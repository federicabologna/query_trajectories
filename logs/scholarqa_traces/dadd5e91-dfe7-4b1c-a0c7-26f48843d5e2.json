{
    "query": "Which papers empirically compare the effectiveness of FastText-based versus BERT-based classifier filtering approaches for pre-training data quality and their impact on downstream performance in large language models such as BERT, GPT-3, and Llama?",
    "user_id": "lib_user",
    "task_id": "dadd5e91-dfe7-4b1c-a0c7-26f48843d5e2",
    "timestamp": "2025-06-23T21:09:25.489780",
    "n_retrieval": 256,
    "n_retrieved": 259,
    "n_candidates": 15,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.16296,
    "decomposed_query": {
        "rewritten_query": "Empirical comparison of FastText-based versus BERT-based classifier filtering approaches for pre-training data quality and their impact on downstream performance in large language models such as BERT, GPT-3, and Llama.",
        "keyword_query": "FastText BERT classifier filtering pre-training data quality downstream performance language models BERT GPT-3 Llama",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010389,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "venue": "International Conference on Computational Linguistics",
            "year": 2020,
            "reference_count": 39,
            "citation_count": 25,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/2020.coling-main.481",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.coling-main.481, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1383074767",
                    "name": "A. Edwards"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                },
                {
                    "authorId": "2750681",
                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                },
                {
                    "authorId": "1762890",
                    "name": "A. Preece"
                }
            ],
            "abstract": "Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
            "corpus_id": 227231089,
            "sentences": [
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
                    "score": 0.730668518893203,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline. 4 BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments. For future work it would be interesting to further delve into the role of unlabeled data in text classification, both in terms of word embeddings (e.g., by making use of meta-embeddings (Yin and Sch\u00fctze, 2016)) and the data used to train language models (Gururangan et al., 2020). Moreover, this quantitative analysis could be extended to more classification tasks and different models, e.g., larger language models such as RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al., 2020), which appear to be more suited to few-shot experiments. However, the generic domain embeddings tend to fail to represent the meaning of more domainspecific words, which may explain their lower performance. This is confirmed by the nearest neighbour analysis (see Table 5) which showed that the generic domain embeddings do not provide accurate representations of more technical words such as 'Windows' and 'Sun'. In the IMDB reviews, words such as 'Toothless', used within a very specific context are also not correctly represented by the generic model. Moreover, tweets are rich with abbreviations which have domain-specific meaning such as 'SF' referring to 'San Francisco'.",
                    "score": 0.5219114823124157,
                    "section_title": "Conclusion and Future Work",
                    "char_start_offset": 14243,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1638
                        },
                        {
                            "start": 1639,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 1902
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 929,
                            "end": 952,
                            "matchedPaperCorpusId": "2820782"
                        },
                        {
                            "start": 997,
                            "end": 1022,
                            "matchedPaperCorpusId": "216080466"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6943359375
                },
                {
                    "corpus_id": "227231089",
                    "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                    "text": "Table 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.",
                    "score": 0.6519833417792988,
                    "section_title": "Effect of training size",
                    "char_start_offset": 9068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 938
                        },
                        {
                            "start": 939,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1119
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65625
                }
            ],
            "relevance_judgement": 0.88037109375,
            "relevance_judgment_input_expanded": "# Title: Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification\n# Venue: International Conference on Computational Linguistics\n# Authors: A. Edwards, Jos\u00e9 Camacho-Collados, H\u00e9l\u00e8ne de Ribaupierre, A. Preece\n## Abstract\nPre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.\n## Effect of training size\nTable 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.\n\n## Conclusion and Future Work\nIn this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline. 4 BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments. For future work it would be interesting to further delve into the role of unlabeled data in text classification, both in terms of word embeddings (e.g., by making use of meta-embeddings (Yin and Sch\u00fctze, 2016)) and the data used to train language models (Gururangan et al., 2020). Moreover, this quantitative analysis could be extended to more classification tasks and different models, e.g., larger language models such as RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al., 2020), which appear to be more suited to few-shot experiments. However, the generic domain embeddings tend to fail to represent the meaning of more domainspecific words, which may explain their lower performance. This is confirmed by the nearest neighbour analysis (see Table 5) which showed that the generic domain embeddings do not provide accurate representations of more technical words such as 'Windows' and 'Sun'. In the IMDB reviews, words such as 'Toothless', used within a very specific context are also not correctly represented by the generic model. Moreover, tweets are rich with abbreviations which have domain-specific meaning such as 'SF' referring to 'San Francisco'.",
            "reference_string": "[227231089 | Edwards et al. | 2020 | Citations: 25]"
        },
        {
            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.01098",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.01098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66266326",
                    "name": "Kun-Peng Ning"
                },
                {
                    "authorId": "2053434618",
                    "name": "Ming Pang"
                },
                {
                    "authorId": "2072874946",
                    "name": "Zheng Fang"
                },
                {
                    "authorId": "2226458237",
                    "name": "Xue Jiang"
                },
                {
                    "authorId": "2226511841",
                    "name": "Xi-Wei Zhao"
                },
                {
                    "authorId": "1949218014",
                    "name": "Changping Peng"
                },
                {
                    "authorId": "2146396439",
                    "name": "Zhangang Lin"
                },
                {
                    "authorId": "22528137",
                    "name": "Jinghe Hu"
                },
                {
                    "authorId": "2118926502",
                    "name": "Jingping Shao"
                }
            ],
            "abstract": "Search query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline BERT model to retrieve more potentially relevant data. Benefiting from its powerful semantic representation, more relevant labels not exposed in the historical data will be added into the training set for better FastText model training. Moreover, a novel distribution-diverse multi-expert learning strategy is proposed to further improve the mining ability of relevant data. By training multiple BERT models from different data distributions, it can respectively perform better at high, middle, and low-frequency search queries. The model ensemble from multi-distribution makes its retrieval ability more powerful. We have deployed two versions of this framework in JD search, and both offline experiments and online A/B testing from multiple datasets have validated the effectiveness of the proposed approach.",
            "corpus_id": 260379057,
            "sentences": [
                {
                    "corpus_id": "260379057",
                    "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
                    "text": "FastText has been widely used in various text classification tasks. \n\nEspecially in E-commerce search tasks, benefited from its high inference efficiency, the FastText with low online latency and stable performance become one of the most popular classification models in the industry. On the other hand, the BERT model has also achieved great success in various natural language processing (NLP) tasks based on its powerful semantic representation. However, there are limited works to investigate the property of both models. Thus we conduct toy experiments on the JD search ads dataset to explore the property of the FastText and the BERT models. Firstly, we collected the training set with nearly 160 million user queries from historical click-through data. Then, we respectively train the FastText and BERT models on this training data. Next, we generate two testing sets from T+0 and T+1 days respectively, and each testing set has nearly 2 million user queries. Importantly, in the testing set of T+0 and T+1 days, 99.89% and 57.47% of test queries have appeared in the training set respectively. Finally, we respectively test both models and record their performance on the T+0 and T+1 days. \n\nTable 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization. This empirical finding motivates us to combine the advantages of both models to further improve the online performance. To achieve this goal, we introduce knowledge condensation, a Teacher-Student learning framework illustrated in the following subsection.",
                    "score": 0.6751290533628439,
                    "section_title": "A Closer Look on FastText and BERT Models",
                    "char_start_offset": 10344,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 70,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1197
                        },
                        {
                            "start": 1200,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2064
                        },
                        {
                            "start": 2065,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73974609375
                }
            ],
            "relevance_judgement": 0.73974609375,
            "relevance_judgment_input_expanded": "# Title: Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search\n# Venue: arXiv.org\n# Authors: Kun-Peng Ning, Ming Pang, Zheng Fang, Xue Jiang, Xi-Wei Zhao, Changping Peng, Zhangang Lin, Jinghe Hu, Jingping Shao\n## Abstract\nSearch query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline BERT model to retrieve more potentially relevant data. Benefiting from its powerful semantic representation, more relevant labels not exposed in the historical data will be added into the training set for better FastText model training. Moreover, a novel distribution-diverse multi-expert learning strategy is proposed to further improve the mining ability of relevant data. By training multiple BERT models from different data distributions, it can respectively perform better at high, middle, and low-frequency search queries. The model ensemble from multi-distribution makes its retrieval ability more powerful. We have deployed two versions of this framework in JD search, and both offline experiments and online A/B testing from multiple datasets have validated the effectiveness of the proposed approach.\n## A Closer Look on FastText and BERT Models\nFastText has been widely used in various text classification tasks. \n\nEspecially in E-commerce search tasks, benefited from its high inference efficiency, the FastText with low online latency and stable performance become one of the most popular classification models in the industry. On the other hand, the BERT model has also achieved great success in various natural language processing (NLP) tasks based on its powerful semantic representation. However, there are limited works to investigate the property of both models. Thus we conduct toy experiments on the JD search ads dataset to explore the property of the FastText and the BERT models. Firstly, we collected the training set with nearly 160 million user queries from historical click-through data. Then, we respectively train the FastText and BERT models on this training data. Next, we generate two testing sets from T+0 and T+1 days respectively, and each testing set has nearly 2 million user queries. Importantly, in the testing set of T+0 and T+1 days, 99.89% and 57.47% of test queries have appeared in the training set respectively. Finally, we respectively test both models and record their performance on the T+0 and T+1 days. \n\nTable 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization. This empirical finding motivates us to combine the advantages of both models to further improve the online performance. To achieve this goal, we introduce knowledge condensation, a Teacher-Student learning framework illustrated in the following subsection.",
            "reference_string": "[260379057 | Ning et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
            "venue": "",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.17661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308480949",
                    "name": "Abhay Gajanan Shanbhag"
                },
                {
                    "authorId": "2308481744",
                    "name": "Suramya Jadhav"
                },
                {
                    "authorId": "2329035171",
                    "name": "Amogh Thakurdesai"
                },
                {
                    "authorId": "2296717726",
                    "name": "Ridhima Sinare"
                },
                {
                    "authorId": "2332349659",
                    "name": "Raviraj Joshi"
                }
            ],
            "abstract": "Natural Language Processing (NLP) for low-resource languages, which lack large annotated datasets, faces significant challenges due to limited high-quality data and linguistic resources. The selection of embeddings plays a critical role in achieving strong performance in NLP tasks. While contextual BERT embeddings require a full forward pass, non-contextual BERT embeddings rely only on table lookup. Existing research has primarily focused on contextual BERT embeddings, leaving non-contextual embeddings largely unexplored. In this study, we analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for tasks such as news classification, sentiment analysis, and hate speech detection in one such low-resource language Marathi. We compare these embeddings with their contextual and compressed variants. Our findings indicate that non-contextual BERT embeddings extracted from the model's first embedding layer outperform FastText embeddings, presenting a promising alternative for low-resource NLP.",
            "corpus_id": 274280560,
            "sentences": [
                {
                    "corpus_id": "274280560",
                    "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
                    "text": "Word embedding is a way of representing words into dense vectors in a continuous space such that the vectors capture the semantic relationship between the words for the models to understand the context and meaning of the text. FastText, a context-independent method, basically captures the subword information, enabling it to learn rare words, misspelled words, and out-of-vocabulary words. It is recognized in the NLP community for its efficient performance in tasks like text classification and sentiment analysis. Despite being relatively old, it still remains one of the most effective alternatives when performing tasks on large datasets across various languages due to its subword-based approach. \n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) word embeddings understand the meaning of a word based on its context in a sentence. The embeddings extracted just before the first embedding layer of the BERT architecture are referred to as non-contextual embeddings, while those obtained from the last hidden layer of BERT are known as contextual embeddings (Refer Fig 1). Numerous variations of BERT like IndicBERT (Kakwani et al., 2020), MuRIL (Khanuja et al., 2021), AfriBERT (Ralethe, 2020), and mBERT (Devlin et al., 2018) to name a few, are available for experiments. \n\nRecent studies have experimented with both Fast-Text and BERT for various tasks; however, most of them focus on exploring contextual BERT embeddings. Experiments of D 'Sa et al. (2020) demonstrated that BERT embeddings outperformed Fast-Text for classifying English text into toxic and nontoxic. Findings of Ahmed et al. (2024) suggested that BERT embeddings outperformed those of Fast-Text with an F1 score of 84% when evaluated for depressive post-detection in Bangla. \n\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored.",
                    "score": 0.49579905337598906,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 702
                        },
                        {
                            "start": 705,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1315
                        },
                        {
                            "start": 1318,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1788
                        },
                        {
                            "start": 1791,
                            "end": 2021
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1158,
                            "end": 1180,
                            "matchedPaperCorpusId": "267935553"
                        },
                        {
                            "start": 1221,
                            "end": 1236,
                            "matchedPaperCorpusId": "218973721"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70556640625
                }
            ],
            "relevance_judgement": 0.70556640625,
            "relevance_judgment_input_expanded": "# Title: Non-Contextual BERT or FastText? A Comparative Analysis\n# Venue: \n# Authors: Abhay Gajanan Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi\n## Abstract\nNatural Language Processing (NLP) for low-resource languages, which lack large annotated datasets, faces significant challenges due to limited high-quality data and linguistic resources. The selection of embeddings plays a critical role in achieving strong performance in NLP tasks. While contextual BERT embeddings require a full forward pass, non-contextual BERT embeddings rely only on table lookup. Existing research has primarily focused on contextual BERT embeddings, leaving non-contextual embeddings largely unexplored. In this study, we analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for tasks such as news classification, sentiment analysis, and hate speech detection in one such low-resource language Marathi. We compare these embeddings with their contextual and compressed variants. Our findings indicate that non-contextual BERT embeddings extracted from the model's first embedding layer outperform FastText embeddings, presenting a promising alternative for low-resource NLP.\n## Introduction\nWord embedding is a way of representing words into dense vectors in a continuous space such that the vectors capture the semantic relationship between the words for the models to understand the context and meaning of the text. FastText, a context-independent method, basically captures the subword information, enabling it to learn rare words, misspelled words, and out-of-vocabulary words. It is recognized in the NLP community for its efficient performance in tasks like text classification and sentiment analysis. Despite being relatively old, it still remains one of the most effective alternatives when performing tasks on large datasets across various languages due to its subword-based approach. \n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) word embeddings understand the meaning of a word based on its context in a sentence. The embeddings extracted just before the first embedding layer of the BERT architecture are referred to as non-contextual embeddings, while those obtained from the last hidden layer of BERT are known as contextual embeddings (Refer Fig 1). Numerous variations of BERT like IndicBERT (Kakwani et al., 2020), MuRIL (Khanuja et al., 2021), AfriBERT (Ralethe, 2020), and mBERT (Devlin et al., 2018) to name a few, are available for experiments. \n\nRecent studies have experimented with both Fast-Text and BERT for various tasks; however, most of them focus on exploring contextual BERT embeddings. Experiments of D 'Sa et al. (2020) demonstrated that BERT embeddings outperformed Fast-Text for classifying English text into toxic and nontoxic. Findings of Ahmed et al. (2024) suggested that BERT embeddings outperformed those of Fast-Text with an F1 score of 84% when evaluated for depressive post-detection in Bangla. \n\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored.",
            "reference_string": "[274280560 | Shanbhag et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
            "venue": "International Conference on Computational Linguistics",
            "year": 2020,
            "reference_count": 43,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.coling-main.543.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.coling-main.543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51895498",
                    "name": "Stephen Mutuvi"
                },
                {
                    "authorId": "35101485",
                    "name": "Emanuela Boros"
                },
                {
                    "authorId": "34796546",
                    "name": "A. Doucet"
                },
                {
                    "authorId": "1774986",
                    "name": "A. Jatowt"
                },
                {
                    "authorId": "47763032",
                    "name": "Ga\u00ebl Lejeune"
                },
                {
                    "authorId": "51909076",
                    "name": "Moses Odeo"
                }
            ],
            "abstract": "In this paper, we approach the multilingual text classification task in the context of the epidemiological field. Multilingual text classification models tend to perform differently across different languages (low- or high-resourced), more particularly when the dataset is highly imbalanced, which is the case for epidemiological datasets. We conduct a comparative study of different machine and deep learning text classification models using a dataset comprising news articles related to epidemic outbreaks from six languages, four low-resourced and two high-resourced, in order to analyze the influence of the nature of the language, the structure of the document, and the size of the data. Our findings indicate that the performance of the models based on fine-tuned language models exceeds by more than 50% the chosen baseline models that include a specialized epidemiological news surveillance system and several machine learning models. Also, low-resource languages are highly influenced not only by the typology of the languages on which the models have been pre-trained or/and fine-tuned but also by their size. Furthermore, we discover that the beginning and the end of documents provide the most salient features for this task and, as expected, the performance of the models was proportionate to the training data size.",
            "corpus_id": 227230342,
            "sentences": [
                {
                    "corpus_id": "227230342",
                    "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
                    "text": "Out of all the models, deep learning BERT-based models were the best performing models, in terms of both F1 score and recall measures. The good performance can be attributed to the deep network architectures and large corpora used to train Transformer-based pre-trained language models (PMLs) such as BERT, which enable learning of rich text representations. Moreover, BERT fine-tuning performed better compared to the feature-based approaches, where FastText and BERT embeddings were used as input features to CNN and BiLSTM classifiers. Essentially, the PLMs end up learning universal language representations that are beneficial to downstream tasks.\n\nThe high precision and low recall noted in the machine learning models suggest that the models are unable to detect the relevant class well but are highly reliable when they do. This implies that while the Figure 1: Impact of data size on performance of the best performing model: BERT (multilingualuncased) fine-tuned.\n\nclassifiers returned reliable results, the machine learning models had a high false-negative rate, hence a few of all relevant results were returned. The approaches based on fine-tuned BERT uncased generally struck a good balance between precision and recall.\n\nVGCN+BERT performed particularly well for Polish, Chinese, and Russian. The model utilizes graph embeddings produced by integrating local information captured by BERT and global information from the vocabulary graph that is based on word co-occurrence information. Both the local and global information interact with each other through a self-attention mechanism during the learning process. The interaction introduces useful global information to BERT, which contributes to the improved results across all the languages, including the low-resource languages.\n\nWith regard to the contribution of various document segments on performance, it was observed from the results that, the beginning and the end of the text combined had the highest recall and F1 score. This was particularly the case for models based on BERT namely, VGCN+BERT and BERT fine-tuned models. This can be explained by the fact that the beginning paragraphs in an article often capture the most important information, which informs the reader what the story is about. On the other hand, the last part of the article tends to provide a summary of the article.\n\nThe performance of the model improved proportionately with training data size. This is in line with neural network models, which require large amounts of",
                    "score": 0.5233765374945781,
                    "section_title": "Discussion",
                    "char_start_offset": 24532,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66455078125
                },
                {
                    "corpus_id": "227230342",
                    "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
                    "text": "Polish  Table 4: F1-micro scores of the analyzed models for the relevant documents per language. The pretrained BERT models are base-multilingual. LR stands for Logistic Regression, RF for Random Forest, and SVM for Support Vector Machines, \u2020 fine-tuned.\n\nRegarding the difference between the fine-tuned BERT-based models and those that use the BERT encoder for generating features only, the performance is slightly better when BERT is fine-tuned on the task. However, in the case of additional layers on top of the BERT encoder, when fine-tuned, a considerable decrease in performance can be seen. Overall, these results suggest that the deep learning approaches are capable of much deeper and complex representations, such that they can utilize previously learned features for newer documents, even when the language of the document differs.\n\nAs observed in Table 4, all the machine learning models (LR, RF, SVM) display similar trends in their unequal performance based on language by not detecting (having the F1 values of zero) the relevant documents in Polish and Chinese. This is likely due to the size of the training data for these particular languages. Similarly, for all the low-resource languages (Polish, Chinese, Russian, and Greek), unsurprisingly, the CNN and BiLSTM -based models with pre-trained FastText embeddings were not able to distinguish relevant documents from irrelevant ones, as indicated by their low F1 scores. This might be due to the low embedding coverage of the languages. The F1 values for Chinese tend to be consistent for all BERT-based models while the performance for Polish varies a lot between models. VGCN+BERT had the highest F1 scores for the low-resourced languages Polish, Chinese, and Russian and the secondhighest for Greek.\n\nIn order to analyze the influence of the documents with a larger quantity of documents (French and English, around 2, 000 news articles) over the classification of low-resource languages, we consider every language as the source language and the other five languages as target languages. At every iteration, the best performing model from the previous experiments is trained on the data in the source language and applied directly to every target language.  Table 5: Evaluation scores of the BERT (multilingual-uncased) \u2020 fine-tuned model for the",
                    "score": 0.5128617833304208,
                    "section_title": "Models",
                    "char_start_offset": 18570,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.4150390625
                }
            ],
            "relevance_judgement": 0.66455078125,
            "relevance_judgment_input_expanded": "# Title: Multilingual Epidemiological Text Classification: A Comparative Study\n# Venue: International Conference on Computational Linguistics\n# Authors: Stephen Mutuvi, Emanuela Boros, A. Doucet, A. Jatowt, Ga\u00ebl Lejeune, Moses Odeo\n## Abstract\nIn this paper, we approach the multilingual text classification task in the context of the epidemiological field. Multilingual text classification models tend to perform differently across different languages (low- or high-resourced), more particularly when the dataset is highly imbalanced, which is the case for epidemiological datasets. We conduct a comparative study of different machine and deep learning text classification models using a dataset comprising news articles related to epidemic outbreaks from six languages, four low-resourced and two high-resourced, in order to analyze the influence of the nature of the language, the structure of the document, and the size of the data. Our findings indicate that the performance of the models based on fine-tuned language models exceeds by more than 50% the chosen baseline models that include a specialized epidemiological news surveillance system and several machine learning models. Also, low-resource languages are highly influenced not only by the typology of the languages on which the models have been pre-trained or/and fine-tuned but also by their size. Furthermore, we discover that the beginning and the end of documents provide the most salient features for this task and, as expected, the performance of the models was proportionate to the training data size.\n## Models\nPolish  Table 4: F1-micro scores of the analyzed models for the relevant documents per language. The pretrained BERT models are base-multilingual. LR stands for Logistic Regression, RF for Random Forest, and SVM for Support Vector Machines, \u2020 fine-tuned.\n\nRegarding the difference between the fine-tuned BERT-based models and those that use the BERT encoder for generating features only, the performance is slightly better when BERT is fine-tuned on the task. However, in the case of additional layers on top of the BERT encoder, when fine-tuned, a considerable decrease in performance can be seen. Overall, these results suggest that the deep learning approaches are capable of much deeper and complex representations, such that they can utilize previously learned features for newer documents, even when the language of the document differs.\n\nAs observed in Table 4, all the machine learning models (LR, RF, SVM) display similar trends in their unequal performance based on language by not detecting (having the F1 values of zero) the relevant documents in Polish and Chinese. This is likely due to the size of the training data for these particular languages. Similarly, for all the low-resource languages (Polish, Chinese, Russian, and Greek), unsurprisingly, the CNN and BiLSTM -based models with pre-trained FastText embeddings were not able to distinguish relevant documents from irrelevant ones, as indicated by their low F1 scores. This might be due to the low embedding coverage of the languages. The F1 values for Chinese tend to be consistent for all BERT-based models while the performance for Polish varies a lot between models. VGCN+BERT had the highest F1 scores for the low-resourced languages Polish, Chinese, and Russian and the secondhighest for Greek.\n\nIn order to analyze the influence of the documents with a larger quantity of documents (French and English, around 2, 000 news articles) over the classification of low-resource languages, we consider every language as the source language and the other five languages as target languages. At every iteration, the best performing model from the previous experiments is trained on the data in the source language and applied directly to every target language.  Table 5: Evaluation scores of the BERT (multilingual-uncased) \u2020 fine-tuned model for the\n\n## Discussion\nOut of all the models, deep learning BERT-based models were the best performing models, in terms of both F1 score and recall measures. The good performance can be attributed to the deep network architectures and large corpora used to train Transformer-based pre-trained language models (PMLs) such as BERT, which enable learning of rich text representations. Moreover, BERT fine-tuning performed better compared to the feature-based approaches, where FastText and BERT embeddings were used as input features to CNN and BiLSTM classifiers. Essentially, the PLMs end up learning universal language representations that are beneficial to downstream tasks.\n\nThe high precision and low recall noted in the machine learning models suggest that the models are unable to detect the relevant class well but are highly reliable when they do. This implies that while the Figure 1: Impact of data size on performance of the best performing model: BERT (multilingualuncased) fine-tuned.\n\nclassifiers returned reliable results, the machine learning models had a high false-negative rate, hence a few of all relevant results were returned. The approaches based on fine-tuned BERT uncased generally struck a good balance between precision and recall.\n\nVGCN+BERT performed particularly well for Polish, Chinese, and Russian. The model utilizes graph embeddings produced by integrating local information captured by BERT and global information from the vocabulary graph that is based on word co-occurrence information. Both the local and global information interact with each other through a self-attention mechanism during the learning process. The interaction introduces useful global information to BERT, which contributes to the improved results across all the languages, including the low-resource languages.\n\nWith regard to the contribution of various document segments on performance, it was observed from the results that, the beginning and the end of the text combined had the highest recall and F1 score. This was particularly the case for models based on BERT namely, VGCN+BERT and BERT fine-tuned models. This can be explained by the fact that the beginning paragraphs in an article often capture the most important information, which informs the reader what the story is about. On the other hand, the last part of the article tends to provide a summary of the article.\n\nThe performance of the model improved proportionately with training data size. This is in line with neural network models, which require large amounts of",
            "reference_string": "[227230342 | Mutuvi et al. | 2020 | Citations: 20]"
        },
        {
            "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89607889",
                    "name": "Md Saroar Jahan"
                },
                {
                    "authorId": "2065873917",
                    "name": "M. Oussalah"
                },
                {
                    "authorId": "26940993",
                    "name": "D. Beddiar"
                },
                {
                    "authorId": "2137750963",
                    "name": "Jhuma Kabir Mim"
                },
                {
                    "authorId": "1644031647",
                    "name": "Nabil Arhab"
                }
            ],
            "abstract": "The surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.",
            "corpus_id": 268819482,
            "sentences": [
                {
                    "corpus_id": "268819482",
                    "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
                    "text": "The experiment was conducted in three stages.Initially, we evaluated a dataset without any augmentation.Subsequently, we applied four distinct augmentation techniques: WordNet, FastText, Back-translation, and BERT masking.In the final stage, we applied BERT-base cosine similarity filtering to each augmented dataset.Additionally, we assessed the augmentation performance of GPT-3.\n\nTable 14 shows results of not using the data augmentation.In this analysis, we chose the optimal classifier architecture.The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score.Consequently, for experiments involving data augmentation, our focus has been solely on BERT, the state-of-the-art (SOTA) model demonstrating the best performance.\n\nTable 15 exhibits the experiment results of non-augmented and augmented datasets with and without BERT-cosine for four different datasets: AskFm, HASOC, TREC, and SST1.The examination of BERT-based classifier performance across these four distinct datasets-AskFm, HASOC, TREC, and SST1-reveals insights into the effects of data augmentation and the role of BERT-cosine similarity filtration.Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores.For example, BERT-mask data augmentation for AskFm has resulted in a drop of 0.3% in accuracy and 0.2% in F1.This suggests that the mere addition of augmented sentences does not guarantee enhanced model performance, emphasizing the necessity for quality over quantity in augmentation strategies.\n\nAmong the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
                    "score": 0.5689038924325909,
                    "section_title": "Classification Results",
                    "char_start_offset": 46644,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 45,
                            "end": 104
                        },
                        {
                            "start": 104,
                            "end": 222
                        },
                        {
                            "start": 222,
                            "end": 317
                        },
                        {
                            "start": 317,
                            "end": 381
                        },
                        {
                            "start": 383,
                            "end": 441
                        },
                        {
                            "start": 441,
                            "end": 504
                        },
                        {
                            "start": 504,
                            "end": 671
                        },
                        {
                            "start": 671,
                            "end": 834
                        },
                        {
                            "start": 836,
                            "end": 1004
                        },
                        {
                            "start": 1004,
                            "end": 1227
                        },
                        {
                            "start": 1227,
                            "end": 1419
                        },
                        {
                            "start": 1419,
                            "end": 1528
                        },
                        {
                            "start": 1528,
                            "end": 1714
                        },
                        {
                            "start": 1716,
                            "end": 1965
                        },
                        {
                            "start": 1965,
                            "end": 2160
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.630859375
                },
                {
                    "corpus_id": "268819482",
                    "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
                    "text": "We chose five benchmark datasets for testing our proposed augmentation method.The overall experimentation methodology includes a four-stage process: (i) dataset selection, (ii) data augmentation, (iii) comparison between different augmentation methods, and (iv) classification performance comparison (accuracy, F1 score), and error analysis.\n\n1. First, we select five publicly available benchmark datasets to test data augmentation using different methods as well as the classifier accuracy for augmented and non-augmented datasets.\n\n2. Second, we expand the selected datasets with different existing augmentation methods (e.g., synonym replacement with WordNet and FastText , Back-translation, BERT mask) and apply our proposed methods (e.g., Backtranslation + BERT cosine).For the testing phase with Large Language Models (LLMs), specifically GPT-3, we utilize the GPT-3 API interface to generate augmented sentences.\n\n3. After data augmentation, we compare our proposed method with other popular methods in terms of: the number of newly generated data; and the quality of the expanded sentence (how close is it to the original sentence).\n\n4. Next, we experiment ML performance with all expanded datasets and not expanded datasets for our proposed method and other existing methods.For that, we use: Accuracy, F1_measure.We exploit different classifiers such as NB, LR, CNN model, and BERT-base.We contrast the results and evaluate the data augmentation process for all four datasets.\n\nFinally, we perform error analysis for both expanded and not-expanded datasets.Besides, we consider semantic fidelity to evaluate the use of BERT-cosine filtration.We measure the statistical significance of BERT-cosine similarity scores compared to human judgment.",
                    "score": 0.5486724690903853,
                    "section_title": "Experiment METHODOLOGY",
                    "char_start_offset": 35023,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 78,
                            "end": 341
                        },
                        {
                            "start": 343,
                            "end": 532
                        },
                        {
                            "start": 534,
                            "end": 775
                        },
                        {
                            "start": 775,
                            "end": 919
                        },
                        {
                            "start": 921,
                            "end": 1140
                        },
                        {
                            "start": 1142,
                            "end": 1284
                        },
                        {
                            "start": 1284,
                            "end": 1323
                        },
                        {
                            "start": 1323,
                            "end": 1397
                        },
                        {
                            "start": 1397,
                            "end": 1486
                        },
                        {
                            "start": 1488,
                            "end": 1567
                        },
                        {
                            "start": 1567,
                            "end": 1652
                        },
                        {
                            "start": 1652,
                            "end": 1752
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.406005859375
                }
            ],
            "relevance_judgement": 0.630859375,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs\n# Venue: arXiv.org\n# Authors: Md Saroar Jahan, M. Oussalah, D. Beddiar, Jhuma Kabir Mim, Nabil Arhab\n## Abstract\nThe surge of interest in data augmentation within the realm of NLP has been driven by the need to address challenges posed by hate speech domains, the dynamic nature of social media vocabulary, and the demands for large-scale neural networks requiring extensive training data. However, the prevalent use of lexical substitution in data augmentation has raised concerns, as it may inadvertently alter the intended meaning, thereby impacting the efficacy of supervised machine learning models. In pursuit of suitable data augmentation methods, this study explores both established legacy approaches and contemporary practices such as Large Language Models (LLM), including GPT in Hate Speech detection. Additionally, we propose an optimized utilization of BERT-based encoder models with contextual cosine similarity filtration, exposing significant limitations in prior synonym substitution methods. Our comparative analysis encompasses five popular augmentation techniques: WordNet and Fast-Text synonym replacement, Back-translation, BERT-mask contextual augmentation, and LLM. Our analysis across five benchmarked datasets revealed that while traditional methods like back-translation show low label alteration rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence diversity but at the cost of higher label alteration rates (over 6%). Our proposed BERT-based contextual cosine similarity filtration markedly reduced label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1 performance. However, augmenting data with GPT-3 not only avoided overfitting with up to sevenfold data increase but also improved embedding space coverage by 15% and classification F1 score by 1.4% over traditional methods, and by 0.8% over our method.\n## Experiment METHODOLOGY\nWe chose five benchmark datasets for testing our proposed augmentation method.The overall experimentation methodology includes a four-stage process: (i) dataset selection, (ii) data augmentation, (iii) comparison between different augmentation methods, and (iv) classification performance comparison (accuracy, F1 score), and error analysis.\n\n1. First, we select five publicly available benchmark datasets to test data augmentation using different methods as well as the classifier accuracy for augmented and non-augmented datasets.\n\n2. Second, we expand the selected datasets with different existing augmentation methods (e.g., synonym replacement with WordNet and FastText , Back-translation, BERT mask) and apply our proposed methods (e.g., Backtranslation + BERT cosine).For the testing phase with Large Language Models (LLMs), specifically GPT-3, we utilize the GPT-3 API interface to generate augmented sentences.\n\n3. After data augmentation, we compare our proposed method with other popular methods in terms of: the number of newly generated data; and the quality of the expanded sentence (how close is it to the original sentence).\n\n4. Next, we experiment ML performance with all expanded datasets and not expanded datasets for our proposed method and other existing methods.For that, we use: Accuracy, F1_measure.We exploit different classifiers such as NB, LR, CNN model, and BERT-base.We contrast the results and evaluate the data augmentation process for all four datasets.\n\nFinally, we perform error analysis for both expanded and not-expanded datasets.Besides, we consider semantic fidelity to evaluate the use of BERT-cosine filtration.We measure the statistical significance of BERT-cosine similarity scores compared to human judgment.\n\n## Classification Results\nThe experiment was conducted in three stages.Initially, we evaluated a dataset without any augmentation.Subsequently, we applied four distinct augmentation techniques: WordNet, FastText, Back-translation, and BERT masking.In the final stage, we applied BERT-base cosine similarity filtering to each augmented dataset.Additionally, we assessed the augmentation performance of GPT-3.\n\nTable 14 shows results of not using the data augmentation.In this analysis, we chose the optimal classifier architecture.The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score.Consequently, for experiments involving data augmentation, our focus has been solely on BERT, the state-of-the-art (SOTA) model demonstrating the best performance.\n\nTable 15 exhibits the experiment results of non-augmented and augmented datasets with and without BERT-cosine for four different datasets: AskFm, HASOC, TREC, and SST1.The examination of BERT-based classifier performance across these four distinct datasets-AskFm, HASOC, TREC, and SST1-reveals insights into the effects of data augmentation and the role of BERT-cosine similarity filtration.Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores.For example, BERT-mask data augmentation for AskFm has resulted in a drop of 0.3% in accuracy and 0.2% in F1.This suggests that the mere addition of augmented sentences does not guarantee enhanced model performance, emphasizing the necessity for quality over quantity in augmentation strategies.\n\nAmong the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
            "reference_string": "[268819482 | Jahan et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
            "venue": "Journal of Computer-Assisted Linguistic Research",
            "year": 2022,
            "reference_count": 38,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://polipapers.upv.es/index.php/jclr/article/download/18224/15516",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4995/jclr.2022.18224?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4995/jclr.2022.18224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "78466463",
                    "name": "Lidiia Melnyk"
                },
                {
                    "authorId": "2192124742",
                    "name": "Linda Feld"
                }
            ],
            "abstract": "This paper explores different options of detecting the stance of German YouTube comments regarding the topic of gender diversity and compares the respective results with those of sentiment analysis, showing that these are two very different NLP tasks focusing on distinct characteristics of the discourse. While an already existing model was used to analyze the comments\u2019 sentiment (BERT), the comments\u2019 stance was first annotated and then used to train different models \u2013 SVM with TF-IDF, DistilBERT, LSTM and CNN \u2013 for predicting the stance of unseen comments. The best results were achieved by the CNN, reaching 78.3% accuracy (92% after dataset normalization) on the test set. Whereas the most common stance identified in the comments is a neutral one (neither completely in favor nor completely against gender diversity), the overall sentiment of the discourse turns out to be negative. This shows that the discourse revolving around the topic of gender diversity in YouTube comments is filled with strong opinions, on the one hand, but also opens up a space for anonymously inquiring and learning about the topic and its implications, on the other. Our research thereby (1) contributes to the understanding and application of different NLP tasks used to predict the sentiment and stance of unstructured textual data, and (2) provides relevant insights into society\u2019s attitudes towards a changing system of values and beliefs.",
            "corpus_id": 253860591,
            "sentences": [
                {
                    "corpus_id": "253860591",
                    "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
                    "text": "The two sentiment classification models Guhr et al. (2020) trained and tested on this corpus were FastText and BERT. BERT, Bidirectional Encoder Representations from Transformers, is a language representation model \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" (Devlin et al. 2019, 4171). It can be fine-tuned by adding just one more output layer, and it can be applied to solving a variety of NLP tasks without task-specific modifications of the model architecture. BERT relies on two core processes: pretraining and fine-tuning. During the pretraining stage, the model is trained on unlabeled data with a variety of tasks, and after initializing the model with the pretrained parameters, these will be fine-tuned with the use of labeled data. The major advantage of BERT is the existence of a wide spectrum of large pretrained models, which minimizes the need for a big amount of training data for the model to provide representative results. Guhr et al. (2020) used a model pretrained by the developers on the German BERT small model, making use of \"bidirectional training of a deep transformer-based network architecture\" (Guhr et al. 2020(Guhr et al. , 1630)), and trained their German BERT model for SA using the implementation provided by the HuggingFace repository. While the authors recognize certain advantages of FastText, being a traditional word embedding model, and the model's performance was relatively high for both data from known domains as well as from an unknown domain, the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%. \n\nWe used a Python script to apply the pretrained model to our data. We sought to predict the sentiment of each comment in our dataset separately and to receive a clear result as to whether the polarity is positive, negative, or neutral.",
                    "score": 0.580810814231812,
                    "section_title": "SENTIMENT ANALYSIS WITH BERT",
                    "char_start_offset": 10302,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1744
                        },
                        {
                            "start": 1747,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1982
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 40,
                            "end": 58,
                            "matchedPaperCorpusId": "218974283"
                        },
                        {
                            "start": 1047,
                            "end": 1065,
                            "matchedPaperCorpusId": "218974283"
                        },
                        {
                            "start": 1228,
                            "end": 1245,
                            "matchedPaperCorpusId": "218974283"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6083984375
                }
            ],
            "relevance_judgement": 0.6083984375,
            "relevance_judgment_input_expanded": "# Title: Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity\n# Venue: Journal of Computer-Assisted Linguistic Research\n# Authors: Lidiia Melnyk, Linda Feld\n## Abstract\nThis paper explores different options of detecting the stance of German YouTube comments regarding the topic of gender diversity and compares the respective results with those of sentiment analysis, showing that these are two very different NLP tasks focusing on distinct characteristics of the discourse. While an already existing model was used to analyze the comments\u2019 sentiment (BERT), the comments\u2019 stance was first annotated and then used to train different models \u2013 SVM with TF-IDF, DistilBERT, LSTM and CNN \u2013 for predicting the stance of unseen comments. The best results were achieved by the CNN, reaching 78.3% accuracy (92% after dataset normalization) on the test set. Whereas the most common stance identified in the comments is a neutral one (neither completely in favor nor completely against gender diversity), the overall sentiment of the discourse turns out to be negative. This shows that the discourse revolving around the topic of gender diversity in YouTube comments is filled with strong opinions, on the one hand, but also opens up a space for anonymously inquiring and learning about the topic and its implications, on the other. Our research thereby (1) contributes to the understanding and application of different NLP tasks used to predict the sentiment and stance of unstructured textual data, and (2) provides relevant insights into society\u2019s attitudes towards a changing system of values and beliefs.\n## SENTIMENT ANALYSIS WITH BERT\nThe two sentiment classification models Guhr et al. (2020) trained and tested on this corpus were FastText and BERT. BERT, Bidirectional Encoder Representations from Transformers, is a language representation model \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" (Devlin et al. 2019, 4171). It can be fine-tuned by adding just one more output layer, and it can be applied to solving a variety of NLP tasks without task-specific modifications of the model architecture. BERT relies on two core processes: pretraining and fine-tuning. During the pretraining stage, the model is trained on unlabeled data with a variety of tasks, and after initializing the model with the pretrained parameters, these will be fine-tuned with the use of labeled data. The major advantage of BERT is the existence of a wide spectrum of large pretrained models, which minimizes the need for a big amount of training data for the model to provide representative results. Guhr et al. (2020) used a model pretrained by the developers on the German BERT small model, making use of \"bidirectional training of a deep transformer-based network architecture\" (Guhr et al. 2020(Guhr et al. , 1630)), and trained their German BERT model for SA using the implementation provided by the HuggingFace repository. While the authors recognize certain advantages of FastText, being a traditional word embedding model, and the model's performance was relatively high for both data from known domains as well as from an unknown domain, the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%. \n\nWe used a Python script to apply the pretrained model to our data. We sought to predict the sentiment of each comment in our dataset separately and to receive a clear result as to whether the polarity is positive, negative, or neutral.",
            "reference_string": "[253860591 | Melnyk et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2021,
            "reference_count": 27,
            "citation_count": 29,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://thesai.org/Downloads/Volume12No11/Paper_53-Transformer_based_Contextual_Model_for_Sentiment_Analysis.pdf",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2021.0121153?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2021.0121153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "90290875",
                    "name": "A. Durairaj"
                },
                {
                    "authorId": "2048021830",
                    "name": "Anandan Chinnalagu"
                }
            ],
            "abstract": "\u2014The Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art language model used for multiple natural language processing tasks and sequential modeling applications. The accuracy of predictions from context-based sentiment and analysis of customer review data from various social media platforms are challenging and time-consuming tasks due to the high volumes of unstructured data. In recent years, more research has been conducted based on the recurrent neural network algorithm, Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, neutral, and traditional text classification algorithms. This paper presents our experimental research work to overcome these known challenges of the sentiment analysis models, such as its performance, accuracy, and context-based predictions. We\u2019ve proposed a fine-tuned BERT model to predict customer sentiments through the utilization of customer reviews from Twitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we compared the results of the proposed model with our custom Linear Support Vector Machine (LSVM), fastText, BiLSTM and hybrid fastText-BiLSTM models, as well as presented a comparative analysis dashboard report. This experiment result shows that the proposed model performs better than other models with respect to various performance measures.",
            "corpus_id": 244895506,
            "sentences": [
                {
                    "corpus_id": "244895506",
                    "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
                    "text": "The pre-trained BERT model's performance, accuracy and approach motivated the authors to experiment with this model for customer sentiment analysis. The main objectives and contributions are as follows; \n\n\uf0b7 Overcome known challenges of SA model performance, accuracy and context-based prediction. \n\n\uf0b7 Train BERT-base-cased model on Twitter, IMDB Movie Reviews, Yelp, and Amazon customer reviews datasets to improve the accuracy and performance of the model. \n\n\uf0b7 Evaluate the custom deep learning sequential model of BiLSTM, hybrid fastText-BiLSTM model and linear models of LSVM, fastText models using the same datasets. \n\n\uf0b7 Compare the results of the BERT model with the results of the deep learning sequential and linear model. \n\n\uf0b7 Customize the data pre-processing steps for hybrid and linear model training. \n\n\uf0b7 Fine-tune the hyperparameters for fastText-BiLSTM models. \n\nThis paper presented with several recent BERT and SA related research papers reviews and major contributions from various researchers in Section II. The literature reviews of BERT, fastText, BiLSTM, and LSVM models are presented in Section III. The experimental setting and model evaluation results are discussed in the Section IV. In Section V, concluded this paper with model results, findings of this research work and future work.",
                    "score": 0.5482692460567311,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 3847,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 202
                        },
                        {
                            "start": 205,
                            "end": 296
                        },
                        {
                            "start": 299,
                            "end": 457
                        },
                        {
                            "start": 460,
                            "end": 620
                        },
                        {
                            "start": 623,
                            "end": 729
                        },
                        {
                            "start": 732,
                            "end": 811
                        },
                        {
                            "start": 814,
                            "end": 873
                        },
                        {
                            "start": 876,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1310
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59033203125
                }
            ],
            "relevance_judgement": 0.59033203125,
            "relevance_judgment_input_expanded": "# Title: Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: A. Durairaj, Anandan Chinnalagu\n## Abstract\n\u2014The Bidirectional Encoder Representations from Transformers (BERT) is a state-of-the-art language model used for multiple natural language processing tasks and sequential modeling applications. The accuracy of predictions from context-based sentiment and analysis of customer review data from various social media platforms are challenging and time-consuming tasks due to the high volumes of unstructured data. In recent years, more research has been conducted based on the recurrent neural network algorithm, Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM) as well as hybrid, neutral, and traditional text classification algorithms. This paper presents our experimental research work to overcome these known challenges of the sentiment analysis models, such as its performance, accuracy, and context-based predictions. We\u2019ve proposed a fine-tuned BERT model to predict customer sentiments through the utilization of customer reviews from Twitter, IMDB Movie Reviews, Yelp, Amazon. In addition, we compared the results of the proposed model with our custom Linear Support Vector Machine (LSVM), fastText, BiLSTM and hybrid fastText-BiLSTM models, as well as presented a comparative analysis dashboard report. This experiment result shows that the proposed model performs better than other models with respect to various performance measures.\n## I. INTRODUCTION\nThe pre-trained BERT model's performance, accuracy and approach motivated the authors to experiment with this model for customer sentiment analysis. The main objectives and contributions are as follows; \n\n\uf0b7 Overcome known challenges of SA model performance, accuracy and context-based prediction. \n\n\uf0b7 Train BERT-base-cased model on Twitter, IMDB Movie Reviews, Yelp, and Amazon customer reviews datasets to improve the accuracy and performance of the model. \n\n\uf0b7 Evaluate the custom deep learning sequential model of BiLSTM, hybrid fastText-BiLSTM model and linear models of LSVM, fastText models using the same datasets. \n\n\uf0b7 Compare the results of the BERT model with the results of the deep learning sequential and linear model. \n\n\uf0b7 Customize the data pre-processing steps for hybrid and linear model training. \n\n\uf0b7 Fine-tune the hyperparameters for fastText-BiLSTM models. \n\nThis paper presented with several recent BERT and SA related research papers reviews and major contributions from various researchers in Section II. The literature reviews of BERT, fastText, BiLSTM, and LSVM models are presented in Section III. The experimental setting and model evaluation results are discussed in the Section IV. In Section V, concluded this paper with model results, findings of this research work and future work.",
            "reference_string": "[244895506 | Durairaj et al. | 2021 | Citations: 29]"
        },
        {
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "venue": "Mathematics",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2227-7390/12/15/2403/pdf?version=1722572320",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math12152403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math12152403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2214766554",
                    "name": "Oumaima Bellar"
                },
                {
                    "authorId": "9281050",
                    "name": "A. Baina"
                },
                {
                    "authorId": "2314661959",
                    "name": "Mostafa Ballafkih"
                }
            ],
            "abstract": "The abundance of publicly available data on the internet within the e-marketing domain is consistently expanding. A significant portion of this data revolve around consumers\u2019 perceptions and opinions regarding the goods or services of organizations, making it valuable for market intelligence collectors in marketing, customer relationship management, and customer retention. Sentiment analysis serves as a tool for examining customer sentiment, marketing initiatives, and product appraisals. This valuable information can inform decisions related to future product and service development, marketing campaigns, and customer service enhancements. In social media, predicting ratings is commonly employed to anticipate product ratings based on user reviews. Our study provides an extensive benchmark comparison of different deep learning models, including convolutional neural networks (CNN), recurrent neural networks (RNN), and bi-directional long short-term memory (Bi-LSTM). These models are evaluated using various word embedding techniques, such as bi-directional encoder representations from transformers (BERT) and its derivatives, FastText, and Word2Vec. The evaluation encompasses two setups: 5-class versus 3-class. This paper focuses on sentiment analysis using neural network-based models for consumer sentiment prediction by evaluating and contrasting their performance indicators on a dataset of reviews of different products from customers of an online women\u2019s clothes retailer.",
            "corpus_id": 271691319,
            "sentences": [
                {
                    "corpus_id": "271691319",
                    "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
                    "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
                    "score": 0.5852683857869778,
                    "section_title": "\u2022",
                    "char_start_offset": 30142,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 816
                        },
                        {
                            "start": 819,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1462
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.552734375
                },
                {
                    "corpus_id": "271691319",
                    "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
                    "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pretrain models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
                    "score": 0.5519652038815089,
                    "section_title": "\u2022",
                    "char_start_offset": 31744,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 815
                        },
                        {
                            "start": 818,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1461
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55078125
                }
            ],
            "relevance_judgement": 0.552734375,
            "relevance_judgment_input_expanded": "# Title: Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers\n# Venue: Mathematics\n# Authors: Oumaima Bellar, A. Baina, Mostafa Ballafkih\n## Abstract\nThe abundance of publicly available data on the internet within the e-marketing domain is consistently expanding. A significant portion of this data revolve around consumers\u2019 perceptions and opinions regarding the goods or services of organizations, making it valuable for market intelligence collectors in marketing, customer relationship management, and customer retention. Sentiment analysis serves as a tool for examining customer sentiment, marketing initiatives, and product appraisals. This valuable information can inform decisions related to future product and service development, marketing campaigns, and customer service enhancements. In social media, predicting ratings is commonly employed to anticipate product ratings based on user reviews. Our study provides an extensive benchmark comparison of different deep learning models, including convolutional neural networks (CNN), recurrent neural networks (RNN), and bi-directional long short-term memory (Bi-LSTM). These models are evaluated using various word embedding techniques, such as bi-directional encoder representations from transformers (BERT) and its derivatives, FastText, and Word2Vec. The evaluation encompasses two setups: 5-class versus 3-class. This paper focuses on sentiment analysis using neural network-based models for consumer sentiment prediction by evaluating and contrasting their performance indicators on a dataset of reviews of different products from customers of an online women\u2019s clothes retailer.\n## \u2022\nFastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.\n...\nFastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pretrain models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "reference_string": "[271691319 | Bellar et al. | 2024 | Citations: 7]"
        },
        {
            "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2019,
            "reference_count": 18,
            "citation_count": 368,
            "influential_citation_count": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-1382.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.11828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2781059",
                    "name": "Yinfei Yang"
                },
                {
                    "authorId": "153533808",
                    "name": "Y. Zhang"
                },
                {
                    "authorId": "7887562",
                    "name": "C. Tar"
                },
                {
                    "authorId": "1387994164",
                    "name": "Jason Baldridge"
                }
            ],
            "abstract": "Most existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.",
            "corpus_id": 201698093,
            "sentences": [
                {
                    "corpus_id": "201698093",
                    "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
                    "text": "We use the latest public multilingual BERT base model with 12 layers 2 and apply the default finetuning strategy with batch size 32 and learning rate 1e-5. For BOW and ESIM, we use our own implementations and 300 dimensional multilingual word embeddings from fastText. 3 We allow finetuning word embeddings during training, which gives better empirical performance.\n\nWe use two metrics: classification accuracy and area-under-curve scores of precision-recall curves (AUC-PR). For BERT, probability scores for the positive class is used to compute AUC-PR. For BOW and ESIM a cosine threshold of 0.5 is used to compute accuracy. In all experiments, the best   model checkpoint is chosen based on accuracy on development sets and report results on testing sets.\n\nResults Table 4 shows the performance of all methods and languages. Table 5 summarizes the average results for the six non-English languages.\n\nModel Comparisons: On both Translate Train and Translate Test, BERT consistently outperforms both BOW and ESIM by a substantial margin (>15% absolute accuracy gains) across all seven languages. BERT Translate Train achieves an average 20% accuracy gain. This result demonstrates that PAWS-X effectively measures models' sensitivity to word order and syntactic structure.\n\nTraining/Evaluation Strategies: As Table 4 and 5 show, the Zero Shot strategy yields the lowest performance compared to other strategies on BERT. This is evidence that machine-translated data helps in the multilingual scenario. Indeed, when training on machine-translated examples in all languages (Merged), the model achieves the best performance, with 8.6% accuracy and 7.1% AUC-PR average gains over Zero Shot.\n\nBERT  gual BERT is pre-trained on over one hundred languages; hence BERT provides better initialization for non-English languages than ESIM (which relies on fastText embeddings). The gap between training on English and on other languages is therefore smaller on BERT than on ESIM, which makes Translate Train work better on BERT.\n\nLanguage Difference: Across all models and approaches, performance on Indo-European languages (German, French, Spanish) is consistently better than CJK (Chinese, Japanese, Korean). The performance difference is particularly noticeable on Zero Shot. This can be explained from two perspectives",
                    "score": 0.5203619374140772,
                    "section_title": "Experiments and Results",
                    "char_start_offset": 7924,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55078125
                }
            ],
            "relevance_judgement": 0.55078125,
            "relevance_judgment_input_expanded": "# Title: PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yinfei Yang, Y. Zhang, C. Tar, Jason Baldridge\n## Abstract\nMost existing work on adversarial data generation focuses on English. For example, PAWS (Paraphrase Adversaries from Word Scrambling) consists of challenging English paraphrase identification pairs from Wikipedia and Quora. We remedy this gap with PAWS-X, a new dataset of 23,659 human translated PAWS evaluation pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual BERT fine-tuned on PAWS English plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-English languages and an average accuracy gain of 23% over the next best model. PAWS-X shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.\n## Experiments and Results\nWe use the latest public multilingual BERT base model with 12 layers 2 and apply the default finetuning strategy with batch size 32 and learning rate 1e-5. For BOW and ESIM, we use our own implementations and 300 dimensional multilingual word embeddings from fastText. 3 We allow finetuning word embeddings during training, which gives better empirical performance.\n\nWe use two metrics: classification accuracy and area-under-curve scores of precision-recall curves (AUC-PR). For BERT, probability scores for the positive class is used to compute AUC-PR. For BOW and ESIM a cosine threshold of 0.5 is used to compute accuracy. In all experiments, the best   model checkpoint is chosen based on accuracy on development sets and report results on testing sets.\n\nResults Table 4 shows the performance of all methods and languages. Table 5 summarizes the average results for the six non-English languages.\n\nModel Comparisons: On both Translate Train and Translate Test, BERT consistently outperforms both BOW and ESIM by a substantial margin (>15% absolute accuracy gains) across all seven languages. BERT Translate Train achieves an average 20% accuracy gain. This result demonstrates that PAWS-X effectively measures models' sensitivity to word order and syntactic structure.\n\nTraining/Evaluation Strategies: As Table 4 and 5 show, the Zero Shot strategy yields the lowest performance compared to other strategies on BERT. This is evidence that machine-translated data helps in the multilingual scenario. Indeed, when training on machine-translated examples in all languages (Merged), the model achieves the best performance, with 8.6% accuracy and 7.1% AUC-PR average gains over Zero Shot.\n\nBERT  gual BERT is pre-trained on over one hundred languages; hence BERT provides better initialization for non-English languages than ESIM (which relies on fastText embeddings). The gap between training on English and on other languages is therefore smaller on BERT than on ESIM, which makes Translate Train work better on BERT.\n\nLanguage Difference: Across all models and approaches, performance on Indo-European languages (German, French, Spanish) is consistently better than CJK (Chinese, Japanese, Korean). The performance difference is particularly noticeable on Zero Shot. This can be explained from two perspectives",
            "reference_string": "[201698093 | Yang et al. | 2019 | Citations: 368]"
        },
        {
            "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification",
            "venue": "International Journal of Electrical and Computer Engineering (IJECE)",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 10,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ijece.iaescore.com/index.php/IJECE/article/download/33495/17172",
                "status": "GOLD",
                "license": "CCBYSA",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.11591/ijece.v14i1.pp1071-1078?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.11591/ijece.v14i1.pp1071-1078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2148592978",
                    "name": "Ghinaa Zain Nabiilah"
                },
                {
                    "authorId": "2186587011",
                    "name": "Islam Nur Alam"
                },
                {
                    "authorId": "2142224011",
                    "name": "Eko Setyo Purwanto"
                },
                {
                    "authorId": "2266784462",
                    "name": "Muhammad Fadlan Hidayat"
                }
            ],
            "abstract": "The rapid increase in social media activity has triggered various discussion spaces and information exchanges on social media. Social media users can easily tell stories or comment on many things without limits. However, this often triggers open debates that lead to fights on social media. This is because many social media users use toxic comments that contain elements of racism, radicalism, pornography, or slander to argue and corner individuals or groups. These comments can easily spread and trigger users vulnerable to mental disorders due to unhealthy and unfair debates on social media. Thus, a model is needed to classify comments, especially toxic ones, in Indonesian. Transformer-based model development and natural language processing approaches can be applied to create classification models. Some previous research related to the classification of toxic comments has been done, but the classification results of the model still require exploration to get optimal results. So, this research uses the proposed model by using different pre-trained models at the embedding and classification stages, in the embedding stage using Indonesia bidirectional encoder representations from transformers (IndoBERT), and classification using multilingual bidirectional encoder representations from transformers (MBERT). The proposed model provides optimal results with an F1 value of 0.9032.",
            "corpus_id": 265216855,
            "sentences": [
                {
                    "corpus_id": "265216855",
                    "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification",
                    "text": "Based on this, pre-trained models can learn language representation as a whole. bidirectional encoder representations from transformers (BERT) are one of the pre-trained models widely used for various NLP tasks. BERT is a model that uses the encoder architecture of the transformer and is designed to use a bidirectional representation [8]. An example of the application of BERT is research conducted by D'Sa et al. [9], by classifying toxic comments on Twitter. In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification. Another study by Nabiilah et al. [10] also used a pre-trained model with BERT architecture. This study compared several pre-trained models that had been trained with Indonesian language corpus data, namely multilingual BERT (MBERT), Indonesia BERT (IndoBERT), and Indonesia robustly optimized BERT pretraining approach (IndoRoBERTa) small. The optimal result of this study is to use IndoBERT with an F1 value of 0.88978. \n\nPrevious research also aligns with recent developments in applying NLP for text classification cases using social media data, where pre-trained models can provide model classification results with better performance in recognizing text patterns. However, the increased access to communication by various groups on social media causes the text and language conveyed to be increasingly unstructured and difficult to analyze the pattern. So, to improve the performance of the model in analyzing the meaning and context of the text conveyed, the development of pre-trained model architecture needs to be further developed. Therefore, this research performs multilabel classification of toxic comments using a BERT-based pre-trained model. The striking difference from this research is using different pre-trained models at several stages.",
                    "score": 0.5036181373877056,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2376,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1280
                        },
                        {
                            "start": 1283,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1717
                        },
                        {
                            "start": 1718,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2017
                        },
                        {
                            "start": 2018,
                            "end": 2117
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 336,
                            "end": 339,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 416,
                            "end": 419,
                            "matchedPaperCorpusId": "213529509"
                        },
                        {
                            "start": 893,
                            "end": 897,
                            "matchedPaperCorpusId": "255690182"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53173828125
                }
            ],
            "relevance_judgement": 0.53173828125,
            "relevance_judgment_input_expanded": "# Title: Indonesian multilabel classification using IndoBERT embedding and MBERT classification\n# Venue: International Journal of Electrical and Computer Engineering (IJECE)\n# Authors: Ghinaa Zain Nabiilah, Islam Nur Alam, Eko Setyo Purwanto, Muhammad Fadlan Hidayat\n## Abstract\nThe rapid increase in social media activity has triggered various discussion spaces and information exchanges on social media. Social media users can easily tell stories or comment on many things without limits. However, this often triggers open debates that lead to fights on social media. This is because many social media users use toxic comments that contain elements of racism, radicalism, pornography, or slander to argue and corner individuals or groups. These comments can easily spread and trigger users vulnerable to mental disorders due to unhealthy and unfair debates on social media. Thus, a model is needed to classify comments, especially toxic ones, in Indonesian. Transformer-based model development and natural language processing approaches can be applied to create classification models. Some previous research related to the classification of toxic comments has been done, but the classification results of the model still require exploration to get optimal results. So, this research uses the proposed model by using different pre-trained models at the embedding and classification stages, in the embedding stage using Indonesia bidirectional encoder representations from transformers (IndoBERT), and classification using multilingual bidirectional encoder representations from transformers (MBERT). The proposed model provides optimal results with an F1 value of 0.9032.\n## INTRODUCTION\nBased on this, pre-trained models can learn language representation as a whole. bidirectional encoder representations from transformers (BERT) are one of the pre-trained models widely used for various NLP tasks. BERT is a model that uses the encoder architecture of the transformer and is designed to use a bidirectional representation [8]. An example of the application of BERT is research conducted by D'Sa et al. [9], by classifying toxic comments on Twitter. In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification. Another study by Nabiilah et al. [10] also used a pre-trained model with BERT architecture. This study compared several pre-trained models that had been trained with Indonesian language corpus data, namely multilingual BERT (MBERT), Indonesia BERT (IndoBERT), and Indonesia robustly optimized BERT pretraining approach (IndoRoBERTa) small. The optimal result of this study is to use IndoBERT with an F1 value of 0.88978. \n\nPrevious research also aligns with recent developments in applying NLP for text classification cases using social media data, where pre-trained models can provide model classification results with better performance in recognizing text patterns. However, the increased access to communication by various groups on social media causes the text and language conveyed to be increasingly unstructured and difficult to analyze the pattern. So, to improve the performance of the model in analyzing the meaning and context of the text conveyed, the development of pre-trained model architecture needs to be further developed. Therefore, this research performs multilabel classification of toxic comments using a BERT-based pre-trained model. The striking difference from this research is using different pre-trained models at several stages.",
            "reference_string": "[265216855 | Nabiilah et al. | 2024 | Citations: 10]"
        },
        {
            "title": "A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data",
            "venue": "IFIP Working Conference on Database Semantics",
            "year": 2021,
            "reference_count": 21,
            "citation_count": 22,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2106.06738",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.06738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115404023",
                    "name": "Jinghui Lu"
                },
                {
                    "authorId": "3407479",
                    "name": "M. Henchion"
                },
                {
                    "authorId": "32147086",
                    "name": "Ivan Bacher"
                },
                {
                    "authorId": "7616304",
                    "name": "Brian Mac Namee"
                }
            ],
            "abstract": "Training deep learning models with limited labelled data is an attractive scenario for many NLP tasks, including document classification. While with the recent emergence of BERT, deep learning language models can achieve reasonably good performance in document classification with few labelled instances, there is a lack of evidence in the utility of applying BERT-like models on long document classification. This work introduces a long-text-specific model -- the Hierarchical BERT Model (HBM) -- that learns sentence-level features of the text and works well in scenarios with limited labelled data. Various evaluation experiments have demonstrated that HBM can achieve higher performance in document classification than the previous state-of-the-art methods with only 50 to 200 labelled instances, especially when documents are long. Also, as an extra benefit of HBM, the salient sentences identified by learned HBM are useful as explanations for labelling documents based on a user study.",
            "corpus_id": 235421844,
            "sentences": [
                {
                    "corpus_id": "235421844",
                    "title": "A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data",
                    "text": "Partially following [15], we compare HBM with several baseline methods. We selected an SVM model using a document representation based on FastText, an SVM model using text representations based on a pre-trained Roberta model and a fine-tuned Roberta model as baselines due to their strong performance. We also used another sentence-level model, Hierarchical Attention Network (HAN) [18], as a baseline. \n\nFastText + SVM: We use 300-dimensional word vectors constructed by a Fast-Text language model pre-trained with the Wikipedia corpus [7]. Averaged word embeddings are used as the representation of the document. For preprocessing, all text is converted to lowercase and we remove all punctuation and stop words. SVM is used as the classifier. We tune the hyper-parameters of the SVM classifier using a grid-search based on 5-fold cross-validation performed on the training set, after that, we re-train the classifier with optimised hyper-parameters. This hyper-parameter tuning method is applied in Roberta + SVM as well. \n\nRoberta + SVM: We use 768-dimensional word vectors generated by a pre-trained Roberta language model [9]. We do not fine-tune the pre-trained language model and use the averaged word vectors as the representation of the document. Since all BERT-based models are configured to take as input a maximum of 512 tokens, we divided the long documents with W words into k = W/511 fractions, which is then fed into the model to infer the representation of each fraction (each fraction has a \"[CLS]\" token in front of 511 tokens, so, 512 tokens in total). Based on the approach of [14], the vector of each fraction is the average embeddings of words in that fraction, and the representation of the whole text sequence is the mean of all k fraction vectors. For preprocessing, the only operation performed is to convert all tokens to lowercase. SVM is used as the classifier. \n\nFine-tuned Roberta: For the document classification task, fine-tuning Roberta means adding a softmax layer on top of the Roberta encoder output and finetuning all parameters in the model.",
                    "score": 0.49495454267229855,
                    "section_title": "Baselines and Setup",
                    "char_start_offset": 14126,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 71
                        },
                        {
                            "start": 72,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 402
                        },
                        {
                            "start": 405,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1024
                        },
                        {
                            "start": 1027,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1861
                        },
                        {
                            "start": 1862,
                            "end": 1892
                        },
                        {
                            "start": 1895,
                            "end": 2082
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 382,
                            "end": 386,
                            "matchedPaperCorpusId": "6857205"
                        },
                        {
                            "start": 1599,
                            "end": 1603,
                            "matchedPaperCorpusId": "153312532"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51806640625
                }
            ],
            "relevance_judgement": 0.51806640625,
            "relevance_judgment_input_expanded": "# Title: A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data\n# Venue: IFIP Working Conference on Database Semantics\n# Authors: Jinghui Lu, M. Henchion, Ivan Bacher, Brian Mac Namee\n## Abstract\nTraining deep learning models with limited labelled data is an attractive scenario for many NLP tasks, including document classification. While with the recent emergence of BERT, deep learning language models can achieve reasonably good performance in document classification with few labelled instances, there is a lack of evidence in the utility of applying BERT-like models on long document classification. This work introduces a long-text-specific model -- the Hierarchical BERT Model (HBM) -- that learns sentence-level features of the text and works well in scenarios with limited labelled data. Various evaluation experiments have demonstrated that HBM can achieve higher performance in document classification than the previous state-of-the-art methods with only 50 to 200 labelled instances, especially when documents are long. Also, as an extra benefit of HBM, the salient sentences identified by learned HBM are useful as explanations for labelling documents based on a user study.\n## Baselines and Setup\nPartially following [15], we compare HBM with several baseline methods. We selected an SVM model using a document representation based on FastText, an SVM model using text representations based on a pre-trained Roberta model and a fine-tuned Roberta model as baselines due to their strong performance. We also used another sentence-level model, Hierarchical Attention Network (HAN) [18], as a baseline. \n\nFastText + SVM: We use 300-dimensional word vectors constructed by a Fast-Text language model pre-trained with the Wikipedia corpus [7]. Averaged word embeddings are used as the representation of the document. For preprocessing, all text is converted to lowercase and we remove all punctuation and stop words. SVM is used as the classifier. We tune the hyper-parameters of the SVM classifier using a grid-search based on 5-fold cross-validation performed on the training set, after that, we re-train the classifier with optimised hyper-parameters. This hyper-parameter tuning method is applied in Roberta + SVM as well. \n\nRoberta + SVM: We use 768-dimensional word vectors generated by a pre-trained Roberta language model [9]. We do not fine-tune the pre-trained language model and use the averaged word vectors as the representation of the document. Since all BERT-based models are configured to take as input a maximum of 512 tokens, we divided the long documents with W words into k = W/511 fractions, which is then fed into the model to infer the representation of each fraction (each fraction has a \"[CLS]\" token in front of 511 tokens, so, 512 tokens in total). Based on the approach of [14], the vector of each fraction is the average embeddings of words in that fraction, and the representation of the whole text sequence is the mean of all k fraction vectors. For preprocessing, the only operation performed is to convert all tokens to lowercase. SVM is used as the classifier. \n\nFine-tuned Roberta: For the document classification task, fine-tuning Roberta means adding a softmax layer on top of the Roberta encoder output and finetuning all parameters in the model.",
            "reference_string": "[235421844 | Lu et al. | 2021 | Citations: 22]"
        },
        {
            "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
            "venue": "Computational and Mathematical Methods in Medicine",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 24,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://downloads.hindawi.com/journals/cmmm/2021/5589829.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8378963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2113619919",
                    "name": "Sajid Hussain"
                },
                {
                    "authorId": "1777100",
                    "name": "H. Afzal"
                },
                {
                    "authorId": "5644298",
                    "name": "Ramsha Saeed"
                },
                {
                    "authorId": "2740595",
                    "name": "N. Iltaf"
                },
                {
                    "authorId": "33827057",
                    "name": "M. Umair"
                }
            ],
            "abstract": "Adverse drug reactions (ADRs) are the undesirable effects associated with the use of a drug due to some pharmacological action of the drug. During the last few years, social media has become a popular platform where people discuss their health problems and, therefore, has become a popular source to share information related to ADR in the natural language. This paper presents an end-to-end system for modelling ADR detection from the given text by fine-tuning BERT with a highly modular Framework for Adapting Representation Models (FARM). BERT overcame the predominant neural networks bringing remarkable performance gains. However, training BERT is a computationally expensive task which limits its usage for production environments and makes it difficult to determine the most important hyperparameters for the downstream task. Furthermore, developing an end-to-end ADR extraction system comprising two downstream tasks, i.e., text classification for filtering text containing ADRs and extracting ADR mentions from the classified text, is also challenging. The framework used in this work, FARM-BERT, provides support for multitask learning by combining multiple prediction heads which makes training of the end-to-end systems easier and computationally faster. In the proposed model, one prediction head is used for text classification and the other is used for ADR sequence labeling. Experiments are performed on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets. The proposed model is compared with the baseline models and state-of-the-art techniques, and it is shown that it yields better results for the given task with the F-scores of 89.6%, 97.6%, 84.9%, and 95.9% on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets, respectively. Moreover, training time and testing time of the proposed model are compared with BERT's, and it is shown that the proposed model is computationally faster than BERT.",
            "corpus_id": 237253665,
            "sentences": [
                {
                    "corpus_id": "237253665",
                    "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
                    "text": "By considering the subword information, fasttext unlike word2vec and glove generates the embeddings for out of vocabulary words as well. The training data used for any machine learning model, no matter how big it may be, can still not include all the words in a language's vocabulary. If such unseen words are found in the test data, their representations are not generated by word2vec and glove embedding models. However, fasttext overcomes this limitation and represents the out of vocabu-lary words by adding the embeddings for the constituent char n-grams found in the vocabulary. \n\nBERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word. In BERT, an input word is represented by the sum of its token embeddings, sentence embeddings, and position embeddings. The proposed model FARM-BERT outperforms all the models by yielding the F-scores of 89.6% and 97.6% on Twitter and PubMed datasets, respectively. FARM-BERT performs better than BERT by 2% on Twitter and by 6% on PubMed datasets. Better performance of FARM-BERT than the standard BERT indicates the effectiveness of fine-tuning BERT with FARM with the modified values of hyperparameters.",
                    "score": 0.5725587296345548,
                    "section_title": "Experiments and Results",
                    "char_start_offset": 24459,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1314
                        },
                        {
                            "start": 1315,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1555
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5146484375
                }
            ],
            "relevance_judgement": 0.5146484375,
            "relevance_judgment_input_expanded": "# Title: Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM\n# Venue: Computational and Mathematical Methods in Medicine\n# Authors: Sajid Hussain, H. Afzal, Ramsha Saeed, N. Iltaf, M. Umair\n## Abstract\nAdverse drug reactions (ADRs) are the undesirable effects associated with the use of a drug due to some pharmacological action of the drug. During the last few years, social media has become a popular platform where people discuss their health problems and, therefore, has become a popular source to share information related to ADR in the natural language. This paper presents an end-to-end system for modelling ADR detection from the given text by fine-tuning BERT with a highly modular Framework for Adapting Representation Models (FARM). BERT overcame the predominant neural networks bringing remarkable performance gains. However, training BERT is a computationally expensive task which limits its usage for production environments and makes it difficult to determine the most important hyperparameters for the downstream task. Furthermore, developing an end-to-end ADR extraction system comprising two downstream tasks, i.e., text classification for filtering text containing ADRs and extracting ADR mentions from the classified text, is also challenging. The framework used in this work, FARM-BERT, provides support for multitask learning by combining multiple prediction heads which makes training of the end-to-end systems easier and computationally faster. In the proposed model, one prediction head is used for text classification and the other is used for ADR sequence labeling. Experiments are performed on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets. The proposed model is compared with the baseline models and state-of-the-art techniques, and it is shown that it yields better results for the given task with the F-scores of 89.6%, 97.6%, 84.9%, and 95.9% on Twitter, PubMed, TwiMed-Twitter, and TwiMed-PubMed datasets, respectively. Moreover, training time and testing time of the proposed model are compared with BERT's, and it is shown that the proposed model is computationally faster than BERT.\n## Experiments and Results\nBy considering the subword information, fasttext unlike word2vec and glove generates the embeddings for out of vocabulary words as well. The training data used for any machine learning model, no matter how big it may be, can still not include all the words in a language's vocabulary. If such unseen words are found in the test data, their representations are not generated by word2vec and glove embedding models. However, fasttext overcomes this limitation and represents the out of vocabu-lary words by adding the embeddings for the constituent char n-grams found in the vocabulary. \n\nBERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word. In BERT, an input word is represented by the sum of its token embeddings, sentence embeddings, and position embeddings. The proposed model FARM-BERT outperforms all the models by yielding the F-scores of 89.6% and 97.6% on Twitter and PubMed datasets, respectively. FARM-BERT performs better than BERT by 2% on Twitter and by 6% on PubMed datasets. Better performance of FARM-BERT than the standard BERT indicates the effectiveness of fine-tuning BERT with FARM with the modified values of hyperparameters.",
            "reference_string": "[237253665 | Hussain et al. | 2021 | Citations: 24]"
        },
        {
            "title": "Geometry matters: Exploring language examples at the decision boundary",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 43,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.07212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2852125",
                    "name": "Debajyoti Datta"
                },
                {
                    "authorId": "2049004359",
                    "name": "Shashwat Kumar"
                },
                {
                    "authorId": "2093120019",
                    "name": "Laura E. Barnes"
                },
                {
                    "authorId": "48763056",
                    "name": "Tom Fletcher"
                }
            ],
            "abstract": "A growing body of recent evidence has highlighted the limitations of natural language processing (NLP) datasets and classifiers. These include the presence of annotation artifacts in datasets, classifiers relying on shallow features like a single word (e.g., if a movie review has the word \"romantic\", the review tends to be positive), or unnecessary words (e.g., learning a proper noun to classify a movie as positive or negative). The presence of such artifacts has subsequently led to the development of challenging datasets to force the model to generalize better. While a variety of heuristic strategies, such as counterfactual examples and contrast sets, have been proposed, the theoretical justification about what makes these examples difficult for the classifier is often lacking or unclear. In this paper, using tools from information geometry, we propose a theoretical way to quantify the difficulty of an example in NLP. Using our approach, we explore difficult examples for several deep learning architectures. We discover that both BERT, CNN and fasttext are susceptible to word substitutions in high difficulty examples. These classifiers tend to perform poorly on the FIM test set. (generated by sampling and perturbing difficult examples, with accuracy dropping below 50%). We replicate our experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews, YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a correlation coefficient of -0.4 between resilience to perturbations and the difficulty score. Similarly we observe a correlation of 0.35 between the difficulty score and the empirical success probability of random substitutions. Our approach is simple, architecture agnostic and can be used to study the fragilities of text classification models. All the code used will be made publicly available, including a tool to explore the difficult examples for other datasets.",
            "corpus_id": 222341529,
            "sentences": [
                {
                    "corpus_id": "222341529",
                    "title": "Geometry matters: Exploring language examples at the decision boundary",
                    "text": "We use multiple model architectures to understand the implications of the FIM. \n\n\u2022 Convolutional Neural Network (CNN): We use the same paradigm Kim [2014] for text classification using GloVe Pennington et al. [2014] embeddings. \n\n\u2022 Long Short Term Memory Network (LSTM): LSTM's have been used extensively in Natural Language Processing for variety of tasks like text classification and language modeling Schmidhuber [2015] \u2022 Bag of Tricks for Efficient Text Classification(Fasttext) We used the Fasttext based model Joulin et al. [2017] for classifying text and examining the quantitative aspects of Fisher Information Metric since the model is incredibly computational efficiency and strong performance across multiple datasets. \n\n\u2022 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. [2018] Large scale pretrained language models have become extremely popular for data efficiency when trained on downstream tasks. However it has been shown recently, that on large datasets ? performance increase from finetuning is often within 1% of BERT. Thus we limited our analysis of using BERT for qualitative analysis for verification of it's relationship with FIM since accuracy difference is minimal and it is computation intensive to calculate the eigenvalue of the FIM of BERT based fine tuned models. We instead used BERT embeddings in downstream tasks for qualitative evaluation of FIM. \n\nFor the Fasttext, CNN and LSTM models we used early stopping with a patience of 5.",
                    "score": 0.6225973176611814,
                    "section_title": "MODEL DESCRIPTIONS",
                    "char_start_offset": 16064,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 81,
                            "end": 227
                        },
                        {
                            "start": 230,
                            "end": 729
                        },
                        {
                            "start": 732,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1427
                        },
                        {
                            "start": 1430,
                            "end": 1512
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 191,
                            "end": 215,
                            "matchedPaperCorpusId": "1957433"
                        },
                        {
                            "start": 404,
                            "end": 422,
                            "matchedPaperCorpusId": "11715509"
                        },
                        {
                            "start": 516,
                            "end": 536,
                            "matchedPaperCorpusId": "1210515"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.505859375
                }
            ],
            "relevance_judgement": 0.505859375,
            "relevance_judgment_input_expanded": "# Title: Geometry matters: Exploring language examples at the decision boundary\n# Venue: arXiv.org\n# Authors: Debajyoti Datta, Shashwat Kumar, Laura E. Barnes, Tom Fletcher\n## Abstract\nA growing body of recent evidence has highlighted the limitations of natural language processing (NLP) datasets and classifiers. These include the presence of annotation artifacts in datasets, classifiers relying on shallow features like a single word (e.g., if a movie review has the word \"romantic\", the review tends to be positive), or unnecessary words (e.g., learning a proper noun to classify a movie as positive or negative). The presence of such artifacts has subsequently led to the development of challenging datasets to force the model to generalize better. While a variety of heuristic strategies, such as counterfactual examples and contrast sets, have been proposed, the theoretical justification about what makes these examples difficult for the classifier is often lacking or unclear. In this paper, using tools from information geometry, we propose a theoretical way to quantify the difficulty of an example in NLP. Using our approach, we explore difficult examples for several deep learning architectures. We discover that both BERT, CNN and fasttext are susceptible to word substitutions in high difficulty examples. These classifiers tend to perform poorly on the FIM test set. (generated by sampling and perturbing difficult examples, with accuracy dropping below 50%). We replicate our experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews, YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a correlation coefficient of -0.4 between resilience to perturbations and the difficulty score. Similarly we observe a correlation of 0.35 between the difficulty score and the empirical success probability of random substitutions. Our approach is simple, architecture agnostic and can be used to study the fragilities of text classification models. All the code used will be made publicly available, including a tool to explore the difficult examples for other datasets.\n## MODEL DESCRIPTIONS\nWe use multiple model architectures to understand the implications of the FIM. \n\n\u2022 Convolutional Neural Network (CNN): We use the same paradigm Kim [2014] for text classification using GloVe Pennington et al. [2014] embeddings. \n\n\u2022 Long Short Term Memory Network (LSTM): LSTM's have been used extensively in Natural Language Processing for variety of tasks like text classification and language modeling Schmidhuber [2015] \u2022 Bag of Tricks for Efficient Text Classification(Fasttext) We used the Fasttext based model Joulin et al. [2017] for classifying text and examining the quantitative aspects of Fisher Information Metric since the model is incredibly computational efficiency and strong performance across multiple datasets. \n\n\u2022 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. [2018] Large scale pretrained language models have become extremely popular for data efficiency when trained on downstream tasks. However it has been shown recently, that on large datasets ? performance increase from finetuning is often within 1% of BERT. Thus we limited our analysis of using BERT for qualitative analysis for verification of it's relationship with FIM since accuracy difference is minimal and it is computation intensive to calculate the eigenvalue of the FIM of BERT based fine tuned models. We instead used BERT embeddings in downstream tasks for qualitative evaluation of FIM. \n\nFor the Fasttext, CNN and LSTM models we used early stopping with a patience of 5.",
            "reference_string": "[222341529 | Datta et al. | 2020 | Citations: 3]"
        },
        {
            "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
            "venue": "Ai & Society",
            "year": 2022,
            "reference_count": 32,
            "citation_count": 19,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s00146-022-01574-0.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s00146-022-01574-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s00146-022-01574-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188338310",
                    "name": "Richard Frissen"
                },
                {
                    "authorId": "2848544",
                    "name": "K. Adebayo"
                },
                {
                    "authorId": "39418246",
                    "name": "Rohan Nanda"
                }
            ],
            "abstract": "In recent years, the work of organizations in the area of digitization has intensified significantly. This trend is also evident in the field of recruitment where job application tracking systems (ATS) have been developed to allow job advertisements to be published online. However, recent studies have shown that recruiting in most organizations is not inclusive, being subject to human biases and prejudices. Most discrimination activities appear early but subtly in the hiring process, for instance, exclusive phrasing in job advertisement discourages qualified applicants from minority groups from applying. The existing works are limited to analyzing, categorizing and highlighting the occurrence of bias in the recruitment process. In this paper, we go beyond this and develop machine learning models for identifying and classifying biased and discriminatory language in job descriptions. We develop and evaluate a machine learning system for identifying five major categories of biased and discriminatory language in job advertisements, i.e., masculine-coded, feminine-coded, exclusive, LGBTQ-coded, demographic and racial language. We utilized the combination of linguistic features with recent state-of-the-art word embeddings representations as input features for various machine learning classifiers. Our results show that the machine learning classifiers were able to identify all the five categories of biased and discriminatory language with a decent accuracy. The Random Forest classifier with FastText word embeddings achieved the best performance with tenfolds cross-validation. Our system directly addresses the bias in the attraction phase of hiring by identifying and classifying biased and discriminatory language and thus encouraging recruiters to write more inclusive job advertisements.",
            "corpus_id": 253015767,
            "sentences": [
                {
                    "corpus_id": "253015767",
                    "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
                    "text": "In this section, we present the results of various machine learning models on the EMSCAD dataset. The dataset was divided into 80% training and 20% testing set. The evaluation metrics: accuracy, precision, recall and F1-score were computed for each model. Figure 1 presents the evaluation metrics for various classifiers with different feature sets. The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier. \n\nWe further evaluate the various machine learning classifiers with different word embedding representations as features using tenfolds cross-validation. Figure 2 presents the macro-averages of the precision, recall and F1-score Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings. The results in Figs. 7 and 8 indicate that all the five classes of biased and discriminatory language were distinguishable from each other. \n\nWe observe a linear improvement in the performance of a sample of our models as training size increases in our primary experiment. We wanted to see if the behavior was by any chance related to these particular models or statistically grounded across all our implemented models. To validate this improvement, we included additional data (3000 additional job descriptions) and ran a new experiment with for the lightweight classifiers (DT, LR and NB). \n\nThe results obtained from the new experiment are shown in Fig. 9 for the regular models (80% training set and 20% test set), and Fig. 10 for the tenfold cross-validated models.",
                    "score": 0.5122133471224306,
                    "section_title": "Results and analysis",
                    "char_start_offset": 21097,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1636
                        },
                        {
                            "start": 1639,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 1916
                        },
                        {
                            "start": 1917,
                            "end": 2088
                        },
                        {
                            "start": 2091,
                            "end": 2267
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50244140625
                }
            ],
            "relevance_judgement": 0.50244140625,
            "relevance_judgment_input_expanded": "# Title: A machine learning approach to recognize bias and discrimination in job advertisements\n# Venue: Ai & Society\n# Authors: Richard Frissen, K. Adebayo, Rohan Nanda\n## Abstract\nIn recent years, the work of organizations in the area of digitization has intensified significantly. This trend is also evident in the field of recruitment where job application tracking systems (ATS) have been developed to allow job advertisements to be published online. However, recent studies have shown that recruiting in most organizations is not inclusive, being subject to human biases and prejudices. Most discrimination activities appear early but subtly in the hiring process, for instance, exclusive phrasing in job advertisement discourages qualified applicants from minority groups from applying. The existing works are limited to analyzing, categorizing and highlighting the occurrence of bias in the recruitment process. In this paper, we go beyond this and develop machine learning models for identifying and classifying biased and discriminatory language in job descriptions. We develop and evaluate a machine learning system for identifying five major categories of biased and discriminatory language in job advertisements, i.e., masculine-coded, feminine-coded, exclusive, LGBTQ-coded, demographic and racial language. We utilized the combination of linguistic features with recent state-of-the-art word embeddings representations as input features for various machine learning classifiers. Our results show that the machine learning classifiers were able to identify all the five categories of biased and discriminatory language with a decent accuracy. The Random Forest classifier with FastText word embeddings achieved the best performance with tenfolds cross-validation. Our system directly addresses the bias in the attraction phase of hiring by identifying and classifying biased and discriminatory language and thus encouraging recruiters to write more inclusive job advertisements.\n## Results and analysis\nIn this section, we present the results of various machine learning models on the EMSCAD dataset. The dataset was divided into 80% training and 20% testing set. The evaluation metrics: accuracy, precision, recall and F1-score were computed for each model. Figure 1 presents the evaluation metrics for various classifiers with different feature sets. The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier. \n\nWe further evaluate the various machine learning classifiers with different word embedding representations as features using tenfolds cross-validation. Figure 2 presents the macro-averages of the precision, recall and F1-score Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings. The results in Figs. 7 and 8 indicate that all the five classes of biased and discriminatory language were distinguishable from each other. \n\nWe observe a linear improvement in the performance of a sample of our models as training size increases in our primary experiment. We wanted to see if the behavior was by any chance related to these particular models or statistically grounded across all our implemented models. To validate this improvement, we included additional data (3000 additional job descriptions) and ran a new experiment with for the lightweight classifiers (DT, LR and NB). \n\nThe results obtained from the new experiment are shown in Fig. 9 for the regular models (80% training set and 20% test set), and Fig. 10 for the tenfold cross-validated models.",
            "reference_string": "[253015767 | Frissen et al. | 2022 | Citations: 19]"
        },
        {
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 22,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.05816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1500242049",
                    "name": "Tristan Thrush"
                },
                {
                    "authorId": "2279335958",
                    "name": "Christopher Potts"
                },
                {
                    "authorId": "2214604036",
                    "name": "Tatsunori Hashimoto"
                }
            ],
            "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier. We have now also updated this paper to include results from preregistered experiments with new pretraining data on an aggregation of 22 benchmarks up to the 1.4B scale, showing increasing improvements of our method over others with more scale. A pip package with full documentation can be found here: https://github.com/TristanThrush/perplexity-correlations.",
            "corpus_id": 272524632,
            "sentences": [
                {
                    "corpus_id": "272524632",
                    "title": "Improving Pretraining Data Using Perplexity Correlations",
                    "text": "In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
                    "score": 0.5484287138600837,
                    "section_title": "INITIAL PRETRAINING EXPERIMENTS",
                    "char_start_offset": 26288,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 654
                        },
                        {
                            "start": 655,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1327
                        },
                        {
                            "start": 1330,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2116
                        },
                        {
                            "start": 2117,
                            "end": 2203
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5009765625
                }
            ],
            "relevance_judgement": 0.5009765625,
            "relevance_judgment_input_expanded": "# Title: Improving Pretraining Data Using Perplexity Correlations\n# Venue: International Conference on Learning Representations\n# Authors: Tristan Thrush, Christopher Potts, Tatsunori Hashimoto\n## Abstract\nQuality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier. We have now also updated this paper to include results from preregistered experiments with new pretraining data on an aggregation of 22 benchmarks up to the 1.4B scale, showing increasing improvements of our method over others with more scale. A pip package with full documentation can be found here: https://github.com/TristanThrush/perplexity-correlations.\n## INITIAL PRETRAINING EXPERIMENTS\nIn addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
            "reference_string": "[272524632 | Thrush et al. | 2024 | Citations: 22]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Pre-trained language models provide the foundations for state-of-the-art performance across a wide range of natural language processing tasks, including text classification. However, most classification datasets assume a large amount labeled data, which is commonly not the case in practical settings. In particular, in this paper we compare the performance of a light-weight linear classifier based on word embeddings, i.e., fastText (Joulin et al., 2017), versus a pre-trained language model, i.e., BERT (Devlin et al., 2019), across a wide range of datasets and classification tasks. In general, results show the importance of domain-specific unlabeled data, both in the form of word embeddings or language models. As for the comparison, BERT outperforms all baselines in standard datasets with large training sets. However, in settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data.",
            "score": 0.730668518893203,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "260379057",
            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
            "text": "FastText has been widely used in various text classification tasks. \n\nEspecially in E-commerce search tasks, benefited from its high inference efficiency, the FastText with low online latency and stable performance become one of the most popular classification models in the industry. On the other hand, the BERT model has also achieved great success in various natural language processing (NLP) tasks based on its powerful semantic representation. However, there are limited works to investigate the property of both models. Thus we conduct toy experiments on the JD search ads dataset to explore the property of the FastText and the BERT models. Firstly, we collected the training set with nearly 160 million user queries from historical click-through data. Then, we respectively train the FastText and BERT models on this training data. Next, we generate two testing sets from T+0 and T+1 days respectively, and each testing set has nearly 2 million user queries. Importantly, in the testing set of T+0 and T+1 days, 99.89% and 57.47% of test queries have appeared in the training set respectively. Finally, we respectively test both models and record their performance on the T+0 and T+1 days. \n\nTable 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization. This empirical finding motivates us to combine the advantages of both models to further improve the online performance. To achieve this goal, we introduce knowledge condensation, a Teacher-Student learning framework illustrated in the following subsection.",
            "score": 0.6751290533628439,
            "section_title": "A Closer Look on FastText and BERT Models",
            "char_start_offset": 10344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "225094545",
            "title": "Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports",
            "text": "We evaluate the performance of the machine learning algorithms by using precision, recall, and F1-measure metrics. The summary results are calculated using microand macro-based measures. Early experiments using Word2Vec embeddings [9] and SVM classifier showed unsatisfactory performance compared to fastText embeddings and GNB classifier. Thus, these results are omitted from Table 2. \n\nThe results in Table 2 show that a simple terminology-based pre-processing step leads to slight improvements over the baseline with micro F1 of 0.59 in comparison to baseline micro-F1 of 0.57. Despite the small amount of data, we found that corpus trained embedding provide a notable advantage over pre-trained embeddings in the classifiers performance. fastText classifier outperformed GNB model, especially when domain-based embeddings were used. A non-verbatim example of a sentence where fastText model, based on corpus-trained embeddings performs better than pre-trained embedding models is: 'The police received information that the subject was selling crack'. A potential reason for fastText to classify correctly this sentence versus the classifiers using pre-trained embeddings is that the word 'crack' has the meaning of a 'drug' in the reports. However, this is not the widely accepted meaning for this word and thus it cannot be interpreted correctly by pre-trained models. The GNB based on pre-trained BERT model outperforms the classifiers based on pre-trained embeddings, however it does not lead to improvements over the domain-based models. Fine-tuning BERT is the best performing classifier with micro-F1 of 0.64 and macro-F1 of 0.59 which gives 0.5 improvement over the baseline. The improvement in the results achieved by fine-tuning BERT indicate the importance of adapting even the more context-aware pre-trained language models to the specific domain, especially when the domain contains highly specialised language. Further, the poor performance of classifiers based on pre-trained word models shows the lack of transferability of pre-trained embeddings for a highly specialised domain such as the safeguarding reports.",
            "score": 0.663043078397215,
            "section_title": "Results",
            "char_start_offset": 10054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "16447573"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423828125
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "Table 2 shows the results with different sizes of training data randomly extracted from the training set. Surprisingly, classification models based on corpus-trained embeddings achieve higher performance with less labelled data compared to the classifier based on pre-trained contextualised models. However, for cases with more than 5,000 training samples, the performance of fine-tuned BERT significantly outperforms fastText corpus-based classifier, especially when domain-trained BERT model (i.e., BERT (Twitter)) is used. Further to that, the fine-tuned model performance improves at a higher rate than the classifier based on corpus-trained embeddings for training sets with more than 2,000 instances. For instance, for the SE-18 dataset, fastText with domain embeddings improves 0.112 micro-F1 points when the entire dataset is used with respect to using only 200 instances, while BERT-Twitter provides a 0.360 absolute improvement. In contrast, fastText with pre-trained embeddings performs similarly to the baseline. This shows the advantage for pre-trained models to be fine-tuned to the given domain and task.",
            "score": 0.6519833417792988,
            "section_title": "Effect of training size",
            "char_start_offset": 9068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "274422859",
            "title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information",
            "text": "Text Filtering. During the construction of pre-training datasets, most raw data from the web contains various forms of noise, such as violence, pornography, advertisements, corrupted characters, and poorly formatted text. To extract high-quality data from these raw contents, numerous text filtering methods have been proposed to automatically remove undesirable elements. Among these methods, handcrafted rules [9,17] are commonly used to filter out explicit noise, followed by deduplication techniques [18] to eliminate duplicate text from different sources. Building on this prior work, this paper introduces a rule-based preprocessing module designed to extract high-quality text from raw web content. \n\nQuality Evaluation. In addition to rule-based methods, quality evaluation approaches are also employed to identify high-quality texts using well-designed classifiers. Unlike rule-based methods, which primarily filter out explicit noise from raw content, quality evaluation approaches offer greater robustness and flexibility. These approaches leverage models such as logistic regression [19], BERT [20], FastText [21], and others to calculate probability scores for each text. Based on these scores, texts are classified as positive or negative according to a predefined threshold. Among these models, BERT has emerged as one of the most widely used architectures for quality evaluation due to its exceptional performance in text classification and understanding tasks. BERT's effectiveness stems from its pre-training objectives, including masked language modeling and next-sentence prediction, which enable it to learn powerful text representation and comprehension capabilities. In this paper, we adopt the BERT architecture to build a robust quality evaluation model for text. Domain Classification. The Pre-training of LLMs on vast amounts of diverse text data often results in a generalized understanding of language. However, to enhance their effectiveness for specific applications, it is advantageous to ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and Fine-grained Information classify and curate data from various domains, such as medicine, law, and finance. This process, known as domain classification, helps refine the model to better capture domain-specific language patterns, terminology, and contextual nuances.",
            "score": 0.6429052564203992,
            "section_title": "Related Work",
            "char_start_offset": 1704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310791015625
        },
        {
            "corpus_id": "272397823",
            "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
            "text": "This filter selects high-quality The Stack v1 and StackOverflow content, significantly enhancing coding performance. FineWeb-Edu [30] employs a linear regressor built on Snowflake-arctic-embed-m [26], an advanced embedding model based on BERT [11]. This regressor, trained on 400k Llama-3 [12] annotations rating the educational value (0-5) of FineWeb dataset documents, significantly enhances STEM performance. DCLM-Baseline [18] uses a fastText [5] filter trained on positives from OpenHermes 2.5 [37], high-scoring posts from r/ExplainLikeImFive, and random negatives. It outperforms FineWeb-Edu in top-10% selection. DeepSeek-Coder-V2 [10] follows DeepSeek-Math [34] by leveraging a multistage fastText-based pipeline to recall high-quality code and math contents. Llama-3 [12] uses fastText for recognizing text referenced by Wikipedia [42] and Roberta-based [22] classifiers trained on Llama-2 [38] predictions. While prior work focuses on initial pretraining, Arctic-SnowCoder demonstrates that high-quality data from the pretraining corpus can significantly enhance model performance during continued pretraining. We are also the first to uncover the secret of data quality, revealing the importance of matching data distribution with downstream tasks. \n\nHigh-quality code data for pretraining Phi-1 [13] is one of the first to study the impact of highquality code data. It first uses a random forest classifier to filter out high-quality code data from \n\nThe Stack v1 and StackOverflow, and then creates synthetic textbook-like data and exercises using GPT-3.5 [29], showing significant coding performance with only 50B+ training tokens. DeepSeek-Coder-V2 [10], pretrained for around 14T tokens in total, achieves state-of-the-art coding performance, with a multi-stage fastText-based [5] pipeline to recall web-related code data as well as high-quality GitHub code.",
            "score": 0.6396642243477804,
            "section_title": "Related Work",
            "char_start_offset": 20921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408447265625
        },
        {
            "corpus_id": "227231751",
            "title": "ALT at SemEval-2020 Task 12: Arabic and English Offensive Language Identification in Social Media",
            "text": "Since we are not provided with a development set, we perform 5-fold cross-validation and then average the results to evaluate different individual classifiers (Table 1). Then, we create a system combination from individual classifiers to generate labels for the test set (Table 2). We report accuracy (Acc.), macro averaged precision (P), recall (R), F1-score (F1). We use macro-averaged F1-score (the official metric) for comparison in our discussion.\n\nfastText fastText (Joulin et al., 2016) is a light-weight deep-learning based system that learns word embedding representation of text and can perform text classification. Although it offers a fast and easy way to perform text classification, it can be outperformed by more sophisticated models. fastText primarily serves purpose of a baseline in our experiments. From Table 1, we can see that fastText (F1-score of 79.8%) is outperformed by all other classifiers.\n\nMultilingual-BERT Deep contextualized transformer models such as Bidirectional Encoder Representations from Transformers (Devlin et al., 2019) have been shown to be extremely effective in many NLP tasks. For our task. we fine-tune BERT base-multilingual (Mult-BERT) which is pretrained on Wikipedia text from 104 languages (including Arabic). The architecture of the model consists of 12 transformer blocks, hidden size of 768 and 12 self-attention heads. During fine-tuning, we add a softmax layer that projects class probabilities of the labels. We use PyTorch 2 implementation by HuggingFace 3 for the fine-tuning.\n\nWe can see from Table 1 that, with F1-score of 82.9%, Mult-BERT is outperformed by all classifiers except fastText. This could be attributed to the fact that wikipedia text differs significantly from informal Twitter text (domain of the shared task data).\n\nConvolutional-Long Short Term Memory (C-LSTM) We design a neural network that consists of two parts. First part of the network acts as a character-level feature extractor. The first layer is an embedding layer that projects input to character embeddings, which is then passed through a",
            "score": 0.62997116314906,
            "section_title": "Experiments and Results",
            "char_start_offset": 5854,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1062,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "276575930",
            "title": "Synthetic vs. Gold: The Role of LLM-Generated Labels and Data in Cyberbullying Detection",
            "text": "Table 4 presents the development and test set accuracy for scenario 3: BERT-based classifiers trained on synthetic datasets generated by Llama, GPT, and Grok. The results indicate that Llama consistently outperforms the other two models across different dataset sizes. GPT follows closely, while Grok exhibits the weakest performance, particularly on the test set, where its accuracy is significantly lower than that of the other models. This suggests that Llama-generated synthetic data is of higher quality and more beneficial for training the classifier, despite the lower label quality observed in scenario 2. \n\nExamining the impact of increasing the size of synthetic data, the results show a general trend of performance improvement for Llama and Grok, but little impact for GPT. This indicates that the",
            "score": 0.6241191383129598,
            "section_title": "Scenario 3: Fully Synthetic Data",
            "char_start_offset": 16820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 809
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1435546875
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "Rule-based Text Filtering. Rule-based text filtering methods are the dominant paradigm to identify content-rich and semantically coherent data from collected raw datasets with handcrafted rules. During the collection of pre-training data, there are a large number of text data on the web. However, these data include a lot of noise, such as violence, pornographic, advertisement and error characters. Consequently, in order to extract high-quality data, several rule-based methods have been proposed to explore how to automatically filter undesired content from noisy web data. In these work, deduplication [10] methods are employed to remove duplicate text from the data, while some handcrafted rules [11; 12] are adopted to filter out violence, pornographic, advertisement and other explicit noisy data. Besides, perplexity [13] is also usually used to evaluate the fluency of the texts. However, these work mainly focus on English and lack a complete tool-chain for Chinese. \n\nText Classification Model. Different from rule-based text filtering methods, text classification model is an alternative approach to identify high-quality data with a well-designed classifier. The simplest text classification model is logistic regression [5], which uses the logistic function to calculate the probability values for each text, and then classifies them into positive or negative with a designed threshold. Currently, BERT [14] and FastText [15] are both commonly used text classification models. BERT is a transformer-based [16] pre-training language model that has achieved remarkable performance in various text classification and understanding tasks. Through pre-training on masked language model and next sentence prediction tasks with a large dataset, this model learns powerful language understanding and representation abilities, which makes it perform well on text classification tasks. FastText [15] is also a neural network based approach which is similar to CBOW [17]. It is characterized by its ability to train efficiently and quickly on large-scale data, while achieving competitive classification performance. In this paper, both of these two approaches will be employed to evaluate the qualities of the Chinese texts. \n\nDatasets for Pre-training. In recent years as the scale of pre-trained language models expands, there is a concomitant increase in the demand for large-scale pre-training datasets.",
            "score": 0.6237949011684422,
            "section_title": "Related Work",
            "char_start_offset": 3950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2412
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1900,
                    "end": 1904,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "5959482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353515625
        },
        {
            "corpus_id": "222341529",
            "title": "Geometry matters: Exploring language examples at the decision boundary",
            "text": "We use multiple model architectures to understand the implications of the FIM. \n\n\u2022 Convolutional Neural Network (CNN): We use the same paradigm Kim [2014] for text classification using GloVe Pennington et al. [2014] embeddings. \n\n\u2022 Long Short Term Memory Network (LSTM): LSTM's have been used extensively in Natural Language Processing for variety of tasks like text classification and language modeling Schmidhuber [2015] \u2022 Bag of Tricks for Efficient Text Classification(Fasttext) We used the Fasttext based model Joulin et al. [2017] for classifying text and examining the quantitative aspects of Fisher Information Metric since the model is incredibly computational efficiency and strong performance across multiple datasets. \n\n\u2022 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Devlin et al. [2018] Large scale pretrained language models have become extremely popular for data efficiency when trained on downstream tasks. However it has been shown recently, that on large datasets ? performance increase from finetuning is often within 1% of BERT. Thus we limited our analysis of using BERT for qualitative analysis for verification of it's relationship with FIM since accuracy difference is minimal and it is computation intensive to calculate the eigenvalue of the FIM of BERT based fine tuned models. We instead used BERT embeddings in downstream tasks for qualitative evaluation of FIM. \n\nFor the Fasttext, CNN and LSTM models we used early stopping with a patience of 5.",
            "score": 0.6225973176611814,
            "section_title": "MODEL DESCRIPTIONS",
            "char_start_offset": 16064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 215,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "11715509"
                },
                {
                    "start": 516,
                    "end": 536,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "261243909",
            "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
            "text": "Pre-training typically requires a vast amount of data to achieve high performance, especially when the LLM parameter scale increases dramatically [2,39,101]. Larger datasets enable models to learn more robust and nuanced language representations. Along the history of the large language model development, we can see that the data scale increased significantly, same as the model parameters, for example, from billion tokens to tens of trillion tokens. However, it is crucial to emphasize that solely high-quality data are essential for performance improvement. Merely augmenting the dataset with low-quality data can result in a decline in performance [102]. Therefore, recently, there is another trend that obtaining high quality data instead of tremendous scale of low quality data for pre-training. \n\nSince the quality of pre-training data significantly impacts the effectiveness of GPT models and ensures the models capture accurate and reliable information during pre-training, we need to ensure data excellence by involving carefully curating and filtering the text corpus to remove noise, errors, and irrelevant content, ultimately leading to more robust and reliable GPT models [103,104]. Next, we will introduce some data preprocessing strategies for data quality improvement in several representative LLMs, including classifier-based filtering, rule-based filtering and utilizing textbook quality data. \n\nClassifier-based Filtering. In this approach, a selection classifier is trained using high-quality texts as examples. This classifier is then used to analyze the remaining data and distinguish between high-quality and low-quality samples. \n\nThe low-quality data is subsequently eliminated from the dataset, ensuring that only the most reliable and valuable information is retained for further pre-training. \n\n\u2022 GPT-3 [2] devised an automated filtering method to effectively eliminate low-quality documents from the Common Crawl dataset. It used a classifier trained on highquality data (WebText, Wikipedia, and web books corpus) to prioritize documents with higher scores, resulting in improved data quality for generative text samples. Moreover, GPT-3 adopted fuzzy deduplication of documents within each dataset and the removal of WebText from Common Crawl, further improving the data quality.",
            "score": 0.6193353787322631,
            "section_title": "Data Quantity and Quality",
            "char_start_offset": 23736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1822
                },
                {
                    "start": 1825,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 653,
                    "end": 658,
                    "matchedPaperCorpusId": "248986979"
                },
                {
                    "start": 1187,
                    "end": 1192,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "258832491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223876953125
        },
        {
            "corpus_id": "258741354",
            "title": "Large-scale text analysis using generative language models: A case study in discovering public value expressions in AI patents",
            "text": "For training the models, we use the 9,000 examples labeled using GPT-4 as the training corpus and the 1,000 GPT-identified, humanvalidated examples as the evaluation corpus. We use two formulations of the classification task: (a) 3-class classification, under which the models are tasked to distinguish between Direct PVE (D-PVE), Contextual PVE (C-PVE), and No-PVE, and (b) 2-class classification, under which the models are tasked to distinguish between PVE and No-PVE. By design, the former setting is more challenging as it involves distinguishing between the sub-categories of PVEs. To quantify the performance of the models, we use macro averages of class-wise F1 scores, precision, recall, and accuracy. \n\nWe evaluate a range of pre-trained language models that are similar to the BERT model. We describe these models and the key differences between them. BERT-base-uncased is a version of BERT with 110 million parameters. It deals with lower-cased English text, allowing it to be smaller and faster while maintaining robust performance on many tasks. BERTlarge-uncased, an expanded version of BERT-base, contains 340 million parameters. Despite being computationally heavier, its larger size grants it improved language understanding capabilities. DistilBERT (Sanh et al., 2019) is a distilled, lighter version of BERT, maintaining comparable performance while being smaller (66 million parameters), faster, and cheaper to run. It uses a teacher-student training paradigm, learning from a larger \"teacher\" BERT model. RoBERTa-large (354 million parameters), or Robustly Optimized BERT, is a variant of BERT that modifies BERT's training process for improved performance, using larger batches of data, more data, and removing the next-sentence prediction task (Liu et al., 2019). ALBERT-xxl-v2 (223 million parameters), or A Lite BERT, is a version of BERT that introduces parameter-reduction techniques to lower memory consumption and increase training speed. It achieves this by sharing parameters across layers and factorizing the embedding layer (Lan et al., 2019).",
            "score": 0.6135217473439247,
            "section_title": "D. Modeling: Training and prediction methods",
            "char_start_offset": 25646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2077
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146728515625
        },
        {
            "corpus_id": "254563733",
            "title": "Domain Adaptation of Transformer-Based Models Using Unlabeled Data for Relevance and Polarity Classification of German Customer Feedback",
            "text": "In the attempt to reach high scores in the first two subtasks, different systems and models were used. For the additional pre-training and fine-tuning of transformer-based models, an NVIDIA A100 GPU was used. The pre-training time usually varied between two and three hours, whereas the fine-tuning took around 8 minutes for the base models and 20 minutes for the large ones. As a baseline, a fastText classifier [30] was trained on the preprocessed text. This classifier is constructed with the goal to predict a class or a label instead of a word, which is the case when using an unsupervised algorithm like continuous bag-of-words (CBOW) to generate word embeddings. In addition to word embeddings, fastText also uses character-level n-grams, which makes it capable of handling morphologically rich languages like German, and sentences with a variety of words. Deviating from the default configuration, the dimensionality of the word vectors was set to 50, the learning rate was initialized with 0.1. For the loss computation, softmax was used, and the number of word n-grams was set to 4. The classifier was then trained for 20 epochs. To ensure the reproducibility of the results, the number of used threads was set to 1. As an additional model, these parameters and the collected tweets were used to generate word embeddings, which then were utilized with the fastText classifier. \n\nTo further improve results, transformer-based models were used, starting with BERT [13]. This model was designed to pre-train deep bidirectional representations from an unlabeled text by jointly exploring left and right contexts on all levels. As a result, the pre-trained BERT language model can be fine-tuned with only one additional output layer to develop models that achieve remarkable results for different tasks. The authors published two models: BERT Base and BERT Large . Details of these models are shown in Table 9. In addition, they also released a multilingual model, which was trained on cased data in 104 languages. \n\nThese models were pre-trained in two phases: \"masked language modeling\", and \"next sentence prediction\".",
            "score": 0.6087133262149981,
            "section_title": "System Description",
            "char_start_offset": 18655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2019
                },
                {
                    "start": 2022,
                    "end": 2126
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1472,
                    "end": 1476,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41455078125
        },
        {
            "corpus_id": "265162335",
            "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking",
            "text": "The results of all models after optimization are presented in Table 2.For fastText, we include the results with and without pre-training to emphasize the improvement of using custom pre-trained word embeddings over the baseline.Since the labels in the ZuL and Bank datasets are organized in two-level hierarchies based on topics, we also report results for predicting first-level labels (suffixed with \"Base\").\n\nOverall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank.Even after an extended hyperparameter search, we were not able to improve the results for this last dataset.We believe that the poor performance for Bank may be due to a combination of a low sample count (727) and a relatively high number of labels (67), which likewise affects the other classifiers.Previous studies have also observed decreased performance in BERT fine-tuning when dealing with a small training set (Pruksachatkun et al., 2020;Zhu et al., 2020).\n\nfastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset.The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets.This suggests a further improvement when using larger domain-specific corpora.Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications.\n\nIn contrast to the improvements from custom word embeddings in fastText, Linear SVC, and ML-KNN show the best results when operating on TF-IDF vectors instead.Linear SVC achieves very similar, albeit lower, performance values than fastText.This is quite interesting, since both work on an identical form of processed text, with Linear SVC not having the benefit of a custom language model.On the other hand, ML-KNN showed the worst performance on our datasets.While it exhibits high precision, it retrieves fewer labels overall.",
            "score": 0.608168052087791,
            "section_title": "Classifier Performance",
            "char_start_offset": 23027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 970
                },
                {
                    "start": 972,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1294
                },
                {
                    "start": 1294,
                    "end": 1409
                },
                {
                    "start": 1411,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1871
                },
                {
                    "start": 1871,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 924,
                    "end": 952,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 952,
                    "end": 969,
                    "matchedPaperCorpusId": "209475786"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464599609375
        },
        {
            "corpus_id": "221319787",
            "title": "Language Models and Word Sense Disambiguation: An Overview and Analysis",
            "text": "Word-specific results for different configurations of BERT and the FastText baseline are shown in Table 5. In general, results are high for all BERT-based models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its lightweight 1NN and in the fine-tuning settings. in all BERT configurations. Also, higher polysemy is not a strong indicator of lower performance (see Table 4.2 for statistics of the twenty words, including polysemy), as one would expect from a classification task with higher number of classes (near zero average correlation across settings). In the following we also discuss other relevant points with respect to Most Frequent Sense (MFS) bias and fine-tuning. \n\nMFS Bias. As expected, macro F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense biases captured by the model which lead to lowered performance for the obscure senses (see the work by Postma et al. (2016) for a more thorough analysis on this issue). However, BERT proves to be much more robust with this respect whereas FastText suffers heavily (highlighted in the macro setting). \n\nImpact of fine-tuning. By average, fine-tuning improves the performance for BERT-Large by 1.6 points in terms of micro-F1 (from 95.8% to 97.5%) but decreases on macro-F1 (from 96.4% to 95.1%). While BERT-Base significantly correlates with BERT-Large in the 1NN setting (Pearson correlation above 0.9 for both micro and macro), it has a relatively low correlation with the fine-tuned BERT-Base (0.60 on Micro-F1 and 0.75 on macro-F1). The same trend is observed for BERT-Large, where the correlation between fine-tuning and 1NN is 0.71 and 0.63 on micro-F1 and macro-F1, respectively. The operating principles behind both approaches are significantly different, which may explain this relatively low correlation. While fine-tuning is optimizing a loss function during training, the 1NN approach is simply memorizing states. By optimizing losses, fine-tuning is more susceptible to overfit on the MFS.",
            "score": 0.6058202315852069,
            "section_title": "Results",
            "char_start_offset": 43862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.471923828125
        },
        {
            "corpus_id": "247618867",
            "title": "Supporting Developers in Addressing Human-Centric Issues in Mobile Apps",
            "text": "We used BERT [34], RoBERTa [35], and DistilBERT [36], which are the state of the art classifiers designed to bidirectionally train on the overall context of data in all layers. BERT is pre-trained on Wikipedia and Book corpus that is helpful in English text. However, since our dataset may have words that are technical and unknown to the model [37], we processed and trained the model on pre-trained data and by adding more output layers. Robustly optimised BERT approach (RoBERTa) is a replication study of BERT pretraining that measures the impact of key hyperparameters and training data size on top of BERT. RoBERTa is trained with dynamic masking, FULL-SENTENCES without Next Sentence Prediction (NSP) loss, large mini-batches and a larger byte-level Byte-Pair Encoding (BPE) [35]. DistilBERT is another extension of BERT that is pre-trained on a smaller general-purpose language representation model and can then be fine-tuned on a wide range of tasks resulting in good performances. DistilBERT is reported to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster, and also cheaper to pre-train [36]. \n\nAs the first step, we needed to transform the data as required for these three models. Hence, we followed data pre-processing that allows them to interpret the data. \n\nPre-processing. For BERT, RoBERTa and DistilBERT classifiers, we followed the same steps as described in Section 5.1.1. Additionally, we needed to transform the text data into numerical values within the word limit specified by Transformer models (512 words at a time). Therefore, we tokenised and encoded the data in a structure as specified by HuggingFace (transformers) library [38]. The package provides API to perform NLP tasks to utilise its capacity over a vast variety of tasks [27]. \n\nClassification Models. The BERT classifier is ready to be utilised directly on the model, but it needs to be finetuned to produce better results. In a text classification task, the BERT base model outputs a vector of length 768 for each word (token) and pools the output.",
            "score": 0.6031874543272185,
            "section_title": "Deep Learning",
            "char_start_offset": 39464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "15756880"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2333984375
        },
        {
            "corpus_id": "277345378",
            "title": "Mining Social Media Data for Influenza Vaccine Effectiveness Using a Large Language Model and Chain-of-Thought Prompting",
            "text": "To evaluate the effectiveness of the CoT few-shot prompting method, we compared it against a traditional supervised baseline using Bidirectional Encoder Representations from Transformers (BERT) [13]. BERT is a smaller-scale language model specifically designed for natural language understanding tasks. In contrast to autoregressive models such as GPT-3 [10] and LLaMA [14], BERT is pretrained on relatively smaller datasets using a masked language modeling objective. While large language models (LLMs) provide broader capabilities at greater computational costs, BERT is more efficient for fine-tuning task-specific NLP tasks and was widely utilized before the recent rise of LLMs. In this study, we applied BERT within a text classification framework and explored three variants of BERT-based models: \n\nBERT-large The large-scale version of BERT released in [13], comprising approximately 340 million parameters. RoBERTa-large An enhanced version of BERT, RoBERTa-large [15] has approximately 355 million parameters and includes improvements such as removal of the next sentence prediction objective, and training on expanded datasets. BERTweet-large BERTweet [16] retains the BERT architecture but is pretrained on 873 million English tweets using the RoBERTa pretraining approach, totaling approximately 355 million parameters. It has demonstrated superior performance compared to previous smaller language models in various tweet-based NLP tasks.",
            "score": 0.6020201755793121,
            "section_title": "Baseline",
            "char_start_offset": 12008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1452
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "218719869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2374267578125
        },
        {
            "corpus_id": "258668089",
            "title": "A Review on Deep-Learning-Based Cyberbullying Detection",
            "text": "The Facebook research team created FastText [36] as a library. It has two uses. The first is efficient word representation learning, and the second is sentence categorization. The method supports both supervised and unsupervised representations of words and sentences. On Facebook, if anyone puts a status update on their Facebook timeline about purchasing a bike, after a few moments, they may see an ad related to bikes. Facebook uses the text data to serve you better ads by using FastText. Figure 6 shows the word embedding for 3-gram in FastText. 4.1.7. BERT Bidirectional Encoder Representation from Transformers (BERT) [37] is based on the transformer architecture. Wikipedia (2500 million words) and Book Corpus (800 million words) are part of a vast corpus of unlabeled text that has been pre-trained. The success of BERT mainly lies in the pre-trained step, which has been trained with a large number of texts. The BERT model gathers information from both the left and right sides of a sentence context. Figure 7 shows an example of bi-directionality. If we forecast the nature of a word by choosing other words to its left or right sides, by selecting both sides of this term, BERT precisely predicts the exact meaning. The transformer is the foundation of the BERT architecture. BERT has two variants: BERT base and BERT large. BERT base has 12 layers of transformer blocks, 12 attention heads, and 110 million of parameters. BERT large, on the other hand, has 24 transformer layers, 16 attention heads, and 340 million parameters. Figure 8 shows the architecture of BERT base and BERT large. Figure 9 and 10 shows the input representation of BERT model and output as the embedding of BERT base respectively. BERT has been pre-trained on two natural language challenges. The first is Masked Language Modeling (MLM), which studies word relationships. The second is Next Sentence Prediction (NSP), which is necessary to comprehend how sentences relate to one another. There are some variations of BERT that are also used for the cyberbullying detection problem. (3) train on longer sequences; and (4) dynamically change the masking pattern over the training data.",
            "score": 0.6002275919468507,
            "section_title": "FastText",
            "char_start_offset": 23151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20849609375
        },
        {
            "corpus_id": "268536862",
            "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
            "text": "The classifiers are pre-trained transformer-based LMs, which were subsequently fine-tuned for the LLM-generated text detection task.As illustrated in Fig. 3, each classifier is associated with a classification head, comprising dropout and dense layers, integrated with the backbone of a transformerbased LM.We employed five distinct transformer-based LMs as the backbones for five individual classifiers.The pre-trained layers of the LM are frozen (not updated during training) to preserve their learned features.Only the classifier \"head\" (the last layer) is trained.In addition, we use cross-entropy loss and Adam optimizer with a learning rate of 5e-4.During model training, each text sample is truncated to a maximum of 256 The summary of the pre-trained LMs is provided below: 1) DistilBert: DistilBERT is developed through the distillation of the BERT base model, is notably smaller and faster [64].We use the \"distil bert base en uncased\" as pretrained backbone weight.2) DeBERTaV3: DeBERTaV3 is an enhanced pretrained language model that outperforms its predecessor by implementing replaced token detection (RTD) and introducing a gradient-disentangled embedding sharing method [65].We use the \"deberta v3 base en\" as pretrained backbone weight.\n\n3) FNet: FNet introduces a novel approach to speed up Transformer encoders by replacing self-attention sublayers with linear transformations, achieving 92-97% of BERT's accuracy on the GLUE benchmark while training significantly faster [66].We use the \"f net base en\" as pretrained backbone weight.4) Albert: Albert presents two parameter-reduction techniques that lower memory usage and accelerate training of BERT, achieving superior scaling and performance [67].We use the \"albert base en uncased\" as pretrained backbone weight.5) XLMRoberta: XLMRoberta is a large-scale multilingual transformer-based LM pretrained on a diverse set of one hundred languages using extensive CommonCrawl data [68].",
            "score": 0.5999787169693777,
            "section_title": "A. Single Classifier Detection",
            "char_start_offset": 10151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1253
                },
                {
                    "start": 1255,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1720
                },
                {
                    "start": 1720,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17138671875
        },
        {
            "corpus_id": "245005741",
            "title": "\u201cWhat Can I Cook with these Ingredients?\u201d - Understanding Cooking-Related Information Needs in Conversational Search",
            "text": "Baseline Models: To generate a baseline for our experiments, we employed some of the models, which have been shown to be effective in past experiments predicting information needs in conversational settings [38,72,74]. For our baseline models, we used 300-dimensional FastText embeddings [47] combined with BytePairEmbeddings provided by Flair [3], which are able to capture out of vocabulary words using sub-word information [42]. This feature was the main reason for choosing FastText embeddings, as it was considered helpful when dealing with spoken dialogue data where out of vocabulary words might occur more likely due to its natural style. For the baseline experiments, we use Gaussian Naive Bayes (=GNB or GaussianNB), Random Forest and SVM classificators provided by the Python package scikit-learn [64].\n\nBERT-based models: Since Schwabl [72] showed that information need detection in the cooking domain benefits from using BERT-based models, we employed them for comparison. BERT was chosen due to its success in natural language understanding problems [98], such as Named Entity Recognition and Text Classification. For our experiments, we leveraged pre-trained BERT models: GermanBERT 6 and the multilingual BERT models mBERT 7 and XLM-RoBERTa. 8 Due to the small amount of data, 85% was kept as training and 15% as test data to ensure that there are still samples available for the test dataset even for categories with few data. For each BERT model, we chose the BERT BAS E architecture which was available for all three models in contrast to BERT LARGE . Since fine-tuning pre-trained BERT models on downstream tasks has shown to improve classification performance [79], this technique was also applied in our experiments. All BERT-related experiments were conducted using the transfer-learning library FARM. 9 To fine-tune each of the 11 binary classifiers, a TextClassficationHead 10 was added on top of the BERT language model. The TextClassificationHead consists of a one-layer feed-forward neural network (FFNN) of dimension (768,2) and a softmax activation function. This follows the recommendations made by Sun et al. [79] and Devlin et al. [31]. The classification head receives the final hidden",
            "score": 0.5989102470929648,
            "section_title": "Models.",
            "char_start_offset": 42679,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "209432405"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "11307799"
                },
                {
                    "start": 344,
                    "end": 347,
                    "matchedPaperCorpusId": "181704107"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "21697629"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "10659969"
                },
                {
                    "start": 1258,
                    "end": 1259,
                    "matchedPaperCorpusId": "215587100"
                },
                {
                    "start": 1681,
                    "end": 1685,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 1825,
                    "end": 1826,
                    "matchedPaperCorpusId": "12719479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432861328125
        },
        {
            "corpus_id": "262826017",
            "title": "When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs",
            "text": "Text scoring and sequence classification has seen significant advancement with the emergence of transformer-based pre-trained language models (PLMs) such as BERT, RoBERTa, and GPT. These models have revolutionized various NLP tasks and have shown promise in enhancing the accuracy and efficiency of automated assessment of text quality and traits. BERT (Bidirectional Encoder Representations from Transformers), a pretrained language model, has been widely adopted in various NLP applications due to its ability to capture contextual information from both left and right contexts in a sentence [29,68]. It has achieved state-of-the-art results in various NLP tasks [42]. BERT has been applied to AES [91], analysis of language competencies [74], automated short-answer assessment [82], and inference of traits from textual data [? ], providing state-of-the-art performance in many cases. BERT-based assessment models excel in handling complex literary devices and where a nuanced understanding of the topic is beneficial [68]. RoBERTa (A Robustly Optimized BERT Pretraining Approach) [50] builds upon the architecture of BERT by further optimizing the pretraining process via greater hyperparameter tuning, inclusion of additional training data, and more parameters [50]. For instance, BERT uses 3.3 million tokens from the BookCorpus and Wikipedia. RoBERTa also uses BookCorpus and Wikipedia, but in addition to common crawl news (CC-News), open web text, and stories, resulting in over 30 million tokens used for pre-training [50]. Both BERT and RoBERTa have performed well on text quality assessment and trait scoring tasks due to their ability to capture rich contextual information and linguistic nuances present in user-generated text [32,38,97]. \n\nThe key research gap we tackle is as follows. With advancements in generative capabilities due to LLMs, in the context involving automated scoring of text using ML, it is unclear how various types of ML assessment methods designed for and trained on human content might assess GPT-based text; and how commonalities/differences between assessment models and those used to generate text might factor into the human versus GPT scoring dynamics.",
            "score": 0.5969437254356784,
            "section_title": "Transformer-based Pre-trained Language Models (PLMs)",
            "char_start_offset": 19202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1752
                },
                {
                    "start": 1755,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "227230267"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "245125483"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "195353092"
                },
                {
                    "start": 1741,
                    "end": 1745,
                    "matchedPaperCorpusId": "234223811"
                },
                {
                    "start": 1745,
                    "end": 1748,
                    "matchedPaperCorpusId": "253236872"
                },
                {
                    "start": 1748,
                    "end": 1751,
                    "matchedPaperCorpusId": "247830293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2099609375
        },
        {
            "corpus_id": "258013230",
            "title": "Ticket automation: An insight into current research with applications to multi-level classification scenarios",
            "text": "Performance metrics across baselines are quite close; in some cases, we found that more recent approaches would perform worse than the SVM-based classifier, which instead performed remarkably well on both datasets. The most crucial advantage that we would expect BERT to have over models based on traditional text representations is the ability to extract more expressive features that can embed both contextual and sequential information from the tokens. However, the Linux Bugs dataset is very noisy, with many grammatical inconsistencies and technical readings, like stack traces or memory addresses. These likely make little sense for a LM pre-trained on more structured natural language. On the other hand, the SVM-based approach utilizes BoW features weighted with TF-IDF; therefore, the classifier only looks at global word frequency without considering any structural information. Indeed, it is conceivable that the strength of the SVM classifier can be explained by the lack of particular expressiveness in the structural information of these datasets. While the Financial dataset is less cluttered in terms of technical jargon, it is still rather noisy, while also being characterized characterized by many structurally unsound sentences. Still, sentence structure is more expressive here, as demonstrated by the stronger performance of both our baseline LMs and our proposed approaches. Again, as mentioned in Section 4.1, the labeling of this dataset is not ideal for the task, which explains the poor performances in terms of precision and recall. \n\nThe FastText classifier obtained worse performances than other methods on both datasets. Given the amount of noise in the datasets we experimented with, it is possible that fine-tuning the embeddings before applying them to the classification task may improve the results of this approach -similar to what we do in our Transformer-based approaches. \n\nDeepTriage obtains decent results, though not on par with BERT-based models. This was to be expected, as Transformers are capable of higher semantic and syntactic understanding as compared to recurrent models, and therefore create more meaningful representations for the documents. A noteworthy disadvantage of DeepTriage is indeed its recurrent nature; the computational expensiveness for training becomes quickly unmanageable when attempting to process longer sequences.",
            "score": 0.5941540180068938,
            "section_title": "Baselines",
            "char_start_offset": 61398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "237485425",
            "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
            "text": "The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant. 15 The difference between seBERT and BERToverflow is not significant The comparison between fastText and BERT BASE shows that the general-domain models may be better than smaller text processing models without pretraining on the issue type prediction. BERT BASE significantly outperforms fastText on the issue type prediction task. The BERT LARGE model from the general domain has severe problems with this use case, i.e., there are several cases where the 14. The full list of polysemes can be found in the supplemental material. \n\n15. Posterior probabilities determined by the Bayesian signed rank test can be found in the supplemental material. models completely failed. This happened with none of the other models and may be an indication that the amount of data is too small to fine-tune such a large model from the general domain on a domain-specific corpus.",
            "score": 0.5920877683382604,
            "section_title": "Issue Type Prediction",
            "char_start_offset": 48155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1513
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358642578125
        },
        {
            "corpus_id": "260993576",
            "title": "A Comprehensive Study of ChatGPT: Advancements, Limitations, and Ethical Considerations in Natural Language Processing and Cybersecurity",
            "text": "Dialogue systems: GPT-3 has been found to be more accurate than BERT in generating natural and coherent responses in dialogue systems such as chatbots [31]. \n\nIt is important to note that while GPT-3 has been found to outperform BERT in many cases, BERT is better in certain tasks such as named entity recognition, which is a task of identifying and classifying named entities in a given text [32]. \n\nTable 4 provides a comparison of popular language models based on various criteria such as model name, training data, model size, structure, performance, advantages, and disadvantages. The table includes information on models such as GPT-2, GPT-3, GPT-4, BERT, RoBERTa, T5, XLNet, Megatron, and ALBERT. It shows the amount of data used to train the model and the number of parameters, the structure of the model, and its performance on certain tasks. It also highlights the advantages and disadvantages of each model, such as computational cost, memory requirements, and ease of fine-tuning. Table 4 can be used as a reference for choosing the appropriate model for a particular task and to understand the trade-offs between the different models [33]. According to this Figure 2, GPT-3, XLNet, and Megatron have the highest performance across all tasks, with average performance of 95%, 90%, and 92%, respectively. While GPT-2, BERT, RoBERTa, and T5 have similar performances, with average performance of 85%, 85%, 85%, and 87%, respectively, and ALBERT has the lowest performance, with an average performance of 89% [34]. The performance percentages show how well the language model performed on each task. We evaluated the model by asking it to classify, summarize, and generate text. Human judges assessed the results based on accuracy, fluency, and coherence. The evaluation was quick and simple, using a metric of simplicity. \n\nGPT-4 is a powerful language model that can process text, photos, and videos. This makes it a versatile tool for marketers, organizations, and individuals alike. GPT-3, on the other hand, is better at processing language but struggles with complex multimedia inputs like video.",
            "score": 0.5893486203132818,
            "section_title": "\u2022",
            "char_start_offset": 18366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "218469094"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "237194658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1695556640625
        },
        {
            "corpus_id": "278394813",
            "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data",
            "text": "Data quality has become a key factor in enhancing model performance with the rapid development of large language models (LLMs). Model-driven data filtering has increasingly become a primary approach for acquiring high-quality data. However, it still faces two main challenges: (1) the lack of an efficient data verification strategy makes it difficult to provide timely feedback on data quality; and (2) the selection of seed data for training classifiers lacks clear criteria and relies heavily on human expertise, introducing a degree of subjectivity. To address the first challenge, we introduce an efficient verification strategy that enables rapid evaluation of the impact of data on LLM training with minimal computational cost. To tackle the second challenge, we build upon the assumption that high-quality seed data is beneficial for LLM training, and by integrating the proposed verification strategy, we optimize the selection of positive and negative samples and propose an efficient data filtering pipeline. This pipeline not only improves filtering efficiency, classifier quality, and robustness, but also significantly reduces experimental and inference costs. In addition, to efficiently filter high-quality data, we employ a lightweight classifier based on fastText, and successfully apply the filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120 billion Chinese tokens. Empirical results demonstrate that the LLMs trained on Ultra-FineWeb exhibit significant performance improvements across multiple benchmark tasks, validating the effectiveness of our pipeline in enhancing both data quality and training efficiency.",
            "score": 0.588857707839229,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41357421875
        },
        {
            "corpus_id": "271691319",
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pre-train models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "score": 0.5852683857869778,
            "section_title": "\u2022",
            "char_start_offset": 30142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "257806425",
            "title": "A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat",
            "text": "Table 2 shows classification results of our method obtained with the threshold optimized in terms of Macro F1 score. Our method achieved 0.942 in terms of average of Macro F1 across the two classes. Table 3 shows results and the effectiveness of each component. Looking at classifier. In Table 3, our ablation study showed that our five features contributed to enhancing F1 score, especially black words and fastText prediction. Thus, we believe that each of our proposed features was independently effective for toxic language detection tasks, and that the improvement was even greater when used together. \n\nTable 4 shows the comparison between our method and BERT-based methods on various configurations. The average inference time for our method was 22.9 seconds for 16,807 test samples, with a standard deviation of 1.18. In the Table , we put QPS score (i.e., Throughput), the number of comments which can be processed per a second. Looking at the table, our method achieved much faster inference speed (734QPS\u00b141) than any other BERT configurations. We found that the differences in inference speed between our method and BERT models were statistically significant at the 1% level, irrespective of the configurations by the two-tailed paired t-test for statistical testing. \n\nBERT-large and ALBERT yielded slightly better performance than our method when the maximum sequence length was 64 (F1 = 0.948) and 32 (F1 = 0.943), respectively. However, these models sacrificed inference speed. For instance, the inference speed (47QPS\u00b16) of BERT-large is 1.7 times slower than BERT-base (82QPS\u00b14) and 15.6 times much slower than our method. On the other hand, if attaching great importance to inference speed,  Distil-BERT achieved the highest inference speed (145QPS\u00b16) of all the BERT-based settings when the maximum sequence length was 4, but F1-score dramatically went down to 0.773. It seems that Poor-Man's BERT and Distil-BERT had harmonized results of BERT settings when the maximum length was 32 or 64. However, all of the values did not reach those for our method.",
            "score": 0.5825701487601758,
            "section_title": "Results",
            "char_start_offset": 13764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265869140625
        },
        {
            "corpus_id": "253860591",
            "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity",
            "text": "The two sentiment classification models Guhr et al. (2020) trained and tested on this corpus were FastText and BERT. BERT, Bidirectional Encoder Representations from Transformers, is a language representation model \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" (Devlin et al. 2019, 4171). It can be fine-tuned by adding just one more output layer, and it can be applied to solving a variety of NLP tasks without task-specific modifications of the model architecture. BERT relies on two core processes: pretraining and fine-tuning. During the pretraining stage, the model is trained on unlabeled data with a variety of tasks, and after initializing the model with the pretrained parameters, these will be fine-tuned with the use of labeled data. The major advantage of BERT is the existence of a wide spectrum of large pretrained models, which minimizes the need for a big amount of training data for the model to provide representative results. Guhr et al. (2020) used a model pretrained by the developers on the German BERT small model, making use of \"bidirectional training of a deep transformer-based network architecture\" (Guhr et al. 2020(Guhr et al. , 1630)), and trained their German BERT model for SA using the implementation provided by the HuggingFace repository. While the authors recognize certain advantages of FastText, being a traditional word embedding model, and the model's performance was relatively high for both data from known domains as well as from an unknown domain, the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%. \n\nWe used a Python script to apply the pretrained model to our data. We sought to predict the sentiment of each comment in our dataset separately and to receive a clear result as to whether the polarity is positive, negative, or neutral.",
            "score": 0.580810814231812,
            "section_title": "SENTIMENT ANALYSIS WITH BERT",
            "char_start_offset": 10302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 58,
                    "matchedPaperCorpusId": "218974283"
                },
                {
                    "start": 1047,
                    "end": 1065,
                    "matchedPaperCorpusId": "218974283"
                },
                {
                    "start": 1228,
                    "end": 1245,
                    "matchedPaperCorpusId": "218974283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "220935910",
            "title": "Multi-node Bert-pretraining: Cost-efficient Approach",
            "text": "The pre-trained language models such as BERT [1], GPT-2 [18], XLNet [19], RoBERTa [20] and GPT-3 [21] have been proven successful on many downstream NLP tasks including text classification, question answering, and natural language inference. In this paper, we will focus on the pre-training of BERT-large. We pick BERT-large because of its wide adoption and state-of-the-art performance. From a systems perspective, BERT-large is also a suitable candidate since the encoder based attention layers [3]' in BERT have similar characteristics to many of the aforementioned models, and is considered as a next generation ML benchmark for the state-of-the-art ML benchmark suites such as MLPerf [22].",
            "score": 0.5795037930268434,
            "section_title": "Related work",
            "char_start_offset": 3691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 694
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 101,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13232421875
        },
        {
            "corpus_id": "267407058",
            "title": "Survey of transformers and towards ensemble learning using transformers for natural language processing",
            "text": "The traditional machine learning NLP method uses the TF-IDF algorithm to train the model. The article compares and analyzes the text analysis experiments of the four methods. In all these classifications, we use two different classifiers: BERT and traditional classifiers. \n\nBaruah et al. [23] use classifiers based on BERT, RoBERTa, and SVM to detect aggressiveness in English, Hindi, and Bengali texts. Their SVM classifier performed very well on the test set, with 3 out of 6 tests ranking second in the official results and fourth in the other. However, through more careful analysis, it can be seen that the SVM classifier performs better because the SVM model has a better classification effect. It is found that the BERT-based classifier can better predict minority groups. It was also discovered that their classifier did not correctly handle spelling errors and deliberate spelling errors. FastText word embedding works better when dealing with orthographic changes. \n\nLee et al. [24] trained the Korean version of the BERT model, KR-BERT, by using a small corpus. Due to the particularity of Korean and the lack of corpus, it is also very important to use the BERT model for language representation. For this reason, they compared different tokenizers and gradually narrowed the minimum range of tokens to build a better vocabulary for their model. After these modifications, the KR-BERT model they proposed can achieve better results even with a small corpus. \n\nIn this paper [25], Li et al. compared the BERT and XLNet models, especially from the comparison of the computational characteristics of the two. Through comparison, they found two points. The first is that the two models have similar computational characteristics. The second is that the XLNet model has a relative position encoding function. On modern CPUs, they have 1.2 times the arithmetic operation and 1.5 times the execution time. At this cost, a better benchmark score was obtained. \n\nAs multiple geographic locations are involved, the data is inherently multilingual, leading to frequent code-mixing. Sentiment analysis of the code-mixed text can provide insights into popular trends in different regions, but it is a challenge due to the nontrivial nature of inferring the semantics of such data.",
            "score": 0.5782029272808683,
            "section_title": "Review of related works",
            "char_start_offset": 37907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2280
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "218974027"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "215722669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396240234375
        },
        {
            "corpus_id": "257806425",
            "title": "A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat",
            "text": "We did experiments with the annotated dataset in \u00a73 to show the effectiveness and the efficiency of our method. Model trainings were conducted on Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz with a single processor. RAM size is 26GB. Cache memory consists of 32KB for L1d, 32KB for L1i, 1024KB for L2, and 28,160KB for L3. The model implementation is as follows: \n\nfastText classification To get the word representations, we create a pre-trained model using the fastText module (Bojanowski et al., 2017) with all the comments in \u00a73. For pre-processing, this data is tokenized by the Japanese morphological analyzer MeCab (Kudo, 2006). Additionally, a word was lemmatized and half-width characters were converted to full-width. Hyperparameters for the pre-training are as follows: The number of dimensions for word representation is 300. We used skip-gram to train word representation. The threshold \u03b8 in \u00a74.2 was determined with development set (\u03b8 = 0.98). \n\nSVM We calibrated the prediction results using a calibrated ClassifierCV 3 provided by scikit-learn to remove the effect of bias in the unbalanced data. \n\nLightGBM This model was optimized with optuna (Akiba et al., 2019). \n\nAs baseline models, we used each component of our method and existing pre-trained BERT-based models. We used the BERT-base4 and BERT-large5 models pre-trained from Japanese Wikipedia. Additionally, it is essential to compare with light BERT models for a fair evaluation of inference speed. Over the last couple of years, variants of BERT have been proposed to make the model size light and inference speed efficient. Specifically, we tried Distil-BERT6 (Sanh et al., 2019), ALBERT7 (Lan et al., 2019), and Poor-Man's BERT (Sajjad et al., 2020). Poor-Man's BERT is the method that removes some layers from an original BERT. Their experimental results showed that dropping the top layers works consistently well across different tasks when dropping 4 and 6 layers.",
            "score": 0.5762216948408545,
            "section_title": "Settings",
            "char_start_offset": 11210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 499,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23583984375
        },
        {
            "corpus_id": "257577511",
            "title": "Predicting health-related quality of life change using natural language processing in thyroid cancer",
            "text": "We used LIWC as an initial benchmark encoding method. LIWC is a dictionary-based text analysis tool widely used to ascertain the linguistic characteristics of text [15]. It assigns every word in the text to one or more categories in an internal dictionary and produces an output specifying the percentage of the total words in the text that fall into each linguistic category. LIWC does not consider the underlying structure or sequence of speech, but only its lexical contents. Though LIWC is limited by the accuracy and comprehensiveness of the dictionary they are based upon, it has been used in many studies evaluating indicators of psychosocial well-being [16] and has also served as a comparison point for deep learning methods [17][18][19]. \n\n1 If transcript 2 was not available, transcript 3 was used in its place. \n\nFor each sequence, frequencies were calculated for 9 LIWC categories of interest (Table 1), selected by an expert in communications research and medical sentiment analysis (JH) based on the research question and dataset properties. The LIWC frequencies became input features for the training of the logistic regression (LR) and support vector machine (SVM) classifiers. \n\n2.3.2. Analysis using BERT-BERT is a transformer-based language model which has achieved state-of-the-art performance on many natural language processing tasks [20]. BERT works by parsing large quantities of data and representing words and their statistical correlations with other words in a high dimensional feature space. BERT is then fine-tuned on a domain-specific corpus of text and applied to downstream tasks. In our experiments, we used the fine-tuned BERT as an encoder to extract sequence embeddings, which were then input into the classifier. Specifically, we added a linear layer followed by a sigmoid function on top of the pre-trained BERT, and they were fine-tuned together for classification. We used the BERT implementation of Wolf et al. [21]. The first token of the BERT output ([CLS]) was used as the sequence embedding, a vector with dimension 768 (Fig. 3a). An independent BERT model was fine-tuned separately for the analysis of each HRQOL measure (PCS, MCS, and average THYCA).",
            "score": 0.5758741000746553,
            "section_title": "Analysis with LIWC features-",
            "char_start_offset": 7436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "23950817"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "226283501"
                },
                {
                    "start": 1357,
                    "end": 1361,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0943603515625
        },
        {
            "corpus_id": "237253665",
            "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM",
            "text": "By considering the subword information, fasttext unlike word2vec and glove generates the embeddings for out of vocabulary words as well. The training data used for any machine learning model, no matter how big it may be, can still not include all the words in a language's vocabulary. If such unseen words are found in the test data, their representations are not generated by word2vec and glove embedding models. However, fasttext overcomes this limitation and represents the out of vocabu-lary words by adding the embeddings for the constituent char n-grams found in the vocabulary. \n\nBERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word. In BERT, an input word is represented by the sum of its token embeddings, sentence embeddings, and position embeddings. The proposed model FARM-BERT outperforms all the models by yielding the F-scores of 89.6% and 97.6% on Twitter and PubMed datasets, respectively. FARM-BERT performs better than BERT by 2% on Twitter and by 6% on PubMed datasets. Better performance of FARM-BERT than the standard BERT indicates the effectiveness of fine-tuning BERT with FARM with the modified values of hyperparameters.",
            "score": 0.5725587296345548,
            "section_title": "Experiments and Results",
            "char_start_offset": 24459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "221173062",
            "title": "Experts and authorities receive disproportionate attention on Twitter during the COVID-19 crisis",
            "text": "Based on the pretrained models described above, we compare downstream classifier performance scores in Fig. S 16. Unexpectedly, BERT models trained on English-only data outperform the multilingual BERT model. Generally, we also see a performance boost due to domain-specific pretraining. The best English-only model (bert-english-pt) gives a F1-macro score of 0.71 and 0.62, on the category and type datasets, respectively. The smaller FastText models (fasttext-english-pt) perform comparably to other models on the type dataset but give slightly lower scores on the category dataset. The best multilingual model (bert-multilang-pt) yields F1-macro scores of 0.56 (category) and 0.63 (type). \n\nFor further analysis we focus on the multilingual BERT model (bert-multilang-pt), which was the final model used for label expansion in this work. When inspecting the confusion matrices (Fig. S 14 and Fig. S 15), classifier scores for this model are generally satisfying. Certain classes for which only very few observations are present show lower scores in comparison. In particular, this is concerning the classes \"Religion\" and \"Public Services\" (for category) and \"Unclear\" (for type). The smallest error rates can be expected for the classes \"Healthcare\", \"News Media\", and \"Government and Politics\". No significant deviations from the mean accuracy could be observed for individual languages. Testing was performed using a two-sided binomial test at significance level \u03b1 = 0.5 (before Bonferroni correction). To alleviate a potential bias in our analysis caused by the hypothetical surge of new users joining the platform during the crisis, we conducted an alternative analysis where we restricted ourselves to a set of users who created their Twitter account before the studied period. In that way, we excluded 482, out of 14000 annotated users. The observed trends are not impacted by the presence of such newcomers.",
            "score": 0.5719645013261817,
            "section_title": "Classifier results",
            "char_start_offset": 37394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1918
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25537109375
        },
        {
            "corpus_id": "278237416",
            "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation",
            "text": "Inspired by successful demonstrations for English with FineWeb [38] and Nemotron-CC [46], we created educational quality classifiers to curate high-quality German web data by training a classifier on scores given by an LLM-as-a-judge. The intuition behind this is to identify web documents that are more likely to be educational, informative, and useful for model training, as opposed to spam, low-quality, or otherwise unhelpful content. \n\nWe first use an LLM-as-a-judge to label a small subset of the data. We then leverage these labelled documents as training data to train more efficient and lightweight fastText and BERT classifiers. This two-stage approach enables us to score a large number of web documents in a computationallyefficient way, while at the same time introducing implicit regularisation to the judging process. \n\nWe again made use of Mistral-Nemo-Instruct-2407, this time to annotate a random set of 600,000 documents from German FineWeb2 according to three criteria: (1) content quality, assessing coherence, informativeness and overall quality of the content; (2) language quality, evaluating the use of language, including formality, objectivity, and the presence of errors or slang; and (3) orthography, assessing the correctness of grammar, spelling, and punctuation, including errors such as typos, incorrect verb conjugation, and incorrect declension. We show the prompt used for the LLM-as-a-judge in Appendix A, which provided scores from one to five for each criterion. \n\nFor each document, we calculated a combined educational quality score by taking the minimum over the three criteria rated by the LLM-as-a-judge. This is because we observed that the judge would often miss poor quality in one criterium, but rarely in all three. We then used these educational quality scores as the training signal for fastText and BERT quality classification models. We trained one BERT model tasked to predict the educational quality scores given the first 512 tokens of the document's text. We also trained a binary classifier based on fastText.",
            "score": 0.5709488114994358,
            "section_title": "Educational quality",
            "char_start_offset": 10707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "270711474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.174560546875
        },
        {
            "corpus_id": "278534737",
            "title": "Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights",
            "text": "Text Classification The classification task involves assigning input samples X = {X1, X2, ..., Xm} to one of the n classes Y = {Y1, Y2, ..., Yn}, or to multiple classes in the case of multi-label classification. A classifier performs a mapping X \u2192 Y aiming to generalize to previously unseen data, based on the knowledge acquired during the training process on a labeled training corpus. Classification plays a crucial role in many tasks of natural language processing (NLP), such as sentiment analysis [32] and topic classification [3]. The introduction of the Transformer model in [33] marked a significant advance in the development of high-quality text classification systems. Building on this foundation, the BERT architecture [6] was introduced, which achieves state-of-the-art performance in a wide range of NLP tasks. BERT is particularly effective for text classification because of its pretraining on large-scale text corpora, which enables it to learn general language representations. These representations can then be fine-tuned for specific tasks such as text classification, resulting in high accuracy even when only limited amounts of labeled data are available, which is a valuable advantage given the cost and effort required for manual annotation. \n\nRecently, decoder-only transformer models such as GPT-3.5 [23] have demonstrated increasing performance across a variety of NLP tasks. This has led some to question whether BERT-based models remain a state-of-the-art solution for text classification. However, studies such as [17,34] have shown that BERT-based models can still outperform decoder-only architectures in text classification tasks. These findings underscore the continued relevance of evaluating the robustness of BERT-based approaches, given their ongoing use in both academic research and practical applications.",
            "score": 0.5700024059730632,
            "section_title": "Related Work",
            "char_start_offset": 3560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1847
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "199131135"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1545,
                    "end": 1549,
                    "matchedPaperCorpusId": "273901477"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407470703125
        },
        {
            "corpus_id": "268819482",
            "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
            "text": "The experiment was conducted in three stages.Initially, we evaluated a dataset without any augmentation.Subsequently, we applied four distinct augmentation techniques: WordNet, FastText, Back-translation, and BERT masking.In the final stage, we applied BERT-base cosine similarity filtering to each augmented dataset.Additionally, we assessed the augmentation performance of GPT-3.\n\nTable 14 shows results of not using the data augmentation.In this analysis, we chose the optimal classifier architecture.The comparison reveals that, in the absence of data augmentation, BERT marginally surpasses CNN, showing a slight improvement of 0.9% in accuracy and 1.0% in F1 score.Consequently, for experiments involving data augmentation, our focus has been solely on BERT, the state-of-the-art (SOTA) model demonstrating the best performance.\n\nTable 15 exhibits the experiment results of non-augmented and augmented datasets with and without BERT-cosine for four different datasets: AskFm, HASOC, TREC, and SST1.The examination of BERT-based classifier performance across these four distinct datasets-AskFm, HASOC, TREC, and SST1-reveals insights into the effects of data augmentation and the role of BERT-cosine similarity filtration.Initially, the impact of data augmentation methods such as WordNet, FastText, and BERT, when applied without BERT-cosine similarity, shows minimal to no improvements in accuracy and F1 scores.For example, BERT-mask data augmentation for AskFm has resulted in a drop of 0.3% in accuracy and 0.2% in F1.This suggests that the mere addition of augmented sentences does not guarantee enhanced model performance, emphasizing the necessity for quality over quantity in augmentation strategies.\n\nAmong the various data augmentation methods evaluated, back-translation demonstrated the best performance, showing a 0.5% improvement despite generating a smaller number of augmented sentences compared to WordNet, FastText, and BERT-masking methods.One possible explanation for this is that, although back-translation produces fewer sentences, it results in less label alteration compared to other methods, as observed in the previous table 12.",
            "score": 0.5689038924325909,
            "section_title": "Classification Results",
            "char_start_offset": 46644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 45,
                    "end": 104
                },
                {
                    "start": 104,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 381
                },
                {
                    "start": 383,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 834
                },
                {
                    "start": 836,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1528
                },
                {
                    "start": 1528,
                    "end": 1714
                },
                {
                    "start": 1716,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "277955900",
            "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models",
            "text": "As the scale of training corpora continues to grow and data-centric AI evolves, there is an increasing need for systematic approaches to select highquality pre-training data. This need has spurred the development of comprehensive pre-training data processing pipelines (Penedo et al., 2023;He et al., 2023He et al., , 2024a)), and data selection methods. Existing pre-training data selection methods can be categorized into three primary approaches: natural language quality-based methods, diversity-based methods, and classifier-based methods. \n\nFor natural language quality-based methods, Gopher (Rae et al., 2021) and RedPajama (Weber et al., 2024) propose empirical rules like controlling the ratio of word and number token in texts to improve language modeling. Additionally, previous works (Muennighoff et al., 2024;Wenzek et al., 2020) have shown that selecting data with perplexity (PPL) scores on validation datasets can lead to superior performance on downstream tasks compared to using the entire dataset. Another notable method is DSIR (Xie et al., 2023b), which streamlines the selection process by employing hashed N-gram features (named as data importance scores) to efficiently identify high-quality data within large datasets. Meanwhile, another line of works utilize clustering (Zhang et al., 2025) or deduplication (Abbas et al., 2023;He et al., 2024b) to enhance diversity of pre-training datasets. \n\nMore recently, more model-based classifiers have been introduced to assess the quality of pretraining data for LLMs. WanjuanCC (Qiu et al., 2024) employs two BERT-based classifiers to filter out data containing excessive advertisements and exhibiting lower fluency. QuRating (Wettig et al., 2024) introduces an innovative framework that simulates human-like text quality assessments, proposing four criteria to guide data selection. Similarly, Fineweb-Edu (Penedo et al., 2024) focuses specifically on assessing the Educational Value of data.",
            "score": 0.5674868717649105,
            "section_title": "Related Work",
            "char_start_offset": 4197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1418
                },
                {
                    "start": 1421,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 822,
                    "end": 842,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1048,
                    "end": 1067,
                    "matchedPaperCorpusId": "256627727"
                },
                {
                    "start": 1334,
                    "end": 1354,
                    "matchedPaperCorpusId": "257557221"
                },
                {
                    "start": 1696,
                    "end": 1717,
                    "matchedPaperCorpusId": "267681974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203125
        },
        {
            "corpus_id": "232278102",
            "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation",
            "text": "Most Frequent Class 50.0 Upperbound\n\nHuman performance 80.0 task consists of deciding whether the two target words in context refer to the same sense or not. Even though no sense inventory is explicitly given, this data set was also constructed based on WordNet. Table 12 shows a few examples from the data set.\n\nBERT-Based Model. Given that the task in WiC is a binary classification, the 1NN model is not applicable because a training to learn sense margins is necessary. Therefore, we experimented with the BERT model fine-tuned on WiC's training data. We followed Wang et al. (2019) and fused the two sentences and fed them as input to BERT. A classifier was then trained on the concatenation of the resulting BERT contextual embeddings.\n\nBaselines. In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al. 2017) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C). As other indicative reference points, we added two language models that are enriched with WordNet (Levine et al. 2020;Loureiro and Jorge 2019b) and another with WordNet and Wikipedia .\n\nResults. Table 13 shows the result of BERT models and the other baselines on the WiC benchmark. 24 We can see that BERT significantly outperforms the FastText static word embedding. The two versions of BERT (Base and Large) perform equally well on this task, achieving results close to the state of the art. As with fine-grained all-words WSD, the additional knowledge drawn from WordNet proves to be beneficial, as shown by the results for KnowBERT and SenseBERT. Table 17 shows for each sense their ID (as per their Wikipedia page title), definition, and example usage from the data set.",
            "score": 0.566767296839287,
            "section_title": "Lowerbound",
            "char_start_offset": 86334,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 586,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 851,
                    "end": 870,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1127,
                    "end": 1147,
                    "matchedPaperCorpusId": "199668663"
                },
                {
                    "start": 1147,
                    "end": 1172,
                    "matchedPaperCorpusId": "195345080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48681640625
        },
        {
            "corpus_id": "231627772",
            "title": "Hostility Detection and Covid-19 Fake News Detection in Social Media",
            "text": "FastText based LSTM + FastText Classifier FastText represents a text by average of word vector and it also allows the word vector to be updated through back-propagation while training. FastText classifier uses (multinomial) logistic regression for training on the vector average and leverages Hierarchical Softmax to reduce computational complexity. We pass the cleaned training data to the FastText, and use grid search to find the optimal hyperparameter for the model. For the multi-label classification we use one-vs-all strategy. \n\nFastText based LSTM + Features In this paper, we built the hostility classification model using the LSTM [8] model with FastText embedding and metadata. We use the LSTM model owing to its internal memory to process long-range dependencies in sequential inputs like tweets. This model takes two inputs, the first being V = {v1, v2, ..., vn} the FastText Embedding, where each v i represents a cleaned tweet in vector space and the second MD = {A, E, H, M, U} the 5-tuplet metadata representation where A, E, H, M, U are the count of abusive words, emoji, hashtag, mention, and URL per clean tweet respectively. We use keras 6 to define and train our model. \n\nBERT-Hindi Embedding with BERT model Recent papers using BERT based contextual embedding models have shown SOTA performance in Hostile language identification [16,17]. We use Hindi-BERT pre-trained model 7 and finetune it on our dataset. The Hindi BERT model has 12 hidden layers, with an encoding layer of 256 dimensions instead of 786, the model has 4 attention heads for each attention layer. The dimension of the feed-forward layer is kept as 1024 instead of 3072 which is the default in BERT models. We find the best learning rate (i.e., 1.2e-4 here) for the model by simulating the training data briefly with different learning rates and choose the one with minimum loss. For training, we pass the preprocessed tweets into the BERT model and train the model for 12 epochs with a batch size of 8.",
            "score": 0.5639077271806134,
            "section_title": "Fig. 2: Data and modeling pipeline for Hindi Hostility Classification",
            "char_start_offset": 12491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1353,
                    "end": 1357,
                    "matchedPaperCorpusId": "204907377"
                },
                {
                    "start": 1357,
                    "end": 1360,
                    "matchedPaperCorpusId": "184483272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4619140625
        },
        {
            "corpus_id": "267406876",
            "title": "The Language Model, Resources, and Computational Pipelines for the Under-Resourced Iranian Azerbaijani",
            "text": "The objective of this research was to establish fundamental pipelines and resources for the Iranian Azerbaijani language. A collection of subword embedding (fastText), transformer language model (BERT), text classification, token classification (POS tagging), and machine translation models for Iranian Azerbaijani NLP is available at Hugging Face repository2 , with corresponding code found on GitHub3 .The obtained results are summarized in Table 1: Embedding intrinsic evaluation: Our fastText model obtained an MRR of 0.46 in word analogy intrinsic evaluation indicating that the model can guess the analogies on average in the second guess. Language modeling perplexity: \n\nWe evaluated the model perplexity of our BERT language model, and achieved a perplexity score of 48.05. Given the constraints of a low-resource language, achieving a perplexity of 48.05 is quite commendable and suggests that despite the scarcity of training data, our model was able to produce relatively accurate predictions. Text classification: our fine-tuned BERT models performed better than the other two models on the text classification task. \n\nAfter the BERT model, the fastText-based baseline showed superior performance in comparison with the TF-IDF baseline (an extrinsic evaluation of the fastText embedding). We conducted a text classification comparison to showcase the impact of transliteration data for Iranian Azerbaijani in BERT masked language model pretraining. Our BERT model, trained on both transliterated and original Iranian Azerbaijani data, achieved an impressive macro-F1 of 0.89 in supervised text categorization. \n\nIn contrast, the BERT model trained solely on Iranian Azerbaijani data attained a significantly lower macro-F1 of 0.48. Moreover, training the model on transliterated data resulted in a mBert score of 0.85 macro-F1, further confirming the efficacy of utilizing transliterated data in transformer language models for downstream tasks. Token classification: The transformer-based tagger achieved a satisfactory performance with an accuracy of 0.86 and an F1-score of 0.67. This performance indicates that the fine-tuned BERT tagger is able to identify and classify language elements in the dataset with a moderate degree of accuracy and completeness.",
            "score": 0.563579788849456,
            "section_title": "Results",
            "char_start_offset": 9346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "221585959",
            "title": "Modern Methods for Text Generation",
            "text": "In order to evaluate BERT and GPT-2, authors prepared different versions of both models (with different number of parameters) and fine-tuned them in different downstream tasks. One version is BERT Base, which has the same number of parameters as GPT (the predecesor of GPT-2) for comparison purposes. Unfortunately, we could not find common benchmarks between BERT and GPT-2 in the same datasets, so we only can compare BERT and GPT in GLUE and SWAG tasks. GLUE [14], which consists of a wide list of NLP tasks (sentiment analysis, similarity, NLI, question-answering...), is one of the dataset where BERT was fine-tuned. In this dataset, BERT Base already outperformed GPT (and prior approaches such as BiLSTM+ELMo) and BERT Large got an even better result 3 , thus it achieved the status SOTA in this collection of tasks. \n\nSWAG [15] consists of 113k multiple choice questions about grounded situations: given a question/situation, we have to pick the correct answer among four choices. Identically as GLUE, BERT outperformed GPT (BERT Large improved GPT score by 8.3%). It is also surprising that BERT Large got a better score than a human expert in a small subset of 100 questions. \n\nThe models described in this article, BERT and GPT-2, demonstrate the benefits of large scale language modeling. Both papers leverage advances in compute and available text corpora to significantly surpass state-of-the-art performance in Natural Language Understanding (NLU), modeling, and generation. Currently, there are new models (based on BERT and GPT-2 architectures) which get even better results. Some of them are RoBERTa (robustly optimized version of BERT with larger training data) [16], ALBERT (lite version of BERT) [17], DistilBERT (also a reduced version of BERT) [18] and StructBERT (incorporate language structures into BERT pre-training) [19] 4 5 6 . \n\nAppart from the mentioned BERT-based models, Nvidia published Megatron-LM [20] in 2019.",
            "score": 0.5619282514870063,
            "section_title": ". Performance",
            "char_start_offset": 15584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1946
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12115478515625
        },
        {
            "corpus_id": "260350873",
            "title": "FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models",
            "text": "We adopt our FinPT to different pretrained foundation models as the backbone model structure. The last Decoder T5Block Flan-T5-XXL [9] 11B 260M \n\nThe last Decoder T5Block LLaMA-7B [32] 7B 202M \n\nThe last Decoder layer LLaMA-13B [32] 13B 317M \n\nThe last Decoder layer \n\n\u2022 BERT [11]: a well-known bidirectional Transformer-based encoder-only language model pretrained with mask language modeling and next-sentence prediction tasks. \u2022 FinBERT [40]: a fine-tuned BERT model, trained with financial text on several classification tasks such as financial sentiment analysis. \u2022 GPT-2 [29]: a Transformer-based decoder-only language model pretrained with autoregressive next-token prediction task, i.e., language modeling. \u2022 T5 [30]: a Transformer-based encoder-decoder architecture that transforms various natural language processing (NLP) tasks into a unified text-to-text format to pretrain the model for general use. \u2022 FLAN-T5 [9]: an instruction-tuned version of T5 model that performs a range of zero-shot NLP and few-shot in-context learning tasks. \u2022 LLaMA [32]: a set of open-sourced large foundation language models trained on trillions of tokens. LLaMA ranges from 7B to 65B parameters and LLaMA-13B outperforms GPT-3 (175B) [7] on many benchmarks. \n\nAs mentioned in Section 3.2, for the concern of computing resources, we use a small classifier-a feed-forward neural network-to leverage the hidden states produced by large foundation models. In addition, we freeze some parts of the foundation models in accordance with their model size, as shown in Table 2.",
            "score": 0.5591791007118239,
            "section_title": "Pretrained Foundation Model as Backbone",
            "char_start_offset": 16433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 143
                },
                {
                    "start": 146,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1561
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08990478515625
        },
        {
            "corpus_id": "226283907",
            "title": "Using BERT for Qualitative Content Analysis in Psychosocial Online Counseling",
            "text": "BERT is a multi-layer bidirectional Transformer encoder based on the original Transformer implementation described in Vaswani et al. (2017). BERT is typically pre-trained on two unsupervised learning tasks. After the pre-training, the model can be fine-tuned according to the downstream task (Vaswani et al., 2017).\n\nFor the classification task in our approach, we used the BertForSequenceClassification implementation from the Hugging Face's Transformers library  that combines the BERT Transformer model with a sequence classification head on top (Hugging Face, 2020).\n\nIn total, we tested thirteen pre-trained BERT models. Among the ten tested German language models, the results varied between a weighted F1 score of 69.3% and 74.4% on the test data set, whereby the best result was achieved with the pre-  All of the following analyses are, therefore, based on the best performing DBMDZ BERT model.\n\nThe hyperparameters used for the fine-tuning were taken from the original BERT publication (Devlin e.a., 2018). Since we are using text sequences with a length of 256 WordPiece tokens, a batch size value of no more than 16 was possible due to GPU memory limitations. Larger models, especially multi-lingual models, even only allowed a batch size of 8. Further testing has shown that the best results can be achieved with a learning rate of 2e-5 and 4 epochs. Table 2 shows the different evaluation metrics for both, the SVM and the best BERT classifier.",
            "score": 0.5591297124939482,
            "section_title": "BERT as Advanced Classifier",
            "char_start_offset": 13965,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.173583984375
        },
        {
            "corpus_id": "272126224",
            "title": "The Training Process and Methods for LLMs Using an Own Knowledge Base",
            "text": "The popular LLMs, such as GPT-4, BERT, LLaMA, and Mistral, represent cutting-edge technology in NLP and CV. These models, pre-trained on vast amounts of text data, have mastered the language's complex structures and semantic relationships [2] , demonstrating excellent language understanding and generation capabilities. \n\n\uf0b7 GPT-4 (Generative Pre-trained Transformer 4th edition), developed by OpenAI, is the latest generation of language models based on the transformer architecture, learning from massive internet text through unsupervised learning to generate high-quality, coherent natural language text. GPT-4 excels in various application scenarios, including text generation, translation, Q&A systems, and chatbots [3] . \n\n\uf0b7 In contrast, BERT (Bidirectional Encoder Representations from Transformers), proposed by Google, innovates its bidirectional encoder architecture, considering contextual information during pre-training to better understand the relationships between words and phrases in sentences [4] . \n\n\uf0b7 BERT has achieved leading performance in various NLP tasks, such as Q&A, sentiment analysis, and text classification. LLaMA, developed by Meta AI (formerly Facebook AI), is a series of largescale pre-trained language models designed for text generation and understanding, available in multiple sizes (e.g., LLaMA-7B, LLaMA-13B) to suit different resource and task requirements. \n\n\uf0b7 Mistral, an open-source large language model, focuses on efficient text generation and understanding, offering improved performance and efficiency through optimised training processes and architecture design, supporting various tasks, including text generation, Q&A, and translation. \n\nThese large language models have several advantages. First, they capture deep linguistic features through large-scale pre-training, achieving superior performance in various downstream tasks. GPT-4's generation capability is powerful, generating coherent and contextually relevant long texts, and it is widely used in automatic writing, content creation, and virtual assistants. BERT excels in understanding and processing complex relationships between sentences, with its bidirectional encoder architecture making it advantageous in tasks requiring precise context understanding [4] . LLaMA offers models of different scales, allowing users to choose the appropriate model based on computing resources.",
            "score": 0.5573051062110644,
            "section_title": "Popular Large Language Models",
            "char_start_offset": 1641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2275
                },
                {
                    "start": 2276,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1012,
                    "end": 1015,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 2270,
                    "end": 2273,
                    "matchedPaperCorpusId": "219531210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154296875
        },
        {
            "corpus_id": "269877815",
            "title": "Automatic identification of role-specific information in product development: a critical review on large language models",
            "text": "It can be seen that the models \"BERT\" and \"RoBERTa\" as well as \"LLaMa2\" fulfil the defined basic requirements.After the pre-selection, in a second step, the three language models are tested for the central task of text classification and, based on this, one model is selected for validation.For reasons of comparability, literature studies are used in which the models \"BERT\", \"RoBERTa\" and \"LLaMa2\" are analysed on the same training and test data sets.The results of the classification taskbased on four different datasets (the datasets differ in terms of content, number of data and number of labels -e.g. the dataset \"AGNews\" contains 120,000 news texts, of which 7,600 have to be classified with the labels \"World\", \"Sport\", \"Economy\" and \"Technology\") -show that the prediction quality of the three models has comparable results in the region of 60%.This means that each model classifies about 60% of the texts correctly.Another finding is that the larger LLaMa2 version with 13 billion parameters leads to a decrease in classification performance.This is due to underfitting, as there is not enough training data to optimally adjust the parameters of the model.(Li et al., 2023) Smaller models, such as BERT and RoBERTa, are particularly suitable for the use case defined here, where only a very small training data set can be used, due to their architecture and the size of the model.According to the studies by Briskilal et al. and Shaheen et al. it can be concluded that RoBERTa has a higher classification accuracy than BERT.In the study by Briskila et al. a classification of idioms is carried out.The results are transferable to standards as a data set, since idioms are text fragments that do not mean what they imply.Standards are also deliberately written implicitly, leaving room for interpretation.Here, the specific language can be seen as a unifying element that causes difficulties in the classification task.1470 pieces of text were classified and RoBERTa classified them correctly with an accuracy of 88%.In addition, the study by Shaheen et al. presents a pairwise comparison of language models in relation to the classification of legal texts.",
            "score": 0.5570304851299739,
            "section_title": "Analysis and selection of a pre-trained language model",
            "char_start_offset": 12876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1815
                },
                {
                    "start": 1815,
                    "end": 1929
                },
                {
                    "start": 1929,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2167
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092041015625
        },
        {
            "corpus_id": "252901000",
            "title": "Assessing Guest Nationality Composition from Hotel Reviews",
            "text": "Multiple machine learning models of varying complexity were trained on the training set. The results from the validation set are displayed in Table I. The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff. Table II shows an excerpt of a qualitative evaluation of the best performing model by displaying its predictions in specific edge cases. On the hidden test set prepared with independent annotators, the winning model achieved an F 1 score of 93.2%, convincingly demonstrating its capability to generalize to reviews from other sources. For comparison, the keyword-based approach without supervised learning would only achieve an F 1 score as low as 31.3%.",
            "score": 0.5566350445148964,
            "section_title": "IV. RESULTS",
            "char_start_offset": 5875,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.473388671875
        },
        {
            "corpus_id": "260386212",
            "title": "A Full-fledged Commit Message Quality Checker Based on Machine Learning",
            "text": "Baselines. For comparison, the following baselines are used: (1) and (2) 5 , which are tools specifically for Task 2, and (3) to 5 These are the only tools we found that detect the mood of the verb of a CM. They both utilize the POS tagger provided by NLTK [1]. (7), which have proven to perform well for a variety of NLP tasks, for Task 2-5.\n\n(1) GitCommitBear (GCB) [9], (2) Bad Commit Message Blocker (BCMB) [12] (3) Support Vector Machines (SVM) trained with TF-IDF embeddings (4) Random Forests (RF) trained with TF-IDF embeddings (5) FastText, a word embedding and classification model that was on par with the performance of deep learning methods as of 2017 [39] (6) Feed-Forward Neural Network (NN) based on self-trained dense vector word embeddings (7) Convolutional Neural Network (CNN) with max pooling over time, similar to the architecture proposed by Collobert et al. [48], also based on self-trained dense vector word embeddings. Transformer-based models. We evaluate models based on BERT [30] that utilize the encoder part of the Transformer architecture [56]. BERT-based models have been pretrained on vast amounts of natural language text. Pretraining facilitates transfer-learning, allowing models to adapt to specific NLP tasks. BERT-based models have been preferred for classification of commit messages in research [32], [33], [51], [55].\n\nWe evaluate the original BERT [30] itself, which is pretrained on English Wikipedia and freely available books.\n\nAlong BERT, we evaluate 2 models that enhance the architecture, parameters and training data of BERT: RoBERTa [41], a robustly optimized version of BERT, and DeBERTa [35], featuring disentangled attention and an enhanced mask decoder, both trained with more data than BERT -DeBERTa also utilizing stories and data from Reddit, and RoBERTa news articles in addition to that.\n\nWe also evaluate DistilBERT",
            "score": 0.5542775560764691,
            "section_title": "A. Evaluation setting",
            "char_start_offset": 25750,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 669,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "233690496"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1688232421875
        },
        {
            "corpus_id": "237485425",
            "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain",
            "text": "We conducted an experiment to evaluate the validity of the SE models seBERT and BERToverflow in comparison to the general domain BERT models. Since seBERT is based on BERT LARGE and BERT BASE , we use both of these models within our comparison. We follow the procedure outlined in Section 5. This means we first compare the vocabularies. Then, we proceed to look at the validity of the models without fine-tuning through their ability to infer the correct meaning of terms. Finally, we evaluated how the different models perform when fine-tuned with a limited amount of labeled data for a prediction task. \n\nAll results, as well as the required scripts and link to the data for pre-training seBERT are available as part of our replication kit. 10 Additionally, we prepared a playground, which can be used to fill in masked words using the models. 11  We restrict our comparison to BERT, as this is the most similar general domain model and, therefore, most suitable to explore the need for SE-specific models for SE domain tasks. Please note that we also exclude CodeBERT from this comparison, because CodeBERT is not natural language model, but rather a bi-modal \"natural language -programming language\" model, i.e., for working with source code. 12  Additionally, we use fastText for the comparison with respect to classification tasks, to understand if smaller NLP models may be sufficient.",
            "score": 0.553985413160196,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 35564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1393
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.320556640625
        },
        {
            "corpus_id": "265213231",
            "title": "German FinBERT: A German Pre-trained Language Model",
            "text": "BERT is a transformer-based (Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin, 2017) deep learning model for natural language processing (NLP). It utilizes a bidirectional approach to encode word representations, enabling it to capture contextual information for every word from both left and right side in a text. In the original paper, BERT's pre-training involves masked language modeling (MLM) and next sentence prediction (NSP), both unsupervised tasks that allow the usage of vast amounts of unlabeled text data. In MLM, certain tokens within a sentence are randomly masked and the model is trained to predict these masked tokens based on their surrounding context. NSP, on the other hand, involves training the model to discern whether a given sentence logically follows a preceding sentence, thereby enhancing the model's understanding of sentence relationships. Fine-tuning BERT on downstream tasks has demonstrated its ability to achieve state-of-the-art performance across various natural language understanding tasks, making it a widely adopted and influential model in the field of NLP. \n\nOriginally, the pre-training of BERT is designed to make it a versatile and general model, making it applicable to a wide range of different tasks of different domains. The authors achieve this property by pre-training BERT on general domain corpora, such as news articles, books or Wikipedia. The performance of such a general model is typically measured and compared by its performance on the GLUE benchmark, which is a collection of diverse natural language understanding tasks designed to evaluate the performance of language models across a range of linguistic capabilities (Wang, Singh, Michael, Hill, Levy, and Bowman, 2018). While its high GLUE score suggests that the original BERT model excels across a wide range of tasks, research has shown that pre-training BERT models on large-scale domain-specific corpora can further enhance their performance on domain-specific tasks compared to a generic model. \n\nFor instance, BioBERT (Lee, Yoon, Kim, Kim, Kim, So, and Kang, 2020) is a BERT model that is pre-trained on biomedical text, pre-trained on more than 21 billion words.",
            "score": 0.5536237379723846,
            "section_title": "Literature Review",
            "char_start_offset": 6542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 109,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153564453125
        },
        {
            "corpus_id": "271691319",
            "title": "Sentiment Analysis: Predicting Product Reviews for E-Commerce Recommendations Using Deep Learning and Transformers",
            "text": "FastText: a Word2Vec plugin that breaks down words into n-grams, or smaller units, like \"application\" into \"app,\" with the goal of teaching word morphology. Every word in the text is converted by the model into a bag of embedded vectors. \n\nPolysemous words might provide difficulties when using Word2Vec and FastText since they always assign the same embedding vector, regardless of context. Researchers have been using transformer-based embeddings, such as BERT and its derivatives, to address this problem. Word contexts from BooksCorpus and Wikipedia were used to pretrain models like BERT, and the resulting embeddings were then used in classifiers to make predictions. These models have demonstrated state of the art performance in natural language processing tasks by providing contextualized word embeddings. \n\nThe bidirectional transformer used in the BERT-based model has been pre-trained on large amounts of unlabeled textual data to provide a language representation that can be tailored to different classification tasks. One noteworthy variation is RoBERTa, which Facebook unveiled. It is an improved version of BERT with increased processing power and expanded prediction capabilities that can handle larger amounts of data. Furthermore, ALBERT, a condensed and effective BERT variant that is far smaller than BERT, was created by Google and Toyota. In particular, two variants of a BERT-based model, RoBERTa and ALBERT, were studied in this work.",
            "score": 0.5519652038815089,
            "section_title": "\u2022",
            "char_start_offset": 31744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "229713941",
            "title": "Dependency parsing of biomedical text with BERT",
            "text": "Deep language models, especially recent models based on the Transformer neural network architecture [43] have had a major impact in natural language processing, leading to a new state of the art performance on a large number of established reference tasks. Arguably the model with the broadest impact to date is the BERT model of Devlin et al. [32]. These language models are pre-trained on a large amounts of raw, unannotated text, and subsequently fine-tuned with annotated task-specific data to create models for specific downstream tasks such as parsing. Since pre-training such models frequently involves fitting hundreds of millions of parameters to examples derived from billions of words of text through millions of minibatch training steps at a non-trivial computational cost, pre-trained models are typically distributed publicly, and the ability to choose the correct pre-trained model for the task at hand from among the large and fast-growing set of published models is an important factor for success. \n\nOne major difference between pre-trained language models is the text domain from which the pre-training data is drawn, which affects e.g. the vocabulary known to the model. Similarly to how the previous generation of context-free word representations benefit from initialization on in-domain data [44,45], deep contextual models such as BERT should generally be pre-trained using data that reflects the domains that the models will be fine-tuned for to maximize performance [46][47][48]. In addition to the pre-training data, the models can also differ in the various training and model size parameters. Two common sizes for BERT models are Base, with 12 Transformer layers and approx. 110 million parameters, and Large, with 24 layers and approx. 340 million parameters, where the exact parameter count varies based on the vocabulary size. While more demanding of computational resources in pre-training, fine-tuning, and prediction, Large models generally provide for better performance, and we here focus on Large variants of BERT models whenever available. \n\nIn order to assess the impact of the choice of the pre-trained model on parsing performance, we here evaluate performance initializing the parser with each of the following BERT models:",
            "score": 0.5512538236875685,
            "section_title": "BERT models",
            "char_start_offset": 7116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1315,
                    "end": 1319,
                    "matchedPaperCorpusId": "3103489"
                },
                {
                    "start": 1319,
                    "end": 1322,
                    "matchedPaperCorpusId": "17458010"
                },
                {
                    "start": 1492,
                    "end": 1496,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 1496,
                    "end": 1500,
                    "matchedPaperCorpusId": "189762009"
                },
                {
                    "start": 1500,
                    "end": 1504,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1917724609375
        },
        {
            "corpus_id": "236986986",
            "title": "A Survey on Vision Transformer",
            "text": "More specifically, when performing sequence-level tasks (e.g., sentiment analysis), BERT uses the representation of the first token for classification; for token-level tasks (e.g., name entity recognition), all tokens are fed into the softmax layer for classification. At the time of its release, BERT achieved the state-of-the-art performance on 11 NLP tasks, setting a milestone in pre-trained language models. Generative Pre-trained Transformer models (e.g., GPT [258], GPT-2 [110]) are another type of PTMs based on the transformer decoder architecture, which uses masked self-attention mechanisms. The main difference between the GPT series and BERT is the way in which pre-training is performed. Unlike BERT, GPT models are unidirectional language models pre-trained using Left-to-Right (LTR) language modeling. Furthermore, BERT learns the sentence separator ([SEP]) and classifier token ([CLS]) embeddings during pre-training, whereas these embeddings are involved in only the fine-tuning stage of GPT. Due to its unidirectional pre-training strategy, GPT achieves superior performance in many natural language generation tasks. More recently, a massive transformerbased model called GPT-3, which has an astonishing 175 billion parameters, was developed [11]. By pre-training on 45 TB of compressed plaintext data, GPT-3 can directly process different types of downstream natural language tasks without fine-tuning. As a result, it achieves strong performance on many NLP datasets, including both natural language understanding and generation. Since the introduction of transformer, many other models have been proposed in addition to the transformer-based PTMs mentioned earlier. We list a few representative models in Table 5 for interested readers, but this is not the focus of our study. TABLE 5: List of representative language models built on transformer. Transformer is the standard encoder-decoder architecture. Transformer Enc. and Dec. represent the encoder and decoder, respectively. Decoder uses mask self-attention to prevent attending to the future tokens. The data of the Table is from [203].",
            "score": 0.5506667777730287,
            "section_title": "A2. Revisiting Transformers for NLP",
            "char_start_offset": 94083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 484,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 2109,
                    "end": 2114,
                    "matchedPaperCorpusId": "212747830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1610107421875
        },
        {
            "corpus_id": "252111214",
            "title": "Latency Adjustable Transformer Encoder for Language Understanding",
            "text": "Recently, Transformer [1] based architectures achieved remarkable success in various Natural Language Processing (NLP) tasks. However, the training difficulty, inference latency, and small size of available datasets are the main concerns with using these models in real applications [2]. Fine-tuning a pretrained language model on downstream tasks mitigates some of these concerns, including training effort and dataset size. BERT [3], XLNet [4], GPT [5], Llama [6], and Gemma [7] represent a spectrum of pre-trained language models, ranging from earlier developments to the latest advancements. These models vary significantly in size, ranging from hundreds of millions to hundreds of billions of parameters. Hence, the computational complexity bears trillions of Floating Point Operations (FLOPs) for processing a single sample. In some cases, the inference phase computational effort is beyond the ability of resource-limited systems such as edge devices. Therefore, baseline Transformer models cause unacceptable latency and energy usage limitations in such devices. \n\nBERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are two landmark architectures in the development of large language models (LLMs). BERT, introduced by Google, uses a bidirectional training approach, allowing it to capture context from both directions of a sentence simultaneously. This makes it particularly effective for tasks like sentence classification, question answering, and named entity recognition, where understanding relationships between words is crucial [3]. On the other hand, GPT, developed by OpenAI, takes an autoregressive approach, generating text word by word in a unidirectional manner. Starting with GPT-1 and evolving through GPT-4, GPT has proven its strength in creative and coherent text generation, conversational AI, and text summarization, making it a highly flexible model for both content generation and interactive applications [5], [8]. \n\nMore recent models like Meta's LLaMA (Large Language Model Meta AI) and Gemma are pushing the boundaries of LLM development. LLaMA is designed to be highly efficient in terms of computational resources, offering a smaller model that still performs well on a wide range of natural language processing tasks.",
            "score": 0.5490025825987404,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 25,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "219182397"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1734619140625
        },
        {
            "corpus_id": "254685547",
            "title": "The Effects of In-domain Corpus Size on pre-training BERT",
            "text": "Pre-training large neural language models based on Transformers (Vaswani et al., 2017) such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) and its variants (Liu et al., 2019;Yang et al., 2019;Lan et al., 2019) has proven to be an excellent strategy and achieved state-of-the-art results on many downstream natural language processing (NLP) tasks. Most models focused their pre-training efforts on general domain text. For example, the original BERT model was trained on Wikipedia and the BookCorpus (Zhu et al., 2015). Many other following efforts focused on adding additional texts to the pre-training process to create even larger models with the intent of improving model performance (Liu et al., 2019;Raffel et al., 2019). However, recent works have shown that, given the general nature of the corpora these models were pre-trained on, they do not perform as well when introduced to domain-specific corpora as found in categories such as biomedicine, law, or finance, to name a few. Several efforts have demonstrated that by pre-training on domainspecific corpora, either through a \"from scratch\" or a continual pre-training approach, these same models achieved much better performance on indomain, downstream NLP tasks. (Beltagy et al., 2019;Lee et al., 2020;Huang et al., 2019) The success of these domain-specific BERT models encouraged practitioners to explore the possibilities of pre-training BERT using corpora in their respective domains to get better performing language models and to better tackle their indomain NLP tasks. One of the biggest challenges presented to NLP practitioners, however, lies in the lack of large, readily available domain-specific corpora; for reference, the original BERT was pretrained on 3.3 billion tokens, or roughly 20 GB of uncompressed raw text. To get enough in-domain text data for pre-training, practitioners must resort to what is either available \"in-house\" or available through public or private resources. Web scraping is one oft-cited method used to gather publicly available documents to increase one's in-domain training corpora.",
            "score": 0.548785913327069,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 233,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 540,
                    "end": 558,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1288,
                    "end": 1305,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "268819482",
            "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
            "text": "We chose five benchmark datasets for testing our proposed augmentation method.The overall experimentation methodology includes a four-stage process: (i) dataset selection, (ii) data augmentation, (iii) comparison between different augmentation methods, and (iv) classification performance comparison (accuracy, F1 score), and error analysis.\n\n1. First, we select five publicly available benchmark datasets to test data augmentation using different methods as well as the classifier accuracy for augmented and non-augmented datasets.\n\n2. Second, we expand the selected datasets with different existing augmentation methods (e.g., synonym replacement with WordNet and FastText , Back-translation, BERT mask) and apply our proposed methods (e.g., Backtranslation + BERT cosine).For the testing phase with Large Language Models (LLMs), specifically GPT-3, we utilize the GPT-3 API interface to generate augmented sentences.\n\n3. After data augmentation, we compare our proposed method with other popular methods in terms of: the number of newly generated data; and the quality of the expanded sentence (how close is it to the original sentence).\n\n4. Next, we experiment ML performance with all expanded datasets and not expanded datasets for our proposed method and other existing methods.For that, we use: Accuracy, F1_measure.We exploit different classifiers such as NB, LR, CNN model, and BERT-base.We contrast the results and evaluate the data augmentation process for all four datasets.\n\nFinally, we perform error analysis for both expanded and not-expanded datasets.Besides, we consider semantic fidelity to evaluate the use of BERT-cosine filtration.We measure the statistical significance of BERT-cosine similarity scores compared to human judgment.",
            "score": 0.5486724690903853,
            "section_title": "Experiment METHODOLOGY",
            "char_start_offset": 35023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 341
                },
                {
                    "start": 343,
                    "end": 532
                },
                {
                    "start": 534,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 919
                },
                {
                    "start": 921,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1323
                },
                {
                    "start": 1323,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1486
                },
                {
                    "start": 1488,
                    "end": 1567
                },
                {
                    "start": 1567,
                    "end": 1652
                },
                {
                    "start": 1652,
                    "end": 1752
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406005859375
        },
        {
            "corpus_id": "272524632",
            "title": "Improving Pretraining Data Using Perplexity Correlations",
            "text": "In addition to these commonsense baselines, we also run DSIR (Xie et al., 2023b), a lightweight training data selection technique based on n-gram overlaps that Li et al. (2024) found to be competitive with proxy LLM-based techniques and was also validated at scale (Parmar et al., 2024). Finally, we run the state-of-the-art method for pretraining data quality filtering found by Li et al., which is a fastText classifier that beats all of the heavier-weight proxy-LLM methods tested. The classifier was trained on a benchmark-agnostic and handcrafted objective, which is to classify data as Common Crawl3 (low quality) or OH2.5 (Teknium, 2023)  Results. We report average rankings over initial benchmarks in Table 1, and we find that our approach significantly outperforms the basic baselines of random sampling, language filtering, and DSIR. Compared to the existing state of the art from Li et al. (2024), our approach beats the performance of the default, English-filtered fastText classifier, but loses slightly once we add in a manual language filtering step to enable better performance on the multilingual LAMBADA datasets. For the maintext comparisons, we use the optional fastText classifier from our algorithm to select pretraining data at the page levels, but we show ablations without the classifier in Appendix H. \n\nFigure 2 shows how each data selection method affects benchmark performance in more detail. Each block of rows represents a data selection method, while an individual row represents an LLM within a method that targets a particular benchmark or set of benchmarks. Columns represent benchmarks. We see that language filtering and perplexity correlations both clearly optimize for the target benchmark: within each block, the benchmark column matching each row typically performs best. The pattern is much less obvious for DSIR -the heatmap looks more uniform across LLMs with different task targets. We also see that while language filtering has significant impacts on model performance, our performance significantly exceeds the impact of language filtering across all tested benchmarks. For some more analysis, including the types of domains selected, see Appendix O and J.",
            "score": 0.5484287138600837,
            "section_title": "INITIAL PRETRAINING EXPERIMENTS",
            "char_start_offset": 26288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "244895506",
            "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT",
            "text": "The pre-trained BERT model's performance, accuracy and approach motivated the authors to experiment with this model for customer sentiment analysis. The main objectives and contributions are as follows; \n\n\uf0b7 Overcome known challenges of SA model performance, accuracy and context-based prediction. \n\n\uf0b7 Train BERT-base-cased model on Twitter, IMDB Movie Reviews, Yelp, and Amazon customer reviews datasets to improve the accuracy and performance of the model. \n\n\uf0b7 Evaluate the custom deep learning sequential model of BiLSTM, hybrid fastText-BiLSTM model and linear models of LSVM, fastText models using the same datasets. \n\n\uf0b7 Compare the results of the BERT model with the results of the deep learning sequential and linear model. \n\n\uf0b7 Customize the data pre-processing steps for hybrid and linear model training. \n\n\uf0b7 Fine-tune the hyperparameters for fastText-BiLSTM models. \n\nThis paper presented with several recent BERT and SA related research papers reviews and major contributions from various researchers in Section II. The literature reviews of BERT, fastText, BiLSTM, and LSVM models are presented in Section III. The experimental setting and model evaluation results are discussed in the Section IV. In Section V, concluded this paper with model results, findings of this research work and future work.",
            "score": 0.5482692460567311,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "273323889",
            "title": "Studying and Benchmarking Large Language Models For Log Level Suggestion",
            "text": "We selected 12 open-source Large Language Models, as enumerated in Table II, for our experimental purposes. These models encompassing both NLP-based and Code-based models, vary in size, ranging from 84 million to 13 billion parameters, and will all be executed using identical datasets in III-A to examine the influence of both model size and type on performance, eliminating the potential confounding factor of using diverse training/testing data. \n\nFill-Mask Models. NLP-based Language Models like BERT base, BERT large, RoBERTa base, and RoBERTa large are chosen for examination to assess whether programming statements can effectively extract semantic information. Specifically, these Language Models undergo pretraining using the Masked Language Modeling (MLM) objective. This involves randomly masking 15% of the words in a sentence, subsequently processing the entire masked sentence through the model, and predicting the masked word [35]. For code-based models, we opted for CodeBERT and CodeBERTa as they are pre-trained on data from various programming languages. We also consider, codebert-java, a specialized CodeBERTa model trained exclusively on Java code, shares the identical model architecture and size (125M parameters) due to their common foundation in the masked-language-modeling task [50]. \n\nText Generation Models. We selected LLaMA 2 models as they provide a benchmark for evaluating large-scale, generalpurpose language understanding and generation [34]. While the CodeLlama models, specifically fine-tuned from LLaMA 2 models for coding tasks, offer insights into how specialized training affects performance in code-related applications. For both LLaMA 2 and CodeLlama, we selected 7B and 13B versions from both LLMs to compare and assess the impact of parameter size on accuracy in log level suggestion and investigate how these models perform under In-Context Learning and fine-tuning scenarios.",
            "score": 0.5479475327292729,
            "section_title": "B. Selecting Large Language Models (LLMs)",
            "char_start_offset": 17339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1924
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "209387706",
            "title": "BERT for Question Generation",
            "text": "Table 2 shows the comparison results using sentence-level context and Table 3 shows the results on paragraph level context. We compare the models using standard metric BLEU and ROUGE-L ( (Papineni et al., 2002)). \n\nWe have the following findings to note about the results. First, as can be observed, BERT-QG offers poor performance. In fact, the performance of BERT-QG is far from the results by other models. This result is expected as BERT-QG generates the sentences without considering the previous decoded results. However, when taking into account the previous decoded results (BERT-SQG), we effectively utilize the power of BERT and yield the state-of-the-art result compared with the existing RNN variants for QG. As shown in Table 2, BERT-SQG outperforms the existing best performing model by 2% on both benchmark datasets. \n\nSecond, the results in Table 3 further show that BERT-SQG successfully processes the paragraphlevel contexts and further push the state-of-the-art from 16.85 to 21.04 in terms of BLEU 4 score. Note that NQG-RC and PLQG both use the RNN architecture, and the RNN-based models all suffer from the issue of consuming long text input. We see that the BERT model based on transformer blocks effectively addresses the issue of processing long text. The results of our BERT-SQG model are consistent in two data set and have achieved the best score at the paragraph level.",
            "score": 0.5478172652599782,
            "section_title": "Evaluation Results",
            "char_start_offset": 9495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1398
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 210,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1268310546875
        },
        {
            "corpus_id": "249240428",
            "title": "Vietnamese hate and offensive detection using PhoBERT-CNN and social media streaming data",
            "text": "The transfer learning model has attracted increasing attention from NLP researchers worldwide for its outstanding performance. One of the SOTA language models as BERT, which stands for Bidirectional Encoder representations from transformers, is published by Devlin et al. [26]. BERT and its variations (BERTology) [63][64][65] such as RoBERTa [27], XLM-R [28], PhoBERT [36] have almost dominated and asserted their strength on natural language processing tasks, even for Vietnamese hate speech detection tasks [10,21,22]. For the aforementioned reasons, we decided to use BERT and its variants to find the optimal solution with good performance and contribute to the successful construction of our proposed solution. \n\nBERT [26]: is a contextualized word representation model pre-trained using bidirectional transformers and based on a masked language model. In this work, we use the train set to fine-tune the pre-trained BERT model before classifying comments or posts from websites or social networks. \n\nRobustly optimized BERT approach (RoBERTa) [27]: is trained with dynamic masking, wherein the system learns to predict intentionally hidden sections of text within otherwise unannotated language examples. RoBERTa, implemented in PyTorch, modifies key hyperparameters in BERT, including removing BERT's next-sentence pretraining objective and training with much larger mini-batches and learning rates. \n\nXLM-RoBERTa (XLM-R) [28]: is a multilingual model trained using over two terabytes of cleaned and filtered CommonCrawl data. Upsampling low-resource languages during training and vocabulary generation, generating a more extensive shared vocabulary, and raising the overall model capacity to 550 million parameters are all important contributions of XLM-R. \n\nPhoBERT [36]: For Vietnamese, the SOTA method was first released and called PhoBERT by Nguyen et al. [36] for solving Vietnamese NLP problems. PhoBERT is a pre-trained model, which has the same idea as RoBERTa, a replication study of BERT is released by Liu et al. [27], and there are modifications to suit Vietnamese.",
            "score": 0.5476352317182575,
            "section_title": "Transfer learning approach",
            "char_start_offset": 37267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "211677475"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "232307803"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "221970944"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "211010498"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1430,
                    "end": 1434,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1776,
                    "end": 1780,
                    "matchedPaperCorpusId": "211677475"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "211677475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1131591796875
        },
        {
            "corpus_id": "248780276",
            "title": "DepressionOne@LT-EDI-ACL2022: Using Machine Learning with SMOTE and Random UnderSampling to Detect Signs of Depression on Social Media Text.",
            "text": "The baselines used are: SVM with TF-IDF Term frequency and inverse document frequency-based vectorization is used to represent the text data, and the support vector machine is used to classify the data. \n\nCNN (Kim, 2014) This convolutional neural network-based text classifier is trained by considering pre-trained FastText word vectors. \n\nBi-LSTM (Hochreiter and Schmidhuber, 1997) A two-layer, bi-directional LSTM text classifier with pre-trained FastText word embeddings as input was considered for the task of text classification. \n\nPre-trained BERT (Devlin et al., 2018) A pretrained BERT model with a feed-forward network for classification",
            "score": 0.545951998332953,
            "section_title": "Baselines",
            "char_start_offset": 5851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 646
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 220,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 348,
                    "end": 382,
                    "matchedPaperCorpusId": "1915014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "267122910",
            "title": "SPCM: A Machine Learning Approach for Sentiment-Based Stock Recommendation System",
            "text": "Subsequent to the categorization of labeled data, we partition the dataset into training and test sets in a 7:3 ratio. Following the methodology proposed by Sousa et al. [53], we employ the BERT model as a pre-trained model, finetuning it on our labeled training set for three-class sentiment classification. We then conduct the performance evaluation using 10-fold cross validation. Remarkably, the BERT model achieves an impressive accuracy of 84.12% on the test set. \n\nTo assess the effectiveness of the BERT-based classification approach, we compare it with commonly used classification algorithms from previous studies, including Support Vector Machines (SVM), Convolutional Neural Networks (CNN), and Long Short-Term Memory networks (LSTM), all trained on the same labeled dataset. For the experiment, we used Intel(R) Core(TM) i5-9300H CPU, 16g DDR4 memory, NVIDIA Geforce GTX 1660Ti, and Python 3.7 environment. The performance results are presented in Appendix Table 4. Notably, the BERT model outperforms all other algorithms in sentiment classification within the Guba discussion. This outcome underscores the predictive superiority of pre-trained models in this context.",
            "score": 0.5457451752959033,
            "section_title": "4) SENTIMENT CLASSIFICATION",
            "char_start_offset": 23849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1182
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "211208214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229736328125
        },
        {
            "corpus_id": "229705027",
            "title": "Service quality monitoring in confined spaces through mining Twitter data",
            "text": "However, in case of Macro-Average F-score values, it can be observed that BERT's performance is more than two times better than the second-best approach, the skip-grambased MLP. In other words, when it comes to aspects with fewer observations, it can be observed that baseline approaches are unable to detect tweets' associations with minor aspects of SQ. This can be justified by the fact that a small number of tweets means fewer www.josis.org occurrences of input features in word-embedding or topic models, which can lead to the low variability challenge mentioned before. Table 8 provides a more detailed comparison between our proposed method and skip-gram-based MLP classifier as the second-best approach in terms of F-score and AUC values. As Table 8 demonstrates, while skip-grambased MLP achieves considerable precision values in \"Comfort\" and \"Personnel\" aspects, i.e., 43% and 63% respectively, unlike BERT, it fails to keep the trade-off between the corresponding recall values. This shows that the model is not sensitive enough to different kinds of feature vectors that can show up. \n\nIn contrast to all baseline methods, BERT is pre-trained on a large corpus of data and can be fine-tuned for the context of public transport, thus, it can bring extra semantic features into the process of text classification, which increases the variability and reduces the bias in the classifier. Additionally, BERT's bidirectional training of a transformer can also provide a deeper understanding of the language context and improves the learning capacity. As Table 7 illustrates, BERT outperforms all baseline approaches in all other aspects of SQ significantly. This discussion can also be confirmed by the resulting AUC metric, where BERT can achieve 17% higher AUC values compared to the second-best approach, the skipgram-based MLP. Moreover, it can be seen that other baseline approaches have an AUC value around 0.5, which reflects their incapacity to effectively separate different classes. After BERT, skip-gram-based classifiers perform better than LDA based approaches. In particular, skip-gram-based MLP has a significantly better performance especially in minor aspects where it achieves an F-score of 55% in the \"Information\" aspect.",
            "score": 0.5449788811892308,
            "section_title": "Aspect",
            "char_start_offset": 54706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.134521484375
        },
        {
            "corpus_id": "261505094",
            "title": "Metamorphic Malware and Obfuscation: A Survey of Techniques, Variants, and Generation Kits",
            "text": "Similar poor results were obtained in [387] which reported F1 scores ranging from 0.33 to 0.72 for the 8 malware types based on a similar LSTM approach. Te work of [388] went one step further and compared an LSTM approach to that of a transformer and fnally to a bidirectional encoder representation from transformers (BERT). BERT relies on learning latent representations from both directional contexts from before and after sequences, meaning that it does a better job encoding context of the API sequence. In [388], they also used the Windows PE malware dataset and found similar issues classifying the 8 classes with a weighted F1 score of 0.51 on their best performing BERT model. One approach that did fnd success using BERT was that of [389] who implemented fastText [390], a text vectorizing technique based on n-gram. While removing redundant API calls, such as NtDelayExecution, accuracies as high as 96.76% using BERT were obtained.",
            "score": 0.5440371498641647,
            "section_title": "Natural Language Processing",
            "char_start_offset": 116224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 943
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 43,
                    "matchedPaperCorpusId": "201621126"
                },
                {
                    "start": 164,
                    "end": 169,
                    "matchedPaperCorpusId": "235384351"
                },
                {
                    "start": 512,
                    "end": 517,
                    "matchedPaperCorpusId": "235384351"
                },
                {
                    "start": 743,
                    "end": 748,
                    "matchedPaperCorpusId": "236190873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1990966796875
        },
        {
            "corpus_id": "269302410",
            "title": "Social Media and Artificial Intelligence for Sustainable Cities and Societies: A Water Quality Analysis Use-case",
            "text": "\u2022 DistilBERT: DistilBERT is another variant of the BERT model aiming at applications with less computational and memory requirements.The concept of knowledge distillation is adopted during pre-training allowing a significant reduction in parameters without a significant impact on the performance of the model.\n\n\u2022 GPT: Generative Pre-trained transformer (GPT) models represent a family of Neural Network (NNs)-based language prediction models built on the Transformer architecture [21].These models are pre-trained on a huge volume of diverse text data.Currently, GPT is available in different versions.However, the first version of the model was introduced in 2018 by Open AI [21].In this work, we used GPT version 3.5 turbo.It is composed of 175 billion parameters, which is significantly higher than the number of parameters used in its previous versions and other transformers, such as BERT.In this work, we used prompt engineering for the classification of tweets through GPT 3.5.\n\n\u2022 Meta-LLAMA: Large Language Model Meta AI (LLMA) is also a family of pre-trained LLMs.Similar to GPT, multiple versions of LLAMA are available having 7B to 70B parameters.In this work, we used LLAMA 2, which is an improved version of the base model LLAMA.\n\nSimilar to the base model, LLAMA 2 is built on the Google transformer architecture with several interesting changes and improvements.For example, the RMSNorm pre-normalization, a SwiGLU activation function, and multi-query attention instead of multi-head attention and AdamW optimizer.The key differences between LLAMA 2 and the original LLAMA include a higher context length (i.e., 4096 compared to 2048 tokens) and grouped-query attention instead of multi-query attention.Similar to GPT 3.5, we used the prompt engineering method for text classification with LLAMA.",
            "score": 0.5437410824919121,
            "section_title": "Classification Via Individual Models",
            "char_start_offset": 12665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 310
                },
                {
                    "start": 312,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1243
                },
                {
                    "start": 1245,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "207870323",
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
            "text": "We assess the quality of the resulting dataset by learning unsupervised word and sentence representations through fastText and BERT models. For fastText, we train 300dimensional word embeddings on the head, middle and tail subsets of the English and Polish CommonCrawl corpora, sorted by document perplexity. We evaluate these on standard semantic and syntactic analogy datasets (Mikolov et al., 2013). We observe in Table 1 a steady increase in performance as we go from the tail to the head of the dataset, confirming the positive impact of our filtering method based on document perplexity. We also train BERT models on the English (en), Russian (ru), Chinese (zh) and Urdu (ur) languages, using either the Wikipedia corpora or our new CommonCrawl datasets. For these languages, we use respectively 16G, 5G, 1.1G and 106M of raw Wikipedia data (full datasets), and we cap the head CommonCrawl data to 21G, 21G, 17G, 2.2G for English, Russian, Chinese and Urdu. That is, we consider roughly the same amount of data for English, but increase the amount of data for Russian, Chinese and Urdu. We train a BERT-BASE architecture (Devlin et al., 2018) on each of these corpora, without next sentence prediction (NSP) as in (Lample and Conneau, 2019). For better comparison, we early-stop all our models after two days of training on 16 Volta32 GPUs, and use the exact same number of steps for each model. We evaluate each model on the XNLI (Conneau et al., 2018) corpus by using the training data in each language. Results presented in",
            "score": 0.5422807528194452,
            "section_title": "Training models on this dataset",
            "char_start_offset": 16864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 1220,
                    "end": 1246,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464111328125
        },
        {
            "corpus_id": "240419828",
            "title": "UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension",
            "text": "We investigate the performance of three models: a baseline approach based on sliding window [19] and two multi-lingual Transformer-based models BERT [20] and XLMRoberta [21]. The Sliding window approach was first introduced in the MCTest paper, and it solves the answer extraction problem in a rule-based manner without any training data needed. BERT is a powerful pre-trained model that recently obtained state-of-the-art performance on various NLP tasks. We use the multi-lingual pre-trained model released by Google to fine-tune the BERT model for the UQuAD1.0 task without applying additional language-specific NLP techniques. The Third model used is XLM-Roberta, a multi-lingual pre-trained transformer model from common Crawl trained on 100 languages, including Urdu. XLM-Roberta outperformed previous multi-lingual models such as mBERT and XLM on a variety of downstream tasks.",
            "score": 0.541481392026801,
            "section_title": "Models",
            "char_start_offset": 13649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 884
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "2100831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14404296875
        },
        {
            "corpus_id": "266977420",
            "title": "Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection",
            "text": "For text classification in all the tasks, we rely on various transformer-based language models including BERT (Bidirectional Encoder Representations from Transformers) and its multiple variations, such as DistilBERT, ALBERT, Roberta (Robustly Optimized BERT), and XML-Roberta. The selection of these algorithms is made on the basis of their performances in similar text classification applications, such as natural disaster analysis [21], water quality analysis in text [2], and healthcare [1]. A brief overview of these transformers-based models is provided as follows. \n\n-BERT: BERT [8] is one of the pre-trained transformer-based models that have been proven very effective in text analysis. It has been trained on a large collection of Wikipedia unlabeled text and can be fine-tuned for text classification in different application domains. One of the key characteristics of BERT includes its ability to extract contextual information/features through bi-directional learning (i.e., left \u2192 right and right\u2192 left). Despite being very effective in text analysis BERT also has some limitations. For instance, it is computationally intensive. The literature also reports that the model has not been efficiently trained. To overcome these limitations, several variants of BERT have been introduced in the literature, such as RoBERTa and Distil BERT. Moreover, the original model is available in two different configurations having a different number of layers and attention heads. We employed the version of the model with 12 layers and attention heads and 110 million parameters. We used the Tensorflow implementation of the model with Adam optimizer and binary cross entropy loss function. We trained the model for 5 epochs with a learning rate of 1e \u2212 5. -DistilBERT: DistilBERT [22] is one of the variants of BERT designed to overcome some limitations of the original implementation. It provides a computationally lighter and faster solution for text analysis. The model is based on the concept of knowledge distillation [11], during the pre-trained phase, resulting in a 40% reduction in model size. While the overall architecture remains similar to the original BERT, DistilBERT removes token-type embeddings and the pooling layer, thereby reducing the number of parameters and model size.",
            "score": 0.5402383160659091,
            "section_title": "Feature Extraction and Classification",
            "char_start_offset": 14789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "58004701"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "250402891"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "247504387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "266162526",
            "title": "Survey on Foundation Models for Prognostics and Health Management in Industrial Cyber-Physical Systems",
            "text": "To address the problems of overfitting and poor generalization of language models, researchers have introduced pre-training techniques in NLP. Initial pre-training is first performed on large-scale text corpora and then fine-tuned on smaller datasets for specific downstream tasks, such as [22], Seq2Seq [23], and LSTM [24]. however, these models learn context-independent word embeddings that need to be retrained and re-tuned when applying to different downstream tasks. Pre-trained models (PTM) based on converter architectures are able to learn generic language representations, and PTMs can improve model initialization without the need to train the model from scratch. \n\nIn 2018, Google introduced BERT [10] with over 300 million model parameters. By fine-tuning only the output layer for a specific downstream task, BERT achieved SOTA performance on multiple NLP tasks. In the same year, OpenAI proposed GPT-1 [25], a generative pre-training model based on autoregressive models. In 2019, OpenAI released GPT-2 [26], which enhanced the model capacity and data diversity. In 2020, OpenAI released GPT-3 [8], a a groundbreaking model with 175 billion parameters, representing a major leap in model The launch of ChatGPT, an AI dialog system based on GPT-3.5 [27], aroused great public attention and interest in AI technology. The success of BERT and GPT triggered a boom in large-scale language modeling research, as shown in Fig. 3. In 2020, Google released the T5 [9], and the following year, Tsinghua University released the GLM [28]. In 2022, Google released the PaLM [29], which has a parameter size of more than a hundred billion, reaching 540 billion. in February 2023, Meta AI released LLaMA [30], which attracted a lot of attention from the research and industrial communities.The LLaMA model achieved excellent scores in various open benchmarks and became the most popular open source language model to date.",
            "score": 0.5393899690033974,
            "section_title": "C. Large-scale Foundation Models for NLP",
            "char_start_offset": 15465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "3488076"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1109,
                    "end": 1112,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1537,
                    "end": 1541,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.078369140625
        },
        {
            "corpus_id": "237385853",
            "title": "LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training",
            "text": "This approach, anterior to the development of BERT, yielded results that were at the time state-ofthe-art on multiple text classification benchmarks. While BERT-based models have later produced better results, it is notable that the ULMFiT approach uses only WikiText-103 (Merity et al., 2017), a subset of Wikipedia that contains just over 100 million words. This is a considerably smaller pre-training set than BERT, which uses a corpus of 3,300 million words (Devlin et al., 2019). Eisenschlos et al. (2019) further explore this approach and obtained then state-of-the-art performances on multiple multilingual classification tasks while using considerably less data and compute than BERT-based approaches. \n\nDespite the strong performance of models based on LSTM language models on text classification downstream tasks, the versatility and good results of BERT-based models have led to these learning approaches being under-studied for specialised domains. This is especially true for the legal domain, which we aim to address with this paper.",
            "score": 0.5390617553704882,
            "section_title": "Related Work",
            "char_start_offset": 5321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1047
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 293,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 462,
                    "end": 483,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 485,
                    "end": 510,
                    "matchedPaperCorpusId": "202558976"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1812744140625
        },
        {
            "corpus_id": "259129434",
            "title": "A Unified Generative Approach to Product Attribute-Value Identification",
            "text": "ing to median frequency and number of values. Tables 7 and 8 list micro and macro F 1 values of each approach for each category of attributes on the MAVE and our in-house datasets, respectively. From the table, we can see that T5 shows the best performance in all categories. This suggests that T5 is more robust than BERT-NER and BERT-MLC in the PAVI task. We can also observe that the performance of BERT-MLC drops significantly for attributes with a small number of training examples compared to those with a large number of training examples; the classification-based approach makes an effort to better classify more frequent attributes. Meanwhile, the performance drops of BERT-NER and T5 are more moderate than BERT-MLC, especially on the MAVE dataset. Moreover, we can see that T5 shows better micro F 1 for attributes that have a smaller number of distinct values on our in-house dataset, whereas it shows better micro F 1 for attributes that have a larger number of distinct values on the MAVE dataset. This implies that, although it is easy for the generation-based approaches to extract diverse values from text, it is still difficult to canonicalize those diverse values.",
            "score": 0.538531165135323,
            "section_title": "Results",
            "char_start_offset": 19689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1183
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09283447265625
        },
        {
            "corpus_id": "271329116",
            "title": "Conversational Rubert for Detecting Competitive Interruptions in ASR-Transcribed Dialogues",
            "text": "We fine-tuned BERT [33] for several reasons.First, despite advances in NLP, we found only one study that applied the large language model to classify interruptions [18].All three papers that utilized BERT (see Section 2) aimed to detect backchannels but not interruptions.As BERT demonstrated good performance on a similar problem, it was worth trying to adapt it to our task.Moreover, BERT is less resource-consuming than GPT-3.5 or GPT-4 [18], which is critical for potential industrial applications.\n\nThe proposed methods were implemented primarily based on the code developed in our previous work [34] since the text classification task in that study and the present study are structurally similar.In addition, the dataset was obtained from the same area and source, and it was transcribed using the same ASR system.As \"Conversational RuBERT\" by DeepPavlov [35,36] demonstrated high performance, we used it in the current study.We fine-tuned the model during five epochs with a batch size of 16, weight decay of 0.01, and maximum length in the tokenizer of 128, using the default optimizer and scheduler.We followed the same nine-fold crossvalidation process and tested the models on the 10th fold, applying standard classification metrics such as ROC AUC and F1.",
            "score": 0.5383944118751282,
            "section_title": "Model and Implementation",
            "char_start_offset": 9072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 502
                },
                {
                    "start": 504,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 820
                },
                {
                    "start": 820,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1267
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "271212209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09124755859375
        },
        {
            "corpus_id": "237605282",
            "title": "Named Entity Recognition and Classification in Historical Documents: A Survey",
            "text": "While Riedl et al. experimented with similar settings (character embeddings and pre-trained modern and historical fastText embeddings), it appears that combining Wikipedia and Common Crawl embeddings leads to better performances, even higher than the transfer learning setting of Riedl et al. using more labelled data. As a second experiment, Schweter et al. use pretrained LM embeddings: flair embeddings newly trained on two historical corpora having temporal overlaps with the test data, and two modern pre-trained BERT models (multilingual and German). On both historical test sets, in-domain LMs yield the best results (outperforming those of Riedl et al.), all the more so when the temporal overlap between embedding and task-specific training data is large. This demonstrates that the selection of the language model corpus plays an important role, and that unlabelled data close in time might have more impact than more (and difficult to obtain) labelled data. \n\nWith the objective of developing a versatile approach that performs decently on texts of different epochs without intense adaptation, Labusch et al. [108] experiment with BERT under different pre-training and fine-tuning settings. In a nutshell, they apply a model based on multilingual BERT embeddings, which is further pre-trained on large OCRed historical German unlabelled data (the Digital Collection of the Berlin State Library) and subsequently fine-tuned on several NE-labelled datasets (CoNLL-03, GermEval, and the German part of Europeana NER corpora). Tested across different contemporary/historical dataset pairs (similar to the all-corpora crossing of Riedl et al. [158]), it appears that additional in-domain pre-training is most of the time beneficial for historical pairs, while performances worsen on contemporary ones. The combination of several task-specific training datasets has positive yet less important impact than BERT pre-training, as already observed by Schweter et al. [172]. Overall, this work shows that an appropriately pre-trained BERT model delivers decent recognition performances in a variety of settings.",
            "score": 0.5383376652371437,
            "section_title": "6.3.4",
            "char_start_offset": 111333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1125,
                    "matchedPaperCorpusId": "208192606"
                },
                {
                    "start": 1649,
                    "end": 1654,
                    "matchedPaperCorpusId": "51868564"
                },
                {
                    "start": 1969,
                    "end": 1974,
                    "matchedPaperCorpusId": "189999820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3369140625
        },
        {
            "corpus_id": "52967399",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "text": "For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks: \n\n\u2022 Batch size: 16, 32 \n\n\u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5 \u2022 Number of epochs: 2, 3, 4 \n\nWe also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set. \n\nA.4 Comparison of BERT, ELMo ,and OpenAI GPT \n\nHere we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons between the model architectures are shown visually in Figure 3. Note that in addition to the architecture differences, BERT and OpenAI GPT are finetuning approaches, while ELMo is a feature-based approach. \n\nThe most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text corpus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained: \n\n\u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). \n\n\u2022 GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only introduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embeddings during pre-training.",
            "score": 0.5382983586005973,
            "section_title": "A.3 Fine-tuning Procedure",
            "char_start_offset": 32313,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1290283203125
        },
        {
            "corpus_id": "202539232",
            "title": "Abductive Reasoning as Self-Supervision for Common Sense Question Answering",
            "text": "We first evaluate our approach on the SWAG dataset and compare against fully supervised and weakly supervised. The results are presented in Table 1. We show the performance of fastText to highlight the importance of commonsense knowledge and the abductive reasoning process. Fast-Text models a given text as a bag of n-grams and predicts the probability of each ending being correct or incorrect. This approach is heavily reliant on word embeddings and does not generalize well to this task. It is also interesting to note that our approach outperforms all weakly supervised baselines such as the dual bag of words approach and the SNLI-based approaches. These approaches are the closest related to our approach since they are not trained directly on the SWAG training split. BERT currently has the best per- We also evaluate our approach on the tougher Hel-laSWAG dataset. The adversarial filtering technique on this dataset has been improved to increase the perplexity of BERT-like models on this task. The effect of this approach can be seen in Table 2. Again, we compare against the same baselines as in SWAG and find that our approach offers competitive performance to the fully supervised baselines. We find that using our abductive reasoning approach on BERT allows the model to retain up to 59% of its performance as compared to a model trained directly on the annotations. It should be noted that the use of ConceptNet-based contextualization on HellaSWAG has one significant drawback: the answers are too similar in their use of semantics, and hence the potential for indifference goes up. We observe that the number of examples where the second-best hypothesis had the same energy as the top hypothesis was 35% when the correct answer was in the top 2. This indifference could, arguably, be diminished by grounding the concepts in vision or other modalities. This is not an unreasonable assumption since the pre-training data used in BERT contains articles from Wikipedia which have vision-based explanations which constrain the semantics to those observed in vision. \n\nWe evaluate the performance of current QA systems un-der a Low Resource environment. We limit the amount of training data available to these models for a fair comparison with our approach.",
            "score": 0.5374415661421404,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 24877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1788330078125
        },
        {
            "corpus_id": "227126558",
            "title": "Studying Taxonomy Enrichment on Diachronic WordNet Versions",
            "text": "We test our methods on non-contextualised fastText (Bojanowski et al., 2017) and contextualised BERT (Devlin et al., 2019) embeddings. We choose fastText embeddings because pre-trained fastText models are easy to deploy, and do not require additional data or training for the out-of-vocabulary words. In this paper we use the fastText embeddings from the official website3 for both English and Russian, trained on Common Crawl from 2019 and Wikipedia CC including lexicon from the previous periods as well. \n\nWhile fastText embeddings can be generated for individual words, BERT requires a context for a word (i.e. a sentence containing it) to generate its embedding. For experiments with English datasets, we extract contexts from Wikipedia. For the experiments with Russian, we use a news corpus provided by the organisers of RUSSE'2020,4 which contains at least 50 occurrences for each word in the dataset. \n\nWe use the pre-trained BERT-base model for English from (Devlin et al., 2019). For Russian, we utilize RuBERT model from (Kuratov and Arkhipov, 2019), which proved to outperform the Multilingual BERT from the original paper. To compute BERT embeddings for orphans and synsets, we extract sentences containing them from the corresponding corpora. If the words are absent in the corpora, we computed the average of lemmas without context for synsets and the embedding of the input word without context. We also averaged word-pieces for the OOV words. We lemmatise corpora with UDPipe (Straka and Strakov\u00e1, 2017) to be able to find not only exact word matches but also their grammatical forms. We rely on UDPipe as it supports many languages and shows reasonable performance on our data. In case of multiple occurrences of the same orphan, we average the retrieved contextualised embeddings.",
            "score": 0.5373019624796873,
            "section_title": "Pre-trained Embeddings",
            "char_start_offset": 14357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 76,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 101,
                    "end": 122,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 968,
                    "end": 989,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1033,
                    "end": 1061,
                    "matchedPaperCorpusId": "158047013"
                },
                {
                    "start": 1494,
                    "end": 1521,
                    "matchedPaperCorpusId": "6205777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "258947576",
            "title": "Are We Really Making Much Progress in Text Classification? A Comparative Review",
            "text": "In-context learning has led to an increased interest in the design and working with prompts, such as Chain-of-Thought (CoT) prompting [131]. \n\nThe recent work by Sun et al. [108] evaluates GPT-3+CoT on text classification and introduces a new prompting strategy Clue and Reasoning Prompting (CARP). We report the numbers of these methods using a uniform sampler, i. e., where  = 16 examples per class are randomly sampled from the training data to be added to be prompt. We denote these models as GPT-3, GPT-3+CoT, and GPT-3+CARP with a 16-shot uniform sampler. The authors also propose another sampling technique that is based on a RoBERTa model fine-tuned for the current downstream task. Using RoBERTa representations of the training documents, the sampler employs a NN search on the examples to sample more representative documents per class to be included in the prompt. We denote this sampling technique as RoBERTa sampler. The best-performing CARP variant additionally employs a majority vote over multiple runs of prompting GPT-3, which we denote as GPT-3+CARP+vote (with a 16-shot RoBERTa sampler). \n\nIn the following, we focus on encoder-only models since they are most suitable for text classification. BERT relies on a masked language modeling objective and a next-sentence-prediction objective for pre-training. Popular follow-up works of BERT are RoBERTa [71], DistilBERT [100], ALBERT [58], DeBERTa [40], and ERNIE 2.0 [109]. RoBERTa improves the pre-training procedure of BERT and removes the next sentence prediction objective. DistilBERT [100] is a distilled version of BERT with 40% reduced parameters and 60% faster inference time that retains 97% of BERT's performance on the GLUE benchmark. ALBERT reduces the memory footprint and increases the training speed of BERT for improved scalability. Like DistilBERT and ALBERT, TinyBERT [47] and MobileBERT [111] are also size-reduced variants of BERT, but these two need the original BERT model for fine-tuning.",
            "score": 0.5365763167771465,
            "section_title": "Sequence-based Methods: Transformers",
            "char_start_offset": 17719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 143,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1978
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1893310546875
        },
        {
            "corpus_id": "268427977",
            "title": "Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks",
            "text": "The difference between GPT and BERT is that BERT processes text bidirectionally to capture context from both directions, while GPT is unidirectional and predicts the next word based on previous context.BERT is suited for natural language processing (NLP) tasks like language inference and question answering [7].Whereas GPT has an advantage over NLP task like text and dialogue generation [18] and summarization.There are studies that conclude the superiority of BERT in different text classification tasks.One study was from Yadav et al. ( 2023) [17] where they examine how people perceive smart farming technologies by conducting sentiment analysis on data extracted from YouTube using 3 transformer models: GPT-2, BERT specifically fine-tuned for the agricultural domain, and a BERT variant, DistilBERT.The study's findings and statistical evaluations suggest that transformer models are effective for categorizing both technical and agricultural texts.Notably, the domain-specific pre-trained BERT model for agriculture demonstrates superior performance compared to the other two models which includes GPT-2.The evaluation metrics for the classification were the macro-F1-score (a metric that balances precision and recall) and accuracy.They also imply that transformers are a viable option for classifying technical and agricultural text.Another comparative study between BERT and GPT was from Rehana et al. ( 2023) [11] where they assessed the ability of various GPT and BERT models to identify protein-protein interactions (PPIs) using a benchmark dataset containing 164 PPIs within 77 sentences from the Learning Language in Logic (LLL) dataset.Though their findings suggests that GPT models are effective in identifying PPIs from text with the GPT-4 exhibited comparable performance to the best BERT models, they also conclude that in general, BERT-based models outperformed the GPT models.Few similarities with Rehana et al.'s approach, this study would add another conclusion in text classification between BERT and GPT by expanding the application of BERT and GPT to in-game communication.This study will be using two models from BERT, the BERT (Base-uncased) and BERT (Large-uncased), and one from GPT-3, the model called 'davinci-002'.",
            "score": 0.5362660618314878,
            "section_title": "Related Studies",
            "char_start_offset": 8224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1899
                },
                {
                    "start": 1899,
                    "end": 2101
                },
                {
                    "start": 2101,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "245124484",
            "title": "Unraveling Social Perceptions & Behaviors towards Migrants on Twitter",
            "text": "We have considered different hyperparameters, such as multiple batch sizes (BS: 16 and 32) and dropout rates (DR: 0.4 and 0.5), for our CNN and Bi-LSTM models. The hidden layer for all these models was 128. We have considered SoftMax activation in our final classification layer to predict the final class. We have considered 'adam' as our optimizer for the modeling. Table 5 reports the model performances. Performances of CNN + fastText models are better than Bi-LSTM + fastText models, and the highest F1weighted Score for CNN is 0.72 (BS -32, DR -0.4).\n\nTransformer-based Neural Models: Next, we have considered two transformer-based models: BERT (Devlin et al. 2019) and RoBERTa  for the classification task. Transformer-based models work reasonably well for text classification tasks because transformers are pretrained on a diverse and large corpus. Core aspects of these models are their multi-head self-attention to extract the input tokens' semantic aspects for contextual representation with multiple layers. Unlike RNNs, these models can handle long-term dependency problems. BERT has successfully performed numerous NLP-related tasks -including the classification task. BERT is a bidirectional unsupervised pre-trained model. Devlin et al. (2019) have considered BooksCorpus and English Wikipedia (16GB) for the training purpose. BERT was introduced in 2018. However, within a year, BERT's performance was further improved by adding more training corpus and incorporating minor adaptations to the training process . This advanced version of BERT is known as RoBERTa. In addition to the pre-training corpus of BERT, RoBERTa also used an additional corpus from CC-News (76 GB), Open Web Text (38 GB), and Storie's dataset (31 GB) for training.\n\nPrior embedding approaches, such as word2vec (Mikolov et al. 2013) or GloVe (Pennington, Socher, and Manning 2014), have considered a single word embedding representation for each word without considering the context of that specific word. Therefore, these language representations failed to capture the context. In contrast, BERT considers the context of a particular word from both directions -both from",
            "score": 0.5349335140133147,
            "section_title": "Results:",
            "char_start_offset": 34845,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 651,
                    "end": 671,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1239,
                    "end": 1259,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.284912109375
        },
        {
            "corpus_id": "270210451",
            "title": "Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification",
            "text": "This is a Sparse Mixture of Experts (SMoE) language model with 56B parameters and an improved version based on Mistral 7B.In this model, a router network selects two experts for each token at each layer to process the current state and combine their results.The model fine-tuned to follow instructions, i.e.Mixtral 8x7B-Instruct, outperformed Gemini Pro and Llama 2 70B on all benchmarks evaluated [57].\u2022 Llama 3 8B and 70B .They represent a new SOTA in a wide range of industry benchmarks for LLMs, compared to the Mistral series and the Llama 2 series [56].We selected their instruction fine-tuned version.Fine-Tuned Models: To compare the performance of LLMs and previous fine-tuned models, we chose BERT and its improved version RoBERTa with few-shot and full-shot finetuning as a baseline.In particular, we use only text and label pairs for training them.\n\n\u2022 BERT.We choose the BERT base with 110 million parameters [58], which is widely used and has shown excellent performance on LEC tasks [3], [55].\n\n\u2022 RoBERTa.This is a robustly optimized BERT with 125 million parameters.RoBERTa has been shown to outperform BERT in several NLP benchmarks due to its optimized training regimen that includes longer sequences and more extensive data during pre-training [59].Many text classification studies regard it as a powerful baseline because of its excellent performance on various text classification tasks [35], [60].\n\nRandom: It is also included as a baseline for better comparisons, i.e. it assigns a random label to a sample from the label list as a prediction result.",
            "score": 0.5337058118186304,
            "section_title": "C. Model Selection",
            "char_start_offset": 22257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1007
                },
                {
                    "start": 1009,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1418
                },
                {
                    "start": 1420,
                    "end": 1572
                }
            ],
            "ref_mentions": [
                {
                    "start": 921,
                    "end": 925,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "246519729"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "256489533"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "258686184"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "261174949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11553955078125
        },
        {
            "corpus_id": "248428847",
            "title": "Effectiveness of Fine-tuned BERT Model in Classification of Helpful and Unhelpful Online Customer Reviews",
            "text": "the sequence length used to fine-tune and evaluate the BERT base model has a significant impact on classification performance.  The accuracy of bag-of-words based classifiers and fine-tuned BERT classifiers is compared in Figure 3. The comparison shows that fine-tuned BERT classifiers outperform bag-of-words-based classifiers. SVM has the highest accuracy and F1 score when compared to other bag-of-words based classifiers (k-NN and NB). SVM achieves slightly higher accuracy than BERT-64 (BERT classifier fine-tuned and evaluated using a sequence length of 64), while the F1 score of SVM is slightly lower than BERT-64. The BERT-320 classifier achieves the highest accuracy of 0.707, which is 0.03 (3%) higher than the accuracy of the SVM classifier, which is 0.679.\n\nWhen the prediction results of SVM and BERT-320 in Table 5 are closely examined, it is clear that the difference in the performance of both classifiers is due to TP and FN predictions. SVM predicted 679 TP and 321 FN, whereas BERT-320 predicted 743 TP and 257 FN. Only a small difference exists between SVM and BERT-320 TN and FN predictions. The good performance of BERT-320 can be attributed to BERT features as well as the length of the sequence of reviews used to fine-tune and evaluate the classifier. In contrast to bag-of-wordsbased features, which remove the majority of the words and do not take into account contextual placement of words, BERT features record bi-directional context of words and do not remove any stop words.\n\nAlthough SVM outperforms k-NN and NB and has comparable accuracy to BERT classifiers, it has been reported that SVM does not perform well on large datasets [59]. Bag-of-words based classifiers require mandatory textual data preprocessing before creating word vectors with TF-IDF to achieve better results. The other problem with bag-of-words based approaches is that it produces a huge list of features from which important features needed to be filtered using some automatic feature selection techniques. Like in this study, 18982 bag-ofwords based features are generated from which",
            "score": 0.5311763260969302,
            "section_title": "Results and Discussion",
            "char_start_offset": 32710,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1664,
                    "end": 1668,
                    "matchedPaperCorpusId": "20951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2216796875
        },
        {
            "corpus_id": "236342985",
            "title": "An Empirical Study on Software Defect Prediction Using CodeBERT Model",
            "text": "Language models can be roughly categorized into N-gram language models and neural language models. Classical neural language models include Word2Vec [26] and Glove [27], which are still popular in software defect prediction [4,5,7]. BERT [28] improves natural language pre-training by using mask-based objectives and a Transformer-based architecture, which has successfully improved many state-of-the-art results for various natural language tasks. It is one of the best pre-training language models for downstream tasks, considering that more powerful models (e.g., GPT3 [42]) are not open-source and not easily accessible. RoBERTa [43] is a replication of the BERT paper which follows BERT's structure and proposes an improved pre-training procedure. CodeBERT [36] follows the architecture of BERT and RoBERTa, i.e., the RoBERTa-large architecture. Unlike BERT and RoBERTa, which target natural languages, CodeBERT utilizes both natural languages and source codes as its input. \n\nThe overall architecture of BERT is shown in Figure 3. The core blocks in BERT are Transformers [44], which follow an encoder-decoder architecture. Based on the attention mechanism, Transformers could convert the distance of any two words in a sentence to 1, which mitigates the long-term dependency problems in natural language processing. The input corpora are first encoded into feature vectors via multi-head attention and fully connected layers. Then, the feature vectors are fed to the decoder, which includes masked multi-head attention, multi-head attention, and fully connected layers, and are finally converted to the conditional probabilities for prediction. Unlike the OpenAI GPT model, BERT uses a bidirectional Transformer that enables extracting context from both directions. \n\nAppl. Sci. 2021, 11, 4793 BERT's structure and proposes an improved pre-training procedure. CodeBERT lows the architecture of BERT and RoBERTa, i.e., the RoBERTa-large architecture BERT and RoBERTa, which target natural languages, CodeBERT utilizes both nat guages and source codes as its input. \n\nThe overall architecture of BERT is shown in Figure 3. The core blocks in B Transformers [44], which follow an encoder-decoder architecture.",
            "score": 0.5311271182494657,
            "section_title": "BERT and CodeBERT",
            "char_start_offset": 16985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "10769502"
                },
                {
                    "start": 227,
                    "end": 229,
                    "matchedPaperCorpusId": "4845285"
                },
                {
                    "start": 229,
                    "end": 231,
                    "matchedPaperCorpusId": "181705380"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147705078125
        },
        {
            "corpus_id": "218595822",
            "title": "WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge",
            "text": "Recently, pre-trained language representation models have achieved significant improvement on the WSC task. In this section, we evaluate the following three models: \n\n1. BERT (Devlin et al., 2019): As a powerful contextualized word representation model, it has been proven helpful in many downstream NLP tasks. As shown in (Kocijan et al., 2019), we can first convert the original WSC task into a token prediction task and then leverage BERT to solve the problem. We denote the base and large model of BERT as BERT (base) and BERT (large) respectively. \n\n2. GPT-2 (Radford et al., 2019): GPT-2 is one of the best pre-trained language models for generation tasks. As reported in the original paper, we can first replace the pronouns with different candidates and leverage the probability of the full or partial sentences to make the prediction. \n\nHere we evaluate the small (117 M parameters) and the large (774 M parameters) models and denote those settings as GPT-2 (small, full), GPT-2 (small, partial), GPT-2 (large, full), and GPT-2 (large, partial) respectively. \n\n3. RoBERTa (Liu et al., 2019): RoBERTa is a recent improved version of BERT with larger amount of training instances and techniques such as dynamic masking, which performs consistently better than BERT over many benchmark datasets. We denote the base and large models of RoBERTa as RoBERTa (base) and RoBERTa (large) respectively. \n\nBesides unsupervised models, as indicated by (Kocijan et al., 2019), fine-tuning BERT with a similar pronoun resolution dataset WSCR (Rahman and Ng, 2012) can help boost the performance. A later work (Sakaguchi et al., 2019) has further enhanced the performance by fine-tuning RoBERTa with a larger and more balanced dataset WinoGrande. \n\nStatistics of these datasets are presented in Table 3. In our experiments, we evaluate the combination of different pre-trained models and fine-tuning datasets, and denote them as BERT (base/large) + WSCR/Grande and RoBERTa (base/large) + WSCR/Grande respectively.",
            "score": 0.5302944837012076,
            "section_title": "Evaluated Methods and Implementation",
            "char_start_offset": 15815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 323,
                    "end": 345,
                    "matchedPaperCorpusId": "155091369"
                },
                {
                    "start": 564,
                    "end": 586,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1448,
                    "end": 1470,
                    "matchedPaperCorpusId": "155091369"
                },
                {
                    "start": 1536,
                    "end": 1557,
                    "matchedPaperCorpusId": "15274877"
                },
                {
                    "start": 1603,
                    "end": 1627,
                    "matchedPaperCorpusId": "199370376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1236572265625
        },
        {
            "corpus_id": "221949376",
            "title": "A little goes a long way: Improving toxic language classification despite data scarcity",
            "text": "We systematically compared eight augmentation techniques on four classifiers, ranging from shallow architectures to BERT (Devlin et al., 2019), a popular pre-trained Transformer network. We used downsampled variants of the Kaggle Toxic Comment Classification Challenge dataset (Jigsaw 2018; \u00a73) as our seed dataset. We focused on the threat class, but also replicated our results on another toxic class ( \u00a74.6). With some classifiers, we reached the same F1-score as when training on the original dataset, which is 20x larger. However, performance varied markedly between classifiers.\n\nWe obtained the highest overall results with BERT, increasing the F1-score up to 21% compared to training on seed data alone. However, augmentation using a fine-tuned GPT-2 ( \u00a73.2.4)a pre-trained Transformer language model (Radford et al., 2019) -reached almost BERT-level performance even with shallow classifiers. Combining multiple augmentation techniques, such as adding majority class sentences to minority class documents ( \u00a73.2.3) and replacing subwords with embedding-space neighbors (Heinzerling and Strube, 2018) ( \u00a73.2.2), improved performance on all classifiers. We discuss the interplay of performance and computational requirements like memory and run-time costs ( \u00a74.5). We release our source code. 1",
            "score": 0.5299347792499491,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1400146484375
        },
        {
            "corpus_id": "249363473",
            "title": "BERT Models for Arabic Text Classification: A Systematic Review",
            "text": "Text classification is a machine-learning task in which a document is assigned to one or more predefined categories according to its content. It is a fundamental task in natural language processing with diverse applications such as sentiment analysis, email routing, offensive language detection, spam filtering, and language identification [1]. Of these applications, sentiment analysis has attracted the most attention. The objective of sentiment analysis is to identify the polarity of text content, which can take the form of a binary positive/negative classification, or a more granular set of categories, such as a five-point satisfaction scale [2]. Despite the progress that has been achieved in improving the performance of text classification, there is much room for improvement, especially for the Arabic language. \n\nBidirectional Encoder Representations from Transformers (BERT) is a language representation model that was introduced in 2018 by Jacob Devlin and his colleagues from Google [3]. Since its introduction, it has become a ubiquitous baseline in natural languageprocessing research [4]. Unlike other language representation models that capture the context unidirectionally, BERT was designed as a bidirectional model that can predict words conditioned on both the left and right context [5]. BERT was also built as an unsupervised model that can be trained using the plain text corpus that is enormously available on the web in most languages. This combination of features allows BERT to demonstrate exceptional performance in various natural language-processing tasks, including text classification [6]. \n\nThere are two main approaches for using BERT: feature extraction and finetuning. In feature extraction, the architecture of the BERT model is preserved, i.e., the model's parameters are 'frozen'. Features are extracted from the pretrained BERT model and then fed into a classifier model to solve a given task. In finetuning, the model's parameters are finetuned by adding extra layers to the original BERT architecture. These new layers are used to train the model on the downstream tasks [3]. \n\nIn the original paper of Devlin, Chang, Lee and Toutanova [3], two BERT models were introduced: BERT-large and BERT-base. They are both in English. They were pretrained from extremely large corpora extracted from the internet.",
            "score": 0.5295603893523417,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2122
                },
                {
                    "start": 2125,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2272
                },
                {
                    "start": 2273,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "22617218"
                },
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "168581656"
                },
                {
                    "start": 1104,
                    "end": 1107,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 1309,
                    "end": 1312,
                    "matchedPaperCorpusId": "211040895"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "229390393"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223388671875
        },
        {
            "corpus_id": "276617859",
            "title": "Kanana: Compute-efficient Bilingual Language Models",
            "text": "After the cascaded filtering, we further apply language-specific model-based filtering on high quality documents (Su et al., 2024;Shao et al., 2024b;Li et al., 2024;Penedo et al., 2024a) separately on English and Korean. For English web documents, we utilize a DCLM (Li et al., 2024) classifier. \n\nFor Korean web documents, due to the lack of publicly available high quality classifiers, we iteratively train edu filter as high quality classifier using FastText (Joulin et al., 2017) based on the FineWeb-Edu pipeline (Penedo et al., 2024a). When applying the FineWeb-Edu pipeline, we observe that most of the documents are classified as uneducational, leading to a distribution imbalance. To address this issue, we iteratively retrain the classifier by augmenting educational documents from the previous iteration. \n\nTo assess the quality of our edu filter and Korean web corpus, we perform experiments by continual pre-training Llama 3 8B with 25B tokens. As shown in Table 2, the quality of our Korean web corpus is comparable to that of FineWeb 2 (Penedo et al., 2024b), which is the largest open-source Korean corpus. Furthermore, when using our edu filter to extract high quality data from Korean web corpus, we observe a significant performance improvement in the experimental results through training. Interestingly, we observe that using high quality English data, regardless of the quality of Korean data, can improve the scores on Korean benchmarks such as KMMLU and HAE-RAE, as well as the English benchmark MMLU. The results from this experiment make a foundation of our intuition for data mixture strategy in the staged pre-training in the following section. In summary, we share two insights to consider when building bilingual corpora with underrepresented language for enhanced computational efficiency. (1) Prioritize quality over quantity. For languages that do not have vast tokens available, such as Korean, prioritizing quality over quantity is an effective solution.",
            "score": 0.5286663599403869,
            "section_title": "Data",
            "char_start_offset": 7679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 130,
                    "matchedPaperCorpusId": "274446256"
                },
                {
                    "start": 149,
                    "end": 165,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "270560330"
                },
                {
                    "start": 462,
                    "end": 483,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1749267578125
        },
        {
            "corpus_id": "235421844",
            "title": "A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data",
            "text": "This work seeks to build on the BERT language models to further explore the effectiveness of taking sentence-level information into account for document classification with limited labelled data. Though BERT and its variants have achieved impressive results in many NLP tasks, including document classification in larger datasets [4,9], few studies are focused on the performance of BERT in low labelled data scenario over other existing NLP paradigms. As suggested in [6], supervised deep learning models will generally achieve acceptable performance at about 5,000 examples per class, and therefore, deep learning models may not be suitable for scenarios with low labelled examples. Interestingly, Usherwood and Smit [15] demonstrated that when one has only 100-1,000 labelled examples per class, BERT is the best performing approach for document classification as compared to other traditional machine learning approaches, due to the utility of pre-training and fine-tuning paradigm. However, their work is an empirical comparison of BERT and other NLP approaches without any modification in BERT. Also, BERT is incapable of processing long texts [5], the datasets used in [15] are Amazon reviews and Twitter that consist of short texts, lacking evidence of the utility of BERT-based models for long text classification with limited labelled data. Hence, our work proposes using a sentence-level language model to alleviate the long-texts problem suffered by BERT. \n\nYang et al. [18] were the first to propose the sentence-level model Hierarchical Attention Network (HAN) for document classification, and they adopted the Bi-GRU with attention as their architecture. There are also some concurrent works investigating the effectiveness of sentence-level model based on BERT. For example, Xu et al. [16] used a sentence-level BERT to a query matching task in Chinese (judge whether the two sentences convey the same meaning), Yang et al. [17] proposed a sentence-level BERT integrated with audio features to forecast the price of a financial asset over a certain period, and Zhang et al. [19] designed HIerachical Bidirectional Encoder Representations from Transformers (HIBERT) for document summarisation. Though all used sentence-level information, these works vary from the concrete design, pre-training/fine-tuning paradigm, the datasets used, the task performed and the domain.",
            "score": 0.5286148504216239,
            "section_title": "Related Work",
            "char_start_offset": 3510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 1150,
                    "end": 1153,
                    "matchedPaperCorpusId": "227276455"
                },
                {
                    "start": 1482,
                    "end": 1486,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1801,
                    "end": 1805,
                    "matchedPaperCorpusId": "203658223"
                },
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "215842684"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30615234375
        },
        {
            "corpus_id": "264172780",
            "title": "Revisiting Sentiment Analysis for Software Engineering in the Era of Large Language Models",
            "text": "\u2022 BERT (bert-base-uncased) [15], which stands for Bidirectional Encoder Representations from Transformers, introduces two key pre-training tasks. The first is mask language modeling (MLM), where BERT learns to predict masked words in a given text. Additionally, BERT incorporates the next sentence prediction (NSP) task, training to determine whether the second sentence logically follows the first or is a random sentence from the training data. \u2022 RoBERTa (roberta-base) [35] is short for \"A Robustly Optimized BERT Pretraining Approach\". RoBERTa is a BERT variant distinguished by its innovative training strategies and hyperparameter choices. Notably, it eliminates the NSP task, employs a larger batch size, trains on a larger corpus than BERT, and utilizes a dynamic masking strategy during training. \u2022 ALBERT (albert-base-v2) [29], or \"A Lite BERT\", is another BERT variant designed to reduce model size and computational requirements while maintaining or improving performance. ALBERT retains the MLM task but replaces the NSP task with the sentence order prediction (SOP) task. In SOP, ALBERT is trained to predict whether pairs of sentences are correctly ordered or if their positions have been swapped. \u2022 XLNet (xlnet-base-cased) [67] primarily focuses on capturing contextual information and long-range dependencies in text. It employs an autoregressive pretraining method and introduces permutation language modeling, where word order in a sentence is randomly shuffled, and the model is trained to predict the original sequence. XLNet also incorporates innovations such as the \"two-stream self-attention\" mechanism. \u2022 DistilBERT (distilbert-base-uncased) [51] is a distilled and smaller version of the BERT model. DistilBERT is designed to be faster and more memory-efficient. DistilBERT adopts model compression or knowledge distillation to learn from a teacher BERT to capture the same knowledge but with fewer parameters. As DistilBERT combines efficiency and strong performance, it has been popular in research and industry settings.",
            "score": 0.5283612180266399,
            "section_title": "Evaluated Language Models",
            "char_start_offset": 23916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1240,
                    "end": 1244,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145751953125
        },
        {
            "corpus_id": "233423520",
            "title": "Evaluating document representations for content-based legal literature recommendations",
            "text": "Regarding the word vector-based methods, we see that the methods which are trained on the legal corpus (Paragraph Vectors, fastText Legal , GloVe Legal ) perform similarly well with a minor advantage by fastText Legal . Moreover, there is a margin between the generic and legal word vectors even though the legal word vectors are trained on a small corpus compared to ones from the generic vectors. The advantage of Paragraph Vectors over TF-IDF is consistent with the results from Mandal et al. [28]. Limiting the document length to 512 or 4096 decreases the effectiveness of fastText Legal . A limit of 512 tokens decreases the MAP score to 59% compared to all tokens on Open Case Book. With 4096 tokens, the performance decline is only minor (90% compared to all tokens). The token limitation effect is also larger on Open Case Book than Wikisource. The 4096 tokens version of fastText Legal even outperforms all Transformer methods.\n\nLongformer-large is the best Transformer for Open Case Book with 0.031 MAP. For Wikisource, Legal-AUEB-BERT achieves the highest MAP of 0.022, closely followed by Legal-JHU-BERT. The Longformer's theoretical advantage of processing 4096 instead of 512 tokens does not lead to better results for Wikisource, for which even BERT scores the same MAP of 0.018. We generally observe that large models outperform their base counterparts 7 . Likewise, RoBERTa has higher scores than BERT as Liu et al. [27] suggested. From the Transformers category, Sentence Transformers yield the worst results. We assume that fine-tuning on the similarity datasets like NLI or STSB does not increase the performance since the models do not generalize well to other domains. However, the language model fine-tuning from Legal-JHU-BERT and Legal-AUEB-BERT  In the category of hybrid methods, the combination of text and citations improves the performance. For Open Case Book, the score summation Poincar\u00e9 + fastText Legal has the same MAP of 0.05 as fastText Legal but a higher MRR of 0.746. The MRR of Poincar\u00e9 + fastText Legal is even higher than the MRR of its sub-",
            "score": 0.5283251414316238,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 18331,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "44070470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11456298828125
        },
        {
            "corpus_id": "227231467",
            "title": "TTUI at SemEval-2020 Task 11: Propaganda Detection with Transfer Learning and Ensembles",
            "text": "In both subtasks, we utilized the transformer-based models BERT and RoBERTa as they have achieved successful performance on several natural language processing tasks recently. Because both models released multiple versions of pre-trained models trained on the different corpora, we examined all the versions to find the proper one for the dataset; The uncased version of BERT-base and BERT-large and RoBERTa-base and RoBERTa-large models were tested in this shared task. These models were fine-tuned on the training set with a learning rate of 2e-5 for 10 epochs. Then, we took the best epoch per model based on the F1 score on the development set. All models were trained on Nvidia P100 using the Tensorflow version of the Huggingface Transformers library (Wolf et al., 2019).\n\nAfter that, we applied an average ensemble on the results across the models. Empirically, bagging ensemble techniques allow better performance and minimize over-fitting (Opitz and Maclin, 1999). Most participants with top-ranked performance in many other machine learning competitions used this technique. We averaged the probability distributions of the classifiers and then took the maximum prediction over the average probability distribution. We adopt the best-performing ensemble models per each task on the development set for our final models on the test set.",
            "score": 0.5273191638190017,
            "section_title": "Fine Tuned Models",
            "char_start_offset": 2754,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "268093756"
                },
                {
                    "start": 948,
                    "end": 972,
                    "matchedPaperCorpusId": "2594813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1331787109375
        },
        {
            "corpus_id": "102350916",
            "title": "An Attentive Survey of Attention Models",
            "text": "XLNet combines BERT and Transformer-XL .\n\nAlthough Transformer has been used for several NLP tasks, Transformer-XL understands context beyond the fixed-length limitation of Transformer and can learn 450% longer dependency, critical to achieve better performance on both short and long sequences. BART is similar to Transformer in architecture but is trained to reconstruct the original text from corrupted input text with an arbitrary noising function. However, BERT uses bi-directional encoder such that it allows the model to consider the context from both the left and the right sides of each word. RoBERTa improves over BERT by using a larger dataset for training, training the model over more iterations, and removing the next sequence prediction training objective. ALBERT uses parameter reduction techniques such as factorized embedding parameterization, cross-layer parameter sharing to reduce the number of parameters by 18\u00d7 and faster training by 1.7\u00d7. Finally XLNet combines the capabilities of BERT and Transformer-XL to achieve state-of-the-art performance on 18 NLP tasks including question answering, natural language inference, sentiment analysis, and document ranking.\n\nOpenAI's GPT is an unsupervised language model trained on a giant collection of free text corpora. GPT specifically uses multi-layer decoder only Transformer as well as does not generate embeddings for usage in downstream NLP tasks but fine-tunes the base model itself. The successor to GPT and GPT-2, GPT-3 is similar to GPT-2, but it uses alternating dense and sparse attention patterns as in Sparse Transformer [Child et al. 2019]. This large scale transformer-based language model has been trained on 175 billion parameters and has shown strong performance for over two dozen NLP tasks. GPT-3 is the largest model so far, and its impressive capabilities have positioned it to outrank other pre-trained models.",
            "score": 0.5263350367409483,
            "section_title": "Natural Language Processing (NLP)",
            "char_start_offset": 48242,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.151123046875
        },
        {
            "corpus_id": "273186794",
            "title": "SkillMatch: Evaluating Self-supervised Learning of Skill Relatedness",
            "text": "The performance on SkillMatch for each method is shown in Table 2. Our contrastive learning objective for the Sentence-BERT model significantly outperforms all other models. This shows the effectiveness of modeling skill relatedness based on skill co-occurrence in job ads. The domain-specific static vectors outperform their generic versions, despite being trained on fewer data. Notably, the generic Word2Vec model performs much worse compared to the fastText model. Further analysis reveals that this is due to words not being modeled in the generic Word2Vec model. The number of skill phrases in SkillMatch without a Word2Vec representation drops from 193 to 36 when using the domain-specific model. fastText does not suffer from this out-of-vocabulary issue by design. Table 2. Comparison of model performance for the skill relatedness task. For the static vector models, the domain-specific versions refer to the models trained from scratch on job ads. For Sentence-BERT, the domain-specific model refers to its fine-tuned variant based on the proposed self-supervised skill co-occurrence objective. \n\nDespite its popularity, the pretrained Sentence-BERT model is outperformed by generic fastText vectors. We assume this is partly because Sentence-BERT models are trained to represent full sentences, which might harm their ability to meaningfully represent shorter phrases. As such, our fine-tuning strategy for Sentence-BERT effectively serves both purposes of learning domain-specific skill relations, as well as learning to effectively represent shorter phrases instead of full sentences. \n\nFinally, qualitative analysis of the skill relatedness scores allows inspecting the effect of our proposed Sentence-BERT fine-tuning approach. Fig. 1 displays one such example: a heatmap of similarity scores between two clusters of IT skills, being three web development related (HTML, CSS, JavaScript) and three machine learning related skills (ML, Deep Learning, NLP ).",
            "score": 0.5260548434528607,
            "section_title": "Results",
            "char_start_offset": 10096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32666015625
        },
        {
            "corpus_id": "247447069",
            "title": "SciNLI: A Corpus for Natural Language Inference on Scientific Text",
            "text": "We fine-tune four transformer based pre-trained language models: (a) BERT (Devlin et al., 2019) -pre-trained by masked language modeling (MLM) on Book-Corpus (Zhu et al., 2015) and Wikipedia; (b) SciB-ERT (Beltagy et al., 2019) -a variant of BERT pretrained with a similar procedure but exclusively on scientific text; (c) RoBERTa (Liu et al., 2019b)an extension of BERT which was pre-trained using dynamic masked language modeling, i.e., unlike BERT, different words were masked in each epoch during training. It was also trained for a longer period of time on a larger amount of text compared with BERT; and (d) XLNet (Yang et al., 2019)pre-trained with a \"Permutation Language Modeling\" objective instead of MLM. We employ the base variants of each of these models using the huggingface 5 transformers library. The input sequence for these models is derived by concatenating the two sentences in a pair with a [SEP] token in between. The [CLS] token is then projected with a weight matrix W \u2208 R d\u00d74 by sending it as the input to a softmax layer to get the output class. We finetune each transformer based model for 5 epochs where we minimize the cross-entropy loss using Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 2e \u2212 5. Early stopping with a patience size 2 is employed.\n\nThe experiments are run on a single Tesla V10 GPU. The transformer based models took approximately four hours to train and the traditional machine learning and neural network models were trained in less than one hour. We run each experiment three times with different random seeds and report the average and standard deviation of the F1 scores for each of the four classes, their Macro av-  erage and overall accuracy in Table 6. Our findings are discussed below.\n\nTransformer based models consistently outperform the traditional models The transformer based models have a very high performance gap with the traditional lexicalized and neural models. Their better performance can be attributed to their superior design for capturing the language semantics and their pre-training on large amounts of texts.\n\nMore sophisticated pre-training methods lead to better performance RoBERTa and XL",
            "score": 0.5252770608534916,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 24530,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 94,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 158,
                    "end": 176,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 205,
                    "end": 226,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 620,
                    "end": 638,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12158203125
        },
        {
            "corpus_id": "260438804",
            "title": "Supply chain emission estimation using large language models",
            "text": "We additionally experimented with fine-tuning encoder based Large Language Models (LLMs) for classifying transaction data into commodity classes. These models are pre-trained using masked language modeling (MLM) and next sentence prediction (NSP) objectives. We tried the following commonly used models available on Huggingface. \n\nbert-base-uncased. The BERT (Bidirectional Encoder Representations from Transformers) is a popular pre-trained model developed by [4]. BERT-base-uncased is uncased version of BERT model which is trained on a massive amount of textual data from books, articles, and websites. The training process involves a transformer-based architecture, which enables BERT to capture bidirectional dependencies between words and context. BERT-base-uncased consists of 12 transformer layers, each with a hidden size of 768, and has 110 million parameters. roberta-base. RoBERTa (Robustly Optimized BERT pretraining Approach) is a variant of the BERT (Bidirectional Encoder Representations from Transformers) model developed by [10]. Similar to BERT, the RoBERTa-base also utilises dynamic masking during pre-training where tokens are randomly masked out. However, RoBERTa-base extends this approach by training with larger batch sizes, more iterations, and data from a more diverse range of sources. It also utilises a larger vocabulary and removes the next sentence prediction objective used in BERT, focusing solely on the masked language modeling objective. It excels in understanding the context and semantics of text, and captures subtle relationships and finegrained nuances in language. It consists of 12 transformer layers, each with a hidden size of 768, and has 125 million parameters. \n\nclimatebert/distilroberta-base-climate-f. The ClimateBERT language model is additionally pre-trained, on top of DistilRoBERTa model, on text corpus comprising climate-related research paper abstracts, corporate and general news and reports from companies [25]. It has been shown improve the performance of various climate related downstream tasks [6]. The model has 6 transformer layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).",
            "score": 0.5250190957395267,
            "section_title": "Supervised fine-tuning for Scope3 (SFT3)",
            "char_start_offset": 16295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 328
                },
                {
                    "start": 331,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1710
                },
                {
                    "start": 1713,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "239768231"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "270551744",
            "title": "Application of Natural Language Processing and Genetic Algorithm to Fine-Tune Hyperparameters of Classifiers for Economic Activities Analysis",
            "text": "Its notable precision and F1-Score underscore its accuracy and suitability for multi-class categorization in analyses of legal text.\n\nLarge Language Models [70] (LLMs) excel in various AI and NLP tasks but can lead to issues like fake news if misused.Detecting AI-generated text is crucial for responsible LLM use.Abburi et al. [71] explores how to distinguish AI from human-generated text and attribute text to specific language models.They propose an ensemble neural model using the probabilities from pre-trained LLMs as features for a Traditional Machine Learning (TML) classifier.For English, their F 1 score is 0.733 for AI-generated text classification.Zhao et al. [72] introduce BERT-Augmented Prompt Engineering in Large Language Models (BAP-LLM), a novel method combining BERT's precision with Foundation Models' extensive training foundations.When applied to the New York Times news corpus, the BAP-LLM approach outperformed existing models, showcasing its potential for automated news categorization and paving the way for hybrid model research.The accuracies of BERT [73], GPT-4 [74], GPT-3 [75], and BAP-LLM are 0.74, 0.76, 0.62, and 0.79, respectively, against a baseline random classifier of 0.19.BAP-LLM surpasses all individual models across metrics, highlighting its efficacy in leveraging BERT's insights and the generative capabilities of Foundation Models.\n\nAs a neural network model based on transformers, BERT leverages deep learning and is pre-trained on a large corpus of texts.BERT generates context-dependent vector representations (embeddings) for words and sentences, which can be utilized in various NLP problems.\n\nTF-IDF is a simpler and computationally cheaper method that can be effective in basic text analysis tasks.It is a statistical technique for evaluating the importance of a word in a document within a collection of documents.It generates weight coefficients for words in a document, which are used to represent documents as fixed-length vectors.",
            "score": 0.5248612822375819,
            "section_title": "Discussion",
            "char_start_offset": 29746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 134,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1378
                },
                {
                    "start": 1380,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1644
                },
                {
                    "start": 1646,
                    "end": 1752
                },
                {
                    "start": 1752,
                    "end": 1869
                },
                {
                    "start": 1869,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "249248651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1790771484375
        },
        {
            "corpus_id": "263642216",
            "title": "On the Question of Authorship in Large Language Models (LLMs)",
            "text": "In 2017 (Vaswani et al.), the advent of a specialized type of neural network, called a Transformer, gave rise to a new era in language modeling. One of the first examples of a Large Language Model is BERT, a Transformer-based language model that advanced the state of the art on many common NLP benchmark tasks (Devlin et al. 2019). \n\nThe shift to Transformer-based language models marked an increase in both the size of models and the data used to train them. BERT has approximately 110 million parameters-which is relatively massive compared to its contemporaries-and a similarly large training corpus: English Wikipedia, which included 2.5 billion words in the version the authors used; and BookCorpus (Zhu et al. 2015), which contained around 800 million words pulled from approximately 11,000 unpublished books scraped from Smashwords. Following BERT was a flood of pre-trained language models, with the notable examples of ERNIE (Zhang et al. 2019), GPT-2 (Radford et al. 2019), XLNet (Yang et al. 2019), BART (Lewis et al. 2020), T5 (Raffel et al. 2020), andGPT-3 (Brown et al. 2020). \n\nThe creation of GPT-3 in 2020 marked the apex of increases to model size; it has a whopping 175 billion parameters, almost 1,600 times larger than BERT. GPT-3's gigantic scale came along with, of course, a gigantic training set, which includes English Wikipedia and BookCorpus along with the CommonCrawl dataset, which is a web crawl dataset consisting of the text from billions of web pages. Like BERT before it, GPT-3 showed impressive performance gains on a variety of NLP tasks. \n\nGPT-3 also marked the beginning of a new LLM paradigm. Previous LLMs were usually not directly applied to specific tasks of interest; instead, researchers would download a pre-trained language model like BERT and train its parameters further on a smaller set of task-specific dataa process known as fine-tuning. Because GPT-3 was released closed-source, its users could not simply download the model and train it further.",
            "score": 0.524148713673245,
            "section_title": "Scaling Up: The Birth of Large Language Models",
            "char_start_offset": 7226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 331,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 991,
                    "end": 1009,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1016,
                    "end": 1035,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1292724609375
        },
        {
            "corpus_id": "259859023",
            "title": "Text Augmentation Using Dataset Reconstruction for Low-Resource Classification",
            "text": "By generating new sentences for our classifier, we risk the creation of low-quality data. This can hap-pen if we set an incorrect pseudo-label or if the language model generates out-of-domain examples. Therefore, it is common to apply a consistency filtering mechanism (Anaby-Tavor et al., 2020;Wang et al., 2022). \n\nThe consistency filtering suggested by Anaby-Tavor et al. (2020) used the pre-trained classifier and considered the top-K sentences (ordered by their softmax scores). Wang et al. ( 2022) also used the trained classifier. However, instead of using the top-K approach, they kept all the generated sentences for which the classifier agrees with the pseudo-label. \n\nClearly, the chosen filtration method has a large effect on the final classifier, as it controls the data quality of the final trained classifier. The top-K approach might be too conservative, keeping a large safety margin, which results in filtering out most of the generated instances. On the other hand, keeping all the instances on which the classifier agrees with the pseudo-label might include many noisy-label sentences, resulting in a degraded classifier. \n\nWe now present Dynamic Consistency Filtering -our filtering approach for a case where an evaluation set exists. In Section 4.5, we discuss the no-evaluation case. Our method relies upon the evaluation dataset to approximate the optimal portion of the generated instances to include in the augmented dataset. We do so by training k classifiers, one of which trained on a different quantile of the generated instances, ordered by their softmax scores (received from the pre-trained classifier C base ). After training the k instances, we choose the best preforming classifier using the evaluation dataset. \n\nIt is important to note that there is a possibility of applying the filtering mechanism in a recursive manner, for example, training a classifier on the filtered data and running that classifier on the original",
            "score": 0.5241305889759378,
            "section_title": "Dynamic Consistency Filtering",
            "char_start_offset": 12310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 295,
                    "matchedPaperCorpusId": "212821571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282958984375
        },
        {
            "corpus_id": "258180358",
            "title": "An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT",
            "text": "With the advent of BERT [7] model based on Transformer architecture, an increasing number of studies related to Natural Language Processing (NLP) have incorporated pre-training + fine-tuning methodologies. The approach that first pre-training on a large amount of unlabeled data and then fine-tuning on a small portion of labeled data has proven to achieve more outstanding results. For example, before BERT, GPT-1 [27] with 117 million parameters has been initially trained using self-supervised pre-training + supervised fine-tuning. It directly used the Transformer decoder to achieve excellent results on natural language inference and question-and-answer tasks. Later, Google proposed the landmark model BERT, which introduced Transformer encoder and further improved the performance by using Mask Language Modeling and Next Sentence Prediction methods in the pre-training stage, where the number of parameters in BERT-Large has reached 340 million. Four months after the release of BERT, GPT-2 [8] was introduced, which further extended the model parameters and training data set based on GPT-1, with Extra Large of GPT-2 model reaching 1.5 billion parameters. In addition, the researchers [8] found that with the expanded training dataset, outstanding results of large language model could be achieved in downstream tasks without using fine-tuning. GPT-3 [28] further expanded the data size and parameter size based on GPT-2, and the maximum parameter reached 175 billion, and its performance on downstream tasks was significantly improved. And they first proposed a training paradigm of unsupervised pre-training + few-shot prompt. \n\nIn comparison to small Pre-trained Language Models (PLMs), LLMs possess superior generalization capability. They can accurately learn potential features of input text and perform effectively across different downstream tasks, even without fine-tuning. One prominent foundational model of a large language model is ChatGPT [14], based on the GPT-3.5 model, which employs training data in conversation mode to facilitate user-friendly human-machine interaction. ChatGPT has been widely integrated into various applications such as education and healthcare, and performs well in tasks such as text classification, data expansion, summarization, and other natural language processing [29]- [34].",
            "score": 0.5239936226082427,
            "section_title": "C. Large Language Model",
            "char_start_offset": 13535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1196,
                    "end": 1199,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145751953125
        },
        {
            "corpus_id": "246996849",
            "title": "Evaluating the construct validity of text embeddings with application to survey questions",
            "text": "In this paper, we investigate whether current text embedding techniques like fast-Text, GloVe, BERT and USE can produce valid representations for survey questions. We focus on pretrained embedding models (as opposed to fine-tuned models) because of their widespread use. However, our approach to construct validity analysis also applies to fine-tuned text embeddings and other vector representations of texts. \n\nTable 1 lists the pretrained embedding models we adopt. For fastText, we use the pretrained model developed by [3]. It is trained on Common Crawl with 600B tokens, and produces word embeddings with 300 dimensions for 2M words. For GloVe, we use the model pretrained on Common Crawl with 840B tokens and a vocabulary of 2.2M words. It also outputs word vectors of 300 dimensions. As for pre-trained Sentence-BERT models, there are many to choose from, which differ not only in the specific natural language tasks that they have been optimised for, but also in their model architecture. We select two pretrained models which have been trained on various data sources (e.g. Reddit, Wikipedia, Yahoo Answers; over 1B pair of sentences) and are thus designed as general purpose models [2]. They are \"All-DistilRoBERTa\" and \"All-MPNet-base\", where \"DistilRoBERTa\" [44,45] and \"MPNet\" [46] are two different extensions of the original BERT. \"Base\" indicates that the embedding dimension is 768, as opposed to \"Large\" where the dimension is 1024. Both \"All-DistilRoBERTa\" and \"All-MPNet-base\" have been shown to have the top average performance across various language tasks. For the purpose of comparison, we also include two pretrained models of the original BERT model [4]: \"BERT-base-uncased\" and \"BERT-large-uncased\". \"Uncased\" refers to BERT treating upper and lower cases equally. Both models have been trained on Wikipedia (2.5B words) and BookCorpus (800M words). \n\nAs for USE, we use the most recent (i.e.",
            "score": 0.5239497646769706,
            "section_title": "Pretrained Text Embedding Models",
            "char_start_offset": 19928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 1192,
                    "end": 1195,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1274,
                    "end": 1277,
                    "matchedPaperCorpusId": "203626972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "227230342",
            "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
            "text": "Out of all the models, deep learning BERT-based models were the best performing models, in terms of both F1 score and recall measures. The good performance can be attributed to the deep network architectures and large corpora used to train Transformer-based pre-trained language models (PMLs) such as BERT, which enable learning of rich text representations. Moreover, BERT fine-tuning performed better compared to the feature-based approaches, where FastText and BERT embeddings were used as input features to CNN and BiLSTM classifiers. Essentially, the PLMs end up learning universal language representations that are beneficial to downstream tasks.\n\nThe high precision and low recall noted in the machine learning models suggest that the models are unable to detect the relevant class well but are highly reliable when they do. This implies that while the Figure 1: Impact of data size on performance of the best performing model: BERT (multilingualuncased) fine-tuned.\n\nclassifiers returned reliable results, the machine learning models had a high false-negative rate, hence a few of all relevant results were returned. The approaches based on fine-tuned BERT uncased generally struck a good balance between precision and recall.\n\nVGCN+BERT performed particularly well for Polish, Chinese, and Russian. The model utilizes graph embeddings produced by integrating local information captured by BERT and global information from the vocabulary graph that is based on word co-occurrence information. Both the local and global information interact with each other through a self-attention mechanism during the learning process. The interaction introduces useful global information to BERT, which contributes to the improved results across all the languages, including the low-resource languages.\n\nWith regard to the contribution of various document segments on performance, it was observed from the results that, the beginning and the end of the text combined had the highest recall and F1 score. This was particularly the case for models based on BERT namely, VGCN+BERT and BERT fine-tuned models. This can be explained by the fact that the beginning paragraphs in an article often capture the most important information, which informs the reader what the story is about. On the other hand, the last part of the article tends to provide a summary of the article.\n\nThe performance of the model improved proportionately with training data size. This is in line with neural network models, which require large amounts of",
            "score": 0.5233765374945781,
            "section_title": "Discussion",
            "char_start_offset": 24532,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "259667515",
            "title": "Detecting Level of Depression from Social Media Posts for the Low-resource Bengali Language",
            "text": "In the pre-processing stage, we have removed various unwanted and noisy elements. At first, 118 duplicated instances are discarded. Then, HTML tags, URLs, punctuation marks, special characters, and digits are removed from the dataset. After that, we eliminated all other languages except Bengali. Lastly, we have performed stop-words removal and stemming [13] operations to the entire dataset. Throughout the empirical experiment, we have observed that stop-word removal and stemming operations degrade the overall performance of the classification system. In addition, retaining stop-words and not performing stemming leads to more variety of features [14], [15]. For that reason, these two pre-processing tasks are not considered in the final experiment. At this stage, we have extracted features from the preprocessed data. Here, we do this task separately for the three types of ML models, classical ML, DL, and transformer-based pre-trained models. For classical ML models, we have utilized character, word, and combinations of character-word N-gram techniques with TF-IDF scoring. Through the experimental observation, we have fixed the value of N as 3 to 5 for the character N-gram and 1 to 2 for the word N-gram. In the case of DL models, we have observed through empirical analysis that the fastText word embedding approach gives better output than the word2vec and GloVe methods. The main reason is that the fastText employs the character N-gram technique to get subword level features, which handle the unknown word vocabulary of the dataset [16], [17]. Here, we have used the max_len (the maximum number of tokens in each text) is 132 and the vector_size (the size of the feature vector) is 100 to maintain the same input dimension. In the case of transformerbased models, they use their own embedding techniques, where the BERT uses the WordPiece 4 technique and the XLM-R utilizes the Sentence Piece Model (SPM) [18] method to extract token-based sub-word level features. The SPM combines two sub-word segmentation methods: the uni-gram language model [19] and byte-pair-encoding (BPE) [20].",
            "score": 0.5233473011331538,
            "section_title": "Data Pre-processing",
            "char_start_offset": 11949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "235079410"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "234904792"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "231598077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24853515625
        },
        {
            "corpus_id": "270870818",
            "title": "EconNLI: Evaluating Large Language Models on Economics Reasoning",
            "text": "Our experiment includes a wide range of language models, from the encoder-only to the decoder-only architecture.Specifically, we examine the following LLMs:\n\nEncoder-only Language Models.For general domain LLMs, we consider (1) BERT (Devlin et al., 2019) and (2) RoBERTa (Liu et al., 2019): two popular language models for sequence classifications.Besides, we also evaluate the LLMs of the financial domain, including (1) FinBERT (Yang et al., 2020), a domain-specific model trained on financial text data.(4) FLANG-BERT and FLANG-ELECTRA (Shah et al., 2022) which use finance domainspecific pre-training with preferential masking to build more robust representations for the domain.\n\nDecoder-only Language Models.We also evaluate the performance of various popular decoderonly language models: (1) LLAMA2 (Touvron et al., 2023b) is a popular open-source LLM pretrained on extensive online data, and we use the LLAMA2-Chat version, which is optimized for dialogue use cases.(2) Alpaca (Taori et al., 2023) is fine-tuned from a LLAMA-7B model (Touvron Table 3: Classification results of the language models after supervised fine-tuning.\n\net al., 2023a) on instruction-following data generated by the the technique of Self-Instruct (Wang et al., 2023).( 3) FINMA (Xie et al., 2023) is a financial LLM based on fine-tuning LLAMA (Touvron et al., 2023a) with instruction data.( 4) Chat-GPT (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) are two advanced LLMs pre-trained on a wide array of textual data and reinforced by human feedback.",
            "score": 0.5230847469036256,
            "section_title": "Evaluated Language Models",
            "char_start_offset": 15639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 156
                },
                {
                    "start": 158,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 714
                },
                {
                    "start": 714,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1135
                },
                {
                    "start": 1137,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 558,
                    "matchedPaperCorpusId": "253244049"
                },
                {
                    "start": 1230,
                    "end": 1249,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11260986328125
        },
        {
            "corpus_id": "235422530",
            "title": "Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces",
            "text": "To achieve good coverage of profane language, we use 300-dimensional German FastText embeddings (Deriu et al., 2017) trained on 50M German tweets for the word-level experiments in Section 5. \n\nThe BERT models (Devlin et al., 2019) used in Section 6 are Bert-Base-German-Cased 4 and Bert-Base-Multilingual-Cased for the monolingual and multilingual experiments respectively, since they pose strong baselines. We fine-tune on the Twitter data (Section 4.1) using the masked language modeling objective and early stopping over the evaluation loss (\u03b4 = 0, patience = 3). All classification experiments use Linear Discriminant Analysis (LDA) as the classifier.",
            "score": 0.5229564220499741,
            "section_title": "Model Specifications",
            "char_start_offset": 12820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 655
                }
            ],
            "ref_mentions": [
                {
                    "start": 96,
                    "end": 116,
                    "matchedPaperCorpusId": "1562663"
                },
                {
                    "start": 209,
                    "end": 230,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1290283203125
        },
        {
            "corpus_id": "269614122",
            "title": "Detecting Anti-Semitic Hate Speech using Transformer-based Large Language Models",
            "text": "Since the invention of the transformer architecture [19], the attention mechanism has been increasingly used in the NLP field due to its good parallel computing capabilities.Also, a transformer-based model can be used to do text classification tasks.The advent of transformer architecture revolutionized NLP by enabling models to process words concerning all other words in a sentence, thus capturing the context more effectively than ever before [19].Building upon this, BERT (Bidirectional Encoder Representations from Transformers), developed by Devlin et al. (2019), represented a landmark in pre-trained models, allowing for an even more nuanced understanding of language through bidirectional training [1].\n\nFollowing the success of BERT, DistilBERT was introduced by Sanh et al. (2019) as a smaller, faster, and lighter version of BERT.DistilBERT optimizes BERT's architecture to maintain 97% of its language understanding capabilities while reducing the model size by 40% and speeding up training significantly [2].This makes it more accessible and practical for real-time applications.\n\nRoBERTa (Robustly Optimized BERT Approach), developed by Liu et al. (2019), extends BERT by modifying key hyperparameters, training with much larger minibatches and learning rates, and training on more data.RoBERTa removes BERT's next-sentence pretraining objective and dynamically changes the masking pattern applied to the training data.This has led to improved performance across a range of NLP tasks compared to BERT [3].\n\nAn interesting advancement in model adaptation is LoRA (Low-Rank Adaptation), which can be applied to models like RoBERTa.LoRA allows for efficiently adapting large pre-trained models with minimal additional parameters, preserving the original parameters while achieving comparable performance to full fine-tuning approaches [5].\n\nRecently, LLaMA-2 was introduced as an evolution of transformer models designed for generative and multitasking capabilities across multiple languages.LLaMA-2 benefits from a large-scale training corpus and has shown significant advancements in understanding and generating language tasks [4].When combined with LoRA, LLaMA-2 can be fine-tuned in a parameter-efficient manner, allowing for custom adaptations without extensive retraining of the model.",
            "score": 0.5226556637219862,
            "section_title": "Transformer Architecture",
            "char_start_offset": 7651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 712
                },
                {
                    "start": 714,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1094
                },
                {
                    "start": 1096,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1435
                },
                {
                    "start": 1435,
                    "end": 1521
                },
                {
                    "start": 1523,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1852
                },
                {
                    "start": 1854,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2147
                },
                {
                    "start": 2147,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2100830078125
        },
        {
            "corpus_id": "270440709",
            "title": "Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification",
            "text": "A growing literature evaluates and compares the performance of fine-tuned and prompt-based LLMs [5,6,8,10,41,15,18,47,46].However, some of this work does not specifically address text classification, which is the primary focus of our paper, but instead concentrates on various other text comprehension tasks [6,41].Other studies primarily compare the performance of different fine-tuning methods [10], or evaluate various zero-shot approaches against each other [47].\n\nThe existing studies that come closest to our work, as they compare fine-tuned and prompt-based models for text classification tasks, are [5] and [15] as well as [46] and [18].However, these papers provide conflicting evidence as to which approach is superior for text classification.\n\nFor example, [15] compare a fine-tuned RoBERTa model with prompt-based models like GPT-3.5 and Meta's Llama models across different classification tasks.The authors find that fine-tuned models (trained on the entire dataset) generally outperform prompt-based models.Similarly, [5] compare the performance of RoBERTa and GPT-3 in a study on an English-language dataset of parliamentary speeches, concluding that fine-tuning yields better results.\n\nIn contrast, recent work by [46] and [18] suggests that ChatGPT has caught up with or even surpassed fine-tuned models.[46] find that ChatGPT is on par with BERT-style models for some text understanding tasks, though results are mixed overall.[18] present results suggesting that ChatGPT even outperforms human annotators for text labeling tasks.\n\nInspired by these works, we aim to provide an up-to-date, systematic empirical analysis that offers a comprehensive view on the performance of fine-tuned versus prompt-based models for text classification.\n\nWhile our results clearly point in the same direction as the studies by [5] and [15], our paper expands on the existing literature in several ways:\n\n\u2022 Comprehensive Comparison: We systematically compare model performance across a diverse set of classification tasks and text categories.This allows us to identify performance variations across tasks and highlight areas where the differences between fine-tuned and prompt-based models are most (and least) pronounced.",
            "score": 0.5225506635140855,
            "section_title": "Related Work and Contribution",
            "char_start_offset": 5513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 467
                },
                {
                    "start": 469,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 753
                },
                {
                    "start": 755,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1200
                },
                {
                    "start": 1202,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1548
                },
                {
                    "start": 1550,
                    "end": 1755
                },
                {
                    "start": 1757,
                    "end": 1904
                },
                {
                    "start": 1906,
                    "end": 2043
                },
                {
                    "start": 2043,
                    "end": 2223
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 101,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 103,
                    "end": 106,
                    "matchedPaperCorpusId": "237507948"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "257766307"
                },
                {
                    "start": 308,
                    "end": 311,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "237507948"
                },
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "257766307"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "257766307"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "257766307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350341796875
        },
        {
            "corpus_id": "237482931",
            "title": "Query-driven Segment Selection for Ranking Long Documents",
            "text": "Compared models. We implemented three baselines models: BERT (First P), BERT (Max P), and BERT (Gold P). Both BERT (First P) and BERT (Max P) are trained by taking the first segment of each document. At inference time, BERT (First P) only uses the first passage while BERT (Max P) uses the maximum score of all passages. BERT (Gold P) fine-tunes BERT by using passages of documents that are relevant to the query according to the MS MARCO passage ranking dataset. This is possible because the TREC 2019 DL dataset for document ranking is built based on the MS MARCO passage ranking dataset, thus the relevant passages of documents can be obtained. After aligning relevant passages to documents dataset by query IDs and contents, 58% of relevant passages are used from MS MARCO passage ranking. We created this baseline because the goal of BeST is to use the most relevant segments of documents. Thus, BERT (Gold P) is an oracle that provides an upper bound to the performance based on ground-truth data.\n\nWe also report the results of IDST which is one of the best performing runs in the original TREC 2019 Deep Leaning TRACK [2]. There are other runs which have better performance than IDST, but they are inappropriate for comparison because they either use full-ranking instead of re-ranking, use larger pre-trained models, or use ensemble models. Still, IDST has a few other factors that contribute to its high performance, such as additional language model pre-training and using the entire passage-level labels as training data [17]. We also consider existing models that are especially designed to handle long inputs. QDS-Transformer is a ranker with local and hierarchical attention which can efficiently model long sequences [5]. RoBERTa (Max) and Longerformer-QA were used as the baselines for QDS-Transformer. TKL processes long sequences by local self-attention mechanism [4]. Unlike other models, TKL uses Glove embeddings instead of pre-trained language models. Unless mentioned otherwise, all the methods use a fixed number of leading tokens (which can vary from 512 to 4000 depending on models) for the document representation in the training.\n\nImplementation details. For TREC 2019 DL, we used pairwise hi",
            "score": 0.5219539389705898,
            "section_title": "EXPERIMENT DESIGN",
            "char_start_offset": 11201,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1533,
                    "end": 1537,
                    "matchedPaperCorpusId": "211481850"
                },
                {
                    "start": 1733,
                    "end": 1736,
                    "matchedPaperCorpusId": "225067971"
                },
                {
                    "start": 1883,
                    "end": 1886,
                    "matchedPaperCorpusId": "218581396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1309814453125
        },
        {
            "corpus_id": "227231089",
            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
            "text": "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline. 4 BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments. For future work it would be interesting to further delve into the role of unlabeled data in text classification, both in terms of word embeddings (e.g., by making use of meta-embeddings (Yin and Sch\u00fctze, 2016)) and the data used to train language models (Gururangan et al., 2020). Moreover, this quantitative analysis could be extended to more classification tasks and different models, e.g., larger language models such as RoBERTa (Liu et al., 2019) and GPT-3 (Brown et al., 2020), which appear to be more suited to few-shot experiments. However, the generic domain embeddings tend to fail to represent the meaning of more domainspecific words, which may explain their lower performance. This is confirmed by the nearest neighbour analysis (see Table 5) which showed that the generic domain embeddings do not provide accurate representations of more technical words such as 'Windows' and 'Sun'. In the IMDB reviews, words such as 'Toothless', used within a very specific context are also not correctly represented by the generic model. Moreover, tweets are rich with abbreviations which have domain-specific meaning such as 'SF' referring to 'San Francisco'.",
            "score": 0.5219114823124157,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 14243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 929,
                    "end": 952,
                    "matchedPaperCorpusId": "2820782"
                },
                {
                    "start": 997,
                    "end": 1022,
                    "matchedPaperCorpusId": "216080466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "256593479",
            "title": "Empirical Study: How Issue Classification Influences Software Defect Prediction",
            "text": "Earlier work in the area of Natuaral language processing (NLP) focused on word2vec models [74], convolutional models [75], [76], [77] recurrent models [78], [79] and, more recently, attention-based models [80], [81]. Substantial earlier work has shown that pre-trained models on large corpora are beneficial for text classification and other NLP tasks [82]. Using pre-trained models offers the benefit of avoiding model training from scratch, thus, speeding up the fine-tuning process and producing higher performance models than those trained only on one specific task. \n\nFastText [73] is a word embedding method which is an extension of the word2vec model. It is considered a bag of words model. Instead of learning each word directly it represents each word as a set on n-grams. In its original publication it was shown to be much faster than the deep models of that time, with comparative performance. \n\nThe current widely adopted language models are BERT [83] and RoBERTa [84]. Both models offer the same architectural design, using the encoder part of the multi-layer bidirectional Transformer architecture [85] which was pre-trained on large text corpora -the BooksCorpus (800M words) [86] and English Wikipedia (2,500M words). The Cloze task [87] inspired the masked language model (MLM) objective used to train BERT and RoBERTa in conjunction with the next sentence prediction task. The method of using a large pre-trained model and fine-tuning in downstream tasks has made a breakthrough in several natural language understanding tasks [83], [84]. \n\nResearchers [11], [12], [13], [14] have looked into issue classification with various motivations. \n\nWang et al. [11] used BERT to recommend GitHub labels based on issue descriptions. Since issue creators often do not label issues, it is left to repository maintainers to label them which can become very time consuming, thus accurate automatic labeling would help reduce the amount of necessary manual labor. \n\nHerbold et al. [12] stress that reported issue types often do not match the description of the issue.",
            "score": 0.5213848122862824,
            "section_title": "D. NATURAL LANGUAGE PROCESSING AND ISSUE CLASSIFICATION",
            "char_start_offset": 18608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1969
                },
                {
                    "start": 1972,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "368182"
                },
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "1240989"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "206666846"
                },
                {
                    "start": 1572,
                    "end": 1576,
                    "matchedPaperCorpusId": "239633540"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "233224674"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "250182232"
                },
                {
                    "start": 1673,
                    "end": 1677,
                    "matchedPaperCorpusId": "239633540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38671875
        },
        {
            "corpus_id": "235755489",
            "title": "A Survey on Data Augmentation for Text Classification",
            "text": "Finally, a classifier determines which generated instances can actually be assigned to the class stated. The authors manage to achieve significant improvements in the classification of sentences. They show that their method outperforms conditional VAEs (unfortunately no sampling technique is described) and even EDA (Section 3.1.1.1) and c-BERT (Section 3.1.2.4) when applied to a severe low data regime. The results of their LAMBADA approach and CVEA implementation are given in Table 6. Abonizio and Junior [109] try to improve this approach by concatenating three random samples as a prompt for the generation. Furthermore, they are using DistilGPT2 by Sanh et al. [113], which is substantially faster and smaller. As can be seen in Table 6, the method consistently outperforms the baseline. While LAMBADA and PREDATOR are only applicable to short texts as instances, Bayer et al. [46] design a GPT-2 based approach to augment short as well as long text tasks. In this way, very high label preservation and diversity is to be achieved by fine-tuning the language model on the class specific data, generating data prompted with specialized training data tokens, and a filtering method based on document embeddings. They can achieve high improvements for constructed and real-world low data regimes. However, they also discuss limitations of their method and useful applications in terms of specific datasets and tasks. The results can also be seen in Table 6. Similarly, Claveau et al. [110] fine-tune the GPT-2 model using the class-specific data and input a random word from the original texts for generation. Afterwards a classifier is applied to filter the generated data instances. They evaluate their approach using English and French datasets (see Table 6). Liu et al. [111] use a reinforcement learning component after the softmax prediction of the GPT-2 model to predict the tokens depending on the class for which the instance is to be generated. The authors tested their method with various model architectures. It consistently improved all of them in all tasks, especially the larger pre-trained models, like BERT and XLNet. The results for XLNet are shown in Table 6.",
            "score": 0.5204217282443702,
            "section_title": "Interpolation",
            "char_start_offset": 41047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 510,
                    "end": 515,
                    "matchedPaperCorpusId": "224771732"
                },
                {
                    "start": 885,
                    "end": 889,
                    "matchedPaperCorpusId": "232379879"
                },
                {
                    "start": 1489,
                    "end": 1494,
                    "matchedPaperCorpusId": "250164113"
                },
                {
                    "start": 1779,
                    "end": 1784,
                    "matchedPaperCorpusId": "226262374"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "201698093",
            "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification",
            "text": "We use the latest public multilingual BERT base model with 12 layers 2 and apply the default finetuning strategy with batch size 32 and learning rate 1e-5. For BOW and ESIM, we use our own implementations and 300 dimensional multilingual word embeddings from fastText. 3 We allow finetuning word embeddings during training, which gives better empirical performance.\n\nWe use two metrics: classification accuracy and area-under-curve scores of precision-recall curves (AUC-PR). For BERT, probability scores for the positive class is used to compute AUC-PR. For BOW and ESIM a cosine threshold of 0.5 is used to compute accuracy. In all experiments, the best   model checkpoint is chosen based on accuracy on development sets and report results on testing sets.\n\nResults Table 4 shows the performance of all methods and languages. Table 5 summarizes the average results for the six non-English languages.\n\nModel Comparisons: On both Translate Train and Translate Test, BERT consistently outperforms both BOW and ESIM by a substantial margin (>15% absolute accuracy gains) across all seven languages. BERT Translate Train achieves an average 20% accuracy gain. This result demonstrates that PAWS-X effectively measures models' sensitivity to word order and syntactic structure.\n\nTraining/Evaluation Strategies: As Table 4 and 5 show, the Zero Shot strategy yields the lowest performance compared to other strategies on BERT. This is evidence that machine-translated data helps in the multilingual scenario. Indeed, when training on machine-translated examples in all languages (Merged), the model achieves the best performance, with 8.6% accuracy and 7.1% AUC-PR average gains over Zero Shot.\n\nBERT  gual BERT is pre-trained on over one hundred languages; hence BERT provides better initialization for non-English languages than ESIM (which relies on fastText embeddings). The gap between training on English and on other languages is therefore smaller on BERT than on ESIM, which makes Translate Train work better on BERT.\n\nLanguage Difference: Across all models and approaches, performance on Indo-European languages (German, French, Spanish) is consistently better than CJK (Chinese, Japanese, Korean). The performance difference is particularly noticeable on Zero Shot. This can be explained from two perspectives",
            "score": 0.5203619374140772,
            "section_title": "Experiments and Results",
            "char_start_offset": 7924,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "249562531",
            "title": "Multimodal Classification of Safety-Report Observations",
            "text": "We used two large pretrained transformer-based text encoders in our experiments: \n\nGreek BERT Greek BERT (https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1, accessed on May 31 2022) was proposed in [19]. The Greek BERT architecture is based on the 12-layer, 768 hidden size version of BERT [39], i.e., bert-base-uncased. For model training, the authors used 29GB of text from the Greek versions of Wikipedia, European Parliament Proceedings Parallel Corpus (Europarl) [92] and OSCAR, which is a filtered, deduplicated version of Common Crawl. Since OSCAR contributed the vast majority of the training data and is based on the very noisy Common Crawl corpus, we find it useful to review the preprocessing steps. The first step of OSCAR preprocessing involves the splitting of pages across languages using a fastText linear classifier [93,94]. The second step of preprocessing involves rejecting lines shorter than 100 UTF-8 encoded characters and deduplicating the text lines. Additionally, the authors of [19] removed accents and diacritics and converted all lines to lower-case. \n\nThe model was trained using the masked language model (MLM) and next sentence prediction (NSP) losses with learning rate of 10 \u22124 . Training took 5 days on a Google Cloud TPU v3-8.",
            "score": 0.5196084764600497,
            "section_title": "Text Encoders",
            "char_start_offset": 42752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 83,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1270
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "221340533"
                },
                {
                    "start": 298,
                    "end": 302,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 845,
                    "end": 848,
                    "matchedPaperCorpusId": "3411445"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "221340533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1380615234375
        },
        {
            "corpus_id": "265658802",
            "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
            "text": "This section provides additional details on the large language models (LLMs) audited in our experiments. \n\nBERT base We use the BERT base model (uncased) [16] provided by HuggingFace8 , which is a pre-trained transformer model designed for the English language, and the corresponding Tokenizer. BERT is trained via self-supervised training on unlabeled raw texts, utilizing two objectives: masked language modeling and next-sentence prediction. Its training data includes a large corpus of English texts, and it is important to note that the model can have biased predictions. The model has been primarily designed for fine-tuning downstream tasks such as sequence classification and token classification. \n\nBERT is constructed as a transformer-based model, which employs an encoder architecture with a self-attention mechanism that allows it to capture contextual information from both preceding and following words in a sentence, creating a bidirectional understanding. The BERT base model encoder has 12 layers, and 12 attention heads per layer. The embedded space of the model has a dimension of 768. In BERT, the [CLS] token is a special token added to the beginning of each input sequence. The final hidden state of the [CLS] token is often used as a summary representation of the entire input sequence for downstream classification tasks. Our analysis focuses on the hidden representations of the [CLS] token, as it is commonly used for downstream classification tasks [28,47]. \n\nOPT We use the Open Pre-trained Transformer (OPT) language model [56] (6.7B) provided by HuggingFace 9 . It has been specifically trained to align with the performance and sizes of the GPT-3 models employing causal language modeling (CLM) objectives. The training data for OPT comprises a fusion of filtered datasets sourced from various origins, such as BookCorpus, CC-Stories, The Pile, Reddit, and CCNewsV2, which is comprised mainly of English text, with a small quantity of non-English data. OPT serves multiple purposes, including text generation and evaluation via prompts, and can be fine-tuned for specific tasks. The training of the pre-trained model spanned approximately 33 days and harnessed the power of multiple GPUs.",
            "score": 0.5182848020618793,
            "section_title": "B LLM Models",
            "char_start_offset": 26402,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "235422673"
                },
                {
                    "start": 1480,
                    "end": 1483,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "220665885",
            "title": "CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE",
            "text": "Language models : Over the years, a great amount of effort has been directed towards creating benchmark datasets that can measure a system's performance on language processing tasks and provide an impetus for the development of new approaches to the tasks. These benchmark tasks have led to many computational models ranging from earlier symbolic and statistical approaches to recent approaches based on deep neural networks; which model context of language, take advantage of external data or knowledge resources, and achieve the state-of-the-art performance, and at times, even near or above human performance. A major landmark in NLP is the development of pre-trained models and embeddings that can be used as features or further fine-tuned for downstream tasks. These models are often trained based on large corpora of textual data to capture different word senses. The defining contribution of Embeddings from Language Models (ELMo) (Peters et al., 2018) is its contextual word embeddings, which are built relying on the entire input sentence that they belong to. The recent Bidirectional Encoder Representations from Transformers (BERT) model outperforms previous competitive approaches by better capturing the context. A more recent model, XLNET (Yang et al., 2019), exceeded the performance of the vanilla BERT variant on several benchmarks. Robustly Optimized BERT Approach (RoBERTa) (Liu et al., 2019) achieved further improvement by making changes to the pre-training approach used in BERT. It includes randomizing masked tokens in the cloze pre-training task for each epoch instead of keeping them the same over epochs. It also augments the next sentence prediction pre-training task with an additional task which compels the model to also predict whether a candidate next sentence comes from the same document or not. A Lite BERT (ALBERT) (Lan et al., 2019) implements several novel parameter reduction techniques to increase the training speed and efficiency of BERT, enabling a much deeper scale-up than the original large variant of BERT while having fewer parameters. ELECTRA (Clark et al., 2020) is used to pre-train transformers with comparatively less computation. This model is similar to the discriminator of a GAN.\n\nCommon sense validation : TriAN (Wang et al., 2018) achieved state-of-the-art performance",
            "score": 0.5180383685036936,
            "section_title": "Related Work",
            "char_start_offset": 3133,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142578125
        },
        {
            "corpus_id": "222359862",
            "title": "Impact Assessment of Food Safety News Using Stacking Ensemble Learning",
            "text": "We carried out comparative experiments to verify the advantage of the proposed method versus existing methods. We first compared the ensemble classifier with the seven base classifiers, and then compared it with four classic classifiers, saying, the original version of NB, SVM, CNN and LSTM without using TF-IDF weights or pretrained word vectors. The result of the comparison is shown in Table 2. From the results in Table 2 we can see that the proposed ensemble classifier achieves the highest scores regarding all the performance indexes. The second runner is FastText, which scores 0.35% lower than the ensemble learner in terms of accuracy. LSTM with Word2Vec embedding performs similarly with FastText, but the version of LSTM with random embedding performs much poorer. By comparing NB, SVM, CNN and LSTM between their classic versions and base classifier versions, it can be seen that using information and knowledge extracted from the corpus can improve the performance of the classifiers. For BERT, which uses the most prior knowledge and represents the state-of-the-art of natural language understanding, it scores below the ensemble classifier, FastText, LSTM(base classifier version) and SVM(base classifier version), but scores above all the other classifiers. This is probably due to the shortened input of BERT.",
            "score": 0.5178861331645777,
            "section_title": "Comparative study",
            "char_start_offset": 14770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36572265625
        },
        {
            "corpus_id": "271874495",
            "title": "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws",
            "text": "High-quality data is crucial for the pre-training performance of large language models. Unfortunately, existing quality filtering methods rely on a known high-quality dataset as reference, which can introduce potential bias and compromise diversity. In this paper, we propose ScalingFilter, a novel approach that evaluates text quality based on the perplexity difference between two language models trained on the same data, thereby eliminating the influence of the reference dataset in the filtering process. An theoretical analysis shows that ScalingFilter is equivalent to an inverse utilization of scaling laws. Through training models with 1.3B parameters on the same data source processed by various quality filters, we find ScalingFilter can improve zero-shot performance of pre-trained models in downstream tasks. To assess the bias introduced by quality filtering, we introduce semantic diversity, a metric of utilizing text embedding models for semantic representations. Extensive experiments reveal that semantic diversity is a reliable indicator of dataset diversity, and ScalingFilter achieves an optimal balance between downstream performance and semantic diversity.",
            "score": 0.5172689297437935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.156982421875
        },
        {
            "corpus_id": "267233307",
            "title": "Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models",
            "text": "To evaluate the utility and effectiveness of synthetic datasets, we fine-tuned pre-trained transformer-based language models, ALBERT and DistilBERT, to train classifiers with each set of the generated synthetic data. We compared the trained 8 https://huggingface.co/google/flan-t5-xxl classifiers with classifiers with similar structures fine-tuned with real-world data as the benchmark model. \n\nALBERT and DistilBERT are two pre-trained language models from the BERT family of LMs. The BERT model was initially proposed by Delvin et al. [62] as a bidirectional language model pretrained on a large corpus comprising the Toronto Book Corpus and Wikipedia. The model is named bidirectional because it can simultaneously gather the context of a word from either direction. Unlike the generative models such as ChatGPT, FlanT5 or Llama, which include a decoder structure, the BERT family of language models are encoder models and can be fine-tuned for specific tasks such as classification tasks. \n\nThe ALBERT model was proposed by Lan et al. [63] to reduce memory consumption and increase the training speed compared to BERT. In other words, ALBERT is a more lightweight version of BERT that maintains its high level of accuracy, making it a powerful tool for various NLP applications. The DistilBERT model was proposed by Sanh et al. [64]. The authors reported it has 40% fewer parameters than BERT and runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark. Both models are designed as lightweight alternatives to BERT, with ALBERT emphasizing parameter efficiency and DistilBERT focusing on knowledge transfer through distillation. Overall, ALBERT, with a smaller number of parameters, shows more efficient performance compared to DistilBERT. \n\nTo fine-tune these models, we utilized the Huggingface library [65]. The Huggingface is an open-source library and data science platform that provides tools to build, train and deploy ML models. We compare our classification results with baseline ALBERT 9 and DistilBERT 10 models finetuned on the UMD dataset by Ghanadian et al. [36]. We used the Trainer 11 class from Huggingface transformers 12 for feature-complete training in PyTorch.",
            "score": 0.5163020850627562,
            "section_title": "C. EVALUATION OF SYNTHETIC DATASET",
            "char_start_offset": 30864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158203125
        },
        {
            "corpus_id": "265445584",
            "title": "CMed-GPT: Prompt Tuning for Entity-Aware Chinese Medical Dialogue Generation",
            "text": "For recent research on dialogue generation, pre-training on large-scale datasets and fine-tuning on downstream tasks have proved to be a successful paradigm and have become the standard models. Two of the typical models are BERT and GPT. \n\nBERT is a bidirectional encoder representation language model based on the transformer architecture.In subsequent studies, the improvement points of BERT are mainly focused on two parts: the construction of large-scale corpus data [1] and model pre-training [12]. In the field of biomedicine, scholars have proposed several approaches to enhance the performance of BERT models by pre-training them with English medical domain datasets. These approaches include the following three ideas: 1) Continued pre-training using medical domain data, such as BioBERT [9] and DilBERT [11]; 2) Pre-training from scratch using medical domain data, such as SciBERT [10] and PubMedBERT [8]; 3) Pre-training based on self-supervised tasks specifically designed for the medical domain, such as MC-BERT [13], SMedBERT [14] and BERT-MK [15]. While various biomedical pre-trained language models based on BERT have achieved great success in natural language understanding and classification tasks, few scholars have worked on the task of generating BERT models due to its Transformer encoder model architecture that limits NLG. \n\nGPT is an autoregressive language model based on Transformer's decoder. Compared to the BERT, GPT has significantly larger training corpora and model parameters such as GPT-2 [16] and GPT-3 [17]. The modeling structure of GPT makes it outstanding for generative tasks. With the emergence of ChatGPT, a new fine-tuning paradigm based on pre-trained language models, p-tuning, has become a major mainstream approach to improve the performance of GPT on downstream tasks. For domain-specific generative tasks, the inclusion of prompt allows the model to generate text that is more consistent with the user's intent. In the field of biomedicine, while GPT is well-suited for medical dialogue generation, there are very few pretrained language models based on GPT for medical text generation.",
            "score": 0.5161863667324106,
            "section_title": "Related Work",
            "char_start_offset": 4391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "244794916"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 1040,
                    "end": 1044,
                    "matchedPaperCorpusId": "236460015"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1553955078125
        },
        {
            "corpus_id": "264935645",
            "title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model",
            "text": "To further enhance data processing efficiency and reduce hardware resource requirements, in this paper we also develop a text evaluation model based on FastText3 in addition to BERTEval. FastText is libiary for efficient learning of word In the last section, we have built a BERT-based evaluation model BERTEval which performs a good performance on the quality evaluation of Chinese texts. With this model, we can classify the preprocessed web data into high-quality texts (positive) and low-quality texts (negative). Inspired by the idea of knowledge distillation, we will use these classified texts to guide the training of our FastText model. In our approach, we select 400,000 high-quality texts classified by BERTEval as our positive data, while choosing 1,600,000 low-quality texts as our negative data. In order to increase the diversity of training data, our positive data also include some high-quality Chinese data from some other websites and books, such as Baidubaike, Zhihu, Cbook, ChinaNews and so on. These data have been manually proofread and processed. In this way, we can build a good training dataset with 3200K samples. As shown in Table 3, it presents the composition of our training data. \n\nAfter collecting these training data, we will use a word segmentation tool to process all the texts, and then input the processed data into FastText to train the model. Through this approach, we can obtain a more efficient quality evaluation model.",
            "score": 0.5156338773824691,
            "section_title": "FastText-based Evaluation Model",
            "char_start_offset": 20656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29248046875
        },
        {
            "corpus_id": "274436448",
            "title": "Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models",
            "text": "The Objective of the competition was to classify the 100 test set samples into one of these 4 classes based on the patterns learned by leveraging the 500 labeled and 1500 unlabeled samples from the training dataset. \n\nFor our preliminary experimentations, We initially tried a classical semi-supervised approach by using NLPAug and SVM classifiers for data augmentation and modeling, respectively. This approach, although highly compute efficient is not that powerful, and its performance on evaluation is not that impressive. This compelled us to shift towards language models, mainly base language models such as BERT and RoBERTa, which are still more efficient than LLMs such as Llama-8B, Mistral models, GPT-3.5, etc. The augmentation for this case has also been done by a GPT-2 language model, which is a better generative model than the base language models.",
            "score": 0.5153801363606556,
            "section_title": "Proposed Methodology",
            "char_start_offset": 11141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07763671875
        },
        {
            "corpus_id": "268379740",
            "title": "Boosting Disfluency Detection with Large Language Model as Disfluency Generator",
            "text": "Dataset. We choose Switchboard (SWBD) [25], the standard and largest disfluency detection benchmark dataset for evaluating the proposed method. Following the experiment setup outlined in Charniak and Johnson [26], we partitioned the Switchboard corpus into training, development, and test Baselines. Transformers [7] and Transformers & BERT [7] employed heuristic methods to generate disfluent data for pretraining, while BERT-CRF [27] and ELECTRA-CRF [27] adopted a multitask approach. BERT-GCN [28] leveraged structural information for span-level detection, which is currently the SOTA method for the disfluency detection task. And we compare the PGmodel [10] (i.e., BERT-GRI3M model and BERT-GRI20M model) and other models that performed disfluency detection using different data augmentation techniques. Marie et al. [2] fine-tuned T5 to rewrite fluent sentences as disfluent ones, generating data for training a disfluency classifier. As the generated data has not been disclosed, we employe the disfluency classifier as the detector for experimental comparisons. In addition, we employ three disfluency detectors, namely bert-base-uncased [29], robertabase [30], and google/electra-base-discriminator [31], using their fine-tuning outcomes on SWBD as baselines for comparison. In order to comprehensively examine the effect of the data enhancement we generated, we also use DISFL-QA [23] as the augmented data for experimental comparison. \n\nImplementation Details. We use OpenAI's text-davinci-003 language model as the disfluency generation model. For disfluency detection, we utilize the Adam optimizer with a learning rate of 1e-5 and batch size 32 during the pretraining and fine-tuning stages. And, we uniformly employe the ELEC-TRA model fine-tuned on SWBD for data filtering, with the confidence threshold hyperparameter \u03bb set to 0.5.",
            "score": 0.5152343093626771,
            "section_title": "IV. EXPERIMENTS A. Experiment Setup",
            "char_start_offset": 11648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1847
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "61412708"
                },
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "5454540"
                },
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "199668628"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "199668628"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "223798991"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "259858875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "262826017",
            "title": "When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs",
            "text": "However, when looking at the transformer PLM results, both BERT and RoBERTa tend to score the GPT essays markedly higher than the human ones. \n\nFor BERT, the difference is approximately 10 points (0.78 versus 0.68) or about 15%. This \"flip\" relative to the feature-based and CNN/RNN deep learning models is important because as noted in our benchmark analysis, the transformer PLM models attain state-of-the-art performance. This \"affinity\" for GPT generated text, relative to human responses, as depicted in Figure 4, could be attributable to a few possible reasons. First, the feature-based ML models and CNN/GRU deep learning models have pre-defined features/vocabularies based on their input feature sets and/or static word embeddings. Conversely, transformer PLMs use a much larger pre-training corpora and also rely on word piece to avoid out-of-vocabulary (OOV) concerns [29,50]. Hence, the absence of GPT generated text in the training phase may be less of a concern for transformer PLMs tasked with assessing such text. Second, as noted in our review of related work, BERT and RoBERTa share a fair amount of common pre-training data with GPT models, namely the Wikipedia corpus, 11K Book and/or Book 1 and Book 2 corpora, and common crawl and web text data [66,21]. Admittedly, with GPT-3.5 onwards, the true extent of the training corpora for ChatGPT models is unclear, and the proportion of overlap is likely smaller. Nevertheless, familiarity in pre-training sources, as well as relative commonalities in the underlying attention/learning mechanisms (at least vis-\u00e0-vis CNN/GRU and SVR/XGB), may contribute to higher quality score assessments for GPT generated text when evaluated by BERT/RoBERTa. Third, it could be that because the transformer PLMs are more accurate (as per results in Table 4), they are less prone to over-fitting, and hence, are more capable of generalizing to the out-of-sample GPT text and assessing its quality. We explore the first two possible explanations in greater detail in the ensuing section related to RQ3.",
            "score": 0.5150589085061359,
            "section_title": "Results when Comparing Human and GPT-3.5 Generated Text",
            "char_start_offset": 50761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 144,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 1266,
                    "end": 1270,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1270,
                    "end": 1273,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06671142578125
        },
        {
            "corpus_id": "244488698",
            "title": "Triple Classification for Scholarly Knowledge Graph Completion",
            "text": "FastText [8] is a technique that uses Skipgram or Bag of Words to compute out-of-vocabulary word vector embeddings. More recently, Flair [1] embeddings try to generalize embeddings to be more context-aware. On the contrary, instead of relying on feature engineering, fine-tuning methods use the model architecture and parameters as a starting point for specific downstream NLP tasks. BERT [15] and Pegasus [48] are a few examples of pre-trained models that can be fine-tuned on a variety of NLP sub-tasks (e.g., Classification, or Summarization). Such models capture the deep semantic patterns in the natural language text and handle text ambiguity and variations. Pre-trained language models have also been used on knowledge graphs, where graph triples are converted into sentences using random walks and then used to train language models [17]. Other approaches aim to enhance BERT's representations with entity semantics [49]. As such, these approaches concentrate on generating new entities and relations. Finally, KG-BERT [46] is a novel approach that utilizes a BERT transformer model to compute plausibility scores of triples based on names or descriptions of entities and relations.",
            "score": 0.5145740348455774,
            "section_title": "RELATED WORK",
            "char_start_offset": 7258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1190
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "52010710"
                },
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 841,
                    "end": 845,
                    "matchedPaperCorpusId": "207228784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219970703125
        },
        {
            "corpus_id": "233189521",
            "title": "A Few Topical Tweets are Enough for Effective User Stance Detection",
            "text": "Since fastText was designed for sentence-level classification, we opted to perform tweet-level classification. During training, we assigned the label of a user to all his/her tweets. During testing, we averaged per class confidence scores across all tweets for a user, and we assigned the label with the highest average confidence to the user. As for features, we used all the words in tweets, and we preprocessed tweets in the manner described earlier for SVM. We opted not use retweeted accounts only as the number of retweeted accounts was arbitrary for each user and fastText is not well suited for long input text. \n\nContextualized Embeddings Over the last several years, pre-trained embedding (Mikolov et al., 2013;Pennington et al., 2014) have helped achieve significant improvements in a wide range of classification tasks in natural language processing. Representing words as vectors in a low-dimensional continuous space and then using them for downstream tasks lowered the need for extensive manual feature engineering. However, these pre-trained vectors are static and fail to handle polysemous words, where different instances of a word have to share the same representation regardless of context. More recently, different deep neural language models have been introduced to create contextualized word representations that can cope with the issue of polysemy and the context-dependent nature of words. Models such as OpenAi GPT (Radford et al., 2018), ELMo (Peters et al., 2018), BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), and UMLFIT (Howard and Ruder, 2018), to name a few, have achieved groundbreaking results in many NLP classification and language understanding tasks. For this paper, we use BERT base-multilingual2 (referred to hereafter simply as BERT), which we fine-tune for stance detection, as this eliminates the need for heavily engineered task-specific architectures. BERT is pre-trained on Wikipedia text from 104 languages and comes with hundreds of millions of parameters. It contains an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads.",
            "score": 0.514198562106905,
            "section_title": "Classification Models",
            "char_start_offset": 13601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 721,
                    "end": 745,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1556,
                    "end": 1577,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1590,
                    "end": 1614,
                    "matchedPaperCorpusId": "40100965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260498046875
        },
        {
            "corpus_id": "272126224",
            "title": "The Training Process and Methods for LLMs Using an Own Knowledge Base",
            "text": ": This paper explores the development of frameworks and training methods for large language models (LLMs), focusing on the importance of self-built data (Own data or Own Knowledge Base), specific processes of model pre-training and fine-tuning, and model performance evaluation and deployment effects. By introducing and analysing the advantages and disadvantages of mainstream large language models (such as GPT-4, BERT, LLaMA, and Mistral), we illustrate the strengths and limitations of large language models in natural language processing tasks. This paper particularly emphasises the critical role of self-built data in enhancing the model's professionalism and accuracy, discussing data collection and processing methods. We detail the steps of model pre-training and their impact on model performance, explore the necessity and implementation of model fine-tuning, and validate the effectiveness of the proposed framework training method through performance evaluation metrics and actual deployment effects.",
            "score": 0.5141421079962667,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0970458984375
        },
        {
            "corpus_id": "272987039",
            "title": "Surveying the MLLM Landscape: A Meta-Review of Current Surveys",
            "text": "The encoder of the Transformer, when used independently and after undergoing pre-training and fine-tuning, becomes BERT; similarly, the decoder, after similar pre-training and fine-tuning, becomes GPT. BERT and GPT are applied to different natural language processing tasks: BERT is primarily used for understanding tasks, while GPT is mainly used for generation tasks. \n\nBERT (Bidirectional Encoder Representations from Transformers) is a pre-training model proposed by Devlin et al. [17]. \n\nThe core idea of BERT is to generate contextual representations of words using a bidirectional Transformer encoder. It achieves performance improvements across various NLP tasks by performing unsupervised pre-training on large-scale corpora, followed by fine-tuning on specific tasks. BERT's pre-training involves two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). \n\nIn the MLM task, BERT randomly masks some words in the input text and predicts these masked words using the model, thereby learning the relationships between words. In the NSP task, BERT learns the relationships between sentences by determining whether two sentences are continuous. \n\nGPT (Generative Pre-trained Transformer) is a series of generative pre-training models proposed by OpenAI [18,19,20]. Unlike BERT, GPT mainly uses a unidirectional Transformer decoder to generate text. GPT's training consists of two stages: first, pre-training on large-scale unlabelled data, and then fine-tuning on specific tasks. During the pre-training stage, the model uses an autoregressive method to predict the next word, thereby learning the sequence information of words. \n\nGPT performs exceptionally well in generation tasks because it can generate coherent and meaningful text based on the given context. With the development of the GPT series, GPT-2 and GPT-3 introduced more parameters and larger model scales, further improving generation quality and task generalization capabilities.",
            "score": 0.5137235915430493,
            "section_title": "BERT and GPT",
            "char_start_offset": 8651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 1279,
                    "end": 1282,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1873779296875
        },
        {
            "corpus_id": "258187362",
            "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task",
            "text": "We utilized pretrained models, fine-tuned them on the training and validation sets, and then evaluated their performance on the test set for comparison. \n\nBERT: In 2018, Google released Bidirectional Encoder Representations from Transformers (BERT) [45], a collection of masked-language models. BERT stands out as the first unsupervised language representation model that is deeply bidirectional, meaning it is capable of analyzing language in both directions. This capability enables BERT to extract high-quality language features that can be fine-tuned on domain-specific data for improved performance. Notably, BERT's pre-training is conducted solely on plain text corpora. \n\nDistilBERT: DistilBERT is a compact and efficient Transformer model based on BERT. By distilling BERT, DistilBert possesses a smaller parameter count than bert-base-uncased by 40% and performs faster by 60%. DistilBERT achieves comparable performance to BERT on the General Language Understanding Evaluation (GLUE) language understanding benchmark, maintaining an accuracy rate of over 95%. \n\nRoBERTa: The RoBERTa model [6] was based on BERT and share same architecture. By adding dynamic masking, removing the next-sentence pretrain-ing objective and training with larger batches, RoBERTa is a better reimplementation of BERT with some modifications to the key hyperparameters. \n\nBioBERT: BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) [46] is a domain-specific language representation model pre-trained on large-scale biomedical corpora PubMed abstracts (PubMed) and PubMed Central full-text articles (PMC). When compared to BERT and other pre-existing models, BioBERT demonstrates superior performance in various biomedical text mining tasks ClinicalBERT: ClinicalBERT [47], which is based on BERT, utilizes bidirectional transformers to analyze clinical notes from the Medical Information Mart for Intensive Care III (MIMIC-III) dataset. The model has been evaluated by medical professionals and has been found to establish high-quality relationships between medical concepts, as evidenced by their judgements.",
            "score": 0.5137140991290153,
            "section_title": "BERT-Based Fine-Tuned Models",
            "char_start_offset": 26699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 1797,
                    "end": 1801,
                    "matchedPaperCorpusId": "102352093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341552734375
        },
        {
            "corpus_id": "256461375",
            "title": "Automatic Classification of Legal Violations in Cookie Banner Texts",
            "text": "In this paper, we compare the performance of the following models, as measured by their classification accuracy: BERT (Devlin et al., 2019) is a widely-used Transformer-based model, which serves as the basis for a variety of text classification tasks, including topic classification, and sentiment analysis. The major advantage of BERT is that it was pretrained on a large corpus, allowing it to be finetuned on a downstream task with a relatively small data set. We encode each cookie banner text segment into a fixed-sized vector using its BERT embedding, using this as input to a classification layer fine-tuned on the training and validation data. BERT with LIWC features. Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) is a dictionary-based text analysis tool with linguistic, psychological and topical categories. LIWC calculates the percentage of words from the cookie banner text that fall into each category and creates a vector of all these percentages. We concatenate BERT embeddings with a LIWC vector representing all 80 categories used by LIWC. The remaining architecture is the same with BERT. For classes like framing, misleading language and technical jargon, we expect that LIWC will increase the performance of the model, since these features reflect the more stylistic aspects of the text. LEGAL-BERT. LEGAL-BERT are a family of BERT models that have been pre-trained on diverse English legal text from several fields, including European legislation (EURLEX1 ), UK legislation2 , and various courts from Europe and the US3 . Since the general LEGAL-BERT model performs better than BERT on domain-specific tasks (Chalkidis et al., 2020), we use the general LEGAL-BERT as a comparison for the BERT model. While cookie banners are not themselves legal texts, they do explain legally relevant provisions; hence, we include this model to address the utility of a domainspecific BERT model in the general legal domain. BART in ZS-setting. Zero-shot (ZS) classification in NLP has been used to classify text on which a model is not specifically trained (Sarkar et al., 2021;Yin et al., 2019a;Ye et al., 2020).",
            "score": 0.5134560545199074,
            "section_title": "Models",
            "char_start_offset": 5793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 139,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2021484375
        },
        {
            "corpus_id": "271963268",
            "title": "A Survey of Large Language Models for European Languages",
            "text": "Both models are trained on a newly crawled web dataset. These models perform well in named entity recognition (NER) tasks and other text classification tasks. For Czech, researchers have primarily focused on encoder-only models. Earlier work on developing a Czech ALBERT model [105] demonstrated the best performance in text classification tasks, but lower performance in QA. Afterwards, Czert [106], a BERT and ALBERT-based model for the Czech language, outperformed multilingual models on nine downstream NLP tasks. Later, RobeCzech [108], specifically trained for Czech and based on the RoBERTa architecture, surpassed previous Czech language models. This includes multilingual models and other Czech-specific models like Czert and ALBERT, achieving better results on five NLP tasks. \n\nRecent advancements have led to a rapid catch-up for Portuguese LLMs, with encoder-only models reaching GPT-3 architectures. BERTimbau [155], a BERT-based model, outperformed previous models on sentence similarity and NER tasks. This model leverages the BERT architecture and is trained on the brWaC corpus [109]. Subsequently, BERTabaporu [156], a new LLM specifically trained for Brazilian Portuguese using a Twitter dataset is built on the BERT model. It surpasses existing general-purpose Portuguese LMs in text classification tasks. More recently, Sabia [110], presents a family of LLMs trained on Portuguese text by using popular GPT-J [163] and LLaMA [34] transformer architectures. Notably, Sabia outperform previous monolingual and multilingual models on various Portuguese NLP tasks, even with less data compared to standard training. However, their performance weakens on English tasks. The openCabrita models, introduced in [157], utilize continuous pretraining methods, resulting in a 3-billion-parameter OpenLLaMA model trained solely on Portuguese. A filtered Portuguese subset of the large public mC4 dataset is used for training. The openCabrita-3B model employs a special tokenizer that significantly reduces the number of tokens needed, yet maintains performance comparable to traditional methods.",
            "score": 0.5134370369146247,
            "section_title": ".1 Monolingual Models",
            "char_start_offset": 47667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 399,
                    "matchedPaperCorpusId": "232335496"
                },
                {
                    "start": 535,
                    "end": 540,
                    "matchedPaperCorpusId": "235166607"
                },
                {
                    "start": 924,
                    "end": 929,
                    "matchedPaperCorpusId": "263818517"
                },
                {
                    "start": 1096,
                    "end": 1101,
                    "matchedPaperCorpusId": "21724921"
                },
                {
                    "start": 1129,
                    "end": 1134,
                    "matchedPaperCorpusId": "265068288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10247802734375
        },
        {
            "corpus_id": "248780352",
            "title": "Explainable Publication Year Prediction of Eighteenth Century Texts with the BERT Model",
            "text": "In addition to our pre-trained ECCO-BERT, we used three other relevant BERT models pre-trained on either historical or modern English: bert-basecased 1 , dbmdz/bert-base-historic-english-cased 2 , and MacBERTh 3 . For each model, a grid search on the development set was performed to find optimal learning rate and number of training steps. The overall results for the year regression task with the BERT models as well as the linear baseline models are summarized in Table 1. All BERT models can be fine-tuned to perform reasonably well, as the fine-tuning dataset is very large. BERT pre-trained on the ECCO dataset performs slightly better than the other models, possibly due to better fitting the OCR noise unique to the dataset. Overall, the best result of mean absolute error of 6.32 years reflects a surprisingly good performance of the BERT model on the task. To gain more insight into the predictions, the histograms of prediction errors relative to the publication year of the work are presented in Figure 2. These show no strong bias, beyond the natural fact that the publication year of older works is more likely to be overestimated and the publication year of newer works is likely to be underestimated, as the model learned 1 https://huggingface.co/ bert-base-cased 2 https://huggingface.co/dbmdz/ bert-base-historic-english-cased 3 https://huggingface.co/emanjavacas/ MacBERTh the prediction range.\n\nThe impact of OCR quality on the results is worth considering. (Jiang et al., 2021) showed that pre-trained BERT on OCR'd historical books was less robust when used in a domain classification task than one trained on 'clean' text, though in that study fine-tuning significantly improved resilience to noise. Other studies on downstream NLP and language modelling tasks show that OCR quality can have a significant effect, though the extent is heavily dependant on the specific task and extent of the OCR error rate. (van Strien. et al., 2020; Hill and Hengchen, 2019) Here, we found a moderate performance difference between the ECCO1 and ECCO2 subsets of the test set",
            "score": 0.5133596026080891,
            "section_title": "Year regression",
            "char_start_offset": 13866,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1478,
                    "end": 1498,
                    "matchedPaperCorpusId": "240290378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12335205078125
        },
        {
            "corpus_id": "260334432",
            "title": "Multi-output Headed Ensembles for Product Item Classification",
            "text": "Macro-F1 scores induce equal weighting of genre performance and hence are a much stricter standard and that is the measure that we use henceforth for comparing models. For all models except AutoML and fastText NNI, the scores reported are averages of five runs. We note that our Ensemble of CNNs baseline (i.e. MoHE without the coupling) is a strong classifier and outperforms MoE and Aggregator baselines significantly. Additionally, MoHE-2 outperforms Ensemble significantly on the validation set. 5 Microsoft's Neural Network Intelligence (NNI): https://github.com/Microsoft/nni and Ray Tune: https://docs.ray.io/en/latest/tune/index.html The main issue with BERT is that it is a more generalized multitask model where fine-tuning is dependent on a large-scale language model, which is trained according to specific objectives of next word prediction based on a suitably chosen context. For the case of classification of item titles, the NSP (Next Sentence Prediction) objective of BERT is irrelevant if we are to even pre-train on item titles and so is SOP (Sentence Order Prediction) of AlBERT [16]. RoBERTa [20] removes the NSP objective, however, its training time does not meet our business SLA. Hence, we drop BERT and similar models from future comparisons in this paper.",
            "score": 0.5132579273897073,
            "section_title": "EXPERIMENTS AND EVALUATION",
            "char_start_offset": 20999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0701904296875
        },
        {
            "corpus_id": "227230342",
            "title": "Multilingual Epidemiological Text Classification: A Comparative Study",
            "text": "Polish  Table 4: F1-micro scores of the analyzed models for the relevant documents per language. The pretrained BERT models are base-multilingual. LR stands for Logistic Regression, RF for Random Forest, and SVM for Support Vector Machines, \u2020 fine-tuned.\n\nRegarding the difference between the fine-tuned BERT-based models and those that use the BERT encoder for generating features only, the performance is slightly better when BERT is fine-tuned on the task. However, in the case of additional layers on top of the BERT encoder, when fine-tuned, a considerable decrease in performance can be seen. Overall, these results suggest that the deep learning approaches are capable of much deeper and complex representations, such that they can utilize previously learned features for newer documents, even when the language of the document differs.\n\nAs observed in Table 4, all the machine learning models (LR, RF, SVM) display similar trends in their unequal performance based on language by not detecting (having the F1 values of zero) the relevant documents in Polish and Chinese. This is likely due to the size of the training data for these particular languages. Similarly, for all the low-resource languages (Polish, Chinese, Russian, and Greek), unsurprisingly, the CNN and BiLSTM -based models with pre-trained FastText embeddings were not able to distinguish relevant documents from irrelevant ones, as indicated by their low F1 scores. This might be due to the low embedding coverage of the languages. The F1 values for Chinese tend to be consistent for all BERT-based models while the performance for Polish varies a lot between models. VGCN+BERT had the highest F1 scores for the low-resourced languages Polish, Chinese, and Russian and the secondhighest for Greek.\n\nIn order to analyze the influence of the documents with a larger quantity of documents (French and English, around 2, 000 news articles) over the classification of low-resource languages, we consider every language as the source language and the other five languages as target languages. At every iteration, the best performing model from the previous experiments is trained on the data in the source language and applied directly to every target language.  Table 5: Evaluation scores of the BERT (multilingual-uncased) \u2020 fine-tuned model for the",
            "score": 0.5128617833304208,
            "section_title": "Models",
            "char_start_offset": 18570,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4150390625
        },
        {
            "corpus_id": "268521849",
            "title": "Detection and impact estimation of social bots in the Chilean Twitter network",
            "text": "We evaluate different machine-learning techniques to build Botcheck. We approach a binary classification problem with a class balance ratio of 45/55. To ensure class balance across folds, we employ a training-testing model selection strategy based on stratified 5-fold cross-validation. This approach guarantees that each fold has a roughly equal proportion of samples from each class. This ensures that the model is evaluated on a representative sample of the dataset. Each model was trained to minimize the binary cross-entropy loss. \n\nWe study the performance of several binary classification techniques. We used Linear Support Vector Classification (Linear SVC), logistic regression, Random Forest, Multilayer Perceptron (MLP), and AdaBoost. We also explored the use of pre-trained language models for Spanish. We used four different language models: FastText 52 , BETO (https:// huggi ngface. co/ dccuc hile/ bert-base-spani sh-wwm-cased), RoBERTa Twitter 32 , and XLM-T 53 . FastText is a pre-trained language model based on subwords that provides word embeddings for Spanish and other languages. BETO is a transformer-based encoder that was trained on a Spanish corpus using the same approach as BERT 54 . RoBERTa Twitter (RoBERTa-tw) is a BERT-based model that modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates. RoBERTa-tw was trained using tweets in many languages. XML-T is a model to train and evaluate multilingual language models in Twitter. XLM-T is a cross-lingual language model based on XLM-R, a pretrained language model on 100 languages based on RoBERTa 55 . \n\nFor each account, we created a document consisting of their 100 most recent tweets, excluding retweets. We converted each document into a sequence of tokens. We processed these sequences using a maximum number of tokens defined by the longest document in the collection, which had 16,831 tokens. Then, we encoded these sequences into fixed-length inputs of 1,736 tokens, which corresponded to the longest sequence of tokens.",
            "score": 0.5127332555972327,
            "section_title": "Model selection",
            "char_start_offset": 27825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 864,
                    "end": 866,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 961,
                    "end": 963,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 976,
                    "end": 978,
                    "matchedPaperCorpusId": "248693350"
                },
                {
                    "start": 1208,
                    "end": 1210,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1664,
                    "end": 1666,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242431640625
        },
        {
            "corpus_id": "232092338",
            "title": "Towards Efficiently Diversifying Dialogue Generation Via Embedding Augmentation",
            "text": "The augmentation results along with two baselines, the original model and replacement method, on two datasets are shown in Table 1. It can be found that our method outperforms them under most conditions in both n-gram accuracy and diversity evaluation. Compared to the original models, our method (EA) can significantly promote the n-gram diversity of responses and slightly benefit n-gram accuracy under most conditions while remaining equivalent under other cases. In other words, EA encourages models to generate diverse texts without the cost of their quality. In contrast, the replacement method usually degrades the generation accuracy despite it is also beneficial for more various replies.\n\nWhen comparing between base models, EA seems to have more effect on transformer than Seq2seq. The reason is that more data is needed to train a robust transformer than RNN due to its depth, and our method pose a similar effect. It can also be observed that the PPL values of both EA and replacement can result in a noticeable PPL decrease for transformer models, but it is not such a case for Seq2seq models. It means transformers may not be well trained using current raw data.\n\nWe also evaluate the performance of Seq2seq / transformer-BERT to better prove the merits of using fastText as a semantic-neighbor word prediction model. The performance of them is shown in Table 2 in terms of accuracy metrics, Ent, Dist, and Sen(the average of Ent/Dist/Sen-n and n=1, 2, 3). EA and BERT variant show equivalent performance on both base models, because BERT embeddings without tuning on a specific domain can even perform worse than a universal embedding [26]. However, the huge BERT model will increase the computational load since the training time of our EA is approximately 1.98\u00d7 time of raw models, while it is 5.87\u00d7 for BERT variant and 1.37\u00d7 for replacement according to experiments. Obviously, it is better to select fastText rather than BERT for augmentation candidates prediction.    Table 3. Results of the ablation study on DailyDialog dataset.",
            "score": 0.5124511819295154,
            "section_title": "Main results",
            "char_start_offset": 11831,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226806640625
        },
        {
            "corpus_id": "259089172",
            "title": "Skill over Scale: The Case for Medium, Domain-Specific Models for SE",
            "text": "They use a case-insensitive 30,522 token WordPiece vocabulary to tokenize their data and restrict inputs to a maximum length of 128 tokens -significantly shorter than most SO posts (Figure 4). Tabassum et al. [15] released a BERT model pre-trained on SO data. Their model, BERTOverflow, was pre-trained on 152 million sentences from SO using a case-sensitive 64,000 token WordPiece vocabulary. Training on individual sentences was a common practice in training BERT-like models, but recent work has found that language models perform substantially better when considering a document-level context [16], albeit at a substantial increase in compute cost. Our findings echo that observation. Tabassum et al. focus on the downstream task of Named Entity Recognition (NER, essentially tagging each word with its syntactic role), on which their model outperforms existing models such as BERTLarge as well as GloVe and ELMo models trained on the same SO data. \n\nSelection Rationale for Comparative Models: We selected BertOverflow as the main specialized baseline in our work. We did not include comparisons to the other models due to differences in model architecture and training data between these models and our proposed SOBert. Specifically, seBERT incorporates additional data sources beyond SO during pretraining, while PTM4Tag only models the Title, Description, and Code from SO posts. We omitted a comparison with Post2Vec because its performance has been substantially surpassed by Transformer-based approaches, e.g., as shown in PTM4Tag [36] on a tag prediction task. Instead, we include BERT-Base and BERT-Large as strong, general models, in line with our broader goal of demonstrating that domain-specific pre-trained models can outperform general models. And we include BertOverflow as a representative, capable domainspecific model to highlight how adopting LLM training practices allows for training significantly more powerful domainspecific models. \n\nWe selected GPT-3.5 and GPT-4 for comparison due to their strong general language understanding capabilities and widespread adoption as closed-source LLMs.",
            "score": 0.5123864744889435,
            "section_title": "B. Modeling StackOverflow data",
            "char_start_offset": 11760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1960
                },
                {
                    "start": 1963,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 1542,
                    "end": 1546,
                    "matchedPaperCorpusId": "247594801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "253015767",
            "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
            "text": "In this section, we present the results of various machine learning models on the EMSCAD dataset. The dataset was divided into 80% training and 20% testing set. The evaluation metrics: accuracy, precision, recall and F1-score were computed for each model. Figure 1 presents the evaluation metrics for various classifiers with different feature sets. The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier. \n\nWe further evaluate the various machine learning classifiers with different word embedding representations as features using tenfolds cross-validation. Figure 2 presents the macro-averages of the precision, recall and F1-score Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings. The results in Figs. 7 and 8 indicate that all the five classes of biased and discriminatory language were distinguishable from each other. \n\nWe observe a linear improvement in the performance of a sample of our models as training size increases in our primary experiment. We wanted to see if the behavior was by any chance related to these particular models or statistically grounded across all our implemented models. To validate this improvement, we included additional data (3000 additional job descriptions) and ran a new experiment with for the lightweight classifiers (DT, LR and NB). \n\nThe results obtained from the new experiment are shown in Fig. 9 for the regular models (80% training set and 20% test set), and Fig. 10 for the tenfold cross-validated models.",
            "score": 0.5122133471224306,
            "section_title": "Results and analysis",
            "char_start_offset": 21097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50244140625
        },
        {
            "corpus_id": "216392944",
            "title": "MIHNet: Combining N-gram, Sequential and Global Information for Text Classification",
            "text": "Text Classification is a classic and important problem in Natural Language Processing(NLP). The task is to assign predefined categories to a given text sequence. In recent years, neural network models that can make use of text representation have been shown to be effective for text classification, including convolution models [1][3] [5], recurrent models [6] [8], and attention mechanisms [9]. \n\nIn Natural Language Processing, pre-trained language representations are beneficial for text classification and other NLP tasks, which can avoid training a new model from scratch. And there are two existing strategies for applying pre-trained language representation to downstream tasks: featurebased and fine-tuning. The featured-based approach including word embedding, such as Word2Vec [11] , Glove [12], Cove [13] and ELMO [14]. The fine-tuning approach, such as BERT [15], OpenAI GPT [16], has brought a new breakthrough in Natural Language Processing(NLP) and become one of the hottest research topics. \n\nIn this paper, we investigate how to maximize the utilization of BERT+ models for the text classification task. We design exhaustive experiments to make a detailed analysis of BERT+ models and proposed a new model called MIHNet, which integrate Global, n-gram, Sequential information together to enhance performance further. \n\nThe contributions of our paper are as follows: \n\n\u2022 We analyse the effect of text length on BERT and propose detailed advice to choose the appropriate length based on the length distribution. \n\n\u2022 We propose a variety of scenarios for long texts and analyse the performance of each scenario on THUCNews and IMDB. \n\n\u2022 We investigate the performance of several text classification models upon BERT and propose a general solution to use those models upon BERT for different datasets.",
            "score": 0.5120408395503937,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 331,
                    "matchedPaperCorpusId": "1306065"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "1957433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "271328899",
            "title": "Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction",
            "text": "The results of our method and baselines are shown in Table 2. Our results are obtained by taking the average of five different random seeds.Although our method did not achieve state-of-the-art results, we saw significant improvements over the baseline BERT model across all datasets by employing it as data augmentation.We believe achieving the performance by an extremely simple BERT-based CSC model is impressive, highlighting the effectiveness of the data filtering mechanism.Since the CSC task does not involve adding and deleting characters, most previous methods adopt non-autoregressive methods.However, we are interested in how large language models (LLMs) perform in the CSC task due to their powerful learning and generalization abilities.So we further conduct experiments on a proprietary LLM (GPT-3.5)and an open-source LLM (ChatGLM).The reason for unsatisfactory CSC performance for LLMs can be two-fold.On the one hand, they will likely give outputs of different lengths.On the other hand, they may replace some correct words according to their understanding, leading to higher recall and lower precision.\n\nOur data filtering strategy is incorporated into a BERT-based model, so we check its effects by comparing the base model.In our subsequent experiments, we use the official evaluation from SIGHAN.BERT* denotes the results from this re-evaluation.Table 3 illustrates that our filtering method achieves an all-around improvement on BERT, including lower FPR, and lower ECE.We can conclude that training on the refined corpus delivers a performant and well-calibrated CSC model, successfully mitigating over-correction.Therefore, we empirically verify the overall effectiveness of our data filtering strategy.",
            "score": 0.512013937240074,
            "section_title": "Main Results",
            "char_start_offset": 16371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1119
                },
                {
                    "start": 1121,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1593017578125
        },
        {
            "corpus_id": "226281828",
            "title": "Hierarchical Multitask Learning Approach for BERT",
            "text": "We made modifications that are explained in Section 3 to the BERT base architecture. We pre-trained all variants of architectures in our pre-training datasets and fine-tuned them on downstream tasks. We select 1e-4 as the learning rate with 1e-4 weight decay for pre-training, and 1e-5 as the learning rate with no weight decay for fine-tuning. These hyperparameters are found by grid search done on BERT base architecture. We set the batch size to be 32 for inputs with length up to 128 tokens and one for longer inputs in the pre-training. Batch size for fine-tuning is set to be one for all downstream tasks. We set dropout rate to 0.1. For all experiments, Adam optimizer (Kingma and Ba, 2014) is used with default beta parameters and AMSGrad (Reddi et al., 2019) option. \n\nFor Lower NSP and Lower Mask architectures, we experimented with all intermediate layers to choose the best level of selected embeddings for the lower classifier. As we used BERT base architecture, which has 12 encoder layers, there are 11 different intermediate layers. As an additional experiment, we use [CLS] concatenation or NSP concatenation for BERT, Lower NSP, and Lower Mask architectures. Apart from these models, we freeze NSP related parts of the model for cases in which Lower NSP classifier overfitted to data before masked LM training. To see the importance of the NSP task, we removed the NSP classifier as in RoBERTa. The best models for each architecture are fine-tuned on downstream tasks. We add three baselines: \n\n1. Pre-trained BERT base model is fine-tuned. \n\n2. BERT base architecture without any pre-training is directly fine-tuned to understand whether the performance is due to architecture or large-scale training. \n\n3. Another modern architecture due to its success, BiLSTM, is fine-tuned with fastText (Bojanowski et al., 2017)  Fine-tuning results of QA classifiers on SQuAD1.1 and SQuAD2.0 validation sets are shown in Table 1. Here, exact match (EM) measures whether the prediction overlaps with the truth exactly.",
            "score": 0.5120087613680941,
            "section_title": "Pre-training and Fine-tuning Results",
            "char_start_offset": 10703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 1810,
                    "end": 1835,
                    "matchedPaperCorpusId": "207556454"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "258065474",
            "title": "Enhancing Spam Message Classification and Detection Using Transformer-Based Embedding and Ensemble Learning",
            "text": "Finally, we decided to use a text embedding technique based on a Transformer model and a classification method employing the Ensemble Learning strategy. \n\nRecently, Natural Language Processing (NLP) has evolved greatly due to the development of pre-trained language models (known also as Transformer models) such as BERT and GPT. One of the most significant turning points in the history of natural language processing was the release of BERT [4] in 2018 by Google. As a result, researchers accomplished top results and provided high-performance models in various tasks, including question answering, text classification, sentiment analysis, and machine translation. BERT opened the door for the appearance of other types of Transformers such as the famous GPT and its three generations that followed. The third generation GPT-3 [5] was released in 2020 with the aim of providing human-like text. The advantage of GPT-3 is its enormous number of learning parameters and the large amounts of data it has been trained with. Our idea is to use one of the Transformer models to encode text messages into high quality vector representations before starting the classification process. For this reason, we tested both BERT and GPT-3 models. The results were very satisfactory for both, with a slight superiority in favor of GPT-3. On the other hand, we tried to design a classification model that was as robust as possible. The idea that interested us the most was using an Ensemble Learning strategy. This involves using several classifiers together and gathering their output results to decide on the final output according to a specific voting technique. \n\nIn this paper, we propose a new model capable of improving the results of SMS classification and spam detection. We first employed a recent text embedding technique based on the GPT-3 Transformer to represent the text message in a dense numerical vector. Then, we gathered four classifiers (SVM, KNN, CNN and LightGBM) in an Ensemble module to classify the vector representations obtained from the previous module. To make the right decision regarding the input instance, we created a weighted voting algorithm that collected the results of the four classifiers and calculated the most suitable output. \n\nThe following is a summary of the main contributions of our work: \n\n\u2022 The proposal of an advanced model that can improve the detection rate of spam messages.",
            "score": 0.5116785752712072,
            "section_title": "Introduction",
            "char_start_offset": 2219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2256
                },
                {
                    "start": 2259,
                    "end": 2324
                },
                {
                    "start": 2327,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2509765625
        },
        {
            "corpus_id": "211171709",
            "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning",
            "text": "Pre-trained feature extractors, such as BERT (Devlin et al., 2018) for natural language processing and VGG (Simonyan and Zisserman, 2014) for computer vision, have become effective methods for improving the performance of deep learning models. In the last year, models similar to BERT have become state-of-the-art in many NLP tasks, including natural language inference (NLI), named entity recognition (NER), sentiment analysis, etc. These models follow a pre-training paradigm: they are trained on a large amount of unlabeled text via a task that resembles language modeling (Yang et al., 2019;Chan et al., 2019) and are then fine-tuned on a smaller amount of \"downstream\" data, which is labeled for a specific task. Pre-trained models usually achieve higher accuracy than any model trained on downstream data alone.\n\nThe pre-training paradigm, while effective, still has some problems. While some claim that language model pre-training is a \"universal language learning task\" (Radford et al., 2019), there is no theoretical justification for this, only empirical evidence. Second, due to the size of the pre-training dataset, BERT models tend to be slow and require impractically large amounts of GPU memory. BERT-Large can only be used with access to a Google TPU, and BERT-Base requires some optimization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware (Sohoni et al., 2019). Training BERT-Base from scratch costs \u223c$7k and emits \u223c1438 pounds of CO 2 (Strubell et al., 2019).\n\nModel compression (Bucila et al., 2006), which attempts to shrink a model without losing accuracy, is a viable approach to decreasing GPU usage. It might also be used to trade accuracy for memory in some low-resource cases, such as deploying to smartphones for real-time prediction. The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible?\n\nTo explore these questions, we compressed English BERT using magnitude weight pruning (Han et al., 2015) and observed the results on",
            "score": 0.5109177667057534,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1560,
                    "end": 1581,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1553955078125
        },
        {
            "corpus_id": "237492056",
            "title": "GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks",
            "text": "Since GradTS is built on Transformer-based models, we select its backend model from 6 common pre-trained Transformer-based models, namely bert-base-uncased, bert-base-cased, bert-  large-uncased, bert-large-cased, roberta-base, and roberta-large. We set up MTL evaluations with MT-DNN on CoLA, MRPC, SST-2, WNLI, QNLI, and RTE to examine the appropriateness of these backend models in GradTS. Specifically, we assess the strength and robustness of these models by comparing the MT-DNN performance with auxiliary task sets selected by GradTS against that without auxiliary task filtering. The same backend model is used for GradTS and MT-DNN in each experiment to eliminate possible discrepancies across models.\n\nFrom Table 8, we notice clear performance gaps between the cased and uncased models as the backend of GradTS. For example, on CoLA and SST-2, GradTS-trial produces worse auxiliary task sets than using all candidate tasks with a bert-baseuncased backend, while GradTS with a bert-basecased backend improves model performance for both GradTS-trial and GradTS-fg. This is intuitive since case information is crucial for grammaticality and sentiment tasks. Among the four cased backend models, RoBERTa (Liu et al., 2019b) does not trigger larger MT-DNN performance improvements than BERT of the same size, implying that larger pre-training corpora do not greatly affect the efficacy of GradTS. While the performance improvement brought by GradTS with the bert-large-cased backend is comparable to that with the bert-basecased backend, its running time and GPU cost are over 100% higher. We thus choose bert-base-cased as the backend of GradTS to balance performance with resource cost, though potentially any cased Transformer-based model is a valid choice.",
            "score": 0.5107489647372003,
            "section_title": "Base Model Selection for GradTS",
            "char_start_offset": 25686,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10992431640625
        },
        {
            "corpus_id": "258668089",
            "title": "A Review on Deep-Learning-Based Cyberbullying Detection",
            "text": "(3) train on longer sequences; and (4) dynamically change the masking pattern over the training data. The authors used a novel dataset, CCNEWS, and suggested that if more data are used during pre-training, downstream tasks can be improved further. Yani et al. [49] utilized RoBERTa to detect cyberbullying on the popular social media platform Twitter. After experimental analysis, they obtained an accuracy score of 86.9% and an F1 score of 77.5%. \n\nALBERT (A Lite BERT) [50]: Improving the model performance is not always possible due to GPU/TPU memory limitations and longer training times. To mitigate the issue, the authors reduced two parameters to lower the memory consumption and to increase the training speed of BERT. A number of studies show that ALBERT presents better performance compared to BERT over GLUE, RACE, and SQuAD benchmarks. Tripathy et al. [51] used an ALBERT-based fine-tuning model for cyberbullying detection, as it does not require large amounts of data for fine-tuning. The experimental results show that their proposed method outperformed the current approaches CNN + word2Vec, CNN + GRU, and BERT implementations in terms of an F1 Score of 95%. \n\nELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) [52]: BERT corrupts the input by replacing some token with MASK and by training a model to reconstruct the original model. The authors corrupted the tokens with plausible alternatives sampled from a small generator network that improves the model performance significantly. \n\nDistilBERT (Distilled BERT) [53]: DistilBERT is a pre-trained smaller general-purpose language representation model that is a faster variant of the BERT model. The version is fine-tuned with good performance on a wide range of tasks and is designed for lowresource environments. It achieves similar performance to the original BERT model while using fewer resources. The approach leveraged knowledge distillation during the pretraining phase and reduced the size of general BERT by 40%, yet it retained a language understanding capability of 97%. The model is faster, smaller and lighter to pre-train.",
            "score": 0.5106359000245891,
            "section_title": "FastText",
            "char_start_offset": 25223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "260938539"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "224929355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "159041722",
            "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
            "text": "We evaluate the difficulty of HellaSwag using a variety of strong baselines, with and without massive pretraining. The models share the same format: given a context and an ending, return a logit for that ending. Accordingly, we train our models using a four-way cross-entropy loss, where the objective is to predict the correct ending. In addition to BERT-Large, our comparisons include: a. OpenAI GPT (Radford et al., 2018): A finetuned 12-layer transformer that was pre-trained on the BookCorpus (Zhu et al., 2015). b. Bert-Base: A smaller version of the BERT model whose architecture size matches GPT. c. ESIM+ELMo (Chen et al., 2017;Peters et al., 2018): This is the best-performing ELMo model for NLI, modified slightly so the final output layer is now a four-way softmax over endings. d. LSTM sentence encoder: This is a randomly initialized two-layer bi-LSTM; the second layer's hidden states are max-pooled and fed into an MLP to predict the logit. We consider three variations: GloVe embeddings, ELMo embeddings, or (frozen) BERT-Base embeddings.9 e. FastText: (Joulin et al., 2017) An off-the-shelf library for bag-of-words text classification. 10 e compare all models to human performance by asking five independent crowd workers to solve the same four-way multiple choice problems; their predictions are combined via majority vote. \n\nOur results, shown in Table 1, hint at the difficulty of the dataset: human performance is over 95%, while overall model performance is below 50% for every model. Surprisingly, despite BERT-Large having been used as the adversarial filter, it still performs the strongest at 47.3% overall. By making the dataset adversarial for BERT, it seems to also have become adversarial for every other model. For instance, while ESIM+ELMo obtained 59% accuracy on SWAG, it obtains only 33.3% accuracy on HellaSwag.",
            "score": 0.5106073815548573,
            "section_title": "Results",
            "char_start_offset": 10750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1849
                }
            ],
            "ref_mentions": [
                {
                    "start": 618,
                    "end": 637,
                    "matchedPaperCorpusId": "34032948"
                },
                {
                    "start": 637,
                    "end": 657,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1070,
                    "end": 1091,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1844482421875
        },
        {
            "corpus_id": "274959171",
            "title": "Contextual Fine-Tuning of Language Models with Classifier-Driven Content Moderation for Text Generation",
            "text": "In today\u2019s digital age, ensuring the appropriateness of content for children is crucial for their cognitive and emotional development. The rise of automated text generation technologies, such as Large Language Models like LLaMA, Mistral, and Zephyr, has created a pressing need for effective tools to filter and classify suitable content. However, the existing methods often fail to effectively address the intricate details and unique characteristics of children\u2019s literature. This study aims to bridge this gap by developing a robust framework that utilizes fine-tuned language models, classification techniques, and contextual story generation to generate and classify children\u2019s stories based on their suitability. Employing a combination of fine-tuning techniques on models such as LLaMA, Mistral, and Zephyr, alongside a BERT-based classifier, we evaluated the generated stories against established metrics like ROUGE, METEOR, and BERT Scores. The fine-tuned Mistral-7B model achieved a ROUGE-1 score of 0.4785, significantly higher than the base model\u2019s 0.3185, while Zephyr-7B-Beta achieved a METEOR score of 0.4154 compared to its base counterpart\u2019s score of 0.3602. The results indicated that the fine-tuned models outperformed base models, generating content more aligned with human standards. Moreover, the BERT Classifier exhibited high precision (0.95) and recall (0.97) for identifying unsuitable content, further enhancing the reliability of content classification. These findings highlight the potential of advanced language models in generating age-appropriate stories and enhancing content moderation strategies. This research has broader implications for educational technology, content curation, and parental control systems, offering a scalable approach to ensuring children\u2019s exposure to safe and enriching narratives.",
            "score": 0.5100503752762116,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2509765625
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "Setting We use checkpoints from models trained on MS MARCO for different numbers of steps to evaluate their performance on the development set, in order to better understand the impact of parameter number and pre-training sufficiency on data efficiency and convergence speed. \n\nWe compare BERT-large, Qwen1.5-0.5B, and Llama-2-7B to explore the impact of data efficiency with model parameter number and pre-training sufficiency. Notably, BERT-large and Qwen1.5- Results and Analysis As presented in Figure 2, our findings indicate that larger model sizes lead to higher data efficiency and faster convergence. Specifically, after 100 training steps on MS MARCO, Llama-2-7B outperforms Qwen1.5-0.5B by 5.4 points and BERT-large by 14.4 points. This suggests that with an increase in parameter number, better performance can be achieved with less labeled data. Furthermore, as shown in Table 1, when comparing the relative score difference between 100 steps and the full training of 3700 steps, Llama-2-7B shows a score difference of 8.8 points, which is smaller than the 9.7 points for Qwen1.5-0.5B and 15.3 points for BERT-large. This indicates that larger models are able to converge faster. \n\nThe experiment results also demonstrate that LLMs have better data efficiency compared to non-LLMs, even with similar parameter sizes. For example, after 100 training steps on MS MARCO, Qwen1.5-0.5B outperforms BERT-large by 9 points. Despite having a similar number of parameters, Qwen1.5-0.5B has undergone more  extensive pre-training (over 3 trillion tokens compared to BERT's 3.3 billion tokens) and employs a decoder architecture, which enhances its language understanding ability and enables faster convergence in the retrieval task where text discriminative ability is crucial.",
            "score": 0.5096130422823884,
            "section_title": "Data Efficiency",
            "char_start_offset": 16639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1780
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1885986328125
        },
        {
            "corpus_id": "258309541",
            "title": "Pre-trained Embeddings for Entity Resolution: An Experimental Analysis",
            "text": "Word2Vec and Glove are the fastest models by far, exhibiting the lowest processing runtimes in practically all cases. Word2Vec is an order of magnitude faster than the third most efficient model per dataset, which interchangeably corresponds to FastText and S-MiniLM. Except for  1 , GloVe outperforms these two models by at least 6 times. In absolute terms, both Word2Vec and GloVe process the eight smallest datasets,  1 - 8 , in much less than 1 second, while requiring less than 2.5 seconds for the two larger ones. \n\nAmong the BERT-based models, DistilBERT is significantly faster than BERT, as expected, with its processing time being lower by 33%, on average. Note, though, that it is slower than FastText by >50%, on average, except for  3 . The next most efficient models of this category are ALBERT and RoBERTa, which outperform BERT by 11% and 13%, on average, respectively. XLNet is the most time consuming BERT-based model, being slower than BERT by \u223c30%, on average across all datasets but  3 . This can be explained by the fact that  3 has larger sentences, as shown in Table 2(a). \n\nAmong the SentenceBERT models, the lowest time is achieved by S-MiniLM, which, as mentioned above, is the third fastest model together with FastText. The second best model in this category is S-DistilRoBERTa, which is slower by 30%, on average. Both models are faster than BERT by 63% and 53%, respectively, on average. In contrast, the quite complex learned model of S-GTR-T5 yields the highest processing time among all models in all datasets but the smallest one. S-MPNet lies between these two extremes. Summary. The static models excel in processing time, but suffer from very high initialization time. More notably, FastText is the slowest model in all datasets, but  9 , where S-GTR-T5 exhibits a much higher processing time. This means that FastText's high initialization cost does not pay off in datasets with few thousand entities like those in Table 2(a). On average, FastText requires 2 minutes per dataset.",
            "score": 0.5095380307072899,
            "section_title": "Supervised Matching",
            "char_start_offset": 51772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "233330456",
            "title": "Automatic detection of procedural knowledge in robotic-assisted surgical texts",
            "text": "In the first four rows of Table 1, we report the classification performance of the classical ML algorithms that exploit TF-IDF as features. The considered ML approaches based on TF-IDF have mediocre performance when used to solve this task. This fact could be due to the unbalanced dataset, which is difficult to be handled by standard ML algorithms. Classical ML approaches are often biased towards the majority class (F1 scores on the procedural class are substantially higher than on the non-procedural one), not considering the data distribution. In the worst case, minority classes are treated as outliers and ignored. Moreover, TF-IDF cannot account for the similarity between the words in a document since each word is independently presented as an index. Among the considered ML algorithms, Random-Forest obtains the highest F1 scores.\n\nThe fifth row of Table 1 summarizes the performance of the FastText classifier. All scores demonstrate that FastText obtains much higher classification performance than the best considered ML method (Ra-Fo). In particular, it provides an improvement of 10.56% over Macro-F1 and 8.15% over Weighted-F1.\n\nWe then fed the FastText word embeddings learned on the dataset to a 1D-CNN and a Bi-LSTM. The adoption of more complex classification models allows, in our task, to sub-   Table show that it is possible to achieve high classification performance also using transformer-based pre-trained language models. In particular, ClinicalBERT performs slightly better than Bi-LSTM (+ 0.12% of Macro-F1 and + 0.22% of Weighted-F1), while BERT performs slightly worse than Bi-LSTM. As somehow expected, given the characteristics of the source material used for pre-training the model (c.f., Sect. 3.3.4), ClinicalBERT (pre-trained on clinical notes) performs better than BERT (pre-trained on Wikipedia and BooksCorpus). Computational wise, fine-tuning transformers-based models on our dataset is 4 times slower than training Bi-LSTM.\n\nWe also investigated whether it is possible to boost classification performance by balancing the dataset. More precisely, we have applied standard random over-sampling techniques (i.e., the addition of random",
            "score": 0.5094276358938213,
            "section_title": "Results and discussion",
            "char_start_offset": 21224,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "231846746",
            "title": "Traceability Transformed: Generating More Accurate Links with Pre-Trained BERT Models",
            "text": "Researchers have addressed the data sparsity problem and the performance issues of training large models through the use of pre-trained DL models for various NLP problems . This approach divides the training stage into pre-training and fine-tuning phases. In the pre-training phase, DL models are constructed using a huge amount of unlabeled data and selfsupervised training tasks. Then in the fine-tuning phase, the models are trained on smaller, labeled datasets in order to perform more specialized 'downstream' tasks. The underlying notion is that knowledge learned from pre-training a model on a larger and more generalized dataset can be effectively transferred to the downstream tasks which have limited labels for supervised training. Furthermore, a pre-trained model provides a better starting point for model optimization than a randomly seeded one. It therefore reduces the likelihood of local optimization traps and improves overall performance. Fine-tuning a pre-trained model on a smaller dataset takes significantly less time than training a deep learning model from scratch. While pre-training a general model is extremely expensive, the pre-training phase only needs to be performed once and can then be reused for various downstream tasks.\n\nBERT-based language models make use of transformers [24] to learn contextual information from corpora in the pre-training stage and then transfer learned knowledge to downstream NLP tasks, such as question answering, document classification, and sentiment recognition [23], [25]. To our knowledge, this is the first study that has applied BERT or other transformer-based methods to the software traceability task. We pose a series of research questions to evaluate whether T-BERT can effectively address the traceability problem. Our first question is defined as follows: RQ1: Given three variants of T-BERT models, based on single, twin, and siamese BERT relation classifiers, which is the best architecture for addressing NLA-PLA traceability with respect to both accuracy and efficiency?\n\nIn addition to investigating the DL model architecture, we also explore different training techniques for improving model accuracy. As discussed by Guo et al. [10], the DL trace model may hit a 'performance glass ceiling' and converge at relatively low accuracy. We therefore define our second research question as:\n\nRQ2: Which training technique improves accuracy without suffering from the previously observed glass ceiling ?\n\nGururangan et",
            "score": 0.5092799098356563,
            "section_title": "II. PROBLEM STATEMENT",
            "char_start_offset": 8204,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1533,
                    "end": 1537,
                    "matchedPaperCorpusId": "196174566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2125244140625
        },
        {
            "corpus_id": "276250250",
            "title": "Efficient Scientific Full Text Classification: The Case of EICAT Impact Assessments",
            "text": "The exponential growth of research publications across various domains (Bornmann et al., 2021) has created an increasing need for automated methods to process scientific texts efficiently. To address this, numerous approaches have been developed to optimize general research workflows, such as literature search (Singh et al., 2023) and summarization (Singha Roy and Mercer, 2024). For more specialized tasks, such as extracting specific information from full texts, proprietary large language models (LLMs) offer potential solutions (Dagdelen et al., 2024). However, these models are not locally deployable, making them expensive to use when processing large datasets. \n\nRecently, open-source large language models have emerged as strong competitors to proprietary systems, offering comparable performance (DeepSeek-AI et al., 2024). Nevertheless, a wider adoption from researchers outside the machine learning research community is unlikely within the next years, primarily due to their significant hardware requirements. Furthermore, both proprietary and open-source LLMs of this scale are highly energy-intensive, raising concerns about their sustainability. This highlights the importance of exploring smaller, more efficient models that can deliver similar performance while minimizing resource consumption, or of exploring other strategies to reduce the computational cost of using these models to solve specific tasks. \n\nTo address these challenges, we investigate the potential of more efficient BERT-based models alongside slightly more resource-intensive local large language models (LLMs) for classification of scientific full texts. As part of this effort, we introduce the EICAT dataset, which consists of scientific full-text papers focused on specific invasive species and their impact on the native ecosystem, as well as labels specifying the impact category of that species with corresponding evidence sentences that were extracted from the papers. \n\nIn our series of experiments, we first evaluate the performance of a standard BERT-based classifier on the EICAT dataset, where full-text inputs must be split into multiple segments due to the limited context length. We then compare its performance to ModernBERT (Warner et al., 2024), a recent BERT variant capable of handling longer contexts, as well as to Llama-3.1 8B (Grattafiori et al., 2024), a state-of-the-art local LLM.",
            "score": 0.5087387886324307,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2398
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 332,
                    "matchedPaperCorpusId": "254018137"
                },
                {
                    "start": 351,
                    "end": 380,
                    "matchedPaperCorpusId": "269804367"
                },
                {
                    "start": 534,
                    "end": 557,
                    "matchedPaperCorpusId": "267700596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193359375
        },
        {
            "corpus_id": "258309541",
            "title": "Pre-trained Embeddings for Entity Resolution: An Experimental Analysis",
            "text": "As a result, the difference between the maximum and minimum F1 of dynamic models ranges from 4.9% ( 1 ) to 17% ( 2 ). \n\nThe second group of models includes the static, context-agnostic ones, which perform relatively poorly, even though they are combined with the best configuration of DeepMatcher, i.e., the stateof-the-art algorithm for this task and type of models. GloVe and FastText are actually combined with a two layer fully connected ReLU HighwayNet classifier followed by a softmax layer in the  3. \n\nclassification module in combination with a hybrid model for the attribute similarity vector module. On average, GloVe and FastText underperform the top model per dataset by 22% and 37%, respectively. Only in  3 and  4 , where all dynamic models exceed 0.95, the two static models exhibit high performance, with their F1 within 5% of the maximum one (this is another indication about the straightforward matching task posed by the bibliographic data of these two datasets). Note that FastText consistently outperforms GloVe across all datasets, since its character-level functionality is capable of addressing the out-of-vocabulary tokens that arise, due to the domain-specific terminology of each dataset, unlike GloVe. Overall, the static models underperform the dynamic ones in most cases, as reported in the literature [25], while the BERT-based models match the SentenceBERT ones, unlike for the previous ER tasks, due to the fine-tuning of their last layer. SentenceBERT models also benefit from fine-tuning, but to a lesser extent, probably because the sentence representing each entity is constructed in an ad-hoc manner, lacking any cohesiveness. Comparison to SotA. Figure 11(d) depicts the performance of the state-of-the-art supervised matching algorithms that leverage static and dynamic models, DeepMatcher+ [18] and DITTO [25]. We actually consider their optimized F1 that is reported in [25]. \n\nComparing DITTO to the dynamic models, we observe that its F1 is directly comparable to the best performing language model in each dataset. In  2 ,  3 and  4 , DITTO's F1 is lower by just \u22640.5%.",
            "score": 0.5084143810433214,
            "section_title": "Supervised Matching",
            "char_start_offset": 47890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 120,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "195069071"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "214743579"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "214743579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.268798828125
        },
        {
            "corpus_id": "228980954",
            "title": "Towards Machine Learning Based Analysis of Quality of User Experience (QoUE)",
            "text": "However, despite these concerns, web crawling is gaining relevance and usage. This calls for more research on how to set up best practices to reach a tradeoff between its importance and potential threat. 4) Bidirectional Encoder Representations from Transformers (BERT): Recent works now employ Google's BERT for predicting experiences and usage in clinical dialogues [26], [27]. Google's BERT is a state-of the-art model released. It is commanding wide usage by researchers in the field of natural language processing. The open issues for researchers centre on the effective usage and adaptation to various scenarios. For instance, in a recent work, there was a reported challenge faced by their researchers on how to use the BERT [26]. They compared the performance of BERT to a rule-based system and found that BERT did not perform optimally as claimed by inventors. However, they quickly accepted that their findings may be due to inability to test all possible ways to use BERT [26]. It is therefore a research direction to see more BERT-based systems emerge in the coming future. Similarly, another research trained their BERT model and compared its performance with that of Google's pre-trained models [27]. The result showed that their model outperformed that of Google's BERT in predicting personal experience using medication-related tweets as a case study [27]. This paper presents an overview of machine learning particularly natural language processing tools for the analysis of users satisfaction measured by QoUE metrics. The results showed that the extracted sentiments using the web scraping technique corroborates the opinion of respondents in the questionnaire. This showed the potential and capability of machine learning in analysing and predicting large database of sentiments or reviews. This approach can be applied to industrial scenarios where quality control units can use QoUE to appraise the perception of users of their products and services. Finally, an improved logistic regression classifier was developed to test, train and classify the user experiences. Comparing the improved logistics regression classifier with standard logistic regression classifier shows that the training accuracy of the improved logistic regression gave 97.67% as against the standard logistic regression which gave accuracy of 86.01%.",
            "score": 0.508320805144223,
            "section_title": "B. Open Research Issues",
            "char_start_offset": 20175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2344
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "202699998"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "204229606"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "202699998"
                },
                {
                    "start": 983,
                    "end": 987,
                    "matchedPaperCorpusId": "202699998"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "204229606"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "204229606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11798095703125
        },
        {
            "corpus_id": "235669706",
            "title": "Making the Most of Small Software Engineering Datasets With Modern Machine Learning",
            "text": "Our results suggest that, in particular for natural language tasks, the most promising approach seems to be to further pre-train models already pre-trained on general English. When available, in-domain data should be used for pretraining, but even close-to-domain data can yield good improvements. For instance, pre-training BERT on Stack Overflow comments helped to improve accuracy also on the app review dataset (Table 8). Further pre-training an already pre-trained model should also be preferred over pre-training from scratch: The further pre-trained models outperformed our model pre-trained from scratch for most tasks and metrics even though pre-training from scratch required considerably more training time and (unlabeled) training data. This comparison comes with a grain of salt: our further pre-trained models have twice as many layers as our pre-trained-from-scratch model, which, in turn, has a much larger vocabulary (52 000 vs 30 522). While not having a larger model pre-trained from scratch is a limitation of this work, it also highlights how expensive it is. What speaks for our small model, and for small models in general, is of course their size: with only half the layers, training and evaluation is roughly twice as fast, the memory footprint is much smaller and, depending on the task, the performance hit may be acceptable. \n\nOur experiments also indicate that further pre-training is effective even with relatively small amounts data. In the sentiment classification task as little as 150 MB (1 million samples) of pre-training data seems to be sufficient and able to \"saturate\" the model. Doubling the amount of pretraining data resulted in virtually negligible improvements (see BERT-SO-1M versus BERT-SO-2M in Table 8). Similarly, for our large model (BERT-SO-1M-large) improvements are marginal: on the Senti4SD test set, i.e., the test set that \"matches\" the training set, it outperforms the base-sized models by only 0.6%, while on the other test tests it lags behind them.",
            "score": 0.508157868370355,
            "section_title": "Pre-Training",
            "char_start_offset": 42841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08209228515625
        },
        {
            "corpus_id": "271601178",
            "title": "Predicting the geolocation of tweets using transformer models on customized data",
            "text": "In addition, there are several different world models that can be utilized for prediction output. While the Plate carr\u00e9e projection is commonly used due to the Twitter geolocation stored in the WGS84 format, other projections such as the Mercator projection, Robinson projection, conic projection, and Winkel-Tripel projection should be considered as possible alternatives in future studies. \n\nBesides the BERT variations, the proposed machine learning techniques have the potential to be applied to other base models. In the realm of NLP, various state-of-the-art models have been developed to enhance text analysis tasks, particularly text classification that provides an output representation for the examined regression task. However, this would require adapting the implementation of the wrapper layer to accommodate the shape of the pooler output vector specific to the chosen model. Currently, the approach is designed for linear regression logic, transforming the vector of size 768, which is common to all BERT-based models, into a predefined number of continuous numerical values. Furthermore, in order to ensure suitability for the downstream task of sentence classification, it is important that the base models possess strong contextual understanding and the ability to capture semantic relationships within sentences. \n\nWhile acknowledging the availability of newer models, we carefully considered the trade-offs and ultimately selected the BERT model, which was widely adopted and extensively studied for text classification tasks, to strike a balance between computational efficiency and our analytical requirements. Apart from the BERT, RoBERTa (Robustly Optimized BERT) [17], DistilBERT (Distilled BERT) [29], ALBERT (A Lite BERT) [14], ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) [6], XL-Net (eXtreme Language understanding Network) [37], and GPT-3 (Generative Pre-trained Transformer 3) [3] are prominent models in this field that can be finetuned to predict location from the text. \n\nIn the downstream task of text classification, all these models can be finetuned to perform well, although computational resource requirements differ among them. XLNet, BERT, and RoBERTa demand substantial resources due to their size and training needs resulting in a strong performance in text classification tasks.",
            "score": 0.5077636006126484,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 55836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 2046
                },
                {
                    "start": 2049,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2365
                }
            ],
            "ref_mentions": [
                {
                    "start": 1951,
                    "end": 1954,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128662109375
        },
        {
            "corpus_id": "265295512",
            "title": "Tensor-Aware Energy Accounting",
            "text": "To evaluate the effectiveness of tensor-based energy accounting, we apply Smaragdine to BERT [10], a widely used text analysis model. In the domain of natural language processing (NLP), BERT plays a central role in powering numerous NLP end-user applications. In \u00a7 5, we show how the BERT application can be hierarchically decomposed into EDDs. We also show how BERT transformers [46] -especially its attention modules -dominate the energy consumption in different stages of BERT use, from pre-training to fine-tuning to prediction. Throughout experiments, we find Smaragdine incurs low overhead while retaining high precision and stability. \n\nTo further demonstrate the usefulness of Smaragdine in building the downstream toolchain, we use BERT in two case studies. First, we provide a white-box study on the impact of hyperparameter tuning of BERT. We show the energy/power consumption trends with different configurations in the number of layers and the number of hidden embeddings. Second, we conduct an evolutionary study to compare BERT with a newer variant, AL-BERT [25]. Through Smaragdine, we show how the energy behavior has evolved from BERT to ALBERT.",
            "score": 0.5077246851247417,
            "section_title": "Understanding BERT Energy Behavior",
            "char_start_offset": 5088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1163
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1063232421875
        },
        {
            "corpus_id": "273821224",
            "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
            "text": "Each of these models represents a distinct approach to natural language processing, with varying techniques and capabilities for handling linguistic data [5]- [8]. While Statistical Language Models and Neural Language Models have been researched for decades, the Pretrained Language Models and Large Language Models draw a lot of popularity in both academia and industry recently and the applications are widely used in various fields. For example, recommendation system [9], [10]. Pre-trained Language models propose train largely amount of text data before fine-tune for specific downstream tasks. Based on the Transformer architecture and the self-attention mechanism [11]- [15], Pre-trained Language Models advances the performance for semanticpurpose language processing. BERT(Bidirectional Encoder Representations from Transformers) serves as an exemplary Pre-trained Language Model, pre-trained on a large corpus of text in an unsupervised manner using masked language modeling and next sentence prediction and fine-tuned for tasks like question-answering, sentiment analysis, and text classification. It has become the dominant paradigm in Pretrained Language models due to its efficiency, scalability, and superior performance across multiple tasks [?], [16], [17]. Large Language Models represent an extension of research based on Pre-trained Language Models, building upon their foundational Transformer architectures to enhance capabilities in natural language processing. It's found that scaling up PLM model size and data size enhances the capacity to perform more effectively on downstream tasks. For example, GPT-3, a significantly larger PLM with 175 billion parameters, shares a similar architecture and pre-training tasks with standard PLMs. However, despite the primary focus of scaling being on model size, GPT-3 shows remarkable abilities and out performs standard PLM in solving complex tasks. In certain creative writing fields [18], GPT-3 model highly capable of producing creative content like poetry, song lyrics, or fiction that are coherent to specific styles or themes whereas BERT(340 million parameters) is not designed for creative writing tasks which only capable of completing sentences or predicting missing words. Due to Large Language Model's attribute that requires practical large-scale data processing and distributed parallel training, conducting repetitive studies to explore various training strategies is highly resource-intensive and costly.",
            "score": 0.5076905329982058,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2250
                },
                {
                    "start": 2251,
                    "end": 2487
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "10959945"
                },
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "203620277"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "270371619"
                },
                {
                    "start": 1952,
                    "end": 1956,
                    "matchedPaperCorpusId": "248079457"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2288818359375
        },
        {
            "corpus_id": "218665566",
            "title": "Spelling Error Correction with Soft-Masked BERT",
            "text": "For comparison, we adopted the following methods as baselines. We report the results of the methods from their original papers. \n\nNTOU is a method of using an n-gram model and a rule-based classifier (Tseng et al., 2015). NCTU-NTUT is a method of utilizing word vectors and conditional random field (Tseng et al., 2015). HanSpeller++ is an unified framework employing a hidden Markov model to generate candidates and a filter to re-rank candidates (Zhang et al., 2015). Hybrid uses a BiLSTM-based model trained on a generated dataset (Wang et al., 2018b). \n\nConfusionset is a Seq2Seq model consisting of a pointer network and copy mechanism (Wang et al., 2019). FASPell adopts a Seq2Seq model for CSC employing BERT as a denoising autoencoder and a decoder (Hong et al., 2019). BERT-Pretrain is the method of using a pre-trained BERT. BERT-Finetune is the method of using a fine-tuned BERT.",
            "score": 0.5075917925049831,
            "section_title": "Baselines",
            "char_start_offset": 13282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 890
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 220,
                    "matchedPaperCorpusId": "18312025"
                },
                {
                    "start": 299,
                    "end": 319,
                    "matchedPaperCorpusId": "18312025"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "2422075"
                },
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "53083061"
                },
                {
                    "start": 641,
                    "end": 660,
                    "matchedPaperCorpusId": "196181315"
                },
                {
                    "start": 757,
                    "end": 776,
                    "matchedPaperCorpusId": "208001011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1282958984375
        },
        {
            "corpus_id": "277502337",
            "title": "Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models",
            "text": "The performance of different BERT-based models for PTSD classification is presented in Table 2. Our analysis revealed substantial performance differences between models pre-trained on general text corpora versus those specialized for mental health domains. \n\nThe domain-specific models consistently outperformed their general-domain counterparts across all evaluation metrics. Mental-BERT achieved an F1 score of 0.615, representing a 28.1% increase from BERT-base. Similarly, Mental-RoBERTa results demonstrate superior performance with an F1 score of 0.643, a 32.6% increase from RoBERTa-base. Balanced accuracy measurements further confirm this pattern, with Mental-BERT (0.734) and Mental-RoBERTa (0.761) substantially outperforming their general-domain counterparts BERT-base (0.625) and RoBERTa-base (0.604), respectively. The discriminative ability of the models, as measured by AUC, shows Mental-BERT and Mental-RoBERTa both achieving 0.754, compared to 0.576 for BERT-base and 0.596 for RoBERTa-base. Notably, while RoBERTa variants typically outperform BERT models in many natural language processing tasks due to their more robust pre-training, in our experiments this advantage was minimal when comparing models within the same domain category (general or mental health specific). Instead, the domain of pre-training emerged as the dominant factor influencing performance, with Mental-RoBERTa showing only marginally better performance than Mental-BERT. These results highlight the importance of domain-specific pre-training when applying transformer-based models to specialized clinical text analysis tasks such as PTSD detection from patient interview transcripts. The LLaMA embedding + DFNN approach achieved the highest performance across all evaluation metrics, with an F1 score of 0.7, balanced accuracy of 0.781, and AUC of 0.865. This represents a substantial improvement over the Mental-RoBERTa model. This represents an 8.9% relative increase in F1 score and a 14.7% relative improvement in AUC compared to Mental-RoBERTa.",
            "score": 0.5074846559365095,
            "section_title": "Comparison of BERT-based Models",
            "char_start_offset": 18146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4140625
        },
        {
            "corpus_id": "276294063",
            "title": "Ontology-based prompt tuning for news article summarization",
            "text": "In our research, we selected BART, BERT, GPT 3.5 Instruct, and GPT 3.5 Turbo as baseline models for comparison due to their prominence in the field of natural language processing (NLP) and summarization by carefully considered each model's architecture, strengths, and typical use cases to ensure a well-rounded evaluation of our ontology-based prompt tuning approach. BART and BERT are widely recognized for their robust language understanding and generative capabilities, making them ideal for benchmarking against our model's performance in abstractive summarization. GPT 3.5 Instruct and GPT 3.5 Turbo were included to provide insights into how our model fares against the latest advancements in large language models, particularly those optimized for instructionbased tasks. These baselines were chosen to cover a broad spectrum of methodologies, from sequence-to-sequence models like BART to transformer-based architectures like BERT and GPT-3.5, ensuring a comprehensive evaluation. \n\nThe comparison criteria centered on key performance metrics such as Precision, Recall, F1 Score, ROUGE, BERTScore, and BARTScore, which offer a multidimensional assessment of summarization quality. However, using these baseline models posed challenges, particularly in aligning them with our domain-specific prompt tuning approach. Each model has distinct strengths, but they also exhibit limitations, such as BERT's reduced efficacy in generative tasks and GPT-3.5's computational demands. Despite these challenges, their inclusion is justified as they represent the current state-of-the-art, providing a relevant benchmark for evaluating the improvements introduced by our ontology-based prompt tuning method. This selection allows for a rigorous comparison, highlighting the advancements and potential areas for improvement in our approach.",
            "score": 0.5073072006950017,
            "section_title": "Baselines",
            "char_start_offset": 30776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1646728515625
        },
        {
            "corpus_id": "271088966",
            "title": "A Review of the Challenges with Massive Web-mined Corpora Used in Large Language Models Pre-Training",
            "text": "The use of pre-trained text quality classification/regression models is a common element of low-quality content filtration methods described in the literature [6,27,35,46].For training classifier models, corpora that are considered to be of good quality are usually used, such as Wikipedia text corpora, BookCorpus2 and other curated dataset [6].\n\nStatistical Models for Automatic Language Identification.Using statistical, lightweight text language identification models such as langid.py3[18], fasttext 4 [15], langdetect5 [37] is another method of filtering qualitative data from CommonCrawl corpora [33,35] described in the literature.\n\nProbabilistic N-grams Language Models.Filtering low-quality content based on n-gram lightweight language models trained on good-quality data collections is an element of data cleaning and filtration processes described in the literature [31].Pipelines like CCNet [46] employ statistical KenLM models to assess text perplexity during the data filtering phase.\n\nThe poor quality of data collected in web corpora is also largely due to the difficulty of extracting the main content from websites.Contamination caused by text extraction from unwanted HTML elements such as menus, headers, footers, breadcrumbs, advertisements, copyright notices, spam largely affects the performance of language models [2,23,26,27,34].Analysis of the most frequently occurring n-grams conducted on OpenWebText, C4, mC4, OSCAR, The Pile and RedPajama indicates a large share of sequences of repeated punctuation characters such as dashes, question marks, dots; HTML tags, and content derived from the extraction of such page elements like headers, menus etc [13].\n\nComparative analyses of the quality of models trained on raw and filtered pre-training corpora confirm the negative impact of low-quality texts in web corpus and point to the need for methods of filtering high-quality data from web corpora in the process of LLMs pre-training [23,35].",
            "score": 0.5072475857571588,
            "section_title": "Data Quality and Noise",
            "char_start_offset": 8855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 346
                },
                {
                    "start": 348,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 639
                },
                {
                    "start": 641,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 883
                },
                {
                    "start": 883,
                    "end": 999
                },
                {
                    "start": 1001,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1682
                },
                {
                    "start": 1684,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "218974065"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "3411445"
                },
                {
                    "start": 525,
                    "end": 529,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "195505104"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "207870323"
                },
                {
                    "start": 1339,
                    "end": 1342,
                    "matchedPaperCorpusId": "1152099"
                },
                {
                    "start": 1348,
                    "end": 1351,
                    "matchedPaperCorpusId": "218974065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09930419921875
        },
        {
            "corpus_id": "258587990",
            "title": "Korean named entity recognition based on language-specific features",
            "text": "We compare the intrinsic results generated by different types of models. By saying \"intrinsic,\" it implies that the results in this subsection differ owing to the ways and approaches of learning from the training data. We compare the performances of the baseline CRFs and our proposed neural models, and we also investigate the variations when our models use additional features in the data. Table 4 summarizes the evaluation results on the test data based on the proposed machine learning models in Section 5. Comparing the transformer-based models with LSTM + crf, we found that both multilingual BERT-based models (bert-multi, xlm-roberta) outperformed LSTM + crf. The comparison reveals a clear trend: the word representation method is the most effective for the NER tasks adopting our proposed scheme. For the LSTM + crf model, we initialized its word representation, v w i , with a fastText word embedding (Joulin et al. 2016). However, once we initialized word representation using BERT, we observed performance improvements up to 3.4 points with the identical neural network structure as shown in Table 4. Meanwhile, there are two notable observations in our experiment. The first observation is that the CRF classifier exhibits slightly better performance than the MLP classifier, with an improvement of 0.23, for xlm-roberta. However, the improvement through the use of the CRF classifier is relatively marginal compared with the reported results of English NER (Ghaddar and Langlais 2018). Moreover, when comparing the multilingual models (bert-multilingual and xlm-roberta) to the monolingual model (klue-roberta), we found that klue-roberta outperforms both bert-multilingual and xlm-roberta. This is because klue-roberta is trained solely on Korean texts and utilizes better tokenizers for the Korean language (Park et al. 2021). Table 5 details results using various sets of features. We use incremental words, UPOS, and XPOS, for the input sequence x and their unigram and bigram features for CRFs. Both LSTM and XLM-RoBERTa achieve their best F 1 score when only the +UPOS feature is attached.",
            "score": 0.5071429495727916,
            "section_title": "Intrinsic results on various types of models",
            "char_start_offset": 40408,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1472,
                    "end": 1499,
                    "matchedPaperCorpusId": "47020916"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36572265625
        },
        {
            "corpus_id": "265255388",
            "title": "ChatGPT as a Commenter to the News: Can LLMs Generate Human-Like Opinions?",
            "text": "In 2019, Devlin et al. introduced a Transformer-based language model called BERT, which stands for Bidirectional Encoder Representations from Transformers [5]. In contrast to GPT, the BERT model is a transformer encoder, without a decoder component. BERT's pre-training makes use of a Masked Language Modeling (MLM) objective. It randomly masks some tokens from the input, after which its objective is to predict the original token based on the context. in addition, it uses Next Sentence Prediction (NSP), which, given two sentences, determines whether the second sentence follows the first. MLM and NSP help BERT understand context across different sentences. BERT models can be fine-tuned for specific tasks, creating new state-of-the-art models for supervised learning tasks. \n\nRobBERT is a large pre-trained general Dutch language model that can be fine-tuned on a given dataset to perform a wide range of NLP tasks [4]. It uses a RoBERTa architecture and pre-training with a Dutch tokenizer. RoBERTa, a Robustly Optimized BERT Pre-training Approach has some modified key hyperparameters, such as removing the NSP objective in pre-training, having much larger batches and training on longer sequences [8]. It has shown state-of-the-art results for various tasks, especially compared to other models when applied to smaller datasets [4]. In this research, we use the Dutch RobBERT model to finetune for our specific use case, classifying comments as GPT-or human-generated.",
            "score": 0.5069484133377123,
            "section_title": "BERT",
            "char_start_offset": 2800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1477
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "210714061"
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "matchedPaperCorpusId": "210714061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1375732421875
        },
        {
            "corpus_id": "257267523",
            "title": "Using Machine Learning to Uncover the Semantics of Concepts: How Well Do Typicality Measures Extracted from a BERT Text Classifier Match Human Judgments of Genre Typicality?",
            "text": "We replicated the comparison of the four BERT-based typicality measures using word embeddings as a language representation instead of the BERT language representation. More precisely, we used a word-embedding layer with pre-trained weights in our classifiers instead of the BERT language representation. (See Appendix: Methodological Details for further details). We employed GloVe word embeddings (Pennington, Socher, and Manning 2014) to transform text documents into vectors. GloVe is a word-embedding model, not a text-embedding model. Accordingly, we needed to combine word positions in the embedding space to create a unique position for text documents (book descriptions from Goodreads.com). We used the average position of the words in the book description as the position of the book description in feature space. \n\nThe results obtained with the four GloVe-based typicality measures are reported in Tables 3 and 4. The overall performance of the GloVe-based typicality measures is very good, although not as high as that obtained with the BERT language representation. Because the BERT classifier is sensitive to bidirectional dependencies between words but the GloVe classifier is not, this unsurprisingly suggests that typicality judgments are also sensitive to such dependencies. \n\nComparison of the performance of the four GloVe-based typicality measures leads to the same conclusion as that obtained from comparing the four BERT-based typicality measures: what is crucial in achieving a good performance is that measure construction involves training a probabilistic classifier.",
            "score": 0.506812798576378,
            "section_title": "Typicality Based on GloVe Embeddings",
            "char_start_offset": 56976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1592
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 436,
                    "matchedPaperCorpusId": "1957433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "266049371",
            "title": "BERT based natural language processing for triage of adverse drug reaction reports shows close to human-level performance",
            "text": "In the wake of even larger generative models like GPT-3, this pre-training and fine-tuning setup is now being replaced by prompting requiring none or very few examples to obtain promising performance [3]. However, results on ScandEval, a recently published benchmark of NLP tasks in several Scandinavian languages [4], show that multiple Swedish and Norwegian BERT models on average perform better in Swedish tasks than generative models including GPT-4 [5] and that large Swedish BERT models along with GPT-4 and GPT-3.5 turbo are among the top for classification tasks in Swedish such as sentiment analysis and linguistic acceptability [6]. \n\nIn addition, large generative models are typically quite demanding in terms of memory and computational requirements, and we cannot process our reports on outside servers due to privacy concerns, which is why we consider a BERT model as the largest architecture for our experiments. \n\nSpecifically, we consider two transformer-based architectures using a Swedish BERT language model, one bag-of-words approach, and one based on LSTM modelling.",
            "score": 0.5067289804968305,
            "section_title": "Introduction",
            "char_start_offset": 2267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1088
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11474609375
        },
        {
            "corpus_id": "276408138",
            "title": "InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning",
            "text": "These fasttext models are then used to recall relevant content from the remaining corpus. \n\nDeduplication An excessive amount of homogeneous data can be detrimental to small language models. therefore, it is crucial to ensure data diversity. To optimize data efficiency while maintaining semantic diversity, we implemented a global Min-Hash algorithm to efficiently detect and eliminate near-duplicate documents. \n\nQuality assessment To ensure comprehensive data quality across diverse domains, we establish a two-step evaluation framework. First, we employ a domain-specific quality assessment tool: \n\n(1) For web content, we leverage FineWeb-eduscorer (Penedo et al., 2024) to evaluate document quality through multiple dimensions; \n\n(2) For mathematical content, we utilize a model-based classifier following (Lozhkov et al., 2024a) that scores reasoning and deduction capabilities on a 1-5 scale, filtering to retain only high-quality samples; \n\n(3) For code data quality validation, we utilize a static analysis tool to validate syntactic correctness and identify potential structural issues. We also perform comprehensive ablation experiments through continued pre-training on LLaMA 3.2-1B, comparing the performance between models trained on raw and filtered datasets. The model trained on filtered data exhibits better performance across multiple code-related benchmarks compared to its counterpart trained on the raw dataset. Moreover, the filtering process facilitates accelerated convergence, enabling the model to achieve desired performance metrics with fewer training steps. \n\nDecontamination To ensure fairness of comparison, we implemented a token-level 10-gram decontamination algorithm to remove potentially contaminated content in standard benchmarks.",
            "score": 0.5060977735689922,
            "section_title": "Heuristic filtering",
            "char_start_offset": 5981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1770
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1492919921875
        },
        {
            "corpus_id": "259964032",
            "title": "Pre-training Two BERT-Like Models for Moroccan Dialect: MorRoBERTa and MorrBERT",
            "text": "The development of pre-trained language models based on the Transformer architecture has revolutionized the field of NLP in recent years. One of the most successful examples is the BERT model proposed by [13], which employs a multi-layer Transformer-encoder architecture and two pre-training tasks, MLM, and Next Sentence Prediction (NSP), to learn contextualized word representations. BERT has achieved state-of-the-art performance in a wide range of NLP tasks, including sentiment analysis, question answering, and natural language inference. In response to the success of BERT, many BERT-based models with varying goals have been proposed in the literature. For instance, RoBERTa [21] optimizes BERT's pre-training methods by removing the NSP pre-training task and significantly increasing the size of the training corpus and batch size. DistilBERT [24] and ALBERT [20] reduce the model size and complexity of BERT by using knowledge distillation and parameter sharing techniques, respectively. \n\nApart from these models, several pre-trained models have been developed specifically for low-resource languages, including XLM [12], which pre-trains a shared multilingual model on parallel data from 100 languages, and mBERT [13], which pre-trains a single model on a large corpus of Wikipedia articles from 104 languages. These models have been shown to improve the performance of downstream NLP tasks in low-resource languages, although they may not capture the nuances and peculiarities of individual languages. \n\nOne of the major challenges in developing NLP models for Arabic dialects is the lack of high-quality, largescale labeled datasets for training and evaluation. This challenge is particularly relevant for the Moroccan dialect, which has unique phonetic and grammatical features that distinguish it from other Arabic dialects. To overcome this challenge, researchers have proposed various pre-trained models for the Moroccan dialect, such as CamelBERT [18], MARBERT [5], QARiB [4] and AraBERT [7], which are based on the BERT architecture and pre-trained on multidialectal corpora. However, these models have limitations due to the relatively low representation of the Moroccan dialect in the training corpus, which can result in reduced performance and accuracy when applied to specific dialectal variations.",
            "score": 0.5060533678654958,
            "section_title": "Related Work",
            "char_start_offset": 5230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "58981712"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1759033203125
        },
        {
            "corpus_id": "235658083",
            "title": "Neural Models for Offensive Language Detection",
            "text": "author emphasized the superiority of the FastText to obtain accurate embeddings for rare words which was a major factor of the performance improvements.\n\nThe work at [ITE20] focused on solving the problem of imbalanced classes using backtranslation based augmentation introduced in [SHB15]. The technique involves generating new data instances for the class that is underrepresented in the dataset by translating sentence to other languages and then translating back to English which held a different sentence with the overall same meaning and then assigning the same class. That method significantly improve the model performance. They have used Google translate API to perform the processing. The authors applied data augmentation for task B and task C dataset since they suffer more from imbalanced classes problem.  The authors at [Sot+20] used BERT based model in OffensEval 2020 for offensive language detection. Since BERT is pretrained on formal text (wikipedia and books) they started with fine tunning the model to our domain. This is accomplished by taking the original model and continuing to train the masked language model and next sentence prediction objectives using a large amount of unlabeled tweets. For predictions they have using an ensemble using the multi-view SVM with the addition of features from the contextualized language model classifier. Specifically, linear SVM classifier using various n-gram ranges are first trained for each task in addition to the BERT-based classifier. Then the output of the view-classifiers are concatenated as a final linear SVM classifier.\n\nAlso the work at [WYB20] noted that fine-tuning BERT yield state of the art results for text classification tasks. Fine tuning BERT is typically done on task specific datasets in a supervised manner, it can also be done in an unsupervised manner by the masked language modeling MLM. In-domain unsupervised MLM pretraining allows for domain adaptation of the model. The authors concluded that since there is no way that the 2020 weakly labeled data could carry useful information to a machine learning system, they decided to use the manually labeled 2019 dataset only. However 2020 dataset was an ample source to build models using unsupervised learning particularly for domain adaptation using MLM training. The problem is that training a transformer",
            "score": 0.50588129785693,
            "section_title": "Methodology reviews on OffensEval shared task",
            "char_start_offset": 33318,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 173,
                    "matchedPaperCorpusId": "227231712"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42626953125
        },
        {
            "corpus_id": "261031193",
            "title": "A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports",
            "text": "We generated embeddings for the bug reports using multiple methods, such as TF-IDF [24], BERT [17], Fasttext [18], Doc2Vec [19], and ADA (GPT3.5) [20]. To establish a baseline model, we specifically utilized TF-IDF embeddings. In numerous research studies, TF-IDF is widely adopted as a text representation technique, making it a well-established and commonly used reference point. By incorporating Scikitlearn's TF-IDF as the baseline, we were able to effectively compare the performance of other models against this established standard. This comparative analysis enabled us to evaluate the effectiveness of alternative approaches by contrasting their performance with the TF-IDF baseline. TF-IDF is expressed as: \n\nFor pre-processing, we employed default techniques for TF-IDF, FastText, ADA, and BERT embeddings, which include lower-casing, tokenization, and stop-word removal. We used regular expressions to refine the tokenization process further, defining the token pattern and facilitating the extraction of English and alphanumeric words. On the other hand, BERT embeddings did not require any specific pre-processing steps. However, for Doc2Vec embeddings, the \"simple preprocess\" method provided by the Gensim library ensured effective data pre-processing. \n\nC. Embedding models GPT3.5, BERT, Fasttext, and Doc2Vec models required loading pre-trained models for their respective embeddings. Specifically, for BERT, we utilized the \"all-mpnet-base-v2\" model, optimized for various use cases and trained on a large and diverse dataset comprising over 1 billion training pairs. For ADA, we used \"text-embedding-ada-002,\" a GPT3.5 large language model for text search, text similarity, and code search. For Fasttext, we employed the \"crawl-300d-2M-subword\" model, which consisted of 2 million word vectors trained with subword information on the Common Crawl dataset, encompassing 600 billion tokens.",
            "score": 0.5058792003326654,
            "section_title": "B. Preprocessing",
            "char_start_offset": 10259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "207556454"
                },
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "18593743"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "258564392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "245853972",
            "title": "The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild",
            "text": "Dev split was used to optimize hyperparameters for different models. The main focus of the hyperparameter search was the number of training epochs to prevent overfitting and optimize micro and macro F1 scores. In accordance to this, 30 epochs for Transformer models and 200 epochs for fastText models were used. For the Transformer models, the sequence length of 512 tokens was used, and the learning rate was set to 10 \u22125 . The models, trained on the train split, were evaluated on the test split via micro F1 and macro F1 to measure both the instance-level and the label-level performance of a specific setup. In each experiment, at least 5 training runs were performed to assure a reasonable sample for measuring statistical significance of differences in performance of specific setups, which was tested via the Mann-Whitney U rank test.\n\nIn the remainder of this section, we present the results of our experiments:  To assess which technology is the most suitable for the automatic genre identification task, we compared fastText (Joulin et al., 2016) and two basesized Transformer-based pre-trained language models -the monolingual Slovenian SloBERTa (Ul\u010dar and Robnik-\u0160ikonja, 2021) (Ljube\u0161i\u0107 and Erjavec, 2018) only marginally increased its performance. The monolingual model SloBERTa and the multilingual model XLM-RoBERTa revealed to be the most suitable for the AGI task, with SloBERTa reaching 0.629 in micro F1 and 0.575 in macro F1, and XLM-RoBERTa 0.624 in micro F1 and 0.579 in macro F1. XLM-RoBERTa was included in the comparison following the findings of Repo et al. (2021) where it outperformed monolingual BERT models in this task. However, in our case it did not perform statistically significantly different from SloBERTa. Two main conclusions can be drawn from these results. For identifying genre, CNN-like classifiers seem not to be up to the task, while Transformer-based models achieve a drastic improvement of the results. Furthermore, similar to results of previous research, multilingual BERT models seem to be as good for modelling the phenomenon as monolingual BERT models. We currently have two hyp",
            "score": 0.5057388607497049,
            "section_title": "Experimental setup",
            "char_start_offset": 21353,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35400390625
        },
        {
            "corpus_id": "268681118",
            "title": "Emotion Detection with Transformers: A Comparative Study",
            "text": "Robustly optimized BERT approach (RoBERTa) is a robustly optimized BERT approach that exceeds the original BERT model performance. The changes that were made to RoBERTa include: model training for longer periods of time, with larger batches, on more data, removing the next sentence prediction objective, training on longer sequences, and dynamically changing the masking pattern applied to the training data [27]. \n\nThe RoBERTa-base model was trained on about 58M tweets and fine-tuned for emotion recognition with the TweetEval benchmark. The TweetEval benchmark is a new evaluation framework for Twitter-specific classification tasks, including sentiment analysis, emotion recognition, offensive language detection, emoji prediction, and irony detection. \n\nThe RoBERTa model has the same architecture as BERT, but it uses a byte-level byte pair encoding (BPE) tokenizer. BPE is a data compression technique that replaces the most frequent pairs of bytes in a sequence with a single, unused byte [14]. The byte-level BPE is a variant of BPE that operates on bytes instead of characters. The BPE tokenizer is more efficient and can handle out-of-vocabulary words better than the WordPiece tokenizer. \n\nIn terms of next sentence prediction, RoBERTa does not use this task during pre-training. Instead, it uses a larger corpus of text and a longer training time to improve the performance of the model. The model was trained on five English-language corpora of varying sizes, totaling over 160GB of uncompressed text. The text includes various datasets such as BOOKCORPUS, CC-NEWS, OPENWEBTEXT, and STORIES. \n\nRoBERTa was evaluated on downstream tasks using the three benchmarks of the General Language Understanding Evaluation (GLUE), the Reading Comprehension from Examination (RACE) and the Stanford Question Answering Dataset (SQuAD). We know, the GLUE benchmark is a collection of 9 datasets for evaluation of natural language understanding systems. SQuAD is a Stanford Question Answering Dataset that is used to evaluate the ability of a model to answer questions by extracting the relevant span from the context [21]. On the other hand, RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions.",
            "score": 0.5055768417770172,
            "section_title": "twitter-roberta-base-emotion",
            "char_start_offset": 15014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1898193359375
        },
        {
            "corpus_id": "241583640",
            "title": "Multilingual Sequence Labeling Approach to solve Lexical Normalization",
            "text": "The proposed approach attempts to finetune already pre-trained language models. The experiments of finetuning language models on a downstream task of lexical normalization are performed on the following pre-trained language models: BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), BERT-multilingual (Devlin et al., 2018), XLM-RoBERTa (Conneau et al., 2019), BERT-German. For the English Language, the variants of RoBERTa performed well as so with BERT (trained on an English corpus). However, these models which were pre-trained on a huge English corpus failed to achieve good performance on the other 11 languages. To make sure that this behavior is consistent, another experiment was performed wherein the unified training data from phase 1 of section 3.2 was taken and this time the finetuning was performed on the BERT-German model which is pre-trained on Open Subtitles, Common-Crawl Wikipedia dump, ParaCrawl EU Bookshop corpus, and News Crawl. This sums up to about 16GB of German text corpus and 2,350,234,427 tokens. This model with BERT-German cased is trained based on the hypothesis that it will perform well for the German language and low in other languages. This hypothesis was not rejected after computing the evaluation results. Hence, it would be a sensible choice to train different models for different languages. However, it would be expensive in terms of resource cost when these models have to be deployed in production. Further, it would have scalability issues to cater to a large number of people. \n\nHence, the proposed approach finetunes the BERT multilingual base model (Devlin et al., 2018). The multilingual BERT model is pre-trained on the top 104 languages that have voluminous Wikipedia data using the masked language modeling (MLM) objective. The training data for each language was collected from the complete Wikipedia dump (excluding user and discussion pages). The major challenge faced while preparing this enormous dataset of 104 languages is the data imbalance, as different languages had different amounts of data. To deal with this data imbalance, the exponentially smoothed weighting of the data is performed.",
            "score": 0.5054996270115814,
            "section_title": "Finetuning of transformer-based language models for the sequence labeling task of lexical normalization",
            "char_start_offset": 6699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09271240234375
        },
        {
            "corpus_id": "256875360",
            "title": "Network embedding aided vaccine skepticism detection",
            "text": "Embedding words of the human language into vector spaces by neural networks (Mikolov et al. 2013) revolutionized several natural language processing tasks, including text classification. As one such method, BERT (Devlin et al. 2018) is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. Its technological innovation lies in the bidirectional training of a Transformer, a popular attention model. The superior performance of BERT in several tasks is also attributed to its novel training procedure, where the model is jointly optimized for the masked language model and next-sentence prediction tasks. With just one additional output layer, a pre-trained BERT model can be easily fine-tuned for a selected downstream task such as Question Answering, Natural Language Inference, Named Entity Recognition and Text Classification. \n\nDue to the hundreds of millions of model parameters, pre-training a BERT model has a high computational cost. To remedy this issue, several works (Sun et al. 2019;Turc et al. 2019;Tang et al. 2019) were proposed to compress or distill knowledge from large BERT models. For example, in Turc et al. (2019), a small BERT model with significantly fewer parameters is trained based on the predictions of a large pre-trained teacher model that resulted in comparable performance to the teacher. \n\nThe original BERT model was trained on BookCorpus, a dataset of unpublished books and English Wikipedia. Since then, several works have been published assessing models tailored for social network contexts, including emojis, hashtags or mentions. For example, Bertweet (Nguyen et al. 2020) is the first model pre-trained for English Tweets. Finally, in 2020 researchers published the first models (Nguyen et al. 2020;M\u00fcller et al. 2020) pre-trained on hundreds of millions of tweets related to COVID-19. Later in 2021, with the ongoing vaccination effort, a BERT model was developed for COVID vaccination-related fact classification.1",
            "score": 0.5054269444896607,
            "section_title": "Vector space embedding for natural language processing",
            "char_start_offset": 8300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 96,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 1628,
                    "end": 1647,
                    "matchedPaperCorpusId": "218719869"
                },
                {
                    "start": 1756,
                    "end": 1776,
                    "matchedPaperCorpusId": "218719869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14208984375
        },
        {
            "corpus_id": "232106462",
            "title": "Interpretability in Word Sense Disambiguation using Tsetlin Machine",
            "text": "Even though the selected datasets have binary sense classification, we will use Micro-F1 and Macro-F1 as the evaluation metrics as shown in (Loureiro et al., 2020). Since, interpretation of WSD is the main concern of the paper, we will compare our work with the latest benchmark (Loureiro et al., 2020). Table 2 show the comparison of Macro and Micro F1 score on CoarseWSD dataset for 4 different methods: FastText-Base (FTX-B), FastText-CommonCrawl (FTX-C), 1 neural network (NN) BERT base, and our proposed TM. FTX-B is a fast text linear classifier without pre-trained embeddings and FTX-C is a fast text linear classifier with pre-trained embedding from Common Crawl. These are considered as the standard baseline for this dataset (Loureiro et al., 2020). Our proposed TM based WSD easily outperforms FTX-B baseline and is close to FTX-C without even considering the pretrained embedding. However, TM falls short of BERT's performance given that it is a huge language model that achieves the state-of-the-art performance on most of the task. This shows that TM not only possesses the interpretation of the WSD but also has performance close to the state of the art.",
            "score": 0.5050724470460002,
            "section_title": "EVALUATIONS",
            "char_start_offset": 20334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49755859375
        },
        {
            "corpus_id": "271926908",
            "title": "LERCause: Deep learning approaches for causal sentence identification from nuclear safety reports",
            "text": "Our main finding is that the considered BERT-based models outperform the tested heuristic method, conventional machine learning methods, and CNN method when telling apart causal and non-causal sentences in the given domain and data. BioBERT performed best among all examined models, with an F1 score of 94.49%. A possible explanation for the comparatively high performance of BERT-based models is their foundation on extensive pre-training. For instance, the BERT variants we utilized were trained on the expansive Books Corpus, spanning over 800 million words. When repurposed for our study, such extensively pre-trained language models seem to be able to consider context and nuances in the LER data. Traditional word embeddings tend to generate uniform vectors for words, irrespective of their use cases. In contrast, BERT-based models use contextsensitive vectors [7]. This enables them to capture different word contexts and meanings, which likely empowered the BERT models to detect truly causal sentences and enhance recall, which seemed harder for the conventional machine learning algorithms. \n\nBesides the mentioned performance advantages, BERT-based models offer procedural simplicity for causal sentence classification: they bypass the pre-processing and feature selection phase which both are indispensable for traditional machine learning methods. One of the practical implications of our experiments is that BERT-based models can deliver stable results even with small data, which can be essential in specialized application cases. Such model resilience to training dataset size can reduce the labor and time invested in data annotation. Furthermore, our methodology allows researchers to refine our models by generating classification probability scores per sentence. Sentences that hover below a defined confidence threshold could then be earmarked for manual classification. \n\nWhile there are numerous other popular models, such as GPT-3 [22], RoBERTa [53], and T5 [54], the selection of three BERT-based models can be justified by their specific training regimes and demonstrated efficacy in handling domain-specific texts. Specifically, SciBERT and BioBERT, which are pre-trained on scientific and biomedical literature, are well-suited for handling the technical and specialized language prevalent in nuclear safety reports.",
            "score": 0.5050352654013174,
            "section_title": "5.",
            "char_start_offset": 29544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "226096901"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33740234375
        },
        {
            "corpus_id": "257631936",
            "title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation",
            "text": "Pre-trained language models (PLMs) based on the transformer architecture, such as the BERT [4] and GPT [47] model families, have revolutionized natural language processing. Compared to previous methods, they deliver stateof-the-art performance on a wide range of downstream tasks and contribute to the rising popularity and democratization of language models. In general, there are three classes of pretrained language models: autoregressive language models (e.g., the decoder-based GPT), masked language models (e.g., the encoder-based BERT), and encoder-decoder models(e.g., BART [48] and T5 [49]). These models typically contain between 100M and 1B parameters [17]. \n\nIn recent years, NLP communities have witnessed the rise of very large language models such as GPT-3 (175B parameters) [6], PaLM (540B parameters) [50], Bloom (176B parameters) [51], OPT (up to 175B parameters) [52], and the FLAN series (FLAN has 137B parameters) [53]. At their core, these large language models are transformer models inspired by BERT and GPT, albeit at a much larger scale. \n\nLarge language models aim to learn accurate latent feature representations of input text. These representations are often context-dependent and domain-dependent. For example, the vector representation of the word \"treat\" might be vastly different between medical domains and the general domain. For smaller pre-trained language models, it is often necessary to continuously pre-train and fine-tune such models to attain acceptable performance [54]. However, very large language models can potentially eliminate the need for fine-tuning while maintaining competitive performance [6], [18], [55]. \n\nExisting studies indicate that pre-trained language models can help augment a dataset with new samples with The follow-up rate after 5 years was 85%. \n\n1. The study showed a follow-up rate of 85% after 5 years. \n\n2. The 5-year follow-up rate was found to be significantly high at 85%. 3. The participants had an impressive follow-up rate of 85% after 5 years.",
            "score": 0.5049389110387866,
            "section_title": "Very Large Language Models",
            "char_start_offset": 10807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 790,
                    "end": 793,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "255776274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06695556640625
        },
        {
            "corpus_id": "273901465",
            "title": "FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation",
            "text": "We investigated two types of language models: models with encoders, from which we probed their embeddings using kNN algorithm, and state-of-theart models that we prompted to classify the target word into one of two possible meanings. Our findings indicate that current SOTA models struggle to accurately disambiguate coarse-grained homonyms when adversarial contexts are added. We observed a performance decrease across all models when comparing results from Set 1 with those from Sets 3 and 4. \n\nIn models containing encoders, this effect is most significant in smaller models like BERT-base and T5-base, and less significant in larger models like T5-FLAN-xxl. Conversely, advanced and larger language models such as GPT-3.5 Turbo and Llama3-70b show a dramatic performance decline when faced with adversarial context changes, with performance drops of 25.6% and 10.4%, respectively in Set 4 compared to Set 1. However, models like GPT-4o exhibit more robustness against realistic opposing context examples (Set 4), with a performance drop of only around 4%, but more vulnerability for adding adversarial adjective (Set 3). Additionally, our findings suggest that models that contain encoder, such as those from the BERT or T5 family, tend to perform better in these tasks, specifically in Set 3. For instance, the BERT-large model with 340 million parameters outperformed the Llama3-8b model, which has around 8 billion parameters on both adversarial tasks (Set 3 and 4). In addition, T5-large and BERT-large performed better than GPT-4o on Set 3 of the dataset. To conclude, our contributions can be summarized as follows: \n\n\u2022 We introduce FOOL, a new coarse-grained WSD dataset that features various test sets with added adversarial context to assess the robustness of pre-trained language models \n\n\u2022 We perform an extensive analysis on various SOTA language models in WSD with experiments on our proposed dataset \n\n\u2022 We show that current state-of-the-art language models are prone to misclassification when faced with adversarial and opposing realistic context",
            "score": 0.5042586068092603,
            "section_title": "Introduction",
            "char_start_offset": 3545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1625
                },
                {
                    "start": 1628,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1917
                },
                {
                    "start": 1920,
                    "end": 2065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08050537109375
        },
        {
            "corpus_id": "272310114",
            "title": "Assessing Generative Language Models in Classification Tasks: Performance and Self-evaluation Capabilities in the Environmental and Climate Change Domain",
            "text": "To establish baselines for assessing the added value of Large and Small Language Models (L/SLMs) in accurately classifying texts within the ecological domain, we reference the performance of six pre-trained BERT-based models. These models have been utilized in recent research across three classification tasks: binary Eco-Relevance classification, multilabel Environmental Impact Analysis, and Stance Detection [11]. Key performance metrics from the studies include: \n\n-BERT and RoBERTa [10,17]: Masked language models trained on large English corpora. These results set a comprehensive baseline for our study, allowing for a direct comparison of the effectiveness of L/SLMs in similar tasks2 . To account for a heterogeneous range of generative language models, we selected three different types of language models for our evaluation, drawing inspiration from [34], to cover closed, open and small-scale models: \n\n-GPT-3.5-Turbo-0125. An advanced iteration within the GPT series developed by OpenAI [1], representing high-capacity closed generative LLMs. Given its widespread use in both public and academic contexts, it serves as our benchmark for closed generative LLMs. -Llama-2-13b-chat-hf. A variant from Meta AI's Llama-2 model series [30]. \n\nDesigned for conversational applications, it exemplifies mid-to-large opendomain language models with its capability for human-like text generation and comprehension. -Gemma-2b-it. A newly introduced small language model (SLM) by Google, derived from the Gemma family [27]. Gemma is a family of lightweight, stateof-the-art open models built from the same research and technology used to create the larger Gemini models.",
            "score": 0.5041473694182983,
            "section_title": "Baselines and Generative LMs employed",
            "char_start_offset": 6955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "269004840"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2039794921875
        },
        {
            "corpus_id": "254313258",
            "title": "Predicting Chinese Phrase-Level Sentiment Intensity in Valence-Arousal Dimensions With Linguistic Dependency Features",
            "text": "In the second set of experiments, the following transformerbased models were compared to demonstrate their performance for phrase-level sentiment intensity prediction. \n\n\u2022 BERT (Bidirectional encoder Representations for transformers) [21] BERT uses an encoder architecture with an attention mechanism to construct a transformer-based neural network architecture, providing state-of-the-art results in a wide variety of natural language processing tasks. BERT was pre-trained on two tasks: 1) Masked Language Models (MLM): a fixed ratio of tokens was masked to train BERT and the model then predicts the original value of the masked words based on the context; 2) Next Sentence Prediction (NSP): BERT was trained to predict whether the following sentence was probable or not based on the previous sentence. Through pretraining, BERT learns contextual embeddings for representations from large-scale data sets. After pre-training, BERT can be fine-tuned on smaller data sets to optimize its performance on specific tasks. \n\n\u2022 RoBERTa (a Robust optimized BERT pre-training approach) [22] RoBERTa is a replication study of BERT pre-training that carefully measures the impact of key parameters and training data size. The model modifications include removing next sentence predictions, dynamically changing the masking pattern applied to the training data, and training in large batches. \n\n\u2022 MacBERT (MLM as correction BERT) [23] MacBERT revisits the Chinese pre-trained language model series and improves upon RoBERTa, particularly the masking strategy that adopts MLM as correction (Mac). This Mac pre-training task was proposed to alleviate the inconsistency problem of pre-training to downstream tasks. \n\n\u2022 ALBERT (A Lite BERT) [24] ALBERT was proposed to improve the training and results of the BERT architecture using three different techniques: factorization of the embedding matrix, cross-layer parameter sharing, and inter-sentence coherence prediction. \n\n\u2022 XLNet [25] XLNet was proposed as a generalized autoregressive pretraining method that 1) enables learning of bidirectional contexts by maximizing the likelihood over all permutations of the factorization order; and 2) overcomes the limitations of BERT in neglecting dependencies between the masked positions.",
            "score": 0.5039040095712068,
            "section_title": "E. SENTIMENT INTENSITY PREDICTION RESULT",
            "char_start_offset": 19562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 170,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2156982421875
        },
        {
            "corpus_id": "272969458",
            "title": "Trustworthy AI: Securing Sensitive Data in Large Language Models",
            "text": "To date, Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP) due to their exemplary capabilities in understanding and producing text that mimics human communication [1]. These models use deep learning frameworks to find complex patterns and contextual relationships in language developed from large datasets. The Transformer model by Vaswani et al. [6] provides a basic architecture for LLMs using selfattention to model text dependencies of long-range much better than previous recurrent architectures. \n\nProminent examples of LLMs are BERT, GPT, RoBERTa, T5, XLNet, and Meta's series of LLaMA. BERT is an abbreviation for Bidirectional Encoder Representations from Transformers. Devlin et al. [1] proposed this, conditioning on both left and right context to capture deep bidirectional representations. It is pre-trained on masked language modeling and next-sentence prediction tasks, therefore performing really well in a wide range of NLP tasks after fine-tuning. \n\nThe best-known GPT series by OpenAI are GPT-2 [39] and GPT-3 [2]. They are autoregressive models that predict the next word in a sequence. GPT-3 is the largest, with 175 billion parameters, and has achieved impressive scores on few-shot learning and text generation. RoBERTa is an improved version of BERT proposed by Liu et al. [3]. RoBERTa works better than the abovedescribed models by training on more data and omitting the objective of next sentence prediction. \n\nText-to-Text Transfer Learning, as proposed by Raffel et al. in T5 [40], generalizes all NLP tasks into a unified task for easier implementation of transfer learning. XLNet, by Yang et al. [41], proposes a permutation-based training objective to model effective bidirectional contexts, which avoids the pretraining constraints of masked language models. Foundation-LLaMA (Large Language Model Meta AI) [42] has 7 and 65 billion parameters going to Meta AI. LLaMA is engineered for being highperformance with high efficiency; one order of magnitude smaller in size than other bulk models, it still achieves the highest performance.",
            "score": 0.5036339022481533,
            "section_title": "Overview of LLM Architectures and Functionalities",
            "char_start_offset": 12492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 401,
                    "end": 404,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 747,
                    "end": 750,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.115966796875
        },
        {
            "corpus_id": "265216855",
            "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification",
            "text": "Based on this, pre-trained models can learn language representation as a whole. bidirectional encoder representations from transformers (BERT) are one of the pre-trained models widely used for various NLP tasks. BERT is a model that uses the encoder architecture of the transformer and is designed to use a bidirectional representation [8]. An example of the application of BERT is research conducted by D'Sa et al. [9], by classifying toxic comments on Twitter. In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification. Another study by Nabiilah et al. [10] also used a pre-trained model with BERT architecture. This study compared several pre-trained models that had been trained with Indonesian language corpus data, namely multilingual BERT (MBERT), Indonesia BERT (IndoBERT), and Indonesia robustly optimized BERT pretraining approach (IndoRoBERTa) small. The optimal result of this study is to use IndoBERT with an F1 value of 0.88978. \n\nPrevious research also aligns with recent developments in applying NLP for text classification cases using social media data, where pre-trained models can provide model classification results with better performance in recognizing text patterns. However, the increased access to communication by various groups on social media causes the text and language conveyed to be increasingly unstructured and difficult to analyze the pattern. So, to improve the performance of the model in analyzing the meaning and context of the text conveyed, the development of pre-trained model architecture needs to be further developed. Therefore, this research performs multilabel classification of toxic comments using a BERT-based pre-trained model. The striking difference from this research is using different pre-trained models at several stages.",
            "score": 0.5036181373877056,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 416,
                    "end": 419,
                    "matchedPaperCorpusId": "213529509"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "255690182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "258291906",
            "title": "Text2Time: Transformer-based Article Time Period Prediction",
            "text": "Bert's performance was evaluated on the downstream date classification task and compared with the naive bayes approach. The classification task was performed with six classes or six decades ranging from 1960s to 2010s. Our BERT classifier was able to achieve an accuracy of 82% on classification using entire dataset. However, there were some doubts on the model performance on whether the article content years data could have resulted in this high accuracy or the length of the articles might have inflated the accuracy.So various tests where performed which have been explained below to prove that the classifier was not predicting based on the article length or the years in article data. Also, comparison was performed across different categories of articles to examine if some types of articles have high relevance in predictions. \n\nFrom the Figure 14 it can be seen the classifier predicts the articles belonging be 1960s, 1970s and 2010s more accurately than other three. Further from the correlation map shown  in Figure 15. The predictions that are wrong are mostly from nearby decades. So from this it can be observed that the articles have distinct common nouns, writing styles, and content. In Figure 15 it can be seen that the 2000s decade is mostly wrongly predicted as 2010s and 1990s. One reasonable answer to this can be due to the fact that nouns might be popular across decades which might trick the classifier into predicting the wrong decade. 1) Comparison with Baseline Approach: A comparison of the BERT results with the Naive Bayes baseline model used was made. BERT classifier outperforms the naive bayes classifier in all the categories and across all the decades. Table II shows the comparison of the naive bayes vs the BERT classifier. From the table it can clearly be seen that the BERT accuracy outperforms the accuracy of naive bayes  Table III shows the accuracy of the naive bayes classifier and BERT classifier across all the categories. It can be seen that the model almost beats the baseline by at least 10% across all the categories. This shows that the BERT is able to handle the context much better than the Naive Bayes. It should be noted that the fine tuning is 10x more expensive than performing naive Bayes classification in terms of time and computational resources. However, With these results we are able to conclude that BERT model outperforms techniques classical machine learning techniques such as Naive Bayes.",
            "score": 0.5035103260481824,
            "section_title": "D. Results",
            "char_start_offset": 14210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1473388671875
        },
        {
            "corpus_id": "259209659",
            "title": "Leveraging Symbolic Knowledge Bases for Commonsense Natural Language Inference Using Pattern Theory",
            "text": "Specifically, we compare against unsupervised such as a simple rule-based reasoning engine using ConceptNet (ConceptNet + Rules) and unsupervised versions of large language models such as GPT, GPT-2, and BERT. We also compare against weakly supervised baselines, which are models trained for textual entailment (i.e., identify entailment, neutral, and contradiction between sentence pairs) on SNLI [69] and fine-tuned for SWAG with these 3-way probabilities as features. Finally, we compare against fully supervised baselines such as fastText [24], ESIM [23], LSTM-based models, and BERT. As shown in Table V, PT+BERT outperforms all unsupervised baselines by large margins. Interestingly, we also outperform the weakly supervised baselines and early supervised baselines such as fastText and an LSTM-based model with GloVe embeddings. We offer competitive performance to other fully supervised baselines without any labeled data. \n\nHellaSWAG: Finally, we evaluate on HellaSWAG, which extends the idea of grounded commonsense natural language entailment by presenting answer choices with targeted adversarial filtering. In addition to video captions, HellaSWAG also introduces a new challenge to evaluate commonsense reasoning by framing the CNLI problem to help complete how-to articles from WikiHow, an online how-to manual. \n\nThe adversarial filtering is stepped up to a more challenging setting by using GPT-2 as a generating mechanism for alternative answer choices, while BERT is used as a strong discriminator to distinguish between the actual and generated answer choices. The resulting dataset poses a significant challenge for commonsense reasoning that requires both a deep understanding of physical interactions and social situations, in addition to broad commonsense knowledge. We summarize the results in Table VI. We report results for both BERT-base (in italics) and BERT-Large (in parentheses). Note that we only train the Base version in PT+BERT to be consistent with all other approaches.",
            "score": 0.5035063660165382,
            "section_title": "A. Quantitative Evaluation",
            "char_start_offset": 54523,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "34032948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.123779296875
        },
        {
            "corpus_id": "252111214",
            "title": "Latency Adjustable Transformer Encoder for Language Understanding",
            "text": "The experimental results of the proposed method applied to BERT-base for a wide range of tasks are presented in Table II. The proposed approach is 2.9 times faster than BERT-base, with only a 0.7% accuracy drop on the test set. Additionally, the model's inference time is up to 4.8 times faster than BERTbase, with just a 0.9% reduction in accuracy. These results demonstrate that the suggested method significantly enhances the performance and resource efficiency of Transformer-based models while maintaining minimal accuracy loss. \n\nThe table shows the experimental speedup, and the expected speedup. The inference speedup is consistent with expected speedup predicted using (7). The slight differences are due to the computational overheads, including the computational effort of the tokenizer and output classifier layers that are not considered in (7). The consistency between the experimental speedup, the expected speedup, approves the hypothesis discussed in Section III-D. \n\nTable III compares the proposed approach and the previously reported speedup methods. BERT-base model (L=12, h=12, d=768) is used as the basis for comparison. The test results, the average accuracy drop compared to BERT-base, the average number of FLOPs, and the average inference speedup are reported for each model. As shown in the table, the proposed approach achieves 2.9 times speedup with 0.73% on average accuracy drops compared to BERT-base model. \n\nCompared to other works, DistilBERT model achieves 1.7 times speedup with around 2.74% accuracy degradation on average. In DistilBERT, a pre-trained general-purpose language presentation model must be trained from scratch with significant effort. The TinyBERT model speedup is 2.0 times with 0.4% accuracy drops on average. TinyBERT uses a twostage learning framework that performs Transformer distillation at pre-training and fine-tuning phases with considerable excessive computational effort. The MobileBERT model speedup is 5.5 times with around 1.19% accuracy drop, but the starting point of this model is BERT large . The PoWER-BERT model speedup is 2.9 times with around 0.92% accuracy drop.",
            "score": 0.5032898482430488,
            "section_title": "D. Experimental Results on Language Understanding Tasks",
            "char_start_offset": 35217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147216796875
        },
        {
            "corpus_id": "258022214",
            "title": "Flood-Related Multimedia Benchmark Evaluation: Challenges, Results and a Novel GNN Approach",
            "text": "Our model is compared against baseline text classification methods (BERT, CNN, BiL-STM) and in multimodal approaches which proved great results in similar text classification tasks that was reproduced for evaluation in the corresponding dataset: \n\n\u2022 BERT [31] is based on transformer encoder architectures and has been successfully used on a variety of tasks in NLP (natural language processing). They compute vectorspace representations of natural language that are suitable for use in deep learning models. BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks. \u2022 CNN [58] classify text by working through the 1-dimensional convolving filters which are used as ngram detectors and each filter specializing in a closely-related family of ngrams. Afterwards, max-pooling over time extracts the relevant ngrams for making a decision. The rest of the network classifies the text based on this information. \u2022 BiLSTM proved to be effective with high accuracy in text classification tasks [59] compared to RNN, CNN and LSTM.",
            "score": 0.5031790497901487,
            "section_title": "Baselines",
            "char_start_offset": 30891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "139108396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1761474609375
        },
        {
            "corpus_id": "253169185",
            "title": "Towards Transfer Learning Techniques\u2014BERT, DistilBERT, BERTimbau, and DistilBERTimbau for Automatic Text Classification from Different Languages: A Case Study",
            "text": "These methods can include text classification [2][3][4], information retrieval [5,6], summarization [7,8], text clustering [9,10], and others, collectively named text mining [2,4,6]. \n\nMany works are available in the literature on text classification tasks using various neural network models. Some typical works include convolutional neural network (CNN) models [11,12], attentional models [13,14], adversarial models [3], and recurrent neural network (RNN) models [13], which particularly outperform many statistics-based models. The previously mentioned works represent text based on words, i.e., word vectors pre-trained over a large-scale corpus are usually used as the sequence features. Such vectors are usually trained via the word2vec tool [15] or Glove [16,17] algorithm based on the presumption that similar words tend to appear in similar contexts. \n\nIn recent years, to avoid specific structures and significantly decrease the parameters to be learned from scratch, as is done in the models presented above, some researchers have contributed in another direction, highlighting the pre-training models for general language and fine-tuning them to downstream tasks. Another problem with traditional NLP approaches worth mentioning is the issue of multilingualism [18]. The Open AI group (https://openai.com/, accessed on 13 July 2022) proposes the GPT (Generative Pre-trained Transformer) using a left-to-right multi-layer Transformer architecture to learn the general language presentations from a large-scale corpus to deal with the abovementioned problems [19]. Later, Google, inspired by GPT, presented a new language representation called BERT (Bidirectional Encoder Representations from Transformers) [20]. BERT is a state-of-the-art language representation model designed to pre-train deep bidirectional representations from unlabeled text and is fine-tuned using labeled text for different NLP tasks [20]. A smaller, faster, and lighter version of BERT architecture, well-known as DistilBERT, was implemented by the HuggingFace team (https://github.com/huggingface/",
            "score": 0.5029185189651448,
            "section_title": "Introduction",
            "char_start_offset": 2086,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "13854782"
                },
                {
                    "start": 82,
                    "end": 84,
                    "matchedPaperCorpusId": "81977235"
                },
                {
                    "start": 100,
                    "end": 103,
                    "matchedPaperCorpusId": "52012819"
                },
                {
                    "start": 103,
                    "end": 105,
                    "matchedPaperCorpusId": "243865577"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "3038960"
                },
                {
                    "start": 179,
                    "end": 181,
                    "matchedPaperCorpusId": "81977235"
                },
                {
                    "start": 367,
                    "end": 370,
                    "matchedPaperCorpusId": "29191669"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 763,
                    "end": 767,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "67856550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1435546875
        },
        {
            "corpus_id": "235683507",
            "title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection",
            "text": "We started our journey by performing experiments with methodology explained in Section 3.2 on the datasets described in Section 5. For initial experiments after the development of polarization index, we randomly divided the datasets into a training set (80%) and a test set (20%). \n\nWe developed models representing the perspectives of individual groups of annotators by employing the subsection 3.4 of our methodology. We created separate gold standards; one for each group based on majority voting and then trained the classifier on that group-based training set. The experiments from previous work gave us an insight how well the classifier performed with that particular group after we evaluated the models. \n\nWe employed the Bidirectional Encoder Representations from Transformers (BERT) [Devlin et al., 2018], the prediction framework for binary classification. BERT was developed by google and it is considered state-of-the-art in all transformer-based models. BERT uses a bi-directional approach to classify textual data and achieve state of the art performance in many NLP tasks when compared to other available NLP models [Yu et al., 2019]. BERT is trained on bidirectional language representations from unlabelled text from different domains obtained from Internet and Wikipedia pages by considering left and right contexts in a multi-layer architecture [Munikar et al., 2019]. Unlike other language models, BERT can be fine-tuned with just one extra output layer for various downstream tasks without depending on task-specific modifications in the model architecture. We fine-tuned the BERT model on the training sets, keeping the test sets fixed for each dataset, for a fair comparison. We used the uncased base English model provided by Google for English (uncased_L-12_H-768_A-12). For Italian, we used AlBERTo [Polignano et al., 2019], a model for Italian, pre-trained on Twitter data. AlBERTo has similar specifications to the BERT English base model. After a preliminary study, we fixed the sequence length at 128 words, batch size to 12 for English and 8 for Italian. The learning rate was set to 1e \u22125 . The prediction results were published in Akhtar et al. [2020] and are shown in Table 10.",
            "score": 0.5026669989702128,
            "section_title": "Experiments",
            "char_start_offset": 55923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 814,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1365,
                    "end": 1387,
                    "matchedPaperCorpusId": "203905846"
                },
                {
                    "start": 1826,
                    "end": 1850,
                    "matchedPaperCorpusId": "204914950"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1278076171875
        },
        {
            "corpus_id": "246738423",
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "text": "Li [17] used the application of the BERT model in text classification at the sub-word level. Baidu proposed the ERNIE (Enhanced Representation through Knowledge Integration) model in 2018 [18]. Compared with BERT, ERNIE has improved two masking strategies; the first is the phrase-based masking strategy, and the second is the entity (name, location, organization, product, etc.) based masking strategy. Although the BERT and ERNIE models can achieve positive results in various tasks in the field of natural language processing, it is difficult for scientific research teams in universities as well as in smalland medium-sized enterprises to pre-train BERT from the base model due to the large size of the model, the large amount of data, and the large number of resources required for training. In the field of computer vision, Hinton et al. [19] proposed a training method based on the \"teacher-student network idea\" in 2015. The basic principle was to transfer the features learned from the \"teacher\" network with a complex network structure and strong learning ability to the \"student\" network with a small number of parameters and weak learning ability. Sau et al. [20] used a noise-based regularizer to train the student from the teacher for a higher training efficiency in 2016. Liao et al. [21] tried to use the BERT model for distillation learning in LSTM and CNN. Sun et al. [22] proposed Patient Knowledge Distillation for the BERT Model. \n\nIn e-commerce, it is usually computationally expensive when the chatbot handles a large number of real customers online; the simple intention classification model can show a beneficial computational performance but is limited to the low classification accuracy. To improve the performance of the small-scale model, the knowledge obtained by the \"teacher\" model can guide the training of the original small-scale model to achieve better results. In this study, we develop a chatbot for after-sale service in e-commerce to help manage the high computational price issue. The pre-trained ERNIE model will distill the FastText model to increase the efficiency and maintain the classification accuracy.",
            "score": 0.502638574443934,
            "section_title": "Introduction",
            "char_start_offset": 7929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 7,
                    "matchedPaperCorpusId": "222020002"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2115478515625
        },
        {
            "corpus_id": "276742203",
            "title": "Predictive Data Selection: The Data That Predicts Is the Data That Teaches",
            "text": "Our core hypothesis is that the data on which the losses can help predict the performance well is the data that can contribute to training effectively, thus the documents' predictive strength scores as defined in Eq. 1 are our metric to select data. However, computing Eq. 1 for every document is expensive since it requires computing losses of several models on all the data, thus we follow Li et al. (2024) and only select a small seed set of documents that are later used to train a fastText classifier (Joulin et al., 2017). As shown in Figure 2, we first randomly sample a small subset of data from the pre-training corpus, which is kept separate from the data used for later training, to compute the normalized losses. Specifically, we pick the 3,000 most frequent domains (e.g., wikipedia.org) in the corpus and randomly sample 300 examples for each domain for a wide coverage over the pre-training corpora, thus obtaining 900K samples in total. Then we choose the models from the Llama 1 and 2 series (Touvron et al., 2023a;b) ranging from 7B to 65B parameters -in total 6 models are used to compute the predictive strength for the data. We do not choose various models from different families to compute the predictive strength on purpose -in our preliminary experiments, we found that models from various families introduce significant evaluation noises, for example, they are sensitive to prompts and the prompt that is good for one model may not be suitable for another. This issue is particularly salient given that we are evaluating base models that are generally more sensitive to prompts than instruction-tuned models. Such noises would dramatically affect the computation of predictive strength, and thus we found that data selected this way could not outperform a random data selection baseline in our initial trials. This design differs from Thrush et al. (2024) which employs diverse models to estimate perplexity correlation. Such distinction probably explains why perplexity correlation fails to improve random data selection baselines significantly on a high-quality, pre-filtered data pool, as shown in the concurrent version of their paper. We run inference of these models on the 900K documents to obtain the loss ranks of these models on each document respectively.",
            "score": 0.5024640925146585,
            "section_title": "The Overall Framework",
            "char_start_offset": 9540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 527,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.179931640625
        },
        {
            "corpus_id": "231847008",
            "title": "Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models",
            "text": "Tables 4 and 5 present W2D and D2W results for BERT, RoBERTa and GPT-2 along with fastText and random baselines. Language models perform clearly better than both baselines. Larger models perform generally better than smaller ones and RoBERTa consistently outperforms BERT. This might be an indication for the correlation between performance on WDLAMPro and downstream performance. However, further investigation is necessary to show the correlation more clearly. For W2D, best performance is achieved by GPT-2 xl for nouns (47.3 P@1, 0.81 RS) and by RoBERTa large for verbs (50.8 P@1, 0.84 RS). Performance on D2W is much lower than for W2D for all models. For nouns, RoBERTa large and GPT-2 xl perform similarly (28.8 and 29.8 P@1, 0.70 and 0.73 RS) while RoBERTa large achieves the best results for verbs (38.6 P@1, 0.80 RS). Poor performance on D2W compared to W2D might be due to language models' ability to distinguish different definitions better than individual words since definitions are more informative than individual words. Overall GPT-2 models perform better than masked language models (with the exception of Roberta large for verbs), despite using a single pattern as opposed to the multiple patterns used by masked language models. This might indicate that the ALM objective is better at learning word meaning than the MLM objective.\n\nTo investigate the effect of frequency, we stratify words into rare (fewer than 10 occurrences), medium (10 to 99 occurrences) and frequent (100 or more occurrences), based on occurrences in WWC 5 (Westbury Wikipedia Corpus, Shaoul (2010)), 5 Targets that have more than 3 tokens (based on NLTK tokenization) are taken as rare without counting.",
            "score": 0.5023318451711505,
            "section_title": "Results",
            "char_start_offset": 9773,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08929443359375
        },
        {
            "corpus_id": "267724195",
            "title": "Categorization of tweets for damages: infrastructure and human damage assessment using fine-tuned BERT model",
            "text": "The BERT model was introduced by Devlin et al. (2018) at Google Lab and it has proven its significance for a variety of text-mining tasks in several application domains (Malik, Imran & Mamdouh, 2023).The benefits of BERT include faster development, automated feature generation, reduced data requirements, and improved performance.It has two architectures and we are interested in fine-tuning the pre-trained BERT model for damage assessment tweet identification task for binary as well as multi-class classification.The BERT model is pre-trained on a large corpus of English data in a self-supervised fashion and uses the context of language in both directions.Furthermore, BERT was pre-trained on next-sentence prediction and masked language modeling objectives.\n\nTo fine-tune the BERT base uncased model (https://huggingface.co/bert-base-uncased), some important steps are required.After applying the above-mentioned pre-processing steps, data transformation and training classifier steps are executed.We have chosen 64 and 128 sequence lengths because a maximum of 280 characters are allowed in a tweet and 128 sequence length is enough to handle the most lengthy tweets.Therefore, all tweets are padded up to the length of 64 and 128.After that, attention masks are added to locate real and padded tokens.The vector output of attention masks is then fed to the BERT model and fine-tuning step is performed.\n\nBERT classifier training: There are seven disasters in the CrisisMMD dataset.For training and validation of BERT classifiers, we split each disaster into 80-20 ratios using the stratified sampling approach.After that, we took 80% data from each disaster and combined them to make the training dataset.The remaining 20% of data from each disaster is used for testing the BERT classifier on that specific disaster data.Furthermore, the combined 80% data is further divided into a 90-10 split, in which 90% is used for training and 10% is used for validation.We utilize the BERT base model which contains 12 transfer layers, 12 attention heads, and 768 hidden layers.All entities (class labels, token ids, and attention masks) are combined into one set.",
            "score": 0.5019648216153013,
            "section_title": "Fine-tunning BERT",
            "char_start_offset": 14551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 331
                },
                {
                    "start": 331,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 764
                },
                {
                    "start": 766,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1411
                },
                {
                    "start": 1413,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 53,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 169,
                    "end": 199,
                    "matchedPaperCorpusId": "257060193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.110107421875
        },
        {
            "corpus_id": "267657481",
            "title": "ScamSpot: Fighting Financial Fraud in Instagram Comments",
            "text": "Despite the adjustment of seed and temperature parameters (Table 5), the results are not deterministic. Three test runs are made for each model and the results can be found in Table 5. The best results are documented in Table 1. \n\n\u2022 Transformer Models: As a representative, we select BERT (\"bert-base-cased\") and fine-tune it based on the annotated data mentioned in Section 2. Its capability to understand context makes it a strong candidate in our selection. \n\nIn this study, we adopt a zero-shot approach with LLMs like GPT-3 and GPT-4, contrasting them with the fine-tuned BERT model. This methodology is chosen to demonstrate the practical usability of general-purpose LLMs in their standard configuration. While models like GPT-3 and GPT-4 show proficiency in general tasks, our findings align with those of Yu et al. (2023), when illustrating their limitations in specific tasks compared to fine-tuned models. This highlights the necessity of model selection tailored to task specificity and resource availability. \n\nDespite the hype around large language models, the results in Table 1 show that even established models like a fine-tuned BERT model can drastically outperform newer models like GPT-4 or GPT-3. While both BERT and LLMs are transformer-based, our research also demonstrates that a fine-tuned BERT model is more successful in specific tasks, which is also shown by Yu et al. (2023). \n\nThis leads us to the decision to select a finetuned BERT model for ScamSpot. The reasons also include: \n\n\u2022 Promising Results: Tests show that the finetuned BERT model far outperforms the other models in detecting fraudulent Instagram comments (Table 1). \n\n\u2022 Stability and Predictability: Our fine-tuned BERT model demonstrates stable and predictable performance, a crucial factor for consistent user experience compared to the tested LLMs (Table 5). \n\n\u2022 spam messages, ensuring our system remains effective in the face of evolving spam tactics. \n\nIn conclusion, our comprehensive testing and evaluation process leads us to choose a fine-tuned BERT model. The configuration for fine-tuning the model can be found in Table 3.",
            "score": 0.5018451611243555,
            "section_title": "Dataset & Data Annotation Study",
            "char_start_offset": 7372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 460
                },
                {
                    "start": 463,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2130
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "270703323",
            "title": "Context-augmented Retrieval: A Novel Framework for Fast Information Retrieval based Response Generation using Large Language Model",
            "text": "This capability is particularly valuable in scenarios where obtaining a large labelled dataset is impractical or time-consuming.For instance, a zero-shot learning model can classify a piece of text that has never been explicitly trained on by using natural language prompts.For example, to classify movie reviews as positive or negative, a prompt like \"Classify the following review as positive or negative: [review text]\" can be used.The model uses its understanding of language and context to generate the correct classification based on the prompt.\n\nThe aforementioned solutions involve LLMs with a high number of parameters, which increases processing time.Therefore, we introduce a query classifier into the existing RAG workflow.\n\nTraditional machine learning models like Random Forest, and Decision Trees have many strengths and can perform various tasks effectively [27] [33] [25] [21] [18].They excel at handling structured data, such as numbers and categories, making them ideal for tasks like predicting house prices, detecting spam emails, and recommending products.These models can learn patterns from the data they are trained on, allowing them to make accurate predictions and decisions.However, they often face difficulties when dealing with natural language.We can improve their accuracy by integrating advanced NLP techniques [23].\n\nTraditional machine learning algorithms are also known for their interpretability and transparency.Models like Decision Trees provide clear insights into how decisions are made, making them valuable in domains where understanding the reasoning behind predictions is crucial, such as in medical diagnosis or financial risk assessment.Moreover, ensemble methods such as Random Forest combine multiple decision trees to improve robustness and generalization, reducing overfitting and enhancing performance on complex datasets with noisy or incomplete information.\n\nTransformer models like BERT have gained widespread popularity due to their ability to model bidirectional dependencies in language data effectively [11].By pre-training on large databases of text data using unsupervised learning objectives like masked language modelling and next-sentence prediction, BERT learns contextual representations of words and sentences that capture their meanings and relationships within the surrounding context.However, BERT's computational complexity and large memory footprint make it challenging to fine-tune specific data [36].\n\nTo address this issue, we used a distilled version of BERT, DistilBERT.",
            "score": 0.5015852539505639,
            "section_title": "Figure 1: RAG Workflow",
            "char_start_offset": 4188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 551
                },
                {
                    "start": 553,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 735
                },
                {
                    "start": 737,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1349
                },
                {
                    "start": 1351,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1911
                },
                {
                    "start": 1913,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2354
                },
                {
                    "start": 2354,
                    "end": 2474
                },
                {
                    "start": 2476,
                    "end": 2547
                }
            ],
            "ref_mentions": [
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "258458433"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "7311285"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "2427083"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "220961531"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11065673828125
        },
        {
            "corpus_id": "252666305",
            "title": "Using Transformer models for gender attribution in Polish",
            "text": "1) FastText: FastText [15] is a shallow neural network library created for fast text classification model training and evaluation. We used a supervised setting with hyperparameter tuning, the word embeddings were initialized randomly. The best result was obtained with wordNgrams set to 2, word dimension set to 156, and context size window set to 5. \n\n2) LSTM: Long Short Term Memory Networks [13] were used to obtain a state-of-the-art results on most NLP tasks before the era of Transformer language models [7]. In our tasks, for bidirectional LSTM, SentencePiece [19] tokenization performs better than word-level lowercase tokenization. Vocab size 50k was used with randomly initialized embeddings of size 100. We tried embedding size 300, but resulted in slightly worse classifier quality. We used one layer of 256 units, trained with Adam [16] optimizer with learning rate 0.001. The batch size used for training was 400 and sequences were trimmed and padded to 256 tokens. \n\n3) Transformer: In the last time Transformer [26] and its modification like BERT [7], RoBERTa [20] or XLM-R [4] achieve state-of-the-art in the benchmarks such as GLUE [29] or SuperGLUE [28] benchmark. Most often used bidirectional Transformers are pre-trained on huge amounts of monolingual data in the Masked Language Model (MLM) process, where the model learns a bidirectional representation of tokens. Next, pre-trained models are finetuned to the specific task. This process reduces the time to train a new model from scratch and can be easily adapted to other tasks. In our case, the downstream task is classification, where the model uses a special token ([CLS], classification token), which represents the whole sentence and helps achieve better results. \n\nWe train self-contained classifier based on the RoBERTa model in two ways: with pre-training and without pre-training (train classifier from the scratch) stage. We only used the data that was available in the HSSS challenge to avoid any data leaks in the other data sets.",
            "score": 0.5013837243876351,
            "section_title": "C. Neural Methods (self-contained)",
            "char_start_offset": 7626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "52051958"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16455078125
        },
        {
            "corpus_id": "269741170",
            "title": "Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing Clinical and Social Effects of Substance Use Derived from Social Media",
            "text": "Transformer-based approaches that use large pre-trained language models achieve state-of-the-art F 1scores for NER tasks when large annotated data available.Due to the sparsity of annotated samples in the Reddit-Impacts dataset, we choose to fine-tune and evaluate two popular transformer-based models as the baseline: BERT [10] and RoBERTa [11].Building on prior research in few-shot learning, we also report performances for DANN (Data Augmentation with Nearest Neighbor classifier) [12], which demonstrated promising performance in few-shot scenarios.\n\nGiven the remarkable success of LLMs in few-shot learning scenarios, we also explore the viability of employing GPT-3.5 for the extraction of named entities in a one-shot setting (by providing one example of input data in the prompt).This evaluation aims to provide insights into the performance of GPT-3.5, further enriching our benchmarking of this dataset.\n\nThe following is an outline of the models we used: 1. BERT [10]: A foundational Transformer-based model, widely recognized for its pre-training on a large corpus of text from books and Wikipedia.\n\n2. RoBERTa [11]: Transformer-based model popular for its training on big batches and long sequences.\n\n3. DANN [12]: A few-shot learning method for NER that uses a data augmentation module combined with a nearest neighbor classifier to solve data sparsity problems.\n\n4. GPT-3.5:An advanced iteration of the Generative Pre-trained Transformer series, known for its enhanced language understanding and generation capabilities, trained on a diverse range of internet text.",
            "score": 0.5010874905791429,
            "section_title": "Models",
            "char_start_offset": 8686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 554
                },
                {
                    "start": 556,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 915
                },
                {
                    "start": 917,
                    "end": 1112
                },
                {
                    "start": 1114,
                    "end": 1214
                },
                {
                    "start": 1216,
                    "end": 1378
                },
                {
                    "start": 1380,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "267211248"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "267211248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1563720703125
        },
        {
            "corpus_id": "235254631",
            "title": "Sentiment analysis in tweets: an assessment study from classical to modern word representation models",
            "text": "Similarly, regarding all classifiers, Table 14 shows that BERTweet outperformed BERT and RoBERTa by a significant difference in terms of the total number of wins for both accuracy and F 1 -macro. Next, we present an overall analysis of using BERT, RoBERTa, and BERTweet models to train each one of the five classification algorithms, examining them as pairs {language model, classifier }. Table 15 presents the average rank position across all 15 possible combinations (3 language models \u00d7 5 classification algorithms), from best to worst, as explained in Section 4. We can observe that BERTweet combined with LR, MLP, and SVM classifiers achieved the best overall performances for both accuracy and F 1 -macro. Conversely, using RF to train the Transformer-based embeddings seems to harm the performance of the models. \n\nTables 16 and 17 show a summary of the results for each model and classifier, respectively, from best to worst, in terms of the average rank position. From Table 16, we can see that BERTweet achieved the best overall classification effectiveness and was ranked first. Also, RoBERTa and BERT achieved  accuracy and in 18 out of the 22 datasets in terms of F 1 -macro. These results may confirm that Twitter sentiment classification benefits most from contextualized language models trained from scratch on Twitter data. Unlike BERT and RoBERTa, which were trained on traditional English texts, BERTweet was trained on a huge amount of 850M tweets. This fact may have helped BERTweet on learning the specificities of tweets, such as their morphological and semantic characteristics. For a better understanding of the results, we present an analysis of the difference between the vocabulary embedded in the assessed models. For this purpose, Table 19 highlights the number of tokens shared between BERT, RoBERTa, and BERTweet. In other words, we show the amount of tokens (in %) embedded in the models presented in each row that are also included in the models presented in each column, i.e., the intersection between their vocabularies. For example, regarding BERT (first row), we can see that 61% of its tokens can be found on RoBERTa (second column). The information below each model name in the columns refers to their vocabulary size (number of embedded tokens).",
            "score": 0.5007618775694278,
            "section_title": "Evaluation of the Transformer-based text representations",
            "char_start_offset": 47320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 819
                },
                {
                    "start": 822,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2286
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13671875
        },
        {
            "corpus_id": "254563733",
            "title": "Domain Adaptation of Transformer-Based Models Using Unlabeled Data for Relevance and Polarity Classification of German Customer Feedback",
            "text": "XLM RoBERTa outperforms previous multilingual approaches by incorporating more training data and languages, including low-resource languages. Although multiple XLM RoBERTa models are available, only the large model was considered for this work. \n\nAnother tested model is ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) [15]. This model introduced a different approach to language pre-training, where it uses another task called replaced token detection (RTD). Instead of masking the input by replacing some words with the token \"[MASK]\" as in BERT, ELECTRA corrupts the input tokens by replacing them with synthetically generated tokens. It then trains the model to distinguish between \"real\" and \"fake\" input data. This is achieved using a discriminator that classifies the tokens and a generator which provides plausible fake tokens. Both transformer-based components are trained jointly. In addition to the two BERT models, Chan et al. [43] also released two German ELECTRA models: a base model and a large one. In their benchmarking, the large model reached state-of-the-art performance in three downstream tasks. \n\nTo adapt the language models to the task domain, multiple experiments were conducted on GBERT Large . These experiments were based on continuing pre-training using masked language modeling on the collected tweets and on combinations of the unlabeled task data and parts of the unlabeled tweets. Additional experiments were conducted by expanding the vocabulary of the pre-trained model with around 20k new words from the unlabeled tweets, where the embeddings of the new tokens are initialized randomly. Moreover, for some experiments instead of masking 15 % of the text during the masked language modeling, 30 % was masked, since based on a recent study [49] masking up more than 15 % of the tokens can be beneficial in some cases. \n\nFor the baseline classifier, the fastText 5 (version 0.9.1) was used. The training and fine-tuning of transformer-based models were conducted using the Transformers [50] library by HuggingFace 6 . For all models, the same hyperparameters were used (see Table 11).",
            "score": 0.5004735760989397,
            "section_title": "System Description",
            "char_start_offset": 22751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 363,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "224814107"
                },
                {
                    "start": 2060,
                    "end": 2064,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2178955078125
        },
        {
            "corpus_id": "259274808",
            "title": "Taqyim: Evaluating Arabic NLP Tasks Using ChatGPT Models",
            "text": "Large Language Models. Several language models have been proposed recently. One of the earliest pre-trained language models is ELMo which was proposed to model the word context (Peters et al., 2018). ELMo learns the word context by pre-training a two-layer bidirectional LSTM network on large data and fine-tuning it on downstream tasks. BERT followed this learning strategy with a Transformer model pre-trained on large datasets (Devlin et al., 2019). The performance of BERT outperformed other models on several downstream tasks. This learning paradigm motivated researchers to propose either new architectures (e.g., BART (Lewis et al., 2019) and GPT-2 (Radford et al., 2019)) or enhanced pre-training techniques (Liu et al., 2019;Sanh et al., 2021;Wang et al., 2022). Scaling language models in terms of model size or data used for model pre-training has shown its effectiveness in several downstream tasks (Zhao et al., 2023). This led to the introduction of the \"large language models (LLM)\" term. These models are trained on large datasets and usually have billions of parameters. Such LLMs showed a better performance compared with the smaller models with similar architectures and pre-training tasks (e.g., GPT-3 (Brown et al., 2020) vs GPT-2). Recently, a significant number of LLMs have been introduced, such as GPT-3, LLaMA (Touvron et al., 2023), PaLM (Chowdhery et al., 2022), BLOOM (Muennighoff et al., 2022), andChinchilla (Hoffmann et al., 2022). ChatGPT2 is one of these LLMs that was developed based on the GPT model series (GPT-3.5 and GPT-4) and showed a powerful performance with dialogue tasks.",
            "score": 0.5000560950277367,
            "section_title": "Related Work",
            "char_start_offset": 3841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 198,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 656,
                    "end": 678,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 752,
                    "end": 770,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14208984375
        },
        {
            "corpus_id": "233189561",
            "title": "Implicitly Abusive Comparisons \u2013 A New Dataset and Linguistic Analysis",
            "text": "As a supervised classifier, we chose BERT-Large (Devlin et al., 2019). We initially experimented with two versions: one in which we fine-tune the model by adding a layer on top of the pre-trained model and a SVM (Joachims, 1999) that is trained on the BERT embeddings of the final layer. Since we did not measure any statistically significant difference between these models, we decided in favor of SVM due to its simplicity. We carry out a 5fold cross validation. The folds comprise mutually exclusive patterns (Table 1). Thus test instances always comprise patterns not observed in the training data. We consider this the most difficult and realistic scenario. We report macro-average precision, recall and F1-score. (The supplementary notes contain more details regarding all classifiers of our experiments.) \n\nAs baselines, we consider a majority classifier, a random classifier and two classifiers trained on fastText: one without and one with pre-trained embeddings (Common Crawl). As an upper bound we also provide a human baseline in which we randomly sampled the judgment of one individual annotator from the crowdsourced annotation. This upper bound may notably differ from the gold standard label since the latter benefited from being calculated from the majority of 5 annotators. \n\nIn order to demonstrate the importance of cleaning/debiasing the dataset and show that, otherwise classification performance will be unrealistically high, we also train a classifier on a biased comparison dataset. For that, we sampled a set of the identical size from the original data we collected via crowdsourcing ( \u00a73.2) as our final debiased dataset (i.e. 1000 comparisons) but skipped the data cleaning step, particularly the steps on balancing the pattern distribution and removing patterns that are highly skewed towards either of the two classes (Table 2). We also arranged the folds at random, so that patterns in the test data could also be observed in the training data. Thus a classifier could benefit from memorizing biased patterns. \n\nTable 5 shows the performance of the different classifiers. FastText strongly benefits from the pretrained embeddings and already outperforms the other baselines by a large degree. BERT outperforms fastText.",
            "score": 0.5000234060481228,
            "section_title": "Classification Performance",
            "char_start_offset": 23452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 69,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 212,
                    "end": 228,
                    "matchedPaperCorpusId": "61116019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43994140625
        },
        {
            "corpus_id": "272464146",
            "title": "AnyMatch - Efficient Zero-Shot Entity Matching with a Small Language Model",
            "text": "Finally, we conduct an ablation study for AnyMatch in order to validate our design decisions from Section 3. For that, we remove and/or replace different components of our model and show that this removal and replacement results in a performance decrease. We evaluate the resulting model variants analogously to Section 6.1, where we measure the average F1 score over the nine benchmark datasets in a zero-shot setting. We summarise the tested variants together with the corresponding results for the individual datasets in Table 5. In addition, we report the performance delta (the reduction in average F1 score) for all tested variants, in comparison to the proposed design of AnyMatch. \n\n6.3.1 Choice of base model. The goal of our first experiment is to validate the choice of the decoder-only language model GPT-2 [27] as base model for AnyMatch. \n\nExperimental setup. We evaluate the impact of replacing GPT-2 in AnyMatch with different similarly sized alternative models. In particular, we evaluate Google's T5 [28] model with an encoderdecoder architecture and the encoder-only model Bert [7], which is for example also used by [19]. \n\nResults and discussion. We find that both alternative models result in a decrease of the overall F1 score, as detailed in Table 5: using T5 results in a decrease of 2.38%, while using Bert leads to the drastic decrease of 9.04% in average F1 score. The performance loss observed when switching to the T5 model likely stems from its better suitability for sequence-to-sequence tasks [28], whereas we treat our task as sequence classification and have specifically tailored the prompt design for this purpose. Moreover, we attribute the significant decrease in predictive quality when using Bert to the following factors: Bert encodes the input into a vectorised representation, to which a prediction head is subsequently appended for making predictions. However, in our approach, we incorporate a task description into the input sequence, accounting for roughly 5% of the total input, which potentially negatively influences the downstream classification. Note while using a different serialization method for the Bert model might improve performance, this is not the focus of this ablation study. 6.3.2 Choice of serialization format.",
            "score": 0.4997321083173504,
            "section_title": "Ablation Study",
            "char_start_offset": 48829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0880126953125
        },
        {
            "corpus_id": "224802956",
            "title": "BERT2DNN: BERT Distillation with Massive Unlabeled Data for Online E-Commerce Search",
            "text": "BERT is a neural network model developed by Google. The release of BERT marks a singular breakthrough in the field of NLP because of its dramatic improvement over earlier states of art as well as its fine-tuning flexibility in a multitude of downstream tasks. BERT's advent has also inspired many recent NLP architectures, training approaches and language models, such as Google's Transformer-XL [14], XLNet [15], Baidu's ERNIE [5], Facebook's RoBERTa [16], etc.\n\nA key factor in the success of BERT is the powerful role of Transformer structure, but Transformer itself has two defects, i.e., hard to capture long-term dependency and context fragmentation. Dai et al. [14] propose Transformer-XL structure which applies recurrence mechanism and relative positional encoding to address both of limitations. XLNet [15] integrates ideas from Transformer-XL into pre-training and achieves improved effects compared to BERT. Based on original BERT, ERNIE [5] designs a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from knowledge graphs to complete the alignments. RoBERTa [16] changes BERT in pre-training strategies by introducing dynamic masking and full-sentences so that it leads to better downstream task performance. ColBERT [17] applied BERT to encode query and item separately, before applying a late interaction function to estimate their relevance.",
            "score": 0.4996468984260869,
            "section_title": "C. BERT and Related Models",
            "char_start_offset": 8316,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 428,
                    "end": 431,
                    "matchedPaperCorpusId": "158046772"
                },
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 950,
                    "end": 953,
                    "matchedPaperCorpusId": "158046772"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15576171875
        },
        {
            "corpus_id": "225639057",
            "title": "Ablations over transformer models for biomedical relationship extraction",
            "text": "Originally, we planned to conduct an analysis comparing a wide range of transformer architectures. However, our preliminary investigations suggested that many were too cumbersome to work with, either in terms of compute required, the quality of the pretrained model or the maturity of the codebase. To this end, we restricted our analysis to the pretrained models BERT Base, BioBERT 1.1, RoBERTa base and RoBERTa large, as described in Table 1. Our principal question compares the evaluation performance of BERT Base, BioBERT, and RoBERTa base, as models of approximately equal parameter counts. However, we additionally decided to include RoBERTa large to explore any potential benefits from using a larger model with a higher quality pretraining regime (based upon General Language Understanding Evaluation benchmark results 15 ). \n\nAll experiments were conducted with the HuggingFace Transformers implementations, version 2.4.1 16 Classification heads Pretrained models are frequently employed in classification tasks, wherein a linear layer is constructed on top of the final layer. Recently, some modifications of this approach have been proposed, to combine specific entity information into the classification layer, to support relationship classification tasks. Wu and He 10 suggested averaging the token pieces representing each entity, and concatenating the output with the sentence vector before applying a fully connected feed forward layer, giving rise to the RBERT classification head and setting a new benchmark in the Semeval 2010 Task 8 dataset. In this work, we compare both the simple linear layer classification head and the RBERT head. \n\nPreprocessing RE is commonly construed as a sentence classification task, wherein the label assigned to the relationship between two entities in a sentence are instead assigned to the sentence. However, such an approach can be problematic; for instance, if there are more than two entities in a sentence, and/or more than two relationships (a common occurrence in biomedical text), leading to a situation where the same sentence can yield two conflicting labels. \n\nTo mitigate this, various strategies have been used, such as substituting the entities of interest with nominal placeholder tokens, such that all strings seen by a classifier are unique and creating the possibility for a classifier to learn the syntactic importance of the placeholder tokens with regard to the relationship that binds them 17 .",
            "score": 0.49951926048235956,
            "section_title": "Pretrained model selection",
            "char_start_offset": 7417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2120
                },
                {
                    "start": 2123,
                    "end": 2467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15380859375
        },
        {
            "corpus_id": "268364106",
            "title": "Impact of Noisy Supervision in Foundation Model Learning",
            "text": "Setup. We evaluate BERT-L [26], RoBERTa-L [27], and GPT-2 [29] on the GLUE [156] and GLUE-X [157] benchmarks for ID and OOD performance. The details of each model and are shown in Table 3. BERT-L and RoBERTa-L are pre-trained on the combination of the BooksCorpus data [158] and English Wikipedia with uncompressed raw text. It is found that the raw pre-training data of BERT can be reduced from 16GB to 12GB with data cleaning [54]. GPT-2 is pre-trained on WebText [29], a scraped web dataset from Common Crawl that contains low-quality raw texts. We also leverage OpenAI's API service \"text-ada-002\". We cannot use larger and more recent language models such as LLaMA [30], since they are unable to fit in a single V100 GPU and we are unsure whether GLUE is in their training data. We use the AdamW optimizer and set the learning rate for LP as 0.01 and for others as 0.001 for all the experiments of language models. For LP, we do not use weight decay, and for others we use a weight decay of 0.0001. All tuning methods are trained for 10 epochs with a linear scheduler. \n\nResults. In Table 5, NMTune consistently achieves the best generalization performance. It presents superior performance gain, especially on OOD tasks of GLUE-X. On the \"text-ada-002\" model with only API access, it also outperforms LP significantly, demonstrating the necessity of mitigating the effect of noise for better generalization. Interestingly, on the ID tasks of GLUE, we also observe a smaller gap of MLP tuning method to LP even with more parameters, showing that the MLP alone may not mitigate the influence of noisy data in pre-training. \n\nResults. For ID tasks, we report the average accuracy for noise related and noise unrelated tasks, as shown in Figure 10a. One can still observe that the slight asymmetric pre-training noise can benefit LP ID performance for unrelated tasks.",
            "score": 0.4994259231679255,
            "section_title": "Language Models and Datasets",
            "char_start_offset": 48627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 75,
                    "end": 80,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 92,
                    "end": 97,
                    "matchedPaperCorpusId": "253523094"
                },
                {
                    "start": 269,
                    "end": 274,
                    "matchedPaperCorpusId": "6866988"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11029052734375
        },
        {
            "corpus_id": "257505129",
            "title": "Detection of Abuse in Financial Transaction Descriptions Using Machine Learning",
            "text": "[12] leveraged machine learning to detect targeted vs untargeted offensive language. This was done by creating a threelevel annotation schema, corresponding to three subtasks. The first Subtask A focussed on purely the language in a dataset, classifying it as either offensive or non offensive. Subtask B further classified the data as targeted or untargeted, i.e. general offensive language or hate speech, and Subtask C classifies whether the hate speech was targeted at an individual or a group. \n\nOther techniques to detect abuse have leveraged systems based on pre-trained language models such as RoBERTa and BERT [17], which have reached new state-of-the-art performances on numerous tasks [18]. In [19], a BERT model finetuned with binary cross-entropy loss was used to identify abusive language in Twitter Hatespeech and Wikipedia datasets. BERT embedded models outperformed other embeddings such as fastText, TextCNN and TextCNN + Character n-grams. One issue that was found with pre-trained models is that they are trained on general datasets, so they have limitations on domain-specific language tasks. Re-training pre-trained models on domain-specific datasets is a popular method to address this as seen in [20]. This is especially useful contexts such as abusive language detection where there is not enough data to train a BERT-like model from scratch. Their model, 'TweetBERT', was re-trained on a Twitter-based corpus and outperformed other BERT based models when analysing Twitter content. \n\nPaper [21] outlined another method to improve BERT based models in order to detect instances of cyberbullying and harmful speech on Australian-based Twitter data. This was done through appending additional features onto BERT as special tokens. The features included emoji paths, metadata such as user information (e.g. age, gender, number of posts), data on their network (e.g. number of follower and friends) and their power (followers/friends ratio). Results showed that BERT with the extra tokens (BERT + emoji + network + power) yielded the most accuracy.",
            "score": 0.4992679868098696,
            "section_title": "III. LITERATURE REVIEW",
            "char_start_offset": 10413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "209088210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3681640625
        },
        {
            "corpus_id": "263799985",
            "title": "Emergency entity relationship extraction for water diversion project based on pre-trained model and multi-featured graph convolutional network",
            "text": "Pretrained language models aim to acquire extensive linguistic knowledge through self-supervised learning using text corpora [47]. Early iterations of pre-trained language models predominantly relied on n-gram and rule-based approaches. In the literature [48], the Global Vectors model was proposed, which accomplishes pre-training by incorporating both word co-occurrence statistics and global semantic information. In the literature [49], the Word Vector model was proposed, which achieves pre-training by representing words as continuous vectors through self-supervised learning. \n\nWith the advancement of deep learning, the literature [50] introduced the Transformer model, which employs self-attention mechanisms for sequence modeling. The Transformer model effectively captures long-range dependencies in text through its self-attention mechanism, leading to significant performance improvements in machine translation tasks. Subsequently, the literature [35] introduced the BERT model, building upon the foundation of Transformer. The BERT model employs a strategy of pre-training and fine-tuning, conducting extensive pre-training on large-scale unsupervised data and subsequently fine-tuning on downstream tasks. The groundbreaking innovation of the BERT model has yielded exceptional results across multiple natural language processing tasks, sparking widespread scholarly attention towards pre-trained language models.",
            "score": 0.4992449566403634,
            "section_title": "Pre-trained language models",
            "char_start_offset": 7016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "227238757"
                },
                {
                    "start": 255,
                    "end": 259,
                    "matchedPaperCorpusId": "67789476"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "232770534"
                },
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "232076027"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "245340702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0966796875
        },
        {
            "corpus_id": "261140832",
            "title": "Unveiling Vaccine Hesitancy on Twitter: Analyzing Trends and Reasons during the Emergence of COVID-19 Delta and Omicron Variants",
            "text": "This action is needed as the use of an unbalanced dataset for training could adversely affect the performance of the considered classifiers. The next step is represented by pre-processing, through which the user mentions, emails, and links are removed, the hashtags are unpacked, the emoticons are replaced by their corresponding words, the elongated words are adjusted, minor spelling mistakes are corrected, and all words are spelled using lowercase. These pre-processing actions are needed, as it has been proven that they are crucial in achieving results that are as close as possible to the users' opinions [3]. To this extent, the ekphrasis library, the Natural Language Toolkit (NLTK) library, and the \"re\" Python module are used [36,[56][57][58]. Next, the text is represented as numbers to ensure that it can be handled by the classification algorithms. For classical machine learning algorithms, the representation is made through the use of the n-gram language model. Additionally, for the algorithms that rely on the word's frequency, the term frequency-inverse document frequency (TF-IDF) is investigated for increasing their performance. \n\nAfterwards, for the classifiers training, a relatively large existing balanced annotated dataset that has been extracted in previous studies based on the same keywords is used. To this set, the pre-processing and representation steps presented above are applied. Once these two steps are fulfilled, the classifiers are trained on the resulting dataset and tested on the dataset that has been annotated in this study. A series of both classical machine learning and deep learning classifiers are considered for training: Multinomial Naive Bayes (MNB) [59], Random Forest (RF) [60], Support Vector Machine (SVM) [61], Bidirectional Encoder Representations from Transformers (BERT) [62], Robustly Optimized BERT Pretraining Approach (RoBERTa) [63], and A Lite BERT (ALBERT) [64]. The performance of the considered classifiers is measured using four metrics [3]:",
            "score": 0.49902798873837995,
            "section_title": "Dataset Collection",
            "char_start_offset": 14657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "53116896"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "232372958"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "26538892"
                },
                {
                    "start": 749,
                    "end": 753,
                    "matchedPaperCorpusId": "257440982"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "2345347"
                },
                {
                    "start": 1833,
                    "end": 1837,
                    "matchedPaperCorpusId": "226096901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048858642578125
        },
        {
            "corpus_id": "255775280",
            "title": "Linking a Hypothesis Network From the Domain of Invasion Biology to a Corpus of Scientific Abstracts: The INAS Dataset",
            "text": "Table 2 displays the classification results in terms of the macro F1 score for both the single-label and the multi-label classification setting. \n\nNotably, the naive Bayes classifier performs reasonably well and even outperforms the standard BERT classifiers, indicating that simple word frequency statistics provide significant information about the correct label. An analysis of the naive Bayes classifier weights revealed that hypothesisspecific concepts, as well as parts of the hypothesis names, were strong indicators for the specific classes, but also some species and country names that mostly appear in the context of specific hypotheses were used as a basis for the classification. The advantage of the naive Bayes classifier compared to the BERT classifiers might originate in the fact that many domain-specific terms might be unknown to the BERT models and the small training set might not be enough to fully learn these new concepts. \n\nThe classifiers based on variants of BERT that are adapted to texts from the biomedical domain consistently outperformed the naive Bayes classifier, which is consistent with earlier results that show that in-domain fine-tuning generally leads to improved performance (Gururangan et al., 2020). Notably, especially the smaller BERT base models show better performance as well as reduced variance, making them the best performing models in our study. We also observe that the ability to do multi-label predictions generally does not yield an improvement, which can be explained by the small number of cases where multi-label prediction is necessary. \n\nEven though the BioBERT and PubMedBERT models show increased performance compared to the naive Bayes classifier, the difference appears to be moderate considering the large difference in complexity. All BERT models should be able to process the same word frequency information as the naive Bayes classifier, meaning that their ability to combine the information from different words and sentences is only responsible for a 7% performance increase. We believe that this indicates that the BERT classifiers are not able to understand the full semantic content of hypothesis statements, especially if they are only made implicitly. Instead, the increase in performance might simply be caused by the classifier's ability to detect slightly more complex patterns than unigrams (e.g., n-grams) and by its ability to nonlinearly combine the information about the presence of these still simple patterns.",
            "score": 0.49873922329789044,
            "section_title": "Results",
            "char_start_offset": 27443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2495
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "268063321",
            "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review",
            "text": "LLMs, such as BERT, GPT, Text-To-Text Transfer Transformer (T5) [45], and Llama, are versatile in NLP. BERT is an encoder-only model with bidirectional attention, GPT employs a transformer decoder for one-directional processing, and T5 transforms NLP problems into text generation tasks. Recent LLMs like GPT-3 [4], Language Model for Dialogue Applications(LaMDA) [52], Pathways Language Model (PaLM) [6], and Vicuna excel in understanding human-like textual knowledge, employing In-Context Learning (ICL) [12] for context-based responses.",
            "score": 0.4987383467436057,
            "section_title": "Large Language Models (LLMs)",
            "char_start_offset": 2535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1177978515625
        },
        {
            "corpus_id": "255569888",
            "title": "API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model",
            "text": "Training. We choose BERT [24] as a relation classifier because its pre-training task (i.e., Next Sentence Prediction) is consistent with our task, both of which are classification tasks. However, the implementation of relation classifier is not limited to BERT, we can also use TextCNN [27] and FastText [28]. In our current implementation, we use the BERT-base classifier to classify each input sentence into N relation types. Based on the N relation types, dynamic prompt generator generates the corresponding dynamic prompt. \n\nA mask language model (BERT) [24] and a linear layer comprise the classifier. Due to the seven API relation types, the linear layer's output dimension is set to 7. We obtain the latent vector from the CLS token when we enter the sentence into BERT. The latent vector obtained from the CLS token characterizes the sentence features better than other positions, resulting in better classification performance. The latent vector is then fed into the linear layer, which produces a vector with seven dimensions, each corresponding to a relation type. Finally, the classifier is trained on the classifier training set. In back propagation, we use the cross-loss entropy to calculate the classifier's loss and adjust the BERT and linear layer parameters. The loss function is formulated as follows, where  = [ 0 , . . . ,  \u22121 ] represents the linear layer's output result, and C represents the sentence's label.",
            "score": 0.4985856787999156,
            "section_title": "BERT-based Relation Classifier",
            "char_start_offset": 20062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1435
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 290,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2298583984375
        },
        {
            "corpus_id": "258832491",
            "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
            "text": "Section Findings \n\n\u2022 Quality and toxicity filters have very different effects. \n\n\u2022 Quality filters improve performance significantly, despite removing training data. \n\n\u2022 Quality filtering effects are not easily predicted by dataset characteristics. Future filters should weigh more than one dimension of quality. \n\n\u2022 Toxicity filtering trades off generalization and toxicity identification ability for reduced risk of toxic generation. \n\n\u2022 When optimizing for toxicity identification tasks, practitioners should use an inverse toxicity filter. \n\nMost modern large language models use some form of quality and/or toxicity filtering for their pretraining datasets (Table 1). To curb toxicity, T5 uses n-gram filters, Gopher and Chinchilla use SafeSearch filters, and LaMDA uses \"safety discriminators\". Quality heuristics are universally applied for web-scraped data, with newer models like LLaMA, the GPT-series and the PaLM-series all relying on quality classifiers. To compare and quantify the effects of these two filter types, we implement quality and toxicity filters at various thresholds, as described in Section 2.2, to vary the quantity of toxic and low-quality text present when pretraining models on the Pile and C4. \n\nQuality filters significantly improve performance across nearly all tasks, despite reducing training data quantity and variety. We see the quality filters improve nearly all downstream tasks: toxicity identification by 2% (Figure 5, right) and most QA task categories by 1-6% (Figure 6). Of most interest, these improvements are realized despite removing 10%+ of the training data, even though we find that removing data usually leads to a decrease in performance (Section 6). While the average performance peaks at T = 0.975 for the QA tasks, greater quality filtering still outperforms the unfiltered baseline on average. For the toxicity identification experiments, the performance is still improving after T = 0.7, where 55% of the dataset has been filtered out. Dataset quality characteristics are not strongly indicative of filtering effects. In Section 3, Books, Wikipedia, and Web data are classified as highest quality. Figure 6 shows that despite this, quality filtering provides the least benefit to QA tasks in these categories, even hurting the performance for Books.",
            "score": 0.49846661408783777,
            "section_title": "Impact of Quality & Toxicity Filters on Pretrained Models",
            "char_start_offset": 27210,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17236328125
        },
        {
            "corpus_id": "254221070",
            "title": "Using Large Pre-Trained Language Model to Assist FDA in Premarket Medical Device Classification",
            "text": "While BERT is trained on sentence paired and employed next sentence prediction during its training, sentence BERT is also trained on the SNLI dataset, which contains 570,000 sentence pairs and the logical relations between them like entailment, contradiction, and neutral. Therefore, the sentence BERT similarity approach also captures the logical relations between the two sentences and is less sensitive to the extra information in the queries. However, unlike models that are based on permuted language models and able to encode stronger contextual information, BERT is a masked language model and less powerful as compared to models that encode both contextual and positional information like MPNet. Sentence transformer like MPNet achieves much more accurate results on the dataset as expected. Sentence transformer based on T5 also achieves accurate results. Although T5 is also a masked language model, it is trained with a masked span infilling objective instead of masked token prediction like BERT. It, therefore, encodes more positional information by predicting a set of phrases instead of words. T5 is also trained on a much larger and cleaner dataset than any other pre-trained models. It also has a larger scale than most of the pre-trained models. Both MPNet and T5 achieve desirable results on the dataset with an average rank of 14.04 and 34.16 respectively. We also trained the CFR dataset on the FastText supervised model with FastText pre-trained embeddings with 50 epochs and a learning rate of 0.8. We then evaluate the model on the 510(k) dataset. The model only achieves an accuracy of 0.25 on 100 most likely labels, which is much lower than finding the cosine similarity with FastText pre-trained embedding with tfidf averaging, which achieves an accuracy of 0.64 using the same metrics. This makes sense because there is only one description for each label in the training set, and the model has a natural proclivity for overfitting. Therefore, a supervised model is not a reasonable choice in our case with such a limited dataset.\n\nIn conclusion, the performance on predicting correct labels, GPT-3 semantic search, MPNet, and T5 all perform accurately on the dataset, the correct labels are usually within the top 20 most likely labels",
            "score": 0.4984520425994927,
            "section_title": "As shown in",
            "char_start_offset": 22056,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24560546875
        },
        {
            "corpus_id": "262159015",
            "title": "On understanding and predicting issue links",
            "text": "In Software Engineering different approaches for duplicate detection have been recently suggested, with two main model architectures. Single-Channel architectures [15] take the word representation (Word2Vec, FastText, or GLOVE) and encode each issue separately with a siamese network, consisting of a CNN, LSTM, and a feed-forward neural network. Then, the two encoded issues are fed into another forward neural network to determine if one issue is a duplicate of the other. Dual-Channel architectures [16] use the word representations of two issues (which are two matrices of word embeddings) to construct a tensor by putting the two matrices on top of each other. Thus, in this architecture, the two issues are encoded jointly. We experimented with BERT, DistilBERT, the Single-Channel, and Dual-Channel architectures using FastText and Word2Vec. DistilBERT is a smaller transformer model trained to imitate BERT through knowledge distillation. It retains most of BERT's general language understanding capabilities while using 40% fewer model weights. \n\nIn preliminary experiments, we found that BERT outperformed all other models in all setups: BERT outperformed DistilBERT by an average of 0.05 F1-score, the Single-Channel models by an average of 0.21, and the Dual-Channel models by an average of 0.26. The source code and results of all evaluated models are included in the replication package. In the remainder of the paper, we focus on discussing BERT's results as the best-performing model. \n\nWe concatenated the title and description of both issues into one string and used this as input for the BERT model. Then, we tokenized this input string with the tokenizer of bert-base-uncased / distilbert-base-uncased which is a trained WordPiece tokenizer. We truncated the token sequence at 192 tokens with a longest first strategy. That is, if possible, we kept all tokens and otherwise truncated the longer of the two issues first. The [CLS] token output of the BERT model was then fed into a dense layer classification head which predicts the label of the link. For the training, we chose AdamW and use the default learning rate of 5e \u22125 and weight decay of 0.1.",
            "score": 0.49835781033117044,
            "section_title": "Machine learning models",
            "char_start_offset": 13354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "19056247"
                },
                {
                    "start": 502,
                    "end": 506,
                    "matchedPaperCorpusId": "221823015"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28564453125
        },
        {
            "corpus_id": "266566817",
            "title": "Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document",
            "text": "The fundamental yet essential component of document classification is the transformation of text into numeric vectors, enabling computers to conduct subsequent calculations and processing. This process is known as text representation [13]. Word embedding is a common method for text representation, which embeds words from the text into a space and expresses them in the form of vectors. One-hot, Bag-Of-Words models, TF-IDF, etc., are frequently used text representation methods. However, the aforementioned feature representations disregard the context relationships within the text, treating each word as independent, which fails to capture semantic information. Moreover, these representations can lead to issues of sparsity and dimensionality disaster in the feature matrix. Consequently, the focus of subsequent research has been on constructing low-dimensional, distributed, dense word vectors. Word2Vec [14] is a neural network language model that considers both contextual semantics and avoids dimensionality issues, yielding superior performance compared to earlier models. Additionally, FastText [15], released by Facebook in 2016, is a tool for word vector computation and document classification. In classification tasks, FastText often achieves accuracy comparable to deep neural networks but with faster training times. However, both Word2Vec and FastText are static models since they have a one-to-one correspondence between words and vectors, making them incapable of dynamically adjusting and optimizing for specific tasks, thus unable to address the issue of polysemy. \n\nIn contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information [16]. BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements.",
            "score": 0.498024329630942,
            "section_title": "A. WORD EMBEDDING",
            "char_start_offset": 7853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "253828309"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "259376879"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "239459434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "235683534",
            "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
            "text": "Contemporary natural language processing (NLP) relies heavily on pretrained language models, which are trained using large-scale unlabeled data (Bommasani et al., 2021). BERT (Devlin et al., 2019) is a particularly popular choice: it has been widely adopted in academia and industry, and aspects of its performance have been reported on in thousands of research papers (see, e.g., Rogers et al., 2020, for an overview). Because pre-training large language models is computationally expensive (Strubell et al., 2019), researchers often rely on the release of model checkpoints through libraries such as HuggingFace Transformers (Wolf et al., 2020), which enable them to use large-scale language models without repeating the pre-training work. Consequently, most published results are based on a small number of publicly released model checkpoints. \n\nWhile this reuse of model checkpoints has lowered the cost of research and facilitated head-to-head comparisons, it limits our ability to draw general scientific conclusions about the performance of a particular class of models (Dror et al., 2019;D'Amour et al., 2020;Zhong et al., 2021). The key issue is that reusing model checkpoints makes it hard to generalize observations about the behavior of a single model artifact to statements about the underlying pre-training procedure which created it. Pre-training such models is an inherently stochastic process which depends on the initialization of the model's parameters and the ordering of training examples; for example, D'Amour et al. \n\n\u2022 We illustrate the approach with a practical use case: we investigate the impact of counterfactual data augmentation on gender bias, in a BERT-based coreference resolution systems (Webster et al., 2020) ( \u00a74). Additional examples are provided in Appendix E, where we document challenges with reproducing the widely-used original BERT checkpoint. \n\nThe release also includes an additional 140 intermediate checkpoints, captured during training for 5 of the runs (28 checkpoints per run), to facilitate studies of learning dynamics. Our checkpoints and statistical libraries are available at: http://goo.gle/multiberts. \n\nAdditional Related Work.",
            "score": 0.49801978955684734,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2159
                },
                {
                    "start": 2162,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 196,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 381,
                    "end": 400,
                    "matchedPaperCorpusId": "211532403"
                },
                {
                    "start": 492,
                    "end": 515,
                    "matchedPaperCorpusId": "174802812"
                },
                {
                    "start": 627,
                    "end": 646,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1077,
                    "end": 1096,
                    "matchedPaperCorpusId": "196167277"
                },
                {
                    "start": 1117,
                    "end": 1136,
                    "matchedPaperCorpusId": "234482939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08819580078125
        },
        {
            "corpus_id": "231718643",
            "title": "KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding",
            "text": "Google has released BERT multilingual model (M-BERT) pre-trained using 104 different languages including the Korean. Karthikeyan et al. [16] show why and how well M-BERT works on many downstream NLP tasks without explicitly training with monolingual corpus. More recently, Facebook AI Research presented crosslingual model (XLM-R) [17] generally outperforming M-BERT. Recent literature argues that a monolingual language model is consistently superior to M-BERT. For French, FlauBERT [18] and Camem-BERT [19] with the same approach as RoBERTa have been released. ALBERTo [20] focuses on Italian social network data. BERTje [21] for Dutch and FinBERT [22] for Finnish have been developed. They both have achieved superior results on the majority of downstream NLP tasks compared to M-BERT.\n\nSome previous work in the Korean language has focused on learning static representations by using language-specific properties [23]. More recently, SKT Brain has released BERT 1 and GPT-2 pre-trained on large Korean corpora. 2 Korean Electronics and Telecommunications Research Institute (ETRI) has released two versions of BERT: the morpheme analytic based and the syllable based model. 3 These models are worthwhile to experiment with and provide good benchmark evaluations in Korean language model research.\n\nBART [24] features interesting denoising approaches for input text used in pre-training such as sentence permutation and text infilling. In the sentence permutation task, an input document is divided into sentences and shuffled in a random order. A combination of text infilling and sentence shuffling tasks has shown significant improvement of the performance over either applied separately. Inspired by BART, we have formulated word order prediction (WOP), a new pre-training loss used alongside the MLM and SOP losses for Kore-ALBERT. Differentiated from BART, which is essentially a sentence-level shuffling, WOP is an intra-sentence, token-level shuffling.",
            "score": 0.49777442738044875,
            "section_title": "B. Related Work",
            "char_start_offset": 8120,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.175537109375
        },
        {
            "corpus_id": "260438442",
            "title": "Local Large Language Models for Complex Structured Tasks.",
            "text": "As part of this effort, we trained over 100 models across multiple datasets, model architectures, sizes, and training configurations. For each dataset (tiny, small, and large), we increased model size, where applicable, and training epochs, until the performance of the testing dataset diminished, which we discuss in detail in Section 3, Results. All training was conducted on a single server with 4XA100 80G GPUs. 34 For LLaMA 7B and 13B parameter models, the average training time was 25 minutes per epoch and two hours per epoch, respectively. In the following sections, we describe the training process for each unique model architecture. \n\nBERT and its successor transformer models are available in three forms 1) Foundational model, 2) Extended language model, and 3) Fine-tuned model. Foundational models, as the name would suggest, are trained on a wide corpus of language, which provides a foundational model for fine-tuned tasks, such as code extraction. \n\nIn areas where common language and words do not adequately represent the applied domain, unsupervised language modeling can be used to train a new model on domain-specific language. For example, the popular BioBERT 35 model, which was trained using biomedical text, has been shown to outperform the foundational BERT model for specific biomedical tasks. Using example Hugging Face transformer language modeling code, 36 we trained our own BERT-based language model using case notes as inputs. Except for the removal of condition code columns, the training data is identical to the format shown in Table 3. \n\nAll BERT models were fine-tuned using example Hugging Face transformer training code. 37 ngFormer is a BERT-like model that makes use of a sliding window and sparse global attention, which allows for an increased maximum input token size of 4096 compared to 512 for BERT. While the majority of gross or diagnostic reports would not exceed the capacity of BERT models, the concatenation of report types across all specimens in the case could easily exceed the 512-token limit. LongFormer models, which provide twice the input token size of our local LLM (2048), allow us to test the impacts of maximum token size on BERT-style model performance.",
            "score": 0.49765032965541844,
            "section_title": "Model Training",
            "char_start_offset": 10601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0723876953125
        },
        {
            "corpus_id": "227231751",
            "title": "ALT at SemEval-2020 Task 12: Arabic and English Offensive Language Identification in Social Media",
            "text": "Since we are not provided with an official development set, we use OLID 2019 (Zampieri et al., 2019a) as development dataset in our experiments (Table 3). We report results of our best system on official test set in Table 4. We use the same metrics as Section 4.\n\nfastText For the training set, we are provided with average confidence score (CONF) of several classifiers and standard deviation (STD) from CONF. Higher CONF means the instance is more likely to be offensive and higher STD means the classifiers are in higher disagreement on the instance. If an instance has CONF >= 0.5, we label it OFF and label it NOT if CONF <= 0.3, and discard rest of the instances. The threshold for CONF is skewed to ensure we have enough OFF labels (since it is underrepresented in the data). By discarding instances with high STD, we eliminate the more confusing instances. Similar to Arabic track, fastText (F1 score of 79.9) is outperformed by other classifiers. Support Vector Machines (SVMs) Adhering to the reasoning described earlier, we experimented with several thresholds and chose the following due to better performance: label OFF if CONF >= 0.40 and STD <= 0.15; label NOT if CONF >= 0.40 and STD <= 0.15. We discard rest of the instances. We create character n-gram vectors in a way similar to Section 4. Other experiments from Section 4 did not improve performance and are omitted. With F1-score of 82% on OLID 2019, SVM trained on character n-gram vector outperforms fastText.\n\nMultilingual-BERT Due to low performance on the training data so far, we explored addition of external datasets. We choose Wikipedia Detox (Wulczyn et al., 2017) as it is a large dataset annotated for personal attacks, aggression and toxicity -components of offensiveness. First, we map labels of given training data with the same thresholds as the ones for SVMs described earlier. After mapping the labels of Wikipedia Detox to OFF or NOT and adding to the training data, we fine-tuned BERT base-multilingual (",
            "score": 0.49758434188342937,
            "section_title": "Experiments and Results",
            "char_start_offset": 12332,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 101,
                    "matchedPaperCorpusId": "67856299"
                },
                {
                    "start": 1623,
                    "end": 1645,
                    "matchedPaperCorpusId": "6060248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224609375
        },
        {
            "corpus_id": "269423133",
            "title": "Automated Assessment of Inferences Using Pre-Trained Language Models",
            "text": "The accuracy of inference classification is significantly influenced by the choice of the base pre-trained language model. The comparison of the F1 scores based on the BERT-base and RoBERTa-base models shows the effect of the training strategies used during the pre-training phase of a language model on the classification accuracies of the fine-tuned models. The more advanced training strategies used for the RoBERTa-base model resulted in significantly higher F1 scores for the inference classification than that of the BERT-base model of the same size. This is consistent with previous findings which indicate that models trained with RoBERTa-base models outperform models trained with BERT-base models for various downstream tasks [22]. \n\nFurthermore, the comparison of the F1 scores based on the RoBERTa-base and RoBERTalarge models suggests that the larger model is not necessarily better for classifying inference in the current dataset. The RoBERTa-base and RoBERTa-large models share the same Transformer architecture and training objective (masked language model with dynamic masking), but the main difference between the two models is the number of layers (12 vs. 24) and the model sizes (110 million vs. 355 million parameters). Despite the greater representational capacity, the RoBERTa-large model did not perform significantly better than the smaller model with the same architecture. The average F1 score became even lower, and the SEM of the F1 scores became larger. The error analysis using the inference subtypes shows that the largest language model (RoBERTa-large) made more errors in classifying evaluative comments, paraphrases, and meaningless responses. \n\nThis lower accuracy of the larger model is probably due to overfitting. In Table 4, the optimal weight decay value was zero for RoBERTa-base and non-zero for RoBERTa-large. This suggests that regularization with weight decay worked for RoBERTa-large, yielding a higher F1 score than RoBERTa-large without weight decay. Without weight decay, the F1 score of RoBERTa-large would be even lower than that of RoBERTa-base. More data are needed to train the larger model, and a well-trained small model (RoBERTa-base) would be the preferred choice for inference classification with a relatively small dataset.",
            "score": 0.4974653908674874,
            "section_title": "Effects of Pre-Trained Language Models on the Classification Accuracy",
            "char_start_offset": 21653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247802734375
        },
        {
            "corpus_id": "219401728",
            "title": "Spoken Dialect Identification in Twitter using a Multi-filter Architecture",
            "text": "The first filter should be designed in a way to favor GSW recall, either by tuning inference thresholds or by using training data that implicitly enforces this bias towards GSW. Here we follow the second approach for this filter by using different domains for training different labels, which is further discussed below. Moreover, we use a more complex (in terms of the number of parameters) model for the first filter, so that it does the main job of removing non-GSW inputs while having reasonable GSW precision (further detail in section 4). The second filter will be later used to improve the pipeline precision by removing a relatively smaller number of non-GSW tweets. \n\nOur first filter is a fine-tuned BERT (Devlin et al., 2018) model for the LID downstream task. As we do not have a large amount of unsupervised GSW data, it will be hard to train the BERT language model (LM) from scratch on GSW itself. Hence, we use the German pre-trained LM (BERT-base-cased model2 ), which is the closest high-resource language to GSW. \n\nHowever, this LM has been trained using sentences (e.g., German Wikipedia) that are quite different from the Twitter domain. Moreover, lack of standard spelling in GSW introduces many new words (unseen in German LM training data) that their respective subwords embedding should be updated in order to improve the downstream task performance. In addition, there are even syntactic differences between German and GSW (and even among different variations of GSW in different regions (Honnet et al., 2017)). For these three reasons, we can conclude that freezing the BERT body (and just training the classifier layer) might not be optimal for this transfer learning between German and our target language. Hence, we also let the whole BERT body be trained during the downstream task, which of course needs a large amount of supervised data to avoid quick overfitting in the fine-tuning phase. \n\nFor this filter, we choose the same eight classes for training LID as Linder et al. (2019) (the dataset classes and their respective sizes can be found in section 3.1).",
            "score": 0.4971973567070871,
            "section_title": "First filter: fine-tuned BERT model",
            "char_start_offset": 4711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1922
                },
                {
                    "start": 1925,
                    "end": 2093
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0740966796875
        },
        {
            "corpus_id": "271319004",
            "title": "AI Language Models: An Opportunity to Enhance Language Learning",
            "text": "High-proficiency L2 learners may have been tasked \n\nwith proportionally more open-ended prompts, encouraging individual expression and contributing to variability in texts, hence lower similarity scores. \n\nWhy are there differences among the three LLMs in these L2 writing assessment tasks? The current work speculates the following: the differences among BERT, Llama2, and T5 arise from variations in their architectures, training datasets, and training objectives. BERT, a bidirectional transformer, is pre-trained on BooksCorpus and English Wikipedia, using a masked language model objective [2]. T5, based on a text-to-text framework, is trained on the Colossal Clean Crawled Corpus (C4) and handles various NLP tasks as text generation problems [3]. Llama2, a fine-tuned transformer, utilizes diverse internet-based data, including recent content, and balances understanding and the generation of tasks through specific training adjustments [23]. These distinctions result in unique performance variations across L2 writing assessment tasks. It is likely that the text-to-text framework is particularly suitable for L2 text detection at the sentence level, enabling T5 to have a more sensitive LLM in L1-and L2-writing differentiation. On the other hand, masked language modeling seems more sensitive for word-level measurements, enabling BERT to have a more appropriate LLM in writing assessment within a sentence (c.f. Table 2).",
            "score": 0.49698440628986257,
            "section_title": "Interpreting LLM Similarity Scores in an L2 Setting",
            "char_start_offset": 43565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 52,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1435
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 753,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1336669921875
        },
        {
            "corpus_id": "253168494",
            "title": "Register identification from the unrestricted open Web using the Corpus of Online Registers of English",
            "text": "Table 4 presents the classification results across different classifiers and dataset versions. First, the highest F1-score of 68% and PR-AUC of 75% were achieved using the propagated data version and BERT Large. BERT Base achieved nearly similar results, whereas the performances of fastText and CNN were clearly lower. Although these scores are not directly comparable to the results presented in previous studies, they can be considered very competitive; for example, Biber and Egbert (2016b) reported a precision of 26.9% and recall of 28.6%. The data in their study was based on CORE and used a multiclass setting, even though the register labels and their assignment strategy were different. Another point of comparison between model performances can be made against Asheghi et al. (2014), who reported an accuracy of 78.88% using the Leeds Web Genre corpus with 15 balanced register categories from a restricted sample of the Web. \n\nHere, we focus on discussing the best results achieved with BERT Large. As already demonstrated by Madjarov et al. (2019), the multilabel setting is better for learning the mapping between the linguistic characteristics and their associated register categories. This is evident from our experiments as well. The number of labels tended to increase significantly with this type of data when operating in a multiclass setting. This increase became drastic when operating with datasets such as CORE that feature an unrestricted sample of Web documents. \n\nIn the current study, a primary point of interest is the difference between propagated and nonpropagated versions of the data. The former provided substantially better discrimination power between the register labels. While both versions of the data contain the same number of register labels, they do deviate from each other in terms of the number of examples per label in training data-in the propagated version, the main register label was repeated with each subregister label, whereas in the nonpropagated version this as not the case. This seems to suggest that improvements could be gained in this task by increasing the size of the training data. In particular, the high precision and low recall of the nonpropagated data indicated that the coverage could be increased by propagating the main register labels. Furthermore, it is also possible that propagation brings forth further structuring present in the data.",
            "score": 0.4969370515915305,
            "section_title": "Classifier performances",
            "char_start_offset": 44660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1488
                },
                {
                    "start": 1491,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2307
                },
                {
                    "start": 2308,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 494,
                    "matchedPaperCorpusId": "64313679"
                },
                {
                    "start": 772,
                    "end": 793,
                    "matchedPaperCorpusId": "17862821"
                },
                {
                    "start": 1038,
                    "end": 1060,
                    "matchedPaperCorpusId": "199131325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21630859375
        },
        {
            "corpus_id": "227231676",
            "title": "KS@LTH at SemEval-2020 Task 12: Fine-tuning Multi- and Monolingual Transformer Models for Offensive Language Detection",
            "text": "We used pre-trained BERT language models without changes to the base architecture. For the finetuning step, we followed the approach for single sentence classification suggested by Devlin et al. (2018). A single fully connected classification layer was added to the base model. A special [CLS] token was prepended to all inputs. The contextual representation of this token was used as an embedding for the complete sentence, and passed to the classification head. The complete base model was fine-tuned during training. Liu et al. (2019) showed that BERT is undertrained. Their model, RoBERTa, uses exactly the same architecture as BERT. RoBERTa outperforms BERT simply by training on more data, with larger batches, for a longer time. Some additional simple changes in the pre-training approach, such as removing one of the pre-training objectives and training on longer sequences, improved the results even further. This is the monolingual model we used for English. There were no pre-trained RoBERTa models available for the other languages. The fine-tuning approach is identical to the one used for BERT. \n\nSimilarly, in the multilingual context, the XLM-RoBERTa (XLM-R) model we used achieves much of its improvement over previous multilingual models by using several orders of magnitude more data (Conneau et al., 2019). Conneau et al. (2019) also find that vocabulary size has a large impact when many languages are used. Again XLM-R uses the same model architecture as BERT. However, the increase of vocabulary size from 30K to 250K leads to an increase of the total number of parameters from 110M and 335M to 270M and 550M for the BASE and LARGE models, respectively. All five languages are present among the 100 languages used during pre-training of XLM-R. The fine-tuning approach is identical to the one used for the previous models. \n\nA summary of the different pre-trained models that we used for each language is provided below:",
            "score": 0.4967508486654854,
            "section_title": "Models",
            "char_start_offset": 10775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1943
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15087890625
        },
        {
            "corpus_id": "258461548",
            "title": "The Diminishing Returns of Masked Language Models to Science",
            "text": "Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770M-parameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model sizes, training data, or compute time does not always lead to significant improvements (i.e.,>1% F1), if at all, in scientific information extraction tasks and offered possible explanations for the surprising performance differences.",
            "score": 0.4967121752459944,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1337890625
        },
        {
            "corpus_id": "268537094",
            "title": "BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models",
            "text": "RoBERTa (Liu et al., 2019) builds upon the foundational principles and training strategies of BERT (Devlin et al., 2018), offering novel alternatives that enhance downstream task performance.RoBERTa refines and optimizes the pre-training methodology initially proposed in BERT, resulting in notable improvements in task performance while maintaining a comparable number of trainable parameters.We use RoBERTa-base and RoBERTa-large for a convenient and fair comparison with LoRA (Hu et al., 2021).\n\nDeBERTa (He et al., 2020) represents an advanced iteration of BERT, having undergone extensive training at a larger scale.DeBERTa demonstrates strong competitiveness when evaluated on the GLUE benchmark.For our experiments, we use DeBERTa-v2-xxlarge which has 1.5 billions of parameters to evaluate the scaling-up capability of BiLoRA and also for a convenient comparison with LoRA.We use DeBERTa-v3-base which has 183 millions parameters for fair comparison with AdaLoRA (Zhang et al., 2023).\n\nGPT-2 (Radford et al., 2019) developed by OpenAI, was once a state-of-the-art language model renowned for its remarkable text generation capabilities.It is a scaled-up version of its predecessor, GPT-1, and is trained on an extensive corpus of text data.GPT-2 has been widely recognized for its proficiency in generating coherent and contextually relevant text across various natural language understanding and generation tasks, showcasing its versatility and potential in the field of natural language processing.",
            "score": 0.49663027785939895,
            "section_title": "A.3. Models",
            "char_start_offset": 24512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 497
                },
                {
                    "start": 499,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 992
                },
                {
                    "start": 994,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 1000,
                    "end": 1022,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13427734375
        },
        {
            "corpus_id": "263297581",
            "title": "A semantically enhanced text retrieval framework with abstractive summarization",
            "text": "In this section, we will present the results on some data sets of our proposed BERT ranking framework that integrates paragraph contexts, and compare them with the unsupervised baseline model and the supervised baseline model.Tables 1 and 2 show the results of BERT and all the comparative models on the Robust04 and GOV2 datasets in terms of MAP@1000, MAP@100, P@20, and NDCG@20, respectively.\"*\" means statistically improvements over BERT-Base (MS MARCO).\n\nIn Tables 1 and 2, the experimental results from both BERT ranking framework that integrates paragraph contexts and each comparison model can be found that the four indicators have been greatly improved, no matter whether it is the unsupervised baseline or the pretraining language model baseline.\n\nBERT-Base (MS MARCO) is the most advanced basic BERT model.It can be seen from Table 1 that the method proposed by us also has great advantages over this model.The P@20 and Map@1000 on Robust04 data set have been increased by 19.3% and 21.7%.The average length of documents in GOV2 data set is about three times that of Robust04 data set.The results of this framework model in this article on GOV2 data set are significantly higher than those of unsupervised model and BERT-Base (MS MARCO).It shows that the retrieval ability of the proposed method is obviously improved.\n\nIn order to further verify the effectiveness of the method proposed in this article, we compare the BERT ranking framework based on paragraph context proposed in this article with the advanced pseudo relevance feedback model and the BERT-based retrieval model.These models include:\n\nBERT-QE for expanding queries using the most relevant fragments and document relevance, and Co-BERT for scoring multiple documents simultaneously.Based on the principle of fairness, the above two advanced models also use the base scale model, but the parameter settings follow TA B L E 1 Effectiveness of SE-BERT relative to baselines and BERT-based re-rankers on Robust04.The comparisons are mainly relative to the best-performed BERT-based model that is fine-tuned on MS Marco, denoted as BERT-MaxP.The best result for each metric is shown in bold.",
            "score": 0.49656634387915866,
            "section_title": "RESULTS",
            "char_start_offset": 23076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 756
                },
                {
                    "start": 758,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1329
                },
                {
                    "start": 1331,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1612
                },
                {
                    "start": 1614,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1987
                },
                {
                    "start": 1987,
                    "end": 2115
                },
                {
                    "start": 2115,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14208984375
        },
        {
            "corpus_id": "222327807",
            "title": "Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching",
            "text": "We present evaluation results of different models over Wiki65K and AAN104K data in Table 5. We summarize our observations as follows: 1) Both SMITH-WP and SMITH-WP+SP models outperform all the baseline methods including the stage-of-the-art long text matching method SMASH and MatchBERT based on pretrained BERT models on both Wiki65K and AAN104K consistently. The comparison between SMITH-WP/ SMITH-Short and Match-BERT shows the effectiveness of introducing hierarchical document structure modeling with sentence level and document level Transformers for long document representation learning and matching. \n\n2) If we compare the SMITH model settings with the pre-training stage (SMITH-Short, SMITH-WP, SMITH-WP+SP) with the SMITH model settings without the pre-training stage (SMITH-NP), we can find that language modeling pre-training can help increase the performance of the downstream document matching task by a large margin. Thus better language understanding via large scale language modeling pre-training will lead to better downstream task performance, which is consistent with the findings by Devlin et al. [6]. 3) Both SMITH-WP and SMITH-WP+SP outperform SMITH-Short, which is initialized by the pre-trained open source BERT model. We think the main reason is that currently SMITH-Short can only process at most 512 tokens due to TPU memory issues, which will hurt the performance. On the other hand, Both SMITH-WP and SMITH-WP+SP can process as long as 2048 tokens, which is a better setting for long document representation learning. 4) If we compare SMITH-WP with SMITH-WP+SP, we can find that adding the masked sentence block prediction task presented in Section 4.2 during the pre-training stage can also be helpful to improve the downstream document matching performance. The masked word prediction task proposed by Devlin et al. [6] can capture the word relations and dependencies in the pre-training corpus, whereas the masked sentence block prediction task can additionally force the model to learn the sentence-level relations and dependencies.",
            "score": 0.4965506636394531,
            "section_title": "Evaluation Results",
            "char_start_offset": 37040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 2067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171630859375
        },
        {
            "corpus_id": "258784982",
            "title": "Combating the COVID-19 infodemic using Prompt-Based curriculum learning",
            "text": "on the general BooksCorpus and   Wikipedia. Consequently, the semantic difference between the corpus of COVID-19-related information and the corpus used to train BERT was significant; the former is on a small to medium scale, which was inadequate to fine-tune BERT to the specific COVID-19 domain. Among other results, the text length of tweets was found to be relatively short. NLP research has shown that the technique usually performs worse for short texts on some tasks (including name entity recognition) due to the lack of sufficient contextual information in short texts (Giuseppe et al., 2017). As a result, the power of LSTM and CNN to extract semantic information from the text cannot be fully exploited. In our experiment, their performances were even worse than that of the simple fastText, which generally does not rely on a deep neural network structure and thus requires less data to train.\n\nAlthough fastText provided the second-best overall performance, it also demonstrated problems. As mentioned above, class imbalance is extreme in the fight agaisnt infodemics, and the texts in our experiment-tweets-are short and contain many informal expressions, which makes feature extraction especially challenging. With normal classifiers, such as fastText, the classes with more training data usually dominate the classification process, and the testing samples tend to be categorized into the dominant class. As can be seen in Fig. 3(a), the confusion matrices of fastText's multi-classification result for Q4 reveal that all of the testing data were classified into class 1-the dominant class. In contrast, our prompt-based curriculum learning approach mitigated that problem through its superior ability to employ the PLM (i. e., BERT) to extract semantic and syntactic features from short texts. Thus, the testing data could be more accurately assigned to the correct classes, as shown in Fig. 3(b). However, for some classification tasks with exceptionally skewed label distribution, the proposed method did not exhibit superior performance. For example, it performed equally well as fastText and BERT on Q5 \u2032 s multi-classification task. The reason for this is that all three methods categorized all testing samples to the same dominant class and thus yielded the same results. Nevertheless, with more balanced data distribution, this problem could be mitigated",
            "score": 0.4965040192287597,
            "section_title": "Overall performance",
            "char_start_offset": 34064,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 578,
                    "end": 601,
                    "matchedPaperCorpusId": "26185046"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242919921875
        },
        {
            "corpus_id": "247005422",
            "title": "Applying BERT Embeddings to Predict Legal Textual Entailment",
            "text": "With the growth in applications for Natural Language Processing (NLP), various fields of software technology such as machine translation, text recognition and text generation have seen a large development in the area of deep learning models adapting to these tasks [8]. Substantial progress in the area of learning embeddings for dense representations of the textual data has been made. Some of them are CoVe [16] (Contextual Word Vectors), ELMo [19] (Embeddings from Language Model), Cross-View Training [3] (CVT), ULMFiT [10] (Universal Language Model Finetuning for Text Classification), GPT [22] (Generative Pre-training Transformer), BERT [4], ALBERT [14] (A Lite BERT) and RoBERTa [15] (Robustly optimized BERT approach). These dense representations are usually learned by training on auxiliary tasks, such as masked language modeling (MLM), next sentence prediction (NSP), machine translation, and transcription. Contextual word embeddings-once learned-can be further fine-tuned for downstream tasks, such as classification, with relatively less effort. The language model BERT [4] and its variants [14,15] have emerged as the most convenient choice for a model concerning these downstream tasks since they condition a word's embedding on the surrounding context. This makes them and similar approaches perform significantly better than other models which learn static embeddings as a dense representation of the textual data. \n\nOne noticeable characteristic of BERT is that it performs better on domain-specific tasks when pre-trained with data of that specific domain. Various examples include BioBERT-cased, PubMedBERT-uncased which both perform better for biomedical data than the original BERT model as discussed by Gu et al. [6]. Similarly, LEGAL-BERT and its variants on legal sub-domains can perform better than the standard BERT on domain-specific tasks as summarized by Chalkidis et al. [2]. For LEGAL-BERT pre-training is carried out on a collection of several fields of English legal text like contracts, court cases, and legislation.",
            "score": 0.4961536924814132,
            "section_title": "Contextual Embeddings from Language Models",
            "char_start_offset": 5548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 413,
                    "matchedPaperCorpusId": "9447219"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "52811641"
                },
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 644,
                    "end": 647,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 656,
                    "end": 660,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1085,
                    "end": 1088,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1106,
                    "end": 1110,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1812744140625
        },
        {
            "corpus_id": "259264891",
            "title": "Comparison of pretrained transformer-based models for influenza and COVID-19 detection using social media text data in Saskatchewan, Canada",
            "text": "Our study investigated the text classification performance of pre-trained BERT and three BERT variant models -RoBERTa, BERTweet, and CT-BERT using annotated influenza and COVID-19 Twitter datasets. Table 3 shows the eight BERTbased models employed in this study. We carried out our experiments using Google Colaboratory (34). \n\nBERT Bidirectional Encoder Representations from Transformers (BERT) is a language representation model using the widely adopted   A high-level study workflow diagram. sizes. We used BERT-base and BERT-large in this study with parameters of 110M and 340M, respectively. RoBERTa Robustly Optimized BERT Pretraining Approach (RoBERTa) ( 23) is an extension of BERT and has the same architecture as BERT. RoBERTa can be seen as an enhanced version of BERT with improved pretraining procedures and additional pre-training data. RoBERTa applies a dynamic masking approach and removes the NSP objective, which sets it apart of BERT. RoBERTa's training time is longer and utilizes a larger corpora in comparison to BERT. RoBERTa is pretrained on a total of 160GB of text data, including the original data used to train BERT (BooksCorpus and Wikipedia), CC-News, OpenWebText, and Stories. For our experiments on COVID-19 and influenza Twitter datasets, we used RoBERTa-base and RoBERTa-large models, which include 124M and 355M parameters, respectively. \n\nBERTweet BERTweet (36) is a variant of BERT trained with a masked language modeling objective. Using the same architecture as BERT-base, BERTweet is trained to process text data from Twitter. The pre-training procedure of BERTweet is based on RoBERTa for improved performance. After preprocessing the raw tweets, BERTweet used 845M English tweets and 5M English tweets related to the COVID-19 pandemic (from 01/2020 to 03/ 2020) to train BERTweet-base and BERTweet-large models. The total pretraining data used for these models amounts to approximately 80GB (16B word tokens).",
            "score": 0.49609490775754483,
            "section_title": "Transformer-based pretrained language models",
            "char_start_offset": 11784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1951
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1510009765625
        },
        {
            "corpus_id": "221655205",
            "title": "Beyond Accuracy: ROI-driven Data Analytics of Empirical Data",
            "text": "The classification algorithms such as RF and NB, have been explored in NLP based SE problems. These algorithms are driven by the feature extraction aspect to a great extent. Thus, could influence their effectiveness on classification outcomes. However, 1 https://huggingface.co/transformers/model_doc/bert.html#bertfornextsentenceprediction 2 https://colab.research.google.com/ feature extraction is problem specific and incurs substantial cost and access to domain expertise.\n\nOn the other hand, BERT eliminates the need for feature extraction since it is a language model based on deep learning. BERT, pre-trained on a large text corpus, can be fine-tuned on specific tasks by providing only a small amount of domain-specific data.\n\nIn this empirical analysis, we conducted classification by utilizing a fraction of the whole dataset for training and testing for a small fixed data set. This was repeated by slowly increasing the fraction of the training set and results were captured.\n\nDuring every classification, and were computed using various parameters explained in Table 1.\n\nis the sum of the data processing costs (( + + + )/60) (in hours) for a fraction (N%) of training set. This is further translated into dollar cost based on hourly charges ( ) of human resources.\n\ncomputations for RDA, assumes reward ( ) for identifying the dependent requirements (TP) while penalizing ( ) instances that were falsely identified as independent (FN). In this empirical analysis, we compared AL with a traditional random sampling based classification-Baseline -using the RF ML algorithm.\n\nBeginning with 60 training samples of each class (REQUIRES, INDEPENDENT and OTHER), we developed multi-class classifiers for both AL and Baseline for this empirical study scenario. When AL used MinMargin sampling technique 3 to identify 20 4 most uncertain instance (requirement pair) for oracle to label, baseline randomly selected 20 instances and added to the training set along with their label, thus, kept the two approaches comparable in all the 20 iterations. Since data is already labeled, for AL, we pretend they are unlabeled until queried and labeled by a simulated oracle in this scenario. The is determined by first computing the sum of total processing time in person hours (= ) taken for data processing ( = + + )),",
            "score": 0.49594059970398485,
            "section_title": "ROI Modeling",
            "char_start_offset": 14614,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "274280560",
            "title": "Non-Contextual BERT or FastText? A Comparative Analysis",
            "text": "Word embedding is a way of representing words into dense vectors in a continuous space such that the vectors capture the semantic relationship between the words for the models to understand the context and meaning of the text. FastText, a context-independent method, basically captures the subword information, enabling it to learn rare words, misspelled words, and out-of-vocabulary words. It is recognized in the NLP community for its efficient performance in tasks like text classification and sentiment analysis. Despite being relatively old, it still remains one of the most effective alternatives when performing tasks on large datasets across various languages due to its subword-based approach. \n\nBERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) word embeddings understand the meaning of a word based on its context in a sentence. The embeddings extracted just before the first embedding layer of the BERT architecture are referred to as non-contextual embeddings, while those obtained from the last hidden layer of BERT are known as contextual embeddings (Refer Fig 1). Numerous variations of BERT like IndicBERT (Kakwani et al., 2020), MuRIL (Khanuja et al., 2021), AfriBERT (Ralethe, 2020), and mBERT (Devlin et al., 2018) to name a few, are available for experiments. \n\nRecent studies have experimented with both Fast-Text and BERT for various tasks; however, most of them focus on exploring contextual BERT embeddings. Experiments of D 'Sa et al. (2020) demonstrated that BERT embeddings outperformed Fast-Text for classifying English text into toxic and nontoxic. Findings of Ahmed et al. (2024) suggested that BERT embeddings outperformed those of Fast-Text with an F1 score of 84% when evaluated for depressive post-detection in Bangla. \n\nWhile BERT consistently outperforms other word embeddings in various tasks for high-resource languages (HRLs) like English (Malik et al. (2021)), its effectiveness in low-resource languages (LRLs) remains relatively underexplored.",
            "score": 0.49579905337598906,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 1158,
                    "end": 1180,
                    "matchedPaperCorpusId": "267935553"
                },
                {
                    "start": 1221,
                    "end": 1236,
                    "matchedPaperCorpusId": "218973721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "273695318",
            "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
            "text": "Classical language models: \n\n\u2022 BERT Devlin et al. ( 2018): BERT stands out as one of the most well-known transformer-based language models. It is pretrained through masked language modeling (MLM), where it learns to predict words that have been masked, using context for guidance. This pretrained model is then fine-tuned for a variety of downstream tasks, showcasing its versatility and effectiveness in diverse applications. In our experiments, we will use BERT-110M. \n\n\u2022 RoBERTa Liu et al. ( 2019): RoBERTa is developed to overcome certain limitations of the original BERT model. This is accomplished by implementing key modifications such as increasing the batch size, extending the training epochs, and employing advanced optimization techniques. \n\nAs a result of these strategic changes, RoBERTa has demonstrated substantial performance improvements over BERT across various NLP benchmarks. In our experiments, we will use RoBERTa-125M. \n\n\u2022 ALBERT Lan et al. (2020): ALBERT is a lite variant of BERT. It is achieved by decoupling the word embedding from the hidden embedding, significantly cutting down the number of parameters. \n\nTo further enhance its efficiency, ALBERT employs cross-layer parameter sharing, ensuring that all layers use the same parameters. The reductions not only minimize memory footprint but also improve the efficiency of the model. In our experiments, we will use ALBERT-12M. \n\n\u2022 \u2022 LLaMA Touvron et al. ( 2023): LLaMa, the Large Language Model developed by Meta AI, represents a cutting-edge advancement in language modeling. Trained on publicly available datasets, LLaMa is available in various sizes to suit different computational needs. Notably, LLaMa-13B demonstrates superior performance over GPT-3 in most benchmarks, highlighting its exceptional effectiveness and capability in NLP tasks. In our experiments, we will use LLaMA-7B. \n\n\u2022 Vicuna Chiang et al. ( 2023): Vicuna is a high-performing, open-source chatbot that impresses with capabilities comparable to GPT-4. Fine-tuned from the LLaMa model, it utilizes user-shared conversations gathered from Share-GPT for its training. Remarkably, Vicuna achieves 90% of the performance level of GPT-4, despite having only 13 billion parameters, showcasing its efficiency and effectiveness.",
            "score": 0.49579244649034915,
            "section_title": "D.2 Backbone Architectures",
            "char_start_offset": 43922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 29,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1870
                },
                {
                    "start": 1873,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1407470703125
        },
        {
            "corpus_id": "229449234",
            "title": "Fine-Grained Mechanical Chinese Named Entity Recognition Based on ALBERT-AttBiLSTM-CRF and Transfer Learning",
            "text": "BERT [20]: BERT is a Transformer-based model, which uses pre-training to learn from text data, and fine-tuning on different downstream tasks, such as NER task. \n\nRoBERTa [41]: RoBERTa builds on the language masking strategy of BERT, modifying key hyper-parameters in BERT, including removing the next sentence of BERT's pre-training target. \n\nALBERT [23]: ALBERT is a lightweight BERT that achieves excellent results, which reduce the number of model parameters through factorization and parameter sharing. \n\nBiLSTM-CRF [42]: The BiLSTM-CRF model takes into account the ability of the BiLSTM model to remember the context address and retains the strength of the CRF algorithm to control the address labeling output by transferring the probability matrix. \n\nHuman Performance [40]: To better understand the difficulty of the task and the performance of the modern model compared to humans, human performance tests were conducted in the experiment. \n\nEn2BiLSTM-CRF [43]: Model contains initial encoding layer, enhanced encoding layer, and decoding layer combines the advantages of pre-trained model encoding, dual BiLSTM, and a resident connection mechanism. \n\nALBERT-AttBiLSTM-CRF (ours): Character-level word embeddings are trained on large-scale text via the ALBERT pretrained language model, which is then fed into the BiLSTM model's contextual information, and finally the corresponding entity labels are obtained via CRF. According to the different sizes of ALBERT pretrained corpus and the number of model parameters, ALBERT editions are divided into tiny, base, large, and so on.",
            "score": 0.4953672022672456,
            "section_title": "Baseline Algorithms",
            "char_start_offset": 52159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "125081185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.115966796875
        },
        {
            "corpus_id": "222450148",
            "title": "Hate Speech Detection in Twitter using Transformer Methods",
            "text": "Specifically, the performance of the LSTM drops as sequence length increases beyond thirty words [7]. \n\nFurther research that is aimed at addressing the problem of long-term dependencies has given birth to the attention mechanism [7]. Attention allows modeling of dependencies irrespective of the distance between input and output sequences [8]. Attention mechanisms work on the assumption that every word in each sentence is relevant. This allows for the capture of context that may be necessary when classifying subjective text such as hate speech. Attention-based models have been investigated with success in text-related tasks [19][20][21]. However, they are used in conjunction with RNNs [9] and therefore are unable to process word sequences in parallel. For a large corpus of text, this may significantly affect the processing time. \n\nRecent adaptations of attention approach have shifted methods progressively from RNNs to self-attention and transformers [22]. Transformer has rapidly become the dominant architecture for natural language processing (NLP), outperforming RNNs in natural language generation and natural language understanding [23]. The transformer architecture scales well with training data and model size while facilitating efficient parallelization and capturing long-range sequence features. In addition, transformers allow transfer learning by fine-tuning large pre-trained language models for downstream NLP tasks with a relatively small number of training examples, resulting in an improved performance regardless of dataset size [24]. This is particularly important when dealing with highly imbalanced datasets with few instances of hate speech. \n\nThere are several types of transformer methods that have been investigated with success in NLP research. Bidirectional encoder representations from text (BERT) [22] has surpassed previous performance benchmarks in common NLP tasks [25]. BERT uses vast unlabeled data for creating models whose parameters can be tuned as desired for smaller supervised data to improve performance. The success of BERT has led to the development of several algorithms based on BERT architecture. These algorithms include RoBERTa [26], DistilBERT [27] and XLNET [28]. RoBERTa is an enhancement of BERT, which is trained on a bigger dataset to improve performance while DistilBERT learns a streamlined version of BERT.",
            "score": 0.4951158213741279,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 6695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "18993998"
                },
                {
                    "start": 694,
                    "end": 697,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2223,
                    "end": 2227,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1900634765625
        },
        {
            "corpus_id": "276558383",
            "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale",
            "text": "FastText [22] family of binary classifiers have been used in prior datasets [9,11] for identifying highquality pre-training documents. Recently, [11] showed that fastText classifier trained on carefully selected data can outperform sophisticated model-based filtering approaches such as AskLLM (prompting an LLM to ask if a document is helpful). Inspired by their effectiveness coupled with the computational efficiency of fastText classifiers, we use fastText classifiers for quality annotations. \n\nWe employ two fastText classifiers: (i) the fastText classifier from [11] trained on a mix of instruction-formatted data (OpenHermes-2.5 [35]) and high-scoring posts from ELI5 subreddit [36] and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value. \n\nSpecifically, we use the supervised fastText package from [22] to train a classifier on 400k documents, equality split between positive (i.e., high-quality) and negative (i.e., low-quality) classes, selected as follows. \n\n\u2022 Positive documents: \n\n-190k synthetic documents randomly sampled from the Cosmopedia dataset -an open synthetic dataset consisting of textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1 [37]. -10k documents with high educational value selected as follows: we annotated 600k random documents from FineWeb-V1.1.0 asking Mixtral-8x22B-Instruct to score each document between 1 to 5 for its educational quality (with 5 being the highest quality), using a prompt similar to the one used by FineWeb-Edu. Next, we selected 10k random documents from the documents with scores \u2265 4. \n\n\u2022 Negative documents: 200k random documents out of the 600k Mixtral-annotated documents with scores \u2264 2. \n\nWe denote the DCLM-fastText as \u03d5 DCLM and our custom fastText as \u03d5 Cosmo .",
            "score": 0.49506449754752213,
            "section_title": "FastText Classifiers",
            "char_start_offset": 10794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 9,
                    "end": 13,
                    "matchedPaperCorpusId": "1210515"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "274140947"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29638671875
        },
        {
            "corpus_id": "235421844",
            "title": "A Sentence-level Hierarchical BERT Model for Document Classification with Limited Labelled Data",
            "text": "Partially following [15], we compare HBM with several baseline methods. We selected an SVM model using a document representation based on FastText, an SVM model using text representations based on a pre-trained Roberta model and a fine-tuned Roberta model as baselines due to their strong performance. We also used another sentence-level model, Hierarchical Attention Network (HAN) [18], as a baseline. \n\nFastText + SVM: We use 300-dimensional word vectors constructed by a Fast-Text language model pre-trained with the Wikipedia corpus [7]. Averaged word embeddings are used as the representation of the document. For preprocessing, all text is converted to lowercase and we remove all punctuation and stop words. SVM is used as the classifier. We tune the hyper-parameters of the SVM classifier using a grid-search based on 5-fold cross-validation performed on the training set, after that, we re-train the classifier with optimised hyper-parameters. This hyper-parameter tuning method is applied in Roberta + SVM as well. \n\nRoberta + SVM: We use 768-dimensional word vectors generated by a pre-trained Roberta language model [9]. We do not fine-tune the pre-trained language model and use the averaged word vectors as the representation of the document. Since all BERT-based models are configured to take as input a maximum of 512 tokens, we divided the long documents with W words into k = W/511 fractions, which is then fed into the model to infer the representation of each fraction (each fraction has a \"[CLS]\" token in front of 511 tokens, so, 512 tokens in total). Based on the approach of [14], the vector of each fraction is the average embeddings of words in that fraction, and the representation of the whole text sequence is the mean of all k fraction vectors. For preprocessing, the only operation performed is to convert all tokens to lowercase. SVM is used as the classifier. \n\nFine-tuned Roberta: For the document classification task, fine-tuning Roberta means adding a softmax layer on top of the Roberta encoder output and finetuning all parameters in the model.",
            "score": 0.49495454267229855,
            "section_title": "Baselines and Setup",
            "char_start_offset": 14126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "6857205"
                },
                {
                    "start": 1599,
                    "end": 1603,
                    "matchedPaperCorpusId": "153312532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "251237780",
            "title": "Clickbait Detection of Indonesian News Headlines using Fine-Tune Bidirectional Encoder Representations from Transformers (BERT)",
            "text": "M. Bilal and A.A. Almazroi [11] used a generalized approach to identify helpful and unhelpful reviews from Yelp shopping reviews by fine-tuning the BERT base model. The bag-of-words approaches with machine learning classifiers including KNN, Na\u00efve Bayes, and SVM were compared. The results demonstrated that BERT-based classifiers with finetuning outperform bag-of-words techniques. S. Gonzalez-Carvajal and E.C. Garrido-Merchan [12] used several scenarios with varying languages and dataset sources to evaluate BERT, and the conventional TF-IDF vocabulary fed to machine learning algorithms. The experiment results added empirical support for using BERT as the default technique in the NLP problem by demonstrating its superiority and independence from NLP problem aspects. \n\nThe BERT method is a state-of-the-art deep learning approach for Natural Language Understanding (NLU) problems based on transformers. This method proposed a faster architecture to train a language model that eliminates recurrences by using a multi-head attention layer. More rapid development, lower data requirements, and improved performance are advantages of employing BERT [13]. B. Willie et al. [14] introduced the first-ever comprehensive resource for training, evaluating, and benchmarking Indonesian NLU (IndoNLU) tasks. This resource includes twelve tasks, ranging from the classification of a single sentence to the labeling sequences of sentences with different complexity levels. This study also provides a collection of pre-trained Indonesian models called IndoBERT. These models were trained using Indo4B, a large, clean Indonesian dataset. This dataset was obtained from publicly accessible websites, social media posts, blogs, and news articles. \n\nThe aim of this paper use fine-tuning IndoBERT to identify clickbait and non-clickbait from the Indonesian news headlines dataset CLICK-ID [8]. The BERT tokenizer generated contextualized token embeddings. Thus, no handcrafted features are required.",
            "score": 0.4949177026760984,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "248428847"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "221640658"
                },
                {
                    "start": 1880,
                    "end": 1883,
                    "matchedPaperCorpusId": "221704627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44140625
        },
        {
            "corpus_id": "232320374",
            "title": "Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection",
            "text": "To check whether our dataset poses a realistic challenge for state-of-the-art classifiers and to establish a performance benchmark, we employed four models to label paragraphs as either original or paraphrased. A prior study showed that current plagiarism detection systems, which are essentially text-matching software, fail to identify machine-paraphrased text reliably while word embeddings, machine-learning classifiers, and particularly Transformer-based models performed considerably better [13]. Therefore, we evaluated the classification effectiveness of the three BERT-related models used for paraphrasing and the fastText + SVM classifier we applied and described for our ablation study (cf. Section III). We limited the number of input tokens for each model to 512 for a fair comparison of the models without losing relevant context information 9 . Unless specified differently, we used all hyperparameters in their default configuration. \n\nWe derived training data exclusively from Wikipedia as it is the largest of the three collections. We used arXiv papers and theses to obtain test sets that allow verifying a model's ability to generalize to data from sources unseen during training. We used BERT to generate the paraphrased training set (Wiki-Train) and BERT, RoBERTa, and Longfomer to create three paraphrased test sets. The classification models were exposed to mutually exclusive paragraphs to avoid memorizing the differences between aligned paragraphs. Evaluating each model using text paraphrased by the same model allows us to verify an assumption from related work, i.e., the best classifier is the language model used to generate the paraphrased text [14]. \n\nTable III shows the F1-Macro scores of each classification model for the paraphrased test sets consisting of arXiv, theses, and Wikipedia paragraphs. The baseline model (fastText +",
            "score": 0.49490147415697355,
            "section_title": "IV. CLASSIFICATION BENCHMARK",
            "char_start_offset": 10202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1866
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382080078125
        },
        {
            "corpus_id": "230510598",
            "title": "Leveraging Pre-Trained Language Model for Summary Generation on Short Text",
            "text": "The research of pre-training language models is mainly aimed at language understanding tasks, which can usually be classified into feature-based models and fine-tuning-based models according to their characteristics [1]. Feature-based methods mainly use pre-training models to provide language representations and features for the downstream tasks [10]. EMLo [3] used a bidirectional LSTM [11] language model to obtain a context-sensitive pre-trained representation. In the supervised task, they are spliced into the word vector input or the top level representation of the model as features. GPT [4] used Transformer [12] network instead of LSTM [11] as a language model to better capture long-distance language structures. When applied to downstream tasks, GPT [4] does not need to rebuild a new model structure, and can effectively improve the generalization ability of supervised models and accelerate convergence. The fine-tuning methods are mainly to pre-train the model on the language modeling target, and then fine-tune the model on the downstream tasks with supervised data. The BERT model uses ''masked language modelling'' and ''next sentence prediction'' methods to train on a large-scale corpus. BERT can be widely used because it can be applied to multiple downstream tasks of natural language processing by fine-tuning and achieving outstanding results. Unlike the BERT pre-training model, ALBERT [6] is faster to train and uses less memory. ALBERT used factors such as factorization and cross-layer parameter sharing to reduce model parameters and improve training speed effectively. \n\nIn past research, pre-trained language models are usually applied to natural language understanding tasks to improve their performance. Recently, many scholars have applied pre-trained language models to generation tasks. \n\nFor example, BERT can fine-tune its parameters with specific generation task parameters. In this article, we try to use the BERT model for the text summary generation task.",
            "score": 0.49465433918263674,
            "section_title": "A. PRETRAINED LANGUAGE MODELS",
            "char_start_offset": 4120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "146808476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "267686996",
            "title": "ESG2PreEM: Automated ESG grade assessment framework using pre-trained ensemble models",
            "text": "The proposed NLP approach involves a transformer architecture-based language modeling methodology that learns general language properties and can be used to identify linguistic characteristics. This approach includes pretraining and downstream steps, in which a large corpus is pre-trained to capture the general attributes of the language. Pretraining helps generalize downstream tasks that work with small datasets. Although pretraining is costly, it improves the performance and convergence of the downstream model. However, the data distribution of a specific domain may differ from that of the pretraining data. Thus, fine-tuning was employed to tune the Language Model (LM) for the data to be handled in the downstream stage. Given a pre-trained general language model, this step can achieve faster convergence and robust performance even on small datasets by learning distinct data specialties [40]. Specifically, by fine-tuning the model using our news dataset, the model reflected the subjectivity and biases inherent in the news itself. The authors experimented with various models, such as BERT, RoBERTa, and ALBERT, which are applicable to the ESG domain. Compared to other LMs, BERT-based language models have the advantage of facilitating optimization during fine-tuning [41]. Moreover, through sequential learning of multiple tasks, BERT improves the overall performance in the downstream stage [42]. \n\nThe encoder component of the transformer was used to develop the BERT model [43]. In the pretraining process, two unsupervised learning tasks were performed using unlabeled data. The first is Masked LM. In this process, a certain percentage of the input tokens were masked, and the masked tokens were predicted. The second is the Next Sentence Prediction (NSP), which learns whether two sentences are connected or not. The data used in these processes are from BookCorpus and English Wikipedia [44]. Subsequently, in the fine-tuning step, the parameters were tuned using the labeled data. Ultimately, BERT presents strength in language understanding tasks. The entire structure is illustrated in Fig. 4. \n\nRoBERTa has emerged to complement the lack of BERT training in various validation experiments. BERT uses static masks, whereas RoBERTa learns different masked sentences in each epoch. Additionally, when the NSP task is excluded, the model performance is similar or slightly better.",
            "score": 0.4946264710912781,
            "section_title": "System architecture: pre-trained language model",
            "char_start_offset": 16170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2121
                },
                {
                    "start": 2124,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2307
                },
                {
                    "start": 2308,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 1410,
                    "end": 1414,
                    "matchedPaperCorpusId": "222296346"
                },
                {
                    "start": 1494,
                    "end": 1498,
                    "matchedPaperCorpusId": "253312691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1419677734375
        },
        {
            "corpus_id": "272753509",
            "title": "InfiMM-WebMath-40B: Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning",
            "text": "Our initial fastText classifier (Sec. 3.1.3), employed positive samples from the OpenWebMath dataset [53] and negative samples from our own data extraction from CommonCrawl. To enhance the accuracy of our labeling process, we leveraged the LLaMA3-70B-Instruct model [17], employing prompt formats inspired by the FineWeb-Edu dataset [45]. This allowed us to score the mathematical quality of each sample on a scale from 0 to 10. Table 6 in Appendix A displays the full prompt. \n\nFrom the data remaining after rule-based filtering, we randomly sample approximately one million entries. Using the vLLM inference engine [32], we assign math quality scores and apply a threshold of 6 to select positive samples for training our updated fastText classifier. This process yielded 640,000 positive samples. We then randomly select an equivalent number of 640,000 negative samples as outlined in Sec. 3.1.3. These positive and negative samples are combined to train the new fastText classifier. 5 uring fastText training, we implement data cleaning rules to optimize the model's performance for mathematical content. Mathematical texts pose unique challenges due to specialized terminology, symbols, formulas, and numeric data, which differ from typical natural language and require more refined preprocessing techniques. \n\nOur goal is to standardize and simplify the input training data while preserving essential mathematical information. Key considerations include maintaining consistency in token representation, minimizing noise from extraneous characters, and standardizing numeric values. The following steps reflect this approach: \n\n\u2022 Utilizing the SpaCy English language model (en_core_web_sm), we preprocess the input text, tokenize it, and process each token by converting it to its lowercase and lemmatized form. Common placeholders are replaced, certain non-alphanumeric characters are removed, and patterns of special characters like dashes and underscores are normalized. We also strip any unnecessary whitespace, ensuring the text is well-prepared for downstream processing. \n\n\u2022 All numeric values are replaced with the <NUM> placeholder to standardize the representation, and line breaks along with carriage returns are removed. Tokens exceeding 100 characters in English are discarded.",
            "score": 0.49456334350633296,
            "section_title": "High-Precision Filtering for Mathematical Content",
            "char_start_offset": 18955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10577392578125
        },
        {
            "corpus_id": "270599834",
            "title": "Googling Politics? Comparing Five Computational Methods to Identify Political and News-related Searches from Web Browser Histories",
            "text": "To overcome drawbacks of dictionary and traditional SML, researchers have recently successfully applied sophisticated Large Language Models (LLMs), many of which are built on the Transformer architecture, such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) and GPT-3 (Brown et al., 2020). LLM methods have shown to outperform other methods for classifying related concepts like news genres in news articles (Lin et al., 2023), sentiment in news headlines (Van Atteveldt et al., 2021), and PNR content in web content (Makhortykh et al., 2022) and website titles (Wojcieszak et al., 2024). \n\nLLMs are pre-trained on large collections of text to learn relationships between words and their surrounding context, allowing them to obtain a general understanding of language. One of the most popular LLMs is BERT, which is a language model developed by Google (Devlin et al., 2018). By fine-tuning a BERT model with a much smaller set of annotated examples compared to Traditional SML, it updates its parameters to be better able to understand PNR search terms while retaining its general understanding of language. This enables a transformer-based classifier to understand the semantic meaning of search terms, even when these terms are not represented in the annotations. For instance, even if the training data does not include a search for \"Joe Biden,\" these models may still be able to understand its political meaning. While sparse search terms may still pose some challenge for transformer-based SML, LLM approaches are expected to better understand searcher's intent expressed in (subtle) textual differences, if such intent is evident from the wording. \n\nThis increase in performance requires sacrificing simplicity and transparency, as well as needs large computational resources, typically requiring GPU access for fine-tuning. Despite the ability of LLMs to understand a wide range of words, the data they are trained on is critical, as they cannot recognize PNR concepts emerged beyond their training date.",
            "score": 0.4944036782477508,
            "section_title": "Transformer-based supervised machine learning",
            "char_start_offset": 15449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 466,
                    "matchedPaperCorpusId": "254408542"
                },
                {
                    "start": 496,
                    "end": 524,
                    "matchedPaperCorpusId": "234007474"
                },
                {
                    "start": 602,
                    "end": 627,
                    "matchedPaperCorpusId": "260819984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2255859375
        },
        {
            "corpus_id": "254854433",
            "title": "Less is More: Parameter-Free Text Classification with Gzip",
            "text": "We further compare the result of gzip under the fewshot-learning setting with deep learning methods. We first carry out experiments on AGNews, DBpedia and SogouNews across both non-pretrained deep neural networks and pre-trained ones using n-shot labeled examples per class from training dataset, where n = {5, 10, 50, 100}. We chose these three datasets as their scale is large enough to cover 100-shot setting and they vary in text lengths as well as languages. We choose methods whose trainable parameters range from zero parameters like word2vec and sentence BERT to hundreds of millions of parameters like BERT, covering both word-based models (HAN) and the n-gram one (fasttext). \n\nThe result is plotted in Figure 1 (detailed numbers are shown in Appendix C). As we can see, gzip outperforms non-pretrained models on 5, 10, 50 settings for all three datasets and especially in the n = 5 setting, gzip outperforms deep learning models by large margin. For example, the accuracy of gzip is 115% better than fasttext on AGNews 5-shot setting. In the 100-shot setting, gzip also outperforms non-pretrained models on AGNews and SogouNews but is a little bit lower than them on DBpedia. \n\nIt's been investigated in the previous work (Nogueira et al., 2020;Zhang et al., 2021) that pre-trained models are excellent few-shot learners. The advantages of BERT and SentBERT on the AGNews are obvious where they achieve the highest and the second highest accuracy on every shot number. However, on SogouNews, both BERT and SentBERT are surpassed by gzip on every shot number, consistent with the result on full dataset. This is reasonable as the inductive bias learned from the pre-training data is so strong-notice how low the accuracy is when only given 5-shot training samples to BERT, that hinders BERT to be applied to the dataset that's significantly different from the pre-trained datasets.",
            "score": 0.49426504743690913,
            "section_title": "Few-Shot Learning",
            "char_start_offset": 16934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 1233,
                    "end": 1256,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1256,
                    "end": 1275,
                    "matchedPaperCorpusId": "237492027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1461181640625
        },
        {
            "corpus_id": "252542956",
            "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
            "text": "It has proven to be a very successful pattern in deep learning to pre-train models on large scale unlabeled data via careful designed self-supervision tasks and then transfer to downstream tasks by fine-tuning on them. Downstream tasks can benefit from the learned representations from the pre-trained models. BERT [2] is a bidirectional transformer based contextualized language model pre-trained on large scale text corpus English Wikipedia and BooksCorpus. It is pretrained via carefully designed self-supervision tasks: masked language modeling (MLM) task where random word tokens of the input text are replaced by a special token [MASK] which is to be predicted by the model from the context, and the next sentence prediction (NSP) task where two sentences are to be predicted whether the second one is probable given the first one. The pre-trained BERT provides contextualized word representations that can be used by downstream tasks by just fine-tuning on the tasks and has achieved great success on various natural language understanding tasks. Subsequent works mainly focus on pre-training on larger-scale data and models [3] and advanced pre-training task [4]. Though BERT and various biomedical BERT models have been successful in language understanding tasks and classification tasks, few efforts have been devoted to generative models. As BERT learns powerful word representations through the Transformer encoder model architecture in a bi-directional way, it limits its ability of generation. \n\nGenerative Pre-trained Transformer (GPT) [5] is proposed for language generation tasks via pre-training Transformer decoder model on large scale text corpus in a classical casual language modeling task where model learns to predict the next word token only dependent on the previous word tokens. Further, GPT-2 [6] and GPT-3 [7] with larger model size pre-trained on larger scale text corpus are proposed with remarkable performance in various downstream tasks (e.g., translation, summarization) including classification tasks (e.g., reading comprehension) even without fine-tuning (zero-shot) via appropriate prompts design.",
            "score": 0.4936599980580529,
            "section_title": "Related Work Pre-trained Language Models",
            "char_start_offset": 3393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 318,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1167,
                    "end": 1170,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1835,
                    "end": 1838,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166015625
        },
        {
            "corpus_id": "153312532",
            "title": "How to Fine-Tune BERT for Text Classification?",
            "text": "Text classification is a classic problem in Natural Language Processing (NLP). The task is to assign predefined categories to a given text sequence. An important intermediate step is the text representation. Previous work uses various neural models to learn text representation, including convolution models, recurrent models, and attention mechanisms. \n\nRecently, pre-trained language models have shown to be useful in learning common language representations by utilizing a large amount of unlabeled data: e.g., ELMo [20], OpenAI GPT [22] and BERT [6]. Among them, BERT is based on a multi-layer bidirectional Transformer [24] and is trained on plain text for masked word prediction and next sentence prediction tasks. \n\nAlthough BERT has achieved amazing results in many natural language understanding (NLU) tasks, its potential has yet to be fully explored. There is little research to enhance BERT to improve the performance on target tasks further. \n\nIn this paper, we investigate how to maximize the utilization of BERT for the text classification task. We explore several ways of fine-tuning BERT to enhance its performance on text classification task. We design exhaustive experiments to make a detailed analysis of BERT. \n\nThe contributions of our paper are as follows: \n\n-We propose a general solution to fine-tune the pre-trained BERT model, which includes three steps: (1) further pre-train BERT on within-task training data or indomain data; (2) optional fine-tuning BERT with multi-task learning if several related tasks are available; (3) fine-tune BERT for the target task. \n\nCorresponding author. \n\n-We also investigate the fine-tuning methods for BERT on target task, including layer-wise learning rate, catastrophic forgetting, and few-shot learning problems. \n\n-We achieve the new state-of-the-art results on eight widely-studied text classification datasets.",
            "score": 0.4936500446566,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1880
                }
            ],
            "ref_mentions": [
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34716796875
        },
        {
            "corpus_id": "265514554",
            "title": "Uncovering the semantics of concepts using GPT-4",
            "text": "The typicality of the text document in a concept is then obtained by taking the logarithm of this categorization probability. In a comparative analysis, we achieved the best performance by \"fine-tuning\" a text classifier based on BERT on a large amount of training data made of labelled text documents. \n\nThe measure we propose in the present article, based on GPT-4, departs from this prior approach in two ways: It does not involve the specification of a classifier at any stage in the construction of the measure, and it does not involve any model training on the research data-it relies on zero-shot learning. \n\nSeveral recent papers have assessed the classification performance of recent LLMs such as GPT-3 (17), GPT-3.5 Turbo (18)(19)(20)(21), FLAN (17,20) and 22). * The overall picture that emerges from these analyses is that recent LLMs, and in particular GPT-4, are excellent text classifiers even in settings with many candidate categories. Based on this prior work, especially the studies that found high classification performance in which the benchmark was not a ground truth but a categorization judgment by experts, it is reasonable to conjecture that the latest generation of LLMs could be used to produce measures of typicality that achieve a high correlation with human judgment. At the same time, we are not aware of prior work reporting an empirical test of this conjecture. \n\nA recent study used prompts submitted to a Large Language Model (LLM) to construct a continuous measure akin to typicality (23). The authors queried GPT-3 to classify passages from party manifestos (initially used in ref. 24) as Liberal, Conservative, or Neither. GPT-3 returns the probability of the top 5 candidate tokens for text completion. They used this functionality to compute a passage's ideology score as the model's estimated \"Conservative\" probability minus the estimated \"Liberal\" probability. In terms of the approach we introduced in our previous paper (6), they used GPT-3 to compute the typicality of a passage in the Conservative concept, its typicality in the Liberal concept, and defined its ideological position as the difference between the two typicalities.",
            "score": 0.4936373893444106,
            "section_title": "Related Literature",
            "char_start_offset": 6364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "257766307"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "257050407"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "14326352"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18505859375
        },
        {
            "corpus_id": "252438677",
            "title": "Scope of Pre-trained Language Models for Detecting Conflicting Health Information",
            "text": "RQ1: How effective pre-trained language models are in detecting conflicting health information? \n\nModel Analysis We report the performance of 5 different transformer-based classifiers in Table 6 as well as a random guess, lexical feature, and word embedding-based baseline. The lexical model, which uses TFIDF (Manning and Prabhakar 2010) for feature generation with a class-weighted Random Forest classifier (Breiman 2001), under-performs random guess on 90% of experiments across all conflict types. This is expected as TFIDF generates features based on word count and frequency, ignoring the fact that the input text is comprised of two disjoint advice statements which need to be compared. Additionally, we note that the Random Forest model was unable to overcome the severe class im- balance with TFIDF features, often over-predicting the nonconflicting class for all labels. \n\nThe static embedding methods, SBERT and GloVE, provide an improvement over our lexical baseline while outperforming random guess in most experiments. However, static embedding methods are not constructed to perform pair-wise comparison of longer texts. Thus, all textual comparison must happen in the classification head, which is shown to be inferior to the dynamic embedding approaches shown in Table 6. \n\nTransformer-based models such as BERT provide significant improvement over both TFIDF and static embeddings, as their contextual embedding approach permits direct comparison of two texts. The BERT baseline, however, is shown to be inferior to more modern transformer models such as RoBERTa and DeBERTa-v3, both of which outperform BERT on 80% of experiments. From our experiments using Bio+Clinical BERT (B+C BERT), we show that BERT's performance issues are not due to BERT's lack of domain specific training data, as B+C BERT under-performs or matches BERT's performance on 70% of experiments. This again follows intuition as, while this task is called health conflict detection, the addition of knowledge from sources like biomedical corpora and clinical notes are intuitively not relevant to this task. Health advice contains common language alongside occasional references to medications or physiological effects (as shown in Table 1).",
            "score": 0.4935744049691844,
            "section_title": "Results",
            "char_start_offset": 22389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1884765625
        },
        {
            "corpus_id": "266167098",
            "title": "Frequency Balanced Datasets Lead to Better Language Models",
            "text": "Historically, using more training data has been considered a key to improving the performance of natural language processing (NLP) tools. The same seems to be true for transformer-based large language models (LLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) that exhibit their best performance when being trained with really massive quantities of texts. However, the question arises whether more training data is the only factor for improving results or whether specific characteristics of the provided texts have a particular impact on the learning efficiency of these powerful models. Recent research (Wei et al., 2021;Razeghi et al., 2022) has found evidence about the impact of the absolute frequency of pretraining data tokens in the prediction capacities of the model. Besides, high-frequency terms have proved to be behind some phenomena related to the geometry of the representations causing problems for so called semantic tasks that rely on similarity assessments (Ethayarajh, 2019;Fuster-Baggetto and Fresno, 2022). In this paper, we report on our experiments to better understand the impact of token frequency in the pre-training data on language model (LM) quality, in terms of perplexity, and on the quality of the representations that these models provide for fine-tuning. Our ultimate goal is to study to what extent the quantity of pre-training data could be lessened, since this could be of great importance for training LM for low-resource languages (LRLs). \n\nTexts are known to follow Zipf's law with some tokens occurring very frequently, some occurring with medium frequency, and many tokens that rarely occur forming a long tail. Different disciplines have addressed the problems of using long-tailed training data where some elements appear on most of the data but most of the elements are under-represented. Under-sampling and oversampling techniques to directly adjust the number of examples by removing and adding data respectively have been proposed (Cui et al., 2019;Raunak et al., 2020). In this context, we propose an under-sampling algorithm whose objective is to balance token frequency by removing the sentences in which high-frequency tokens (and bigrams) occur, thus improving the estimation of model probabilities for low-frequency tokens.",
            "score": 0.49326948490362,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 249,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 626,
                    "end": 644,
                    "matchedPaperCorpusId": "237513713"
                },
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "256631096"
                },
                {
                    "start": 997,
                    "end": 1015,
                    "matchedPaperCorpusId": "202120592"
                },
                {
                    "start": 1015,
                    "end": 1048,
                    "matchedPaperCorpusId": "256631003"
                },
                {
                    "start": 2019,
                    "end": 2039,
                    "matchedPaperCorpusId": "222290834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12841796875
        },
        {
            "corpus_id": "243865615",
            "title": "Detecting Health Advice in Medical Research Literature",
            "text": "LinearSVM: We chose the SVM algorithm with different vectorization methods to train the advicetype classifier using the Scikit-learn python package (Pedregosa et al., 2011). The penalty value C in LinearSVM was set to 1. A comparison of different word vector representation methods showed that the tf-idf vectorization performed similarly to the count vectorization, and adding bigrams also improved the SVM model's performance. BERT: BERT is a recent method for pre-training language representations, and it has achieved stateof-the-art results in a number of NLP tasks (Devlin et al., 2019;Fan et al., 2020). As for suggestion mining, the BERT-based model also outperformed  (Lee et al., 2020). In this study, we used the same BERT parameter settings as described above, except with the utilization of the BioBERT pre-trained model rather than the cased BERT-base one. We hypothesized that BioBERT would perform the best, followed by BERT and SVM.",
            "score": 0.4932210226524618,
            "section_title": "Prediction Model",
            "char_start_offset": 14295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 949
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 592,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 592,
                    "end": 609,
                    "matchedPaperCorpusId": "208092878"
                },
                {
                    "start": 677,
                    "end": 695,
                    "matchedPaperCorpusId": "59291975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.248046875
        },
        {
            "corpus_id": "221802461",
            "title": "FarsTail: A Persian Natural Language Inference Dataset",
            "text": "We used different methods for representing the input sentences ranging from traditional TF-IDF to more recent word embedding methods such as word2vec3 [11], fastText 4 [12], ELMo5 [13], and BERT [14].For the BERT method, we fine-tuned two pre-trained models from the Hugging Face Transformers library [32], ParsBERT [33] and BERT-base-multilingual-cased (mBERT).\n\nAs the classifier, we exploited different methods including Support Vector Machine (SVM), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU) along with three models developed specially for the NLI task including DecompAtt [16], ESIM [17], and HBMP [18].\n\nOne popular approach in learning with small labeled training datasets is to train a language model (LM) on a large unlabeled corpus and fine-tune it on the downstream task.Besides the BERT-based models which lie in this category, we tested ULMFiT [19] with three steps: LM pre-training, LM fine-tuning, and classifier fine-tuning.In the first step, a language model was trained on a general-domain corpus.We used the Persian Wikipedia for this purpose.Then, the trained LM was fine-tuned on the target task texts without considering their labels.Finally, the pre-trained language model was augmented with additional layers which were trained on the labeled dataset of the target task.\n\nWe also tested LASER6 [15] as an embedding space which is shared between multiple languages.Since LASER provides sentence embeddings rather than word embeddings, a simple deep model was trained on the computed representations.\n\nThe hyper-parameters were chosen based on the models' accuracy on the validation set.Most importantly, we selected the following values for the BERT models: 3 epochs of training with a learning rate of 2e-5, a batch size of 32, and a weight decay of 0.5.",
            "score": 0.4932097661802848,
            "section_title": "Models",
            "char_start_offset": 22247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 362
                },
                {
                    "start": 364,
                    "end": 628
                },
                {
                    "start": 630,
                    "end": 802
                },
                {
                    "start": 802,
                    "end": 960
                },
                {
                    "start": 960,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1314
                },
                {
                    "start": 1316,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1542
                },
                {
                    "start": 1544,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1798
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 195,
                    "end": 199,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 597,
                    "end": 601,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 608,
                    "end": 612,
                    "matchedPaperCorpusId": "34032948"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "174798261"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "56895585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.192138671875
        },
        {
            "corpus_id": "248300080",
            "title": "An Exploratory Study on Code Attention in BERT",
            "text": "There are two variants of BERT: BERT-Large and BERT-Base [10]. BERT-Base model contains 12 stacked encoders, with 12 attention heads in each layer. In our study, we use the BERT-Base cased language model as a direct comparison against the natural language, English. The cased language model is used so that the BERT model can learn the Java coding conventions for the identifiers, which promotes the use of different casing styles. As we intend to compare our results of the BERT model trained on code with natural language, we need to keep the models the same. There are multiple implementations available for BERT and its variants trained on code, which are pre-trained with different objectives (e.g., MLM and Replaced Token Detection). \n\nHowever, for the comparison, we required a BERT model that is pre-trained with the same objectives as the one used for the NLP study [8] and also should purely be pre-trained on code. We are unable to find an open-source pre-trained BERT-Base model that is trained solely on code. Therefore, we trained a BERT-Base cased model from scratch. All the pre-training decisions like the number of layers in the encoder and decoder, the casing of tokens, and the pre-training objectives are consistent with the experimental settings in the NLP study by Clark et al. [8]. For pre-training the BERT model, we used the benchmark dataset released by Husain et al. [18]. This dataset contains a large corpus for six different programming languages collected from open source repositories available on GitHub. However, we only confine our work to the Java programming language in this study (we do not require a multilingual model) to keep it consistent with the NLP study where the BERT model is only trained on a single natural language. Java is chosen as the language in our study because it is a widely adopted programming language to study program comprehension in the software engineering community. It is also a commonly used programming language in the industry. The statistics of the dataset used is shown in Table 1.",
            "score": 0.49296376600995717,
            "section_title": "Experimental Setup",
            "char_start_offset": 14043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 875,
                    "end": 878,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "184486746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0994873046875
        },
        {
            "corpus_id": "255941578",
            "title": "Which Model Shall I Choose? Cost/Quality Trade-offs for Text Classification Tasks",
            "text": "There have been great advances in building realworld natural language processing applications. The success is mostly built on deep neural networks, which typically require a large amount of annotated supervised data for training (Minaee  et al., 2021). Built on these neural architectures, researchers have taken a step further in training large language models (Radford et al., 2019; Raffel  et al., 2020). These large language models have demonstrated great performance under few-shot or even zero-shot settings, where only a small amount of annotated data is needed (Brown et al., 2020). It has emerged as a promising approach for many downstream applications. \n\nGiven this new trend, a natural question to ask is: after seeing that large language models, such as GPT-3, work at least reasonably well given only a handful of examples, whether one should directly use it in practice or still need to train one's own model? Such questions are particularly crucial for industry practitioners, as they often need to consider a number of factors when deploying a model. For example, industry practitioners need to reach a balance between reasonable model performances and a relatively low cost. Specifically, a clear picture that covers what kind of model along with how much annotation is needed to get a high performance, and what the cost implications are, is beneficial for making an informed model selection. \n\nIn this paper, we provide a thorough empirical study of the trade-offs between costs and quality for text classification models. Our choice of the text classification task is inspired by our Inquisitive Natural Conversation Agent (INCA) platform (Seltzer  et al., 2022), which provides real-world value for identifying users' information intent during their interactions with a virtual agent. We experiment with a representative selection of models, ranging from non-neural models (logistic regression and support vector machine) and traditional neural models (convolutional neural network) to the latest pre-trained language models (BERT, T5, and GPT-3). We evaluate the classification accuracy of these classifiers under a few-shot learning setting, so as to simulate situations of having only limited resources to annotate a certain number of samples.",
            "score": 0.49289914731093054,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2230224609375
        },
        {
            "corpus_id": "273022916",
            "title": "End-to-End Speech Recognition with Pre-trained Masked Language Model",
            "text": "In Table VI, we compare the BECTRA results obtained from using various pre-trained masked LMs other than BERT BASE . RoBERTa BASE 5 is an extension of BERT that is constructed with an improved pre-training procedure [80]. However, there was little improvement over the results using vanilla BERT BASE . BECTRA greatly benefited from increasing the capacity of a pre-trained LM, with BERT LARGE 6 achieving the best overall performance. \n\nBECTRA incurs a high computational cost, especially during inference, primarily due to the multiple forward passes in BERT (i.e., K = 10 times) with the O(N 2 ) computational and memory complexities in self-attention layers. To mitigate this drawback, we explored lightweight variants, including DistilBERT BASE 7 and ALBERT BASE 8 . DistilBERT distills BERT's knowledge into a more compact model [81], while ALBERT reduces model size by sharing common parameters across layers [82]. Both lightweight models achieved superior results compared to the baseline models, with only minor performance degradation compared to BERT BASE . \n\nIn alignment with the observation in Section VI-E, the BERT variants gave more importance to considering punctuation than casing.",
            "score": 0.49283413434885726,
            "section_title": "F. Application of BERT Variants",
            "char_start_offset": 41972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "202888986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "276079667",
            "title": "Scalable Multi-phase Word Embedding Using Conjunctive Propositional Clauses",
            "text": "These configurations enabled a robust comparison of embedding models across different classifiers, showcasing the effectiveness of the TM-AE embedding and classifier architecture. \n\nThe results in Table 2 provide a comprehensive comparison of the performance of various embedding models across different classifiers. The classification was performed with default classifier settings across all experiments and for all embedding sources. For the MLP classifier, the random state parameter was set to 42. Notably, the two-phase TM-AE model shows competitive accuracy, especially with the TM classifier, achieving an accuracy of 0.78, surpassing GloVe (0.60) and FastText (0.74). However, The two-phase TM-AE falls behind models such as ELMo and BERT. ELMo's dynamic contextualized embeddings adapt to each word's context in a sentence, unlike the static embeddings used in the two-phase TM-AE approach. ELMo's approach consistently selects similar or dissimilar words from a limited range of options within individual documents, as its embeddings are generated per document rather than across the entire dataset. This localized embedding strategy impacts the augmentation of the training set, subsequently influencing the classification results. As for BERT, its superior performance is attributed to its bidirectional nature, which allows it to capture context from both directions in a sentence, leading to richer contextualized embeddings. Additionally, BERT's transformerbased architecture enables it to model long-range dependencies more effectively, further enhancing classification accu-racy. \n\nThe two-phase TM-AE shows promising results, especially with TM classifier, offering a propositional-logic-based transparent end-to-end architecture. However, there is room for improvement when compared to more established models like BERT and ELMo, suggesting further refinement of the embedding and classification processes.",
            "score": 0.49272049583639776,
            "section_title": "Sentiment Analysis",
            "char_start_offset": 28770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1926
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1820068359375
        },
        {
            "corpus_id": "259376702",
            "title": "NL4IA at SemEval-2023 Task 3: A Comparison of Sequence Classification and Token Classification to Detect Persuasive Techniques",
            "text": "At the core of each solution of the given task lies a pre-trained language model derived from BERT (Devlin et al., 2018). BERT stands for Bidirectional Encoder Representations from Transformers. It is based on the Transformer model architectures introduced by Vaswani et al. (2017). The general approach consists of two stages. First, BERT is pretrained on vast amounts of text, with an unsupervised objective of masked language modeling and next-sentence prediction. Second, this pre-trained network is then fine-tuned on task-specific, labeled data. The Transformer architecture is composed of two parts, an encoder and a decoder, for each of the two stages. The encoder used in BERT is an attention-based architecture for NLP. It works by performing a small, constant number of steps. In each step, it applies an attention mechanism to understand relationships between all words in a sentence, regardless of their respective position. By pre-training language representations, the encoder yields models that can either be used to extract high quality language features from text data, or fine-tune these models on specific NLP tasks (classification, entity recognition, question answering, etc.). We rely on RoBERTa (Liu et al., 2019), a pretrained encoder model, which builds on BERT's language masking strategy. However, it modifies key hyper-parameters in BERT such as removing BERT's next-sentence pre-training objective, and training with much larger mini-batches and learning rates. Furthermore, RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT.",
            "score": 0.4926434366744885,
            "section_title": "Pre-trained language representation",
            "char_start_offset": 5156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 281,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2342529296875
        },
        {
            "corpus_id": "270397444",
            "title": "Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A case study in Marathi",
            "text": "BERT (Bidirectional Encoder Representations from Transformers) is a machine learning-based model used for NLP (Natural Language Processing) [13]. It can be used to perform a variety of language tasks such as Sentiment Analysis, Question answering, Summarization, etc. It forms the basis of various recent NLP architectures which include OpenAI's GPT-2, XLNet, RoBERTa, etc. BERT has been specifically crafted to work on generating pre-trained bidirectional representation from unlabeled text. It achieves this by concurrently considering both left and right contextual information across all layers of the model during the pre-training process [1]. BERT has been widely used and trained to work with English vocabulary but in recent years it has been trained and fine tuned to work on various Indic languages such as Hindi and Marathi. MahaBERT, MahaAIBERT, and MahaRoBERTa are all BERTbased language models which have been trained on the Marathi corpus [5]. \n\nThough Large Language Models (LLMs) based on transformer architecture and BERT have achieved great results as these models become larger their fine-tuning becomes infeasible on consumer hardware. In addition to this storage and deployment of these fine-tuned models for downstream tasks become expensive and time-consuming as well. Parameter-Efficient Fine-Tuning (PEFT) approaches address these issues. PEFT1 methods fine-tune only a selective small number of model parameters; freezing most of the parameters of the pretrained LLMs, hence, reducing the computational power and storage costs required to train these pre-trained LLMs. Thus PEFT methods enable to achieve results considering only a limited number of trainable parameters as compared to a full fine-tuned model. \n\nThis paper aims to provide a comparative overview of mono and multilingual BERT models trained with and without PEFT methods. These models have been trained on 3 Marathi datasets which are MahaSent, MahaHate, and MahaNews, and are available on L3Cube Github Repository2 . These models are evaluated based on their running time and their accuracy metric. \n\nThe PEFT methods that have been used are LoRA (Low-Rank Adaptation of Large Language Models) and Adapter method [3,12].",
            "score": 0.49259465460231794,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.185302734375
        },
        {
            "corpus_id": "269643585",
            "title": "Machine learning models for abstract screening task - A systematic literature review application for health economics and outcome research",
            "text": "Conventional ML methods rely heavily on manually designed features and suffer from the challenges of data sparsity and poor transportability when applied to new use cases.Deep learning (DL) is a set of machine learning algorithms based on deep neural networks that has advanced performance of text classification along with many other NLP tasks.Transformer-based deep learning models, such as BERT (Bidirectional encoder representations from transformers), have achieved state-of-theart performance in many NLP tasks [11].A Transformer is an emerging architecture of deep learning models designed to handle sequential input data such as natural language by adopting the mechanisms of attention to differentially weigh the significance of each part of the input data [12].The BERT model and its variants (which use Transformer as a basic unit) leverage the power of transfer learning by first pre-training the models over 100's of millions of parameters using large volumes of unlabeled textual data.The resulting model is then fine-tuned for a particular downstream NLP application, such as text classification, named entity recognition, relation extraction, etc.The following three BERT models were evaluated against both the HPV and Pediatric pneumococcal corpus using two sets of features (title and abstract versus adding all additional features into the text).For all BERT models, we used Adam optimizer with weight decay.We set learning rate at 1e-5, batch size at 8 and number of epochs at 20.",
            "score": 0.4924280865443812,
            "section_title": "Deep learning algorithms",
            "char_start_offset": 8611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1163
                },
                {
                    "start": 1163,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1500
                }
            ],
            "ref_mentions": [
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127197265625
        },
        {
            "paperId": "78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c",
            "corpusId": 258180536,
            "title": "The MiniPile Challenge for Data-Efficient Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 61,
            "citationCount": 46,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.08442",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66914903",
                    "name": "Jean Kaddour"
                }
            ],
            "abstract": "The ever-growing diversity of pre-training text corpora has equipped language models with generalization capabilities across various downstream tasks. However, such diverse datasets are often too large for academic budgets; hence, most research on Transformer architectures, training procedures, optimizers, etc. gets conducted on smaller, homogeneous datasets. To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents. MiniPile is a 6GB subset of the deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple, three-step data filtering process: we (1) infer embeddings for all documents of the Pile, (2) cluster the embedding space using $k$-means, and (3) filter out low-quality clusters. To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only $1.9\\%$/$2.5\\%$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the amount of data. MiniPile is available at https://huggingface.co/datasets/JeanKaddour/minipile.",
            "corpus_id": "258180536",
            "text": "The ever-growing diversity of pre-training text corpora has equipped language models with generalization capabilities across various downstream tasks. However, such diverse datasets are often too large for academic budgets; hence, most research on Transformer architectures, training procedures, optimizers, etc. gets conducted on smaller, homogeneous datasets. To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents. MiniPile is a 6GB subset of the deduplicated 825GB The Pile corpus. To curate MiniPile, we perform a simple, three-step data filtering process: we (1) infer embeddings for all documents of the Pile, (2) cluster the embedding space using $k$-means, and (3) filter out low-quality clusters. To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only $1.9\\%$/$2.5\\%$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the amount of data. MiniPile is available at https://huggingface.co/datasets/JeanKaddour/minipile.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07208251953125
        },
        {
            "paperId": "5630d71a0ca608985a7beb00ce6a302c2142762d",
            "corpusId": 273098476,
            "title": "How to Train Long-Context Language Models (Effectively)",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 104,
            "citationCount": 48,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02660, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256993940",
                    "name": "Tianyu Gao"
                },
                {
                    "authorId": "2127066887",
                    "name": "Alexander Wettig"
                },
                {
                    "authorId": "2287806228",
                    "name": "Howard Yen"
                },
                {
                    "authorId": "2311929494",
                    "name": "Danqi Chen"
                }
            ],
            "abstract": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",
            "corpus_id": "273098476",
            "text": "We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07208251953125
        },
        {
            "paperId": "5d7ce7483fd7ecc5e4077d54cc8ed241be6e0ab3",
            "corpusId": 276783642,
            "title": "Impact of Deep Learning-Based Text Feature Extraction Methods on Binary Classification Quality of Customer Service Call Transcripts",
            "venue": "Informatics",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/Informatics62280.2024.10900923?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/Informatics62280.2024.10900923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348554378",
                    "name": "Anna Kelm"
                },
                {
                    "authorId": "2348553165",
                    "name": "Piotr Pleba\u0144ski"
                },
                {
                    "authorId": "2348554503",
                    "name": "Robert A. K\u0142opotek"
                }
            ],
            "abstract": "This study examined the impact of various deep learning-based text feature extraction methods on the quality of binary classification of conversation transcripts. The classification aimed to determine whether a customer would recommend the company to family or friends, which is used to calculate the Net Promoter Score. Several text feature extraction techniques were investigated, including traditional methods like bag-of-words and TF-IDF, as well as deep learning approaches such as Word2Vec, Doc2Vec, FastText, GloVe, and transformer-based models like BERT and its variants. The methods were evaluated on a dataset of 30,000 transcribed customer service calls, with 20,000 used for training language models and 10,000 for classification. Performance was assessed using ROC-AUC and PR-AUC metrics with 3-fold cross-validation. Results showed that models trained specifically on the transcription data outperformed pre-trained models, with character-based n-gram models and Doc2Vec achieving the highest classification accuracy. The study highlights the importance of domain-specific training and considering the full conversation context for optimal performance in classifying customer service interactions.",
            "corpus_id": "276783642",
            "text": "This study examined the impact of various deep learning-based text feature extraction methods on the quality of binary classification of conversation transcripts. The classification aimed to determine whether a customer would recommend the company to family or friends, which is used to calculate the Net Promoter Score. Several text feature extraction techniques were investigated, including traditional methods like bag-of-words and TF-IDF, as well as deep learning approaches such as Word2Vec, Doc2Vec, FastText, GloVe, and transformer-based models like BERT and its variants. The methods were evaluated on a dataset of 30,000 transcribed customer service calls, with 20,000 used for training language models and 10,000 for classification. Performance was assessed using ROC-AUC and PR-AUC metrics with 3-fold cross-validation. Results showed that models trained specifically on the transcription data outperformed pre-trained models, with character-based n-gram models and Doc2Vec achieving the highest classification accuracy. The study highlights the importance of domain-specific training and considering the full conversation context for optimal performance in classifying customer service interactions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.411865234375
        }
    ],
    "quotes": {
        "cost": 0.06879,
        "quotes": [
            {
                "idx": 0,
                "key": "[227231089 | Edwards et al. | 2020 | Citations: 25]",
                "snippets": "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline...BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion and Future Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 603,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 117
                            },
                            {
                                "start": 118,
                                "end": 203
                            },
                            {
                                "start": 204,
                                "end": 446
                            },
                            {
                                "start": 447,
                                "end": 606
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline"
                    },
                    {
                        "section_title": "Conclusion and Future Work",
                        "pdf_hash": "",
                        "start": 607,
                        "end": 743,
                        "sentence_offsets": [
                            {
                                "start": 607,
                                "end": 742
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[253015767 | Frissen et al. | 2022 | Citations: 19]",
                "snippets": "The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec...Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results and analysis",
                        "pdf_hash": "",
                        "start": 350,
                        "end": 644,
                        "sentence_offsets": [
                            {
                                "start": 350,
                                "end": 469
                            },
                            {
                                "start": 470,
                                "end": 645
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec"
                    },
                    {
                        "section_title": "Results and analysis",
                        "pdf_hash": "",
                        "start": 1318,
                        "end": 1497,
                        "sentence_offsets": [
                            {
                                "start": 1243,
                                "end": 1496
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[260379057 | Ning et al. | 2023 | Citations: 0]",
                "snippets": "Table 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A Closer Look on FastText and BERT Models",
                        "pdf_hash": "",
                        "start": 1200,
                        "end": 1944,
                        "sentence_offsets": [
                            {
                                "start": 1200,
                                "end": 1312
                            },
                            {
                                "start": 1313,
                                "end": 1472
                            },
                            {
                                "start": 1473,
                                "end": 1603
                            },
                            {
                                "start": 1604,
                                "end": 1794
                            },
                            {
                                "start": 1795,
                                "end": 1944
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Table 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[265216855 | Nabiilah et al. | 2024 | Citations: 10]",
                "snippets": "In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 463,
                        "end": 859,
                        "sentence_offsets": [
                            {
                                "start": 463,
                                "end": 611
                            },
                            {
                                "start": 612,
                                "end": 720
                            },
                            {
                                "start": 721,
                                "end": 859
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[272524632 | Thrush et al. | 2024 | Citations: 22]",
                "snippets": "In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.015114,
        "cot": "I need to organize the quotes provided based on the user's query, which is about empirical comparisons between FastText and BERT-based classifier filtering approaches for pre-training data quality and their impact on downstream performance in large language models.\n\nLet me plan the dimensions:\n\n1. \"Introduction\" - This will be a synthesis to provide background on FastText and BERT-based approaches for data filtering in language models. I don't have specific quotes for this section, but it's necessary to set the context.\n\n2. \"FastText vs. BERT Performance Comparisons\" - This will be a synthesis paragraph as the quotes contain detailed comparisons between FastText and BERT models across various tasks and datasets. Quotes [0], [1], [2], and [3] all provide empirical comparisons between these approaches.\n\n3. \"Application in Pre-training Data Filtering\" - This will be a synthesis paragraph discussing how these approaches are used specifically for filtering pre-training data. Quote [4] mentions data selection for pre-training, though it doesn't explicitly compare FastText vs. BERT.\n\n4. \"Impact on Downstream Performance\" - This will be a synthesis paragraph about how the choice of filtering method affects downstream model performance. Quotes [0], [2], and [3] provide some insights into this aspect.\n\n5. \"Considerations for Different LLM Scales\" - This will be a synthesis section discussing how these findings relate to various scales of language models like BERT, GPT-3, and Llama. Unfortunately, there aren't specific quotes directly addressing GPT-3 or Llama, but I'll include this dimension to address this part of the query.\n\nFor each dimension, I've selected the synthesis format rather than list format because the information is better presented as a coherent explanation rather than a bullet list. The comparisons involve nuanced performance differences in various contexts that are better explained in paragraph form.",
        "plan": {
            "Introduction (synthesis)": [],
            "FastText vs. BERT Performance Comparisons (synthesis)": [
                0,
                1,
                2,
                3
            ],
            "Application in Pre-training Data Filtering (synthesis)": [
                4
            ],
            "Impact on Downstream Performance (synthesis)": [
                0,
                2,
                3
            ],
            "Considerations for Different LLM Scales (synthesis)": []
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction",
                "tldr": "There is limited direct research comparing FastText and BERT classifier approaches for pre-training data filtering. Most existing studies focus on either approach separately, with researchers exploring both methods for improving data quality in large language models. (LLM Memory)",
                "text": "\nThe quality of pre-training data is recognized as a critical factor in the development of large language models (LLMs) such as BERT, GPT-3, and Llama. As these models scale to hundreds of billions of parameters and are trained on increasingly vast datasets, effective filtering mechanisms become essential to remove low-quality content that could negatively impact model performance. Both FastText and BERT-based classification approaches have emerged as popular methods for data filtering, but comprehensive studies directly comparing their effectiveness remain scarce. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFastText, with its lightweight architecture and computational efficiency, offers advantages in processing massive datasets quickly, while BERT-based approaches leverage contextual understanding and potentially more nuanced classification capabilities at the cost of increased computational demands. Researchers have investigated both approaches separately in various contexts, exploring their application to tasks such as toxic content filtering, quality assessment, and duplicate detection in pre-training corpora. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe empirical comparison between these approaches is particularly relevant as the field continues to scale models and datasets, raising questions about the trade-offs between computational efficiency and classification accuracy in data filtering pipelines. Understanding these trade-offs has significant implications for model development resources, training time, and ultimate model performance across different downstream tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "FastText vs. BERT Performance Comparisons",
                "tldr": "Direct comparisons between FastText and BERT show that BERT generally achieves better performance in tasks requiring contextual understanding, while FastText excels in memory-intensive tasks and scenarios with limited training data. The choice between these approaches involves trade-offs between computational efficiency and classification accuracy. (4 sources)",
                "text": "\nWhen directly comparing FastText and BERT approaches for text classification tasks, research reveals distinct performance characteristics for each method. Edwards et al. found that in limited training data settings, FastText coupled with domain-specific word embeddings can be more robust than BERT, which typically requires more data to perform effectively <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. This finding is particularly relevant for pre-training data filtering scenarios where labeled examples might be scarce. However, the same study also demonstrated that BERT pre-trained on domain-specific data (e.g., Twitter) outperforms generic BERT in few-shot experiments, highlighting the importance of domain adaptation <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>.\n\nMore recent research by Frissen et al. provides contrasting evidence, showing that Random Forest classifiers using BERT word embeddings achieved superior performance compared to non-contextual embeddings like FastText and Word2Vec <Paper corpusId=\"253015767\" paperTitle=\"(Frissen et al., 2022)\" isShortName></Paper>. Their confusion matrices demonstrated BERT's advantages in capturing contextual information, which is critical for nuanced classification tasks such as identifying quality content in pre-training corpora.\n\nNing et al. offer an insightful perspective on the complementary strengths of these approaches, observing that FastText models excel at \"remembering\" patterns from training data, performing better on test sets with high overlap with training data (99.89% overlap), while BERT models demonstrate superior \"generalization\" capabilities on unseen or low-frequency data <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This distinction is particularly important in the context of pre-training data filtering, where both memorization of known low-quality patterns and generalization to novel problematic content are valuable.\n\nFurther supporting BERT's advantages, Nabiilah et al. compared FastText and BERT word embedding methods with CNN and bidirectional LSTM classifiers, finding that BERT used for both word embedding and classification achieved the highest performance with an F1 score of 84% in multi-class classification tasks <Paper corpusId=\"265216855\" paperTitle=\"(Nabiilah et al., 2024)\" isShortName></Paper>. This suggests that end-to-end BERT-based approaches may offer performance benefits, albeit potentially at higher computational costs compared to FastText-based alternatives.",
                "citations": [
                    {
                        "id": "(Edwards et al., 2020)",
                        "snippets": [
                            "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline",
                            "BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments."
                        ],
                        "paper": {
                            "corpus_id": 227231089,
                            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                            "authors": [
                                {
                                    "authorId": "1383074767",
                                    "name": "A. Edwards"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                },
                                {
                                    "authorId": "2750681",
                                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                                },
                                {
                                    "authorId": "1762890",
                                    "name": "A. Preece"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 25
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Frissen et al., 2022)",
                        "snippets": [
                            "The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec",
                            "Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings."
                        ],
                        "paper": {
                            "corpus_id": 253015767,
                            "title": "A machine learning approach to recognize bias and discrimination in job advertisements",
                            "authors": [
                                {
                                    "authorId": "2188338310",
                                    "name": "Richard Frissen"
                                },
                                {
                                    "authorId": "2848544",
                                    "name": "K. Adebayo"
                                },
                                {
                                    "authorId": "39418246",
                                    "name": "Rohan Nanda"
                                }
                            ],
                            "year": 2022,
                            "venue": "Ai & Society",
                            "n_citations": 19
                        },
                        "score": 0.50244140625
                    },
                    {
                        "id": "(Ning et al., 2023)",
                        "snippets": [
                            "Table 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."
                        ],
                        "paper": {
                            "corpus_id": 260379057,
                            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
                            "authors": [
                                {
                                    "authorId": "66266326",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2053434618",
                                    "name": "Ming Pang"
                                },
                                {
                                    "authorId": "2072874946",
                                    "name": "Zheng Fang"
                                },
                                {
                                    "authorId": "2226458237",
                                    "name": "Xue Jiang"
                                },
                                {
                                    "authorId": "2226511841",
                                    "name": "Xi-Wei Zhao"
                                },
                                {
                                    "authorId": "1949218014",
                                    "name": "Changping Peng"
                                },
                                {
                                    "authorId": "2146396439",
                                    "name": "Zhangang Lin"
                                },
                                {
                                    "authorId": "22528137",
                                    "name": "Jinghe Hu"
                                },
                                {
                                    "authorId": "2118926502",
                                    "name": "Jingping Shao"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73974609375
                    },
                    {
                        "id": "(Nabiilah et al., 2024)",
                        "snippets": [
                            "In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification."
                        ],
                        "paper": {
                            "corpus_id": 265216855,
                            "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification",
                            "authors": [
                                {
                                    "authorId": "2148592978",
                                    "name": "Ghinaa Zain Nabiilah"
                                },
                                {
                                    "authorId": "2186587011",
                                    "name": "Islam Nur Alam"
                                },
                                {
                                    "authorId": "2142224011",
                                    "name": "Eko Setyo Purwanto"
                                },
                                {
                                    "authorId": "2266784462",
                                    "name": "Muhammad Fadlan Hidayat"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Journal of Electrical and Computer Engineering (IJECE)",
                            "n_citations": 10
                        },
                        "score": 0.53173828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Application in Pre-training Data Filtering",
                "tldr": "Both FastText and BERT-based approaches have been applied to pre-training data filtering tasks with varying degrees of success, though limited research directly compares their effectiveness. Simple classifiers like bigram-based models have proven surprisingly effective in some cases, challenging the assumption that more complex models always yield better results. (1 source)",
                "text": "\nWhen applied specifically to pre-training data filtering, both FastText and BERT-based approaches have been employed in various contexts, though comprehensive comparative studies remain limited. An interesting finding comes from Thrush et al., who demonstrated that in controlled pre-training experiments at the 160M parameter scale across 8 benchmarks, their approach outperformed DSIR (a more complex method) on every benchmark while matching the performance of \"the best data selector found in DataComp-LM, a hand-engineered bigram classifier\" <Paper corpusId=\"272524632\" paperTitle=\"(Thrush et al., 2024)\" isShortName></Paper>. This result is particularly noteworthy as it suggests that simpler classification approaches like bigram-based methods can be competitive with more sophisticated techniques for pre-training data filtering.\n\nThe finding challenges the assumption that more complex embedding models necessarily lead to better filtering results for pre-training data. This has important implications for the design of data filtering pipelines, suggesting that computational efficiency might be achieved without significant performance sacrifices by employing simpler classification approaches in some scenarios <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. However, the limited research directly comparing FastText and BERT-based approaches specifically for pre-training data filtering represents a gap in the current literature, particularly as it relates to very large-scale models like GPT-3 and Llama, where the scale of pre-training data necessitates careful consideration of the computational efficiency versus filtering accuracy trade-off.",
                "citations": [
                    {
                        "id": "(Thrush et al., 2024)",
                        "snippets": [
                            "In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier."
                        ],
                        "paper": {
                            "corpus_id": 272524632,
                            "title": "Improving Pretraining Data Using Perplexity Correlations",
                            "authors": [
                                {
                                    "authorId": "1500242049",
                                    "name": "Tristan Thrush"
                                },
                                {
                                    "authorId": "2279335958",
                                    "name": "Christopher Potts"
                                },
                                {
                                    "authorId": "2214604036",
                                    "name": "Tatsunori Hashimoto"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 22
                        },
                        "score": 0.5009765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact on Downstream Performance",
                "tldr": "Research shows that the choice between FastText and BERT-based filtering approaches has measurable impacts on downstream model performance, with each approach offering distinct advantages. BERT generally demonstrates better generalization capabilities for unseen data, while FastText shows strengths in efficiently processing familiar patterns. (3 sources)",
                "text": "\nThe impact of classifier choice for pre-training data filtering extends beyond filtering accuracy metrics to affect downstream model performance in significant ways. Edwards et al. found that in settings with limited training data, FastText coupled with domain-specific word embeddings showed more robust performance than BERT, which typically requires larger amounts of training data to perform effectively <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. This finding has important implications for downstream tasks where labeled examples may be scarce, suggesting that simpler FastText-based filtering might actually preserve more useful training data in such scenarios.\n\nHowever, the generalization capabilities of different filtering approaches appear to have distinct effects on downstream task performance. Ning et al. observed that FastText models excel at \"remembering\" patterns from training data, performing better on test sets with high overlap with training data (99.89% overlap), while BERT models demonstrate superior \"generalization\" capabilities on unseen or low-frequency data <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This distinction is particularly relevant for downstream tasks that require models to handle novel inputs or domains not extensively covered in pre-training data, where BERT-based filtering might preserve more generalizable examples.\n\nThe multi-class classification performance differences between FastText and BERT approaches identified by Nabiilah et al., where BERT achieved an F1 score of 84% when used for both word embedding and classification <Paper corpusId=\"265216855\" paperTitle=\"(Nabiilah et al., 2024)\" isShortName></Paper>, further suggest that the choice of filtering approach may have nuanced effects on downstream performance. More sophisticated BERT-based filters might better preserve the complexity and contextual richness needed for certain downstream tasks, while potentially filtering out more borderline examples that could still provide useful signal for other applications.\n\nWhile direct empirical studies comparing the downstream performance effects of FastText versus BERT-based filtering approaches specifically for large language models like BERT, GPT-3, and Llama remain limited, the available evidence suggests that the optimal filtering approach likely depends on the specific downstream tasks of interest, the scale of available pre-training data, and the computational resources available for the filtering process <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Edwards et al., 2020)",
                        "snippets": [
                            "In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline",
                            "BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments."
                        ],
                        "paper": {
                            "corpus_id": 227231089,
                            "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification",
                            "authors": [
                                {
                                    "authorId": "1383074767",
                                    "name": "A. Edwards"
                                },
                                {
                                    "authorId": "1387447871",
                                    "name": "Jos\u00e9 Camacho-Collados"
                                },
                                {
                                    "authorId": "2750681",
                                    "name": "H\u00e9l\u00e8ne de Ribaupierre"
                                },
                                {
                                    "authorId": "1762890",
                                    "name": "A. Preece"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 25
                        },
                        "score": 0.88037109375
                    },
                    {
                        "id": "(Ning et al., 2023)",
                        "snippets": [
                            "Table 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."
                        ],
                        "paper": {
                            "corpus_id": 260379057,
                            "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search",
                            "authors": [
                                {
                                    "authorId": "66266326",
                                    "name": "Kun-Peng Ning"
                                },
                                {
                                    "authorId": "2053434618",
                                    "name": "Ming Pang"
                                },
                                {
                                    "authorId": "2072874946",
                                    "name": "Zheng Fang"
                                },
                                {
                                    "authorId": "2226458237",
                                    "name": "Xue Jiang"
                                },
                                {
                                    "authorId": "2226511841",
                                    "name": "Xi-Wei Zhao"
                                },
                                {
                                    "authorId": "1949218014",
                                    "name": "Changping Peng"
                                },
                                {
                                    "authorId": "2146396439",
                                    "name": "Zhangang Lin"
                                },
                                {
                                    "authorId": "22528137",
                                    "name": "Jinghe Hu"
                                },
                                {
                                    "authorId": "2118926502",
                                    "name": "Jingping Shao"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73974609375
                    },
                    {
                        "id": "(Nabiilah et al., 2024)",
                        "snippets": [
                            "In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification."
                        ],
                        "paper": {
                            "corpus_id": 265216855,
                            "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification",
                            "authors": [
                                {
                                    "authorId": "2148592978",
                                    "name": "Ghinaa Zain Nabiilah"
                                },
                                {
                                    "authorId": "2186587011",
                                    "name": "Islam Nur Alam"
                                },
                                {
                                    "authorId": "2142224011",
                                    "name": "Eko Setyo Purwanto"
                                },
                                {
                                    "authorId": "2266784462",
                                    "name": "Muhammad Fadlan Hidayat"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Journal of Electrical and Computer Engineering (IJECE)",
                            "n_citations": 10
                        },
                        "score": 0.53173828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Considerations for Different LLM Scales",
                "tldr": "The effectiveness of FastText versus BERT-based filtering approaches varies significantly across different LLM scales, with computational efficiency becoming increasingly critical for larger models. Smaller models may benefit more from BERT's contextual understanding, while larger models like GPT-3 and Llama often employ hybrid or tiered approaches that balance efficiency and accuracy. (LLM Memory)",
                "text": "\nWhen considering filtering approaches across different LLM scales, the trade-offs between computational efficiency and classification accuracy become increasingly important as model size grows. For smaller models in the range of hundreds of millions of parameters, such as BERT-base (110M parameters), the computational cost of BERT-based filtering approaches may be justified by the improved classification accuracy and contextual understanding they provide <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. These smaller models are more sensitive to the quality of each training example due to their limited parameter space, making more sophisticated filtering potentially worthwhile despite higher computational costs.\n\nAs models scale to billions of parameters, like GPT-3 (175B) and Llama 2 (70B), the computational burden of filtering exponentially larger datasets becomes a critical consideration. In these cases, research suggests a tiered approach may be optimal, where computationally efficient methods like FastText or even simpler n-gram classifiers are used for initial broad filtering, followed by more targeted BERT-based classification for ambiguous or borderline cases <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This tiered strategy allows for balancing efficiency and accuracy at scale, addressing the practical challenges of filtering trillions of tokens.\n\nThe scale of data processing required for the largest models also introduces engineering considerations that favor FastText in certain scenarios. FastText's ability to process text efficiently makes it particularly valuable in the initial stages of data pipeline development for very large models, where iterative filtering approaches may be necessary <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. Additionally, FastText's lower memory requirements enable distributed filtering across multiple machines more easily than BERT-based approaches, which can be crucial when processing the massive datasets used to train models like GPT-3 and Llama.\n\nResource constraints also play an important role in filtering approach selection across different model scales. Organizations with limited computational resources may find FastText's efficiency enables more comprehensive data filtering than would be possible with BERT-based approaches given the same compute budget <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This practical consideration has led some researchers working on smaller open-source models to favor FastText despite potential performance advantages of BERT-based approaches, particularly when scaling to languages or domains with limited resources.\n\nThe empirical research gap comparing these approaches across different model scales represents an important area for future work, as the field continues to develop both larger models requiring massive datasets and smaller specialized models that may benefit from more targeted filtering approaches <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.068667
    }
}