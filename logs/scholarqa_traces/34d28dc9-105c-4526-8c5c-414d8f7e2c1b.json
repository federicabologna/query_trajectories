{
    "query": "What are the key architectural differences between autoregressive language models and traditional dual-encoder models in dense passage retrieval, and how do these differences affect retrieval accuracy and computational efficiency?",
    "user_id": "lib_user",
    "task_id": "34d28dc9-105c-4526-8c5c-414d8f7e2c1b",
    "timestamp": "2025-06-23T22:20:26.389810",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 37,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.467298,
    "decomposed_query": {
        "rewritten_query": "Key architectural differences between autoregressive language models and traditional dual-encoder models in dense passage retrieval, and how these differences affect retrieval accuracy and computational efficiency.",
        "keyword_query": "autoregressive language models dual-encoder dense passage retrieval architecture differences retrieval accuracy computational efficiency",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010032,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.16821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276748536",
                    "name": "Haifeng Li"
                },
                {
                    "authorId": "2157052",
                    "name": "Mo Hai"
                },
                {
                    "authorId": "2276610303",
                    "name": "Dong Tang"
                }
            ],
            "abstract": "Ranker and retriever are two important components in dense passage retrieval. The retriever typically adopts a dual-encoder model, where queries and documents are separately input into two pre-trained models, and the vectors generated by the models are used for similarity calculation. The ranker often uses a cross-encoder model, where the concatenated query-document pairs are input into a pre-trained model to obtain word similarities. However, the dual-encoder model lacks interaction between queries and documents due to its independent encoding, while the cross-encoder model requires substantial computational cost for attention calculation, making it difficult to obtain real-time retrieval results. In this paper, we propose a dense retrieval model called MD2PR based on multi-level distillation. In this model, we distill the knowledge learned from the cross-encoder to the dual-encoder at both the sentence level and word level. Sentence-level distillation enhances the dual-encoder on capturing the themes and emotions of sentences. Word-level distillation improves the dual-encoder in analysis of word semantics and relationships. As a result, the dual-encoder can be used independently for subsequent encoding and retrieval, avoiding the significant computational cost associated with the participation of the cross-encoder. Furthermore, we propose a simple dynamic filtering method, which updates the threshold during multiple training iterations to ensure the effective identification of false negatives and thus obtains a more comprehensive semantic representation space. The experimental results over two standard datasets show our MD2PR outperforms 11 baseline models in terms of MRR and Recall metrics.",
            "corpus_id": 266573365,
            "sentences": [
                {
                    "corpus_id": "266573365",
                    "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
                    "text": "Ranker and retriever are two important components in dense passage retrieval. The retriever typically adopts a dual-encoder model, where queries and documents are separately input into two pre-trained models, and the vectors generated by the models are used for similarity calculation. The ranker often uses a cross-encoder model, where the concatenated query-document pairs are input into a pre-trained model to obtain word similarities. However, the dual-encoder model lacks interaction between queries and documents due to its independent encoding, while the cross-encoder model requires substantial computational cost for attention calculation, making it difficult to obtain real-time retrieval results. In this paper, we propose a dense retrieval model called MD2PR based on multi-level distillation. In this model, we distill the knowledge learned from the cross-encoder to the dual-encoder at both the sentence level and word level. Sentence-level distillation enhances the dual-encoder on capturing the themes and emotions of sentences. Word-level distillation improves the dual-encoder in analysis of word semantics and relationships. As a result, the dual-encoder can be used independently for subsequent encoding and retrieval, avoiding the significant computational cost associated with the participation of the cross-encoder. Furthermore, we propose a simple dynamic filtering method, which updates the threshold during multiple training iterations to ensure the effective identification of false negatives and thus obtains a more comprehensive semantic representation space. The experimental results over two standard datasets show our MD2PR outperforms 11 baseline models in terms of MRR and Recall metrics.",
                    "score": 0.5588311757638993,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.841796875
                },
                {
                    "corpus_id": "266573365",
                    "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
                    "text": "This model can capture more contextual information and generally performs better than the dual-encoder model. However, these dense retrieval methods still have 3 main problems: (1) The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert [15] further increases the latency to 458 milliseconds, nearly 13 times of the BM25. (3) Negative samples are typically randomly sampled from the current batch of data or the entire dataset, lacking true annotations. These samples may only be highly similar pseudo-negative ones to the query. Simply pushing away the query from them can impact the sentence representation and affect the overall model performance. \n\nCurrent research has focused on the combined use of crossencoder and dual-encoder models in dense passage retrieval. In this paper, we propose a multi-level distillation model called MD2PR, in which a dual-encoder model is utilized for large-scale candidate retrieval during the retrieval phase, while a cross-encoder model is employed for fine ranking of the retrieved results during the ranking phase. This method can not only improve the quality of the retrieval results but also decrease the computational cost. The main contributions are shown as follows. \n\n(1) We use a knowledge distillation method, by which the knowledge learned by the ranker is transferred to the retriever at both sentence and word levels.",
                    "score": 0.682767924383774,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2039,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 605
                        },
                        {
                            "start": 606,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1404
                        },
                        {
                            "start": 1407,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 1922
                        },
                        {
                            "start": 1923,
                            "end": 1967
                        },
                        {
                            "start": 1970,
                            "end": 2124
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 177,
                            "end": 180,
                            "matchedPaperCorpusId": "211096730"
                        },
                        {
                            "start": 991,
                            "end": 995,
                            "matchedPaperCorpusId": "216553223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7041015625
                }
            ],
            "relevance_judgement": 0.841796875,
            "relevance_judgment_input_expanded": "# Title: A Multi-level Distillation based Dense Passage Retrieval Model\n# Venue: arXiv.org\n# Authors: Haifeng Li, Mo Hai, Dong Tang\n## Abstract\nRanker and retriever are two important components in dense passage retrieval. The retriever typically adopts a dual-encoder model, where queries and documents are separately input into two pre-trained models, and the vectors generated by the models are used for similarity calculation. The ranker often uses a cross-encoder model, where the concatenated query-document pairs are input into a pre-trained model to obtain word similarities. However, the dual-encoder model lacks interaction between queries and documents due to its independent encoding, while the cross-encoder model requires substantial computational cost for attention calculation, making it difficult to obtain real-time retrieval results. In this paper, we propose a dense retrieval model called MD2PR based on multi-level distillation. In this model, we distill the knowledge learned from the cross-encoder to the dual-encoder at both the sentence level and word level. Sentence-level distillation enhances the dual-encoder on capturing the themes and emotions of sentences. Word-level distillation improves the dual-encoder in analysis of word semantics and relationships. As a result, the dual-encoder can be used independently for subsequent encoding and retrieval, avoiding the significant computational cost associated with the participation of the cross-encoder. Furthermore, we propose a simple dynamic filtering method, which updates the threshold during multiple training iterations to ensure the effective identification of false negatives and thus obtains a more comprehensive semantic representation space. The experimental results over two standard datasets show our MD2PR outperforms 11 baseline models in terms of MRR and Recall metrics.\n## INTRODUCTION\nThis model can capture more contextual information and generally performs better than the dual-encoder model. However, these dense retrieval methods still have 3 main problems: (1) The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert [15] further increases the latency to 458 milliseconds, nearly 13 times of the BM25. (3) Negative samples are typically randomly sampled from the current batch of data or the entire dataset, lacking true annotations. These samples may only be highly similar pseudo-negative ones to the query. Simply pushing away the query from them can impact the sentence representation and affect the overall model performance. \n\nCurrent research has focused on the combined use of crossencoder and dual-encoder models in dense passage retrieval. In this paper, we propose a multi-level distillation model called MD2PR, in which a dual-encoder model is utilized for large-scale candidate retrieval during the retrieval phase, while a cross-encoder model is employed for fine ranking of the retrieved results during the ranking phase. This method can not only improve the quality of the retrieval results but also decrease the computational cost. The main contributions are shown as follows. \n\n(1) We use a knowledge distillation method, by which the knowledge learned by the ranker is transferred to the retriever at both sentence and word levels.",
            "reference_string": "[266573365 | Li et al. | 2023 | Citations: 1]"
        },
        {
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 60,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.findings-emnlp.39.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.08241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1656672968",
                    "name": "Jiduan Liu"
                },
                {
                    "authorId": "2108421184",
                    "name": "Jiahao Liu"
                },
                {
                    "authorId": "2152915671",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2109593338",
                    "name": "Jingang Wang"
                },
                {
                    "authorId": "2118256028",
                    "name": "Wei Wu"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                },
                {
                    "authorId": "144539156",
                    "name": "Rui Yan"
                }
            ],
            "abstract": "Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.",
            "corpus_id": 260656514,
            "sentences": [
                {
                    "corpus_id": "260656514",
                    "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                    "text": "Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.",
                    "score": 0.5943362987542351,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84033203125
                },
                {
                    "corpus_id": "260656514",
                    "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                    "text": "On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss. To improve the performance of dual-encoders, some efforts have been made to incorporate more complicated structures (i.e., late interaction) such as attention layers (Humeau et al., 2019;Tang et al., 2021), the sum of maximum similarity computations (Khattab and Zaharia, 2020), and the transformer layers (Cao et al., 2020;Chen et al., 2020) into encoding. These late interaction strategies bring considerable improvements on retrieval performance but also increase computational overhead. Moreover, interaction information is still neglected in earlier encoding of query and passage. \n\nIn this work, we aim to achieve both efficiency and effectiveness in passage retrieval. The key idea is to maintain two independent encoders, and keep as much interaction information as possible in the meanwhile. To this end, we propose a novel approach that explicitly fuses query (passage) information into passage (query) embeddings through a graph neural network (GNN), and name the model GNN-encoder. Our model is built upon the dualencoder, and learns query-interactive passage representations and passage-interactive query representations through a graph neural network. Specifically, given a query set, we retrieve top passages for each query, and form a graph whose nodes are the queries and the passages, and edges reflect correspondence between query-passage pairs (i.e., if a passage is retrieved by the query). Then, we initialize the GNN model with the representations of the pre-trained dual-encoder and cross-encoder, and then perform information propagation on the graph. To avoid information leakage, we further design a new training algorithm and name it Masked Graph Training (MGT), in which the query set used for training GNN is no longer used to construct the query-passage graph in each training epoch. Finally, the passage embeddings could be pre-computed offline corresponding to the GNN. Thus our model holds the efficiency advantage inherited from the dual structure, and at the same time takes query-passage interaction into account. \n\nOur contributions can be summarized as follows:",
                    "score": 0.5225999704407291,
                    "section_title": "Introduction",
                    "char_start_offset": 1969,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 715
                        },
                        {
                            "start": 718,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 1123
                        },
                        {
                            "start": 1124,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2032
                        },
                        {
                            "start": 2033,
                            "end": 2180
                        },
                        {
                            "start": 2183,
                            "end": 2230
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 317,
                            "end": 335,
                            "matchedPaperCorpusId": "234338073"
                        },
                        {
                            "start": 380,
                            "end": 407,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 436,
                            "end": 454,
                            "matchedPaperCorpusId": "218487151"
                        },
                        {
                            "start": 454,
                            "end": 472,
                            "matchedPaperCorpusId": "222177208"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83984375
                },
                {
                    "corpus_id": "260656514",
                    "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                    "text": "Given a query q, dense retriever is required to retrieve k most relevant passages {p i } k i=1 from a large corpus consisting of hundreds of thousands of passages. For the sake of retrieval efficiency, the dual-encoder architecture is widely adopted, where query encoder E Q (\u2022) and passage encoder E P (\u2022) are used to embed query q and passage p into ddimensional vectors, respectively. The similarity between query q and passage p can be computed as the dot product of their vectors: \n\n(1) \n\nThe training objective of the dual-encoder is to learn embeddings of queries and passages to make positive query-passage pairs have higher similarity than the negative query-passage pairs in training data. Hence, the contrastive-learning loss function is adopted for the dual-encoder: \n\nwhere q and p + represent query and positive passage, respectively, and {p \u2212 } represents the set of negative passages. \n\nIn practical retrieval systems, passage embeddings are usually pre-computed offline, while query embeddings are computed by the query encoder in an ad hoc manner. Therefore we can obtain better passage embeddings through a complicated encoder as long as it does not increase the online inference latency.",
                    "score": 0.531257349238572,
                    "section_title": "Preliminary",
                    "char_start_offset": 7943,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 485
                        },
                        {
                            "start": 488,
                            "end": 491
                        },
                        {
                            "start": 494,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 778
                        },
                        {
                            "start": 781,
                            "end": 900
                        },
                        {
                            "start": 903,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1207
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67822265625
                },
                {
                    "corpus_id": "260656514",
                    "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                    "text": "Large-scale query-passage retrieval is a core task in search systems, which aims to rank a collection of passages based on their relevance with regard to a query. To balance efficiency and effectiveness, existing work typically adopts a two-stage retrieval pipeline (Ren et al., 2021b;Zhu et al., 2021). The first-stage aims to retrieve a subset of candidate passages by a recall model from the entire corpus and the second stage aims to rerank the retrieved passages. In the first-stage retrieval, traditional approaches (Chen et al., 2017) implemented term-based retriever (e.g. TF-IDF and BM25) by weighting terms based on their frequency, which have limitations on representing semantics of text. Recently, dense passage retrieval is drawing more and more attention in the task of passage retrieval (Karpukhin et al., 2020). The underlying idea is to represent both queries and passages as embeddings, so that the semantic relevance can be measured via embeddings similarity. With the great success of pre-trained language models (PLMs) such as BERT/RoBERTa (Devlin et al., 2019;Liu et al., 2019) in natural language processing tasks, dense retrieval models parameterized by PLMs is emerging as the new state-of-the-art in a variety of passage retrieval tasks (Karpukhin et al., 2020;Xiong et al., 2020). \n\nTwo paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss.",
                    "score": 0.6676279777217735,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 828
                        },
                        {
                            "start": 829,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1308
                        },
                        {
                            "start": 1311,
                            "end": 1428
                        },
                        {
                            "start": 1431,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1763
                        },
                        {
                            "start": 1764,
                            "end": 1861
                        },
                        {
                            "start": 1862,
                            "end": 1953
                        },
                        {
                            "start": 1954,
                            "end": 2083
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 266,
                            "end": 285,
                            "matchedPaperCorpusId": "238857121"
                        },
                        {
                            "start": 522,
                            "end": 541,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 803,
                            "end": 827,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1062,
                            "end": 1083,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1264,
                            "end": 1288,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1288,
                            "end": 1307,
                            "matchedPaperCorpusId": "220302524"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67626953125
                }
            ],
            "relevance_judgement": 0.84033203125,
            "relevance_judgment_input_expanded": "# Title: GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jiduan Liu, Jiahao Liu, Yang Yang, Jingang Wang, Wei Wu, Dongyan Zhao, Rui Yan\n## Abstract\nRecently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.\n## Introduction\nLarge-scale query-passage retrieval is a core task in search systems, which aims to rank a collection of passages based on their relevance with regard to a query. To balance efficiency and effectiveness, existing work typically adopts a two-stage retrieval pipeline (Ren et al., 2021b;Zhu et al., 2021). The first-stage aims to retrieve a subset of candidate passages by a recall model from the entire corpus and the second stage aims to rerank the retrieved passages. In the first-stage retrieval, traditional approaches (Chen et al., 2017) implemented term-based retriever (e.g. TF-IDF and BM25) by weighting terms based on their frequency, which have limitations on representing semantics of text. Recently, dense passage retrieval is drawing more and more attention in the task of passage retrieval (Karpukhin et al., 2020). The underlying idea is to represent both queries and passages as embeddings, so that the semantic relevance can be measured via embeddings similarity. With the great success of pre-trained language models (PLMs) such as BERT/RoBERTa (Devlin et al., 2019;Liu et al., 2019) in natural language processing tasks, dense retrieval models parameterized by PLMs is emerging as the new state-of-the-art in a variety of passage retrieval tasks (Karpukhin et al., 2020;Xiong et al., 2020). \n\nTwo paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss.\n...\nOn the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss. To improve the performance of dual-encoders, some efforts have been made to incorporate more complicated structures (i.e., late interaction) such as attention layers (Humeau et al., 2019;Tang et al., 2021), the sum of maximum similarity computations (Khattab and Zaharia, 2020), and the transformer layers (Cao et al., 2020;Chen et al., 2020) into encoding. These late interaction strategies bring considerable improvements on retrieval performance but also increase computational overhead. Moreover, interaction information is still neglected in earlier encoding of query and passage. \n\nIn this work, we aim to achieve both efficiency and effectiveness in passage retrieval. The key idea is to maintain two independent encoders, and keep as much interaction information as possible in the meanwhile. To this end, we propose a novel approach that explicitly fuses query (passage) information into passage (query) embeddings through a graph neural network (GNN), and name the model GNN-encoder. Our model is built upon the dualencoder, and learns query-interactive passage representations and passage-interactive query representations through a graph neural network. Specifically, given a query set, we retrieve top passages for each query, and form a graph whose nodes are the queries and the passages, and edges reflect correspondence between query-passage pairs (i.e., if a passage is retrieved by the query). Then, we initialize the GNN model with the representations of the pre-trained dual-encoder and cross-encoder, and then perform information propagation on the graph. To avoid information leakage, we further design a new training algorithm and name it Masked Graph Training (MGT), in which the query set used for training GNN is no longer used to construct the query-passage graph in each training epoch. Finally, the passage embeddings could be pre-computed offline corresponding to the GNN. Thus our model holds the efficiency advantage inherited from the dual structure, and at the same time takes query-passage interaction into account. \n\nOur contributions can be summarized as follows:\n\n## Preliminary\nGiven a query q, dense retriever is required to retrieve k most relevant passages {p i } k i=1 from a large corpus consisting of hundreds of thousands of passages. For the sake of retrieval efficiency, the dual-encoder architecture is widely adopted, where query encoder E Q (\u2022) and passage encoder E P (\u2022) are used to embed query q and passage p into ddimensional vectors, respectively. The similarity between query q and passage p can be computed as the dot product of their vectors: \n\n(1) \n\nThe training objective of the dual-encoder is to learn embeddings of queries and passages to make positive query-passage pairs have higher similarity than the negative query-passage pairs in training data. Hence, the contrastive-learning loss function is adopted for the dual-encoder: \n\nwhere q and p + represent query and positive passage, respectively, and {p \u2212 } represents the set of negative passages. \n\nIn practical retrieval systems, passage embeddings are usually pre-computed offline, while query embeddings are computed by the query encoder in an ad hoc manner. Therefore we can obtain better passage embeddings through a complicated encoder as long as it does not increase the online inference latency.",
            "reference_string": "[260656514 | Liu et al. | 2022 | Citations: 6]"
        },
        {
            "title": "Dense Passage Retrieval: Architectures and Augmentation Methods",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2023,
            "reference_count": 3,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3539618.3591796?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3539618.3591796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2091044662",
                    "name": "T. Rajapakse"
                }
            ],
            "abstract": "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries. Considering out-of-domain queries, i.e., queries originating from domains other than the one the model was trained on, the loss in accuracy may be significant. A main factor for this is the mismatch in the information available to the context encoder and the query encoder during training. Common retireval training datasets contain an overwhelming majority of passages with one query from a passage. I hypothesize that this could lead the dual-encoder model, particularly the passage encoder, to overfit to a single potential query from a given passage to the detriment of out-of-domain performance. Based on this, I seek to answer the following research question: (RQ1.1) Does training a DPR model on data containing multiple queries per passage improve the generalizability of the model? To answer RQ1.1, I build generated datasets that have multiple queries for most passages, and compare dense passage retriever models trained on these datasets against models trained on (mostly) single query per passage datasets. I show that training on passages with multiple queries leads to models that generalize better to out-of-distribution and out-of-domain test datasets [2]. Language can be considered another domain in the context of a dense retrieval. Training a dense retrieval model is especially challenging in languages other than English due to the scarcity of training data. I propose a novel training technique, clustered training, aimed at improving the retrieval quality of dense retrievers, especially in out-of-distribution and zero-shot settings. I address the following research questions: (RQ2.1)Does clustered training improve the effectiveness of multilingual DPR models on in-distribution data? (RQ2.2) Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.2 Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.3) Does clustered training help multilingual DPR models to generalize to new languages (zero-shot)? I show that clustered training improves the out-of-distribution and zero-shot performance of a DPR model without a clear loss in in-distribution performance using the Mr. TyDi [3] dataset. Finally, I propose a modified dual-encoder architecture that can perform both retrieval and reranking with the same model in a single forward pass. While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder. I aim to answer the following research questions in this work: (RQ3.1), Can the same model be trained to effectively generate two representations intended for two uses? RQ3.2 Can the retrieval quality of the model be improved by simultaneously performing retrieval and reranking? (RQ3.3 What is the tradeoff between retrieval quality vs. latency and compute resource efficiency for the proposed method vs. a two stage retriever? I expect that my proposed architecture would improve the dual encoder retrieval quality without sacrificing throughput or needing more computational resources.",
            "corpus_id": 259949811,
            "sentences": [],
            "relevance_judgement": 0.837890625,
            "relevance_judgment_input_expanded": "# Title: Dense Passage Retrieval: Architectures and Augmentation Methods\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: T. Rajapakse\n## Abstract\nThe dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries. Considering out-of-domain queries, i.e., queries originating from domains other than the one the model was trained on, the loss in accuracy may be significant. A main factor for this is the mismatch in the information available to the context encoder and the query encoder during training. Common retireval training datasets contain an overwhelming majority of passages with one query from a passage. I hypothesize that this could lead the dual-encoder model, particularly the passage encoder, to overfit to a single potential query from a given passage to the detriment of out-of-domain performance. Based on this, I seek to answer the following research question: (RQ1.1) Does training a DPR model on data containing multiple queries per passage improve the generalizability of the model? To answer RQ1.1, I build generated datasets that have multiple queries for most passages, and compare dense passage retriever models trained on these datasets against models trained on (mostly) single query per passage datasets. I show that training on passages with multiple queries leads to models that generalize better to out-of-distribution and out-of-domain test datasets [2]. Language can be considered another domain in the context of a dense retrieval. Training a dense retrieval model is especially challenging in languages other than English due to the scarcity of training data. I propose a novel training technique, clustered training, aimed at improving the retrieval quality of dense retrievers, especially in out-of-distribution and zero-shot settings. I address the following research questions: (RQ2.1)Does clustered training improve the effectiveness of multilingual DPR models on in-distribution data? (RQ2.2) Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.2 Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.3) Does clustered training help multilingual DPR models to generalize to new languages (zero-shot)? I show that clustered training improves the out-of-distribution and zero-shot performance of a DPR model without a clear loss in in-distribution performance using the Mr. TyDi [3] dataset. Finally, I propose a modified dual-encoder architecture that can perform both retrieval and reranking with the same model in a single forward pass. While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder. I aim to answer the following research questions in this work: (RQ3.1), Can the same model be trained to effectively generate two representations intended for two uses? RQ3.2 Can the retrieval quality of the model be improved by simultaneously performing retrieval and reranking? (RQ3.3 What is the tradeoff between retrieval quality vs. latency and compute resource efficiency for the proposed method vs. a two stage retriever? I expect that my proposed architecture would improve the dual encoder retrieval quality without sacrificing throughput or needing more computational resources.\n",
            "reference_string": "[259949811 | Rajapakse | 2023 | Citations: 7]"
        },
        {
            "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
            "venue": "ECML/PKDD",
            "year": 2022,
            "reference_count": 62,
            "citation_count": 16,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.07841",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.07841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1423651904",
                    "name": "Kun Zhou"
                },
                {
                    "authorId": "49544272",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2171182",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "71790825",
                    "name": "Daxin Jiang"
                },
                {
                    "authorId": "46429989",
                    "name": "Nan Duan"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ],
            "abstract": "Pre-trained Transformers (\\eg BERT) have been commonly used in existing dense retrieval methods for parameter initialization, and recent studies are exploring more effective pre-training tasks for further improving the quality of dense vectors. Although various novel and effective tasks have been proposed, their different input formats and learning objectives make them hard to be integrated for jointly improving the model performance. In this work, we aim to unify a variety of pre-training tasks into the bottlenecked masked autoencoder manner, and integrate them into a multi-task pre-trained model, namely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder architecture that can construct a representation bottleneck to compress the abundant semantic information across tasks into dense vectors. Based on it, we integrate three types of representative pre-training tasks: corrupted passages recovering, related passages recovering and PLMs outputs recovering, to characterize the inner-passage information, inter-passage relations and PLMs knowledge. Extensive experiments have shown that our approach outperforms competitive dense retrieval methods. Our code and data are publicly released in \\url{https://github.com/microsoft/SimXNS}.",
            "corpus_id": 254685782,
            "sentences": [
                {
                    "corpus_id": "254685782",
                    "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
                    "text": "In this section, we introduce the task definition of this work and present the typical fine-tuning process of dense retrieval. \n\nTask Definition. Given a query q, the dense passage retrieval task aims to retrieve the most relevant top-k passages {p i } k i=1 from a large candidate pool P. To achieve this goal, the dual-encoder architecture is widely used. It consists of a query encoder E q and a passage encoder E p , mapping the query q and passage p into k-dimensional dense vectors h q and h p , respectively. Then, the semantic relevance score of q and p will be computed using dot product as \n\n(1) \n\nExisting work mostly adopts pre-trained Transformers (e.g., BERT (Devlin et al., 2019)) as the two encoders, using the representations of the [CLS] token as the dense vectors. In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations. Formally, given a pre-training corpus and a Transformer encoder, we focus on devising several tasks to pre-train the parameters of it. Then, the pre-trained Transformer will be used as the backbone of the query encoder E q and passage encoder E p , and can be fine-tuned on downstream dense retrieval tasks. \n\nFine-tuning Dense Retrievers. In the fine-tuning stage, the learning objective is to pull the representations of a query q and its relevant passages P + together (as positives), while pushing apart irrelevant ones P \u2212 = P \\ P + (as negatives). Therefore, high-quality negatives are critical to the effectiveness of dense retrievers. Existing work commonly leverages the BM25 negatives (Karpukhin et al., 2020) or the top-k ranked negatives mined by a well-trained dense retriever (Qu et al., 2021;Xiong et al., 2021), denoted as D\u2212 . Then, the optimization objective can be formulated as:",
                    "score": 0.5115930547226488,
                    "section_title": "PRELIMINARY",
                    "char_start_offset": 8942,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 129,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 599
                        },
                        {
                            "start": 602,
                            "end": 605
                        },
                        {
                            "start": 608,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1295
                        },
                        {
                            "start": 1298,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1630
                        },
                        {
                            "start": 1631,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 1886
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 673,
                            "end": 694,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1683,
                            "end": 1707,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1778,
                            "end": 1795,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1795,
                            "end": 1814,
                            "matchedPaperCorpusId": "220302524"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8369140625
                }
            ],
            "relevance_judgement": 0.8369140625,
            "relevance_judgment_input_expanded": "# Title: MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers\n# Venue: ECML/PKDD\n# Authors: Kun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, Ji-rong Wen\n## Abstract\nPre-trained Transformers (\\eg BERT) have been commonly used in existing dense retrieval methods for parameter initialization, and recent studies are exploring more effective pre-training tasks for further improving the quality of dense vectors. Although various novel and effective tasks have been proposed, their different input formats and learning objectives make them hard to be integrated for jointly improving the model performance. In this work, we aim to unify a variety of pre-training tasks into the bottlenecked masked autoencoder manner, and integrate them into a multi-task pre-trained model, namely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder architecture that can construct a representation bottleneck to compress the abundant semantic information across tasks into dense vectors. Based on it, we integrate three types of representative pre-training tasks: corrupted passages recovering, related passages recovering and PLMs outputs recovering, to characterize the inner-passage information, inter-passage relations and PLMs knowledge. Extensive experiments have shown that our approach outperforms competitive dense retrieval methods. Our code and data are publicly released in \\url{https://github.com/microsoft/SimXNS}.\n## PRELIMINARY\nIn this section, we introduce the task definition of this work and present the typical fine-tuning process of dense retrieval. \n\nTask Definition. Given a query q, the dense passage retrieval task aims to retrieve the most relevant top-k passages {p i } k i=1 from a large candidate pool P. To achieve this goal, the dual-encoder architecture is widely used. It consists of a query encoder E q and a passage encoder E p , mapping the query q and passage p into k-dimensional dense vectors h q and h p , respectively. Then, the semantic relevance score of q and p will be computed using dot product as \n\n(1) \n\nExisting work mostly adopts pre-trained Transformers (e.g., BERT (Devlin et al., 2019)) as the two encoders, using the representations of the [CLS] token as the dense vectors. In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations. Formally, given a pre-training corpus and a Transformer encoder, we focus on devising several tasks to pre-train the parameters of it. Then, the pre-trained Transformer will be used as the backbone of the query encoder E q and passage encoder E p , and can be fine-tuned on downstream dense retrieval tasks. \n\nFine-tuning Dense Retrievers. In the fine-tuning stage, the learning objective is to pull the representations of a query q and its relevant passages P + together (as positives), while pushing apart irrelevant ones P \u2212 = P \\ P + (as negatives). Therefore, high-quality negatives are critical to the effectiveness of dense retrievers. Existing work commonly leverages the BM25 negatives (Karpukhin et al., 2020) or the top-k ranked negatives mined by a well-trained dense retriever (Qu et al., 2021;Xiong et al., 2021), denoted as D\u2212 . Then, the optimization objective can be formulated as:",
            "reference_string": "[254685782 | Zhou et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.05156",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.05156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "145204655",
                    "name": "Hao Fang"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ],
            "abstract": "Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https://github.com/microsoft/taser.",
            "corpus_id": 252816088,
            "sentences": [
                {
                    "corpus_id": "252816088",
                    "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
                    "text": "Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https://github.com/microsoft/taser.",
                    "score": 0.5025950807789776,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8310546875
                },
                {
                    "corpus_id": "252816088",
                    "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
                    "text": "During training, the model is optimized based on a contrastive learning objective,\n\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question. Although the bi-encoder dense retrieval architecture has achieved impressive results in ODQA, few work has attempted to improve its parameter efficiency. Further, compared to the spare vector space model BM25 (Robertson and Walker, 1994), such bi-encoder dense retrievers sometimes suffer from inferior generalization performance, e.g., when the training data is extremely biased (Lebret et al., 2016; or when there is a distribution shift (Sciavolino et al., 2021). In this paper, we conjecture that the unstable generalization performance is partially related to the sheer number of learnable parameters in the model. Therefore, we develop a task-aware specialization architecture for dense retrieval with parameter sharing between the question and passage encoders, which turns out to improve both parameter efficiency and generalization performance.",
                    "score": 0.5599007411128336,
                    "section_title": "Background",
                    "char_start_offset": 5876,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 861,
                            "end": 882,
                            "matchedPaperCorpusId": "1238927"
                        },
                        {
                            "start": 921,
                            "end": 946,
                            "matchedPaperCorpusId": "237562875"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7392578125
                }
            ],
            "relevance_judgement": 0.8310546875,
            "relevance_judgment_input_expanded": "# Title: Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Hao Cheng, Hao Fang, Xiaodong Liu, Jianfeng Gao\n## Abstract\nGiven its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https://github.com/microsoft/taser.\n## Background\nDuring training, the model is optimized based on a contrastive learning objective,\n\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question. Although the bi-encoder dense retrieval architecture has achieved impressive results in ODQA, few work has attempted to improve its parameter efficiency. Further, compared to the spare vector space model BM25 (Robertson and Walker, 1994), such bi-encoder dense retrievers sometimes suffer from inferior generalization performance, e.g., when the training data is extremely biased (Lebret et al., 2016; or when there is a distribution shift (Sciavolino et al., 2021). In this paper, we conjecture that the unstable generalization performance is partially related to the sheer number of learnable parameters in the model. Therefore, we develop a task-aware specialization architecture for dense retrieval with parameter sharing between the question and passage encoders, which turns out to improve both parameter efficiency and generalization performance.",
            "reference_string": "[252816088 | Cheng et al. | 2022 | Citations: 6]"
        },
        {
            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 50,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.11708",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.11708, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1754500",
                    "name": "Xingwei He"
                },
                {
                    "authorId": "2171182",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "15796861",
                    "name": "Alex Jin"
                },
                {
                    "authorId": "15629561",
                    "name": "Weizhen Qi"
                },
                {
                    "authorId": "2119077859",
                    "name": "Hang Zhang"
                },
                {
                    "authorId": "2143968416",
                    "name": "Jian Jiao"
                },
                {
                    "authorId": "2109061043",
                    "name": "Bartuer Zhou"
                },
                {
                    "authorId": "2055922979",
                    "name": "Biao Cheng"
                },
                {
                    "authorId": "2060901199",
                    "name": "Sm Yiu"
                },
                {
                    "authorId": "46429989",
                    "name": "Nan Duan"
                }
            ],
            "abstract": "Commonsense generation aims to generate a realistic sentence describing a daily scene under the given concepts, which is very challenging, since it requires models to have relational reasoning and compositional generalization capabilities. Previous work focuses on retrieving prototype sentences for the provided concepts to assist generation. They first use a sparse retriever to retrieve candidate sentences, then re-rank the candidates with a ranker. However, the candidates returned by their ranker may not be the most relevant sentences, since the ranker treats all candidates equally without considering their relevance to the reference sentences of the given concepts. Another problem is that re-ranking is very expensive, but only using retrievers will seriously degrade the performance of their generation models. To solve these problems, we propose the metric distillation rule to distill knowledge from the metric (e.g., BLEU) to the ranker. We further transfer the critical knowledge summarized by the distilled ranker to the retriever. In this way, the relevance scores of candidate sentences predicted by the ranker and retriever will be more consistent with their quality measured by the metric. Experimental results on the CommonGen benchmark verify the effectiveness of our proposed method: (1) Our generation model with the distilled ranker achieves a new state-of-the-art result. (2) Our generation model with the distilled retriever even surpasses the previous SOTA.",
            "corpus_id": 253080873,
            "sentences": [
                {
                    "corpus_id": "253080873",
                    "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
                    "text": "Recently, with the success of large pre-trained models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021) have surpassed the sparse vector space models, becoming the new de facto method. Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019;Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers. Recently, the retrieve-then-rank pipeline has also been applied to solve CommonGen (Wang et al., 2021;Li et al., 2021;Liu et al., 2022). Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus. To alleviate this, recent work, such as AR2 (Zhang et al., 2022), has focused on improving the retriever by distilling knowledge from the ranker. In this paper, we further extend this idea by distilling the order knowledge between the candidates and gold references to the ranker and retriever.",
                    "score": 0.6078145015070715,
                    "section_title": "Experiment on Keyword Generation",
                    "char_start_offset": 25985,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 654
                        },
                        {
                            "start": 657,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1567
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 69,
                            "end": 90,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 146,
                            "end": 170,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 170,
                            "end": 189,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 189,
                            "end": 205,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 855,
                            "end": 874,
                            "matchedPaperCorpusId": "201307832"
                        },
                        {
                            "start": 1039,
                            "end": 1058,
                            "matchedPaperCorpusId": "235166616"
                        },
                        {
                            "start": 1058,
                            "end": 1074,
                            "matchedPaperCorpusId": "244119606"
                        },
                        {
                            "start": 1074,
                            "end": 1091,
                            "matchedPaperCorpusId": "245144423"
                        },
                        {
                            "start": 1317,
                            "end": 1337,
                            "matchedPaperCorpusId": "238419331"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82861328125
                }
            ],
            "relevance_judgement": 0.82861328125,
            "relevance_judgment_input_expanded": "# Title: Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xingwei He, Yeyun Gong, Alex Jin, Weizhen Qi, Hang Zhang, Jian Jiao, Bartuer Zhou, Biao Cheng, Sm Yiu, Nan Duan\n## Abstract\nCommonsense generation aims to generate a realistic sentence describing a daily scene under the given concepts, which is very challenging, since it requires models to have relational reasoning and compositional generalization capabilities. Previous work focuses on retrieving prototype sentences for the provided concepts to assist generation. They first use a sparse retriever to retrieve candidate sentences, then re-rank the candidates with a ranker. However, the candidates returned by their ranker may not be the most relevant sentences, since the ranker treats all candidates equally without considering their relevance to the reference sentences of the given concepts. Another problem is that re-ranking is very expensive, but only using retrievers will seriously degrade the performance of their generation models. To solve these problems, we propose the metric distillation rule to distill knowledge from the metric (e.g., BLEU) to the ranker. We further transfer the critical knowledge summarized by the distilled ranker to the retriever. In this way, the relevance scores of candidate sentences predicted by the ranker and retriever will be more consistent with their quality measured by the metric. Experimental results on the CommonGen benchmark verify the effectiveness of our proposed method: (1) Our generation model with the distilled ranker achieves a new state-of-the-art result. (2) Our generation model with the distilled retriever even surpasses the previous SOTA.\n## Experiment on Keyword Generation\nRecently, with the success of large pre-trained models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021) have surpassed the sparse vector space models, becoming the new de facto method. Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019;Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers. Recently, the retrieve-then-rank pipeline has also been applied to solve CommonGen (Wang et al., 2021;Li et al., 2021;Liu et al., 2022). Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus. To alleviate this, recent work, such as AR2 (Zhang et al., 2022), has focused on improving the retriever by distilling knowledge from the ranker. In this paper, we further extend this idea by distilling the order knowledge between the candidates and gold references to the ranker and retriever.",
            "reference_string": "[253080873 | He et al. | 2022 | Citations: 11]"
        },
        {
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 42,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.09612",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264184691",
                    "name": "Noah Ziems"
                },
                {
                    "authorId": "38767143",
                    "name": "W. Yu"
                },
                {
                    "authorId": "72871419",
                    "name": "Zhihan Zhang"
                },
                {
                    "authorId": "2152153656",
                    "name": "Meng Jiang"
                }
            ],
            "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.",
            "corpus_id": 258714822,
            "sentences": [
                {
                    "corpus_id": "258714822",
                    "title": "Large Language Models are Built-in Autoregressive Search Engines",
                    "text": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
                    "score": 0.6548066316476866,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 2069
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 175,
                            "end": 199,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 579,
                            "end": 603,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 657,
                            "end": 679,
                            "matchedPaperCorpusId": "220302658"
                        },
                        {
                            "start": 883,
                            "end": 910,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1276,
                            "end": 1294,
                            "matchedPaperCorpusId": "249395549"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81787109375
                },
                {
                    "corpus_id": "258714822",
                    "title": "Large Language Models are Built-in Autoregressive Search Engines",
                    "text": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.",
                    "score": 0.5265511560929534,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8115234375
                },
                {
                    "corpus_id": "258714822",
                    "title": "Large Language Models are Built-in Autoregressive Search Engines",
                    "text": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
                    "score": 0.552239774014343,
                    "section_title": "Traditional Document Retrievers",
                    "char_start_offset": 4664,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 944
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 148,
                            "end": 178,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 178,
                            "end": 196,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 220,
                            "end": 244,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 558,
                            "end": 578,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 594,
                            "end": 614,
                            "matchedPaperCorpusId": "258865472"
                        },
                        {
                            "start": 637,
                            "end": 655,
                            "matchedPaperCorpusId": "230435683"
                        },
                        {
                            "start": 824,
                            "end": 848,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 848,
                            "end": 867,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 921,
                            "end": 943,
                            "matchedPaperCorpusId": "220302658"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67041015625
                }
            ],
            "relevance_judgement": 0.81787109375,
            "relevance_judgment_input_expanded": "# Title: Large Language Models are Built-in Autoregressive Search Engines\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Noah Ziems, W. Yu, Zhihan Zhang, Meng Jiang\n## Abstract\nDocument retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.\n## Introduction\nAlong with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).\n\n## Traditional Document Retrievers\nTraditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
            "reference_string": "[258714822 | Ziems et al. | 2023 | Citations: 42]"
        },
        {
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 41,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.09114",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1754500",
                    "name": "Xingwei He"
                },
                {
                    "authorId": "2171182",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "15796861",
                    "name": "Alex Jin"
                },
                {
                    "authorId": "2119077859",
                    "name": "Hang Zhang"
                },
                {
                    "authorId": "3300216",
                    "name": "Anlei Dong"
                },
                {
                    "authorId": "2143968416",
                    "name": "Jian Jiao"
                },
                {
                    "authorId": "145964453",
                    "name": "S. Yiu"
                },
                {
                    "authorId": "46429989",
                    "name": "Nan Duan"
                }
            ],
            "abstract": "The dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on obtaining query-informed document representations. During training, it expands the document with a real query, but during inference, it replaces the real query with a generated one. This inconsistency between training and inference causes the dense retrieval model to prioritize query information while disregarding the document when computing the document representation. Consequently, it performs even worse than the vanilla dense retrieval model because its performance heavily relies on the relevance between the generated queries and the real query.In this paper, we propose a curriculum sampling strategy that utilizes pseudo queries during training and progressively enhances the relevance between the generated query and the real query. By doing so, the retrieval model learns to extend its attention from the document alone to both the document and query, resulting in high-quality query-informed document representations. Experimental results on both in-domain and out-of-domain datasets demonstrate that our approach outperforms previous dense retrieval models.",
            "corpus_id": 254853896,
            "sentences": [
                {
                    "corpus_id": "254853896",
                    "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
                    "text": "On the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder. As shown in Figure 1 (c), this architecture computes the query-related document representation by expanding the document with a real or pseudo query. \n\nCompared with late interaction, dual-crossencoder (i.e., dense retrieval with document expansion) gets the query-related document representation without sacrificing the retrieval efficiency at inference. However, there exists a discrepancy between training and inference in the current dualcross-encoder retriever. Specifically, during training, the document is expanded using a real query, whereas during inference, the document is enriched with a generated query. This discrepancy causes the learned retriever overly focus on the query, yet neglect the document, when computing the document representation. During inference, if the generated query q\u2032 significantly differs from the user-input query q, the query-related document representation will be misled by q\u2032, thus degrading the performance. That is why the dual-cross-encoder even underperforms the vanilla dual-encoder. To address this issue, Li et al. (2022) proposed a solution by computing multiview document representations using different generated queries for each document. While multiview document representations improve retrieval performance, they come at the cost of significantly increased retrieval latency, which scales linearly with the number of views. \n\nIn this paper, we propose CAPSTONE, a curriculum sampling for dense retrieval with document expansion, to bridge the gap between training and inference for dual-cross-encoder. Our motivation is to expect the dual-cross-encoder retrieval model can utilize both the document d and pseudo query q\u2032 to compute the query-informed document representation. To achieve this, we train the dual-cross-encoder retriever by gradually increasing the relevance of the pseudo query q\u2032 to the gold query q. Specifically, at the early training stage, a pseudo query q\u2032 irrelevant to q is selected, causing the retriever to solely rely on the document. As we progress to the late training stage, a highly related pseudo query q\u2032 is chosen, allowing the retriever to learn to augment the document representation with the pseudo query.",
                    "score": 0.5160281926643,
                    "section_title": "Introduction",
                    "char_start_offset": 2228,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 250
                        },
                        {
                            "start": 253,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1481
                        },
                        {
                            "start": 1484,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2118
                        },
                        {
                            "start": 2119,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79345703125
                },
                {
                    "corpus_id": "254853896",
                    "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
                    "text": "Text retrieval aims to find the relevant documents for a given query from a large collection of documents, playing an indispensable role in opendomain question answering (Chen et al., 2017), fact verification (Thorne et al., 2018) and retrievalaugmented generation (Lewis et al., 2020;He et al., 2022). At the early stage, sparse retrieval methods such as TF-IDF or BM25 dominated passage retrieval by relying mainly on lexical term matching to compute relevance between the query and document. Recently, there has been a surge of research interest in neural network-based dense retrieval (Karpukhin et al., 2020;Xiong et al., 2021). Different from sparse retrieval, dense retrieval resorts to neural encoders to compute the dense representations of the query and document. This enables dense retrieval to infer the relevance between them at the semantic level rather than the surface level, thus circumventing the term mismatch problem suffered by the sparse retrieval models. \n\nIn recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder. \n\nTo enhance the retrieval models' ability to capture interactions between queries and documents while maintaining retrieval efficiency, previous work mainly focuses on generating query-informed document representations. One approach, known as late interaction (Khattab and Zaharia, 2020), involves encoding the query and document independently in the early layers, while the later layers model their interactions. Late interaction combines dual-encoder and cross-encoder, making a trade-off between retrieval efficiency and performance. On the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder.",
                    "score": 0.5632799152192681,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 977
                        },
                        {
                            "start": 980,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1674
                        },
                        {
                            "start": 1677,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2089
                        },
                        {
                            "start": 2090,
                            "end": 2212
                        },
                        {
                            "start": 2213,
                            "end": 2313
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 189,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 209,
                            "end": 230,
                            "matchedPaperCorpusId": "53645946"
                        },
                        {
                            "start": 265,
                            "end": 285,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 589,
                            "end": 613,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 613,
                            "end": 632,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1936,
                            "end": 1963,
                            "matchedPaperCorpusId": "216553223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76123046875
                }
            ],
            "relevance_judgement": 0.79345703125,
            "relevance_judgment_input_expanded": "# Title: Curriculum Sampling for Dense Retrieval with Document Expansion\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xingwei He, Yeyun Gong, Alex Jin, Hang Zhang, Anlei Dong, Jian Jiao, S. Yiu, Nan Duan\n## Abstract\nThe dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on obtaining query-informed document representations. During training, it expands the document with a real query, but during inference, it replaces the real query with a generated one. This inconsistency between training and inference causes the dense retrieval model to prioritize query information while disregarding the document when computing the document representation. Consequently, it performs even worse than the vanilla dense retrieval model because its performance heavily relies on the relevance between the generated queries and the real query.In this paper, we propose a curriculum sampling strategy that utilizes pseudo queries during training and progressively enhances the relevance between the generated query and the real query. By doing so, the retrieval model learns to extend its attention from the document alone to both the document and query, resulting in high-quality query-informed document representations. Experimental results on both in-domain and out-of-domain datasets demonstrate that our approach outperforms previous dense retrieval models.\n## Introduction\nText retrieval aims to find the relevant documents for a given query from a large collection of documents, playing an indispensable role in opendomain question answering (Chen et al., 2017), fact verification (Thorne et al., 2018) and retrievalaugmented generation (Lewis et al., 2020;He et al., 2022). At the early stage, sparse retrieval methods such as TF-IDF or BM25 dominated passage retrieval by relying mainly on lexical term matching to compute relevance between the query and document. Recently, there has been a surge of research interest in neural network-based dense retrieval (Karpukhin et al., 2020;Xiong et al., 2021). Different from sparse retrieval, dense retrieval resorts to neural encoders to compute the dense representations of the query and document. This enables dense retrieval to infer the relevance between them at the semantic level rather than the surface level, thus circumventing the term mismatch problem suffered by the sparse retrieval models. \n\nIn recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder. \n\nTo enhance the retrieval models' ability to capture interactions between queries and documents while maintaining retrieval efficiency, previous work mainly focuses on generating query-informed document representations. One approach, known as late interaction (Khattab and Zaharia, 2020), involves encoding the query and document independently in the early layers, while the later layers model their interactions. Late interaction combines dual-encoder and cross-encoder, making a trade-off between retrieval efficiency and performance. On the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder.\n...\nOn the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder. As shown in Figure 1 (c), this architecture computes the query-related document representation by expanding the document with a real or pseudo query. \n\nCompared with late interaction, dual-crossencoder (i.e., dense retrieval with document expansion) gets the query-related document representation without sacrificing the retrieval efficiency at inference. However, there exists a discrepancy between training and inference in the current dualcross-encoder retriever. Specifically, during training, the document is expanded using a real query, whereas during inference, the document is enriched with a generated query. This discrepancy causes the learned retriever overly focus on the query, yet neglect the document, when computing the document representation. During inference, if the generated query q\u2032 significantly differs from the user-input query q, the query-related document representation will be misled by q\u2032, thus degrading the performance. That is why the dual-cross-encoder even underperforms the vanilla dual-encoder. To address this issue, Li et al. (2022) proposed a solution by computing multiview document representations using different generated queries for each document. While multiview document representations improve retrieval performance, they come at the cost of significantly increased retrieval latency, which scales linearly with the number of views. \n\nIn this paper, we propose CAPSTONE, a curriculum sampling for dense retrieval with document expansion, to bridge the gap between training and inference for dual-cross-encoder. Our motivation is to expect the dual-cross-encoder retrieval model can utilize both the document d and pseudo query q\u2032 to compute the query-informed document representation. To achieve this, we train the dual-cross-encoder retriever by gradually increasing the relevance of the pseudo query q\u2032 to the gold query q. Specifically, at the early training stage, a pseudo query q\u2032 irrelevant to q is selected, causing the retriever to solely rely on the document. As we progress to the late training stage, a highly related pseudo query q\u2032 is chosen, allowing the retriever to learn to augment the document representation with the pseudo query.",
            "reference_string": "[254853896 | He et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.15563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51511290",
                    "name": "Fan Jiang"
                },
                {
                    "authorId": "2268408815",
                    "name": "Tom Drummond"
                },
                {
                    "authorId": "2256997782",
                    "name": "Trevor Cohn"
                }
            ],
            "abstract": "Although existing neural retrieval models reveal promising results when training data is abundant and the performance keeps improving as training data increases, collecting high-quality annotated data is prohibitively costly. To this end, we introduce a novel noisy self-training framework combined with synthetic queries, showing that neural retrievers can be improved in a self-evolution manner with no reliance on any external models. Experimental results show that our method improves consistently over existing methods on both general-domain (e.g., MS-MARCO) and out-of-domain (i.e., BEIR) retrieval benchmarks. Extra analysis on low-resource settings reveals that our method is data efficient and outperforms competitive baselines, with as little as 30% of labelled training data. Further extending the framework for reranker training demonstrates that the proposed method is general and yields additional gains on tasks of diverse domains.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/Self-Training-DPR}}",
            "corpus_id": 265457282,
            "sentences": [
                {
                    "corpus_id": "265457282",
                    "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
                    "text": "In contrast to traditional IR methods, such as BM25 (Robertson and Zaragoza, 2009), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings: \n\nwhere E \u2022 (\u2022; \u03b8) is an encoder parameterised by \u03b8. \n\nThe adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2021) can be performed to fetch the top-k passages. Contrastive learning is applied to train the dualencoder. Given a query q, we have a positive passage p + and a set of n negative passages P \u2212 q = {p \u2212 i } n i=1 . The model is being optimised by minimising the negative log likelihood of the positive passage: \n\nP \u2212 q is the set of irrelevant passages constructed from in-batch negatives (Chen et al., 2020) (i.e. positive passages of other queries in the same minibatch) and mined hard negatives from existing retrievers (Karpukhin et al., 2020;Xiong et al., 2021).",
                    "score": 0.5852308426801376,
                    "section_title": "Dense Passage Retrieval",
                    "char_start_offset": 5840,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 512
                        },
                        {
                            "start": 513,
                            "end": 583
                        },
                        {
                            "start": 586,
                            "end": 636
                        },
                        {
                            "start": 639,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 925
                        },
                        {
                            "start": 926,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1185
                        },
                        {
                            "start": 1188,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1442
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 52,
                            "end": 82,
                            "matchedPaperCorpusId": "202786778"
                        },
                        {
                            "start": 348,
                            "end": 372,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 857,
                            "end": 879,
                            "matchedPaperCorpusId": "926364"
                        },
                        {
                            "start": 1264,
                            "end": 1283,
                            "matchedPaperCorpusId": "211096730"
                        },
                        {
                            "start": 1398,
                            "end": 1422,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1422,
                            "end": 1441,
                            "matchedPaperCorpusId": "220302524"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7822265625
                }
            ],
            "relevance_judgement": 0.7822265625,
            "relevance_judgment_input_expanded": "# Title: Noisy Self-Training with Synthetic Queries for Dense Retrieval\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Fan Jiang, Tom Drummond, Trevor Cohn\n## Abstract\nAlthough existing neural retrieval models reveal promising results when training data is abundant and the performance keeps improving as training data increases, collecting high-quality annotated data is prohibitively costly. To this end, we introduce a novel noisy self-training framework combined with synthetic queries, showing that neural retrievers can be improved in a self-evolution manner with no reliance on any external models. Experimental results show that our method improves consistently over existing methods on both general-domain (e.g., MS-MARCO) and out-of-domain (i.e., BEIR) retrieval benchmarks. Extra analysis on low-resource settings reveals that our method is data efficient and outperforms competitive baselines, with as little as 30% of labelled training data. Further extending the framework for reranker training demonstrates that the proposed method is general and yields additional gains on tasks of diverse domains.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/Self-Training-DPR}}\n## Dense Passage Retrieval\nIn contrast to traditional IR methods, such as BM25 (Robertson and Zaragoza, 2009), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings: \n\nwhere E \u2022 (\u2022; \u03b8) is an encoder parameterised by \u03b8. \n\nThe adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2021) can be performed to fetch the top-k passages. Contrastive learning is applied to train the dualencoder. Given a query q, we have a positive passage p + and a set of n negative passages P \u2212 q = {p \u2212 i } n i=1 . The model is being optimised by minimising the negative log likelihood of the positive passage: \n\nP \u2212 q is the set of irrelevant passages constructed from in-batch negatives (Chen et al., 2020) (i.e. positive passages of other queries in the same minibatch) and mined hard negatives from existing retrievers (Karpukhin et al., 2020;Xiong et al., 2021).",
            "reference_string": "[265457282 | Jiang et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2023,
            "reference_count": 99,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3631939",
                "status": "HYBRID",
                "license": "CCBYSA",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.01263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40974185",
                    "name": "Jurek Leonhardt"
                },
                {
                    "authorId": "2265590892",
                    "name": "Henrik M\u00fcller"
                },
                {
                    "authorId": "2042376",
                    "name": "Koustav Rudra"
                },
                {
                    "authorId": "2125218238",
                    "name": "Megha Khosla"
                },
                {
                    "authorId": "2161343505",
                    "name": "Abhijit Anand"
                },
                {
                    "authorId": "2004208434",
                    "name": "Avishek Anand"
                }
            ],
            "abstract": "Dual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency. We propose Fast-Forward indexes\u2014vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size and show that, by dynamically dropping irrelevant document tokens, the index maintenance efficiency can be improved substantially. We perform an evaluation to show the effectiveness and efficiency of Fast-Forward indexes\u2014our method has low latency and achieves competitive results without the need for hardware acceleration, such as GPUs.",
            "corpus_id": 264935522,
            "sentences": [
                {
                    "corpus_id": "264935522",
                    "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
                    "text": "Dual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency. We propose Fast-Forward indexes\u2014vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size and show that, by dynamically dropping irrelevant document tokens, the index maintenance efficiency can be improved substantially. We perform an evaluation to show the effectiveness and efficiency of Fast-Forward indexes\u2014our method has low latency and achieves competitive results without the need for hardware acceleration, such as GPUs.",
                    "score": 0.5100694302349865,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7724609375
                },
                {
                    "corpus_id": "264935522",
                    "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
                    "text": "This allows them to perform query-document attention in order to compute the corresponding relevance score. These models are typically used as re-rankers. Dual-encoder models employ two language models to independently encode queries and documents as fixed-size vector representations. Usually, a similarity metric between query and document vector determines their relevance. As a result, dual-encoders are mostly used for dense retrieval, but also, less commonly, for re-ranking. \n\nWe divide the remainder of the related work section into subcategories for cross-attention models, dual-encoder models, and hybrid models, which employ both lexical and semantic rankers. Finally, we briefly cover inference efficiency for BERT-based models.",
                    "score": 0.5117902078067856,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 7783,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 481
                        },
                        {
                            "start": 484,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 740
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63427734375
                }
            ],
            "relevance_judgement": 0.7724609375,
            "relevance_judgment_input_expanded": "# Title: Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Jurek Leonhardt, Henrik M\u00fcller, Koustav Rudra, Megha Khosla, Abhijit Anand, Avishek Anand\n## Abstract\nDual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency. We propose Fast-Forward indexes\u2014vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size and show that, by dynamically dropping irrelevant document tokens, the index maintenance efficiency can be improved substantially. We perform an evaluation to show the effectiveness and efficiency of Fast-Forward indexes\u2014our method has low latency and achieves competitive results without the need for hardware acceleration, such as GPUs.\n## RELATED WORK\nThis allows them to perform query-document attention in order to compute the corresponding relevance score. These models are typically used as re-rankers. Dual-encoder models employ two language models to independently encode queries and documents as fixed-size vector representations. Usually, a similarity metric between query and document vector determines their relevance. As a result, dual-encoders are mostly used for dense retrieval, but also, less commonly, for re-ranking. \n\nWe divide the remainder of the related work section into subcategories for cross-attention models, dual-encoder models, and hybrid models, which employ both lexical and semantic rankers. Finally, we briefly cover inference efficiency for BERT-based models.",
            "reference_string": "[264935522 | Leonhardt et al. | 2023 | Citations: 7]"
        },
        {
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47396497",
                    "name": "Chao-Wei Huang"
                },
                {
                    "authorId": "2286884447",
                    "name": "Yun-Nung Chen"
                }
            ],
            "abstract": "Effective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense retrieval have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent comparisons. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively. Our source code and trained models are released at https://github.com/MiuLab/PairDistill",
            "corpus_id": 273026177,
            "sentences": [
                {
                    "corpus_id": "273026177",
                    "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
                    "text": "The goal of dense passage retrieval is to retrieve a subset of relevant passages, denoted as D + , from a large collection of passages \n\nIn order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval. \n\nThe relevance of a query q to a passage d i is measured using a similarity function: \n\nwhere a higher similarity score indicates a greater relevance of the passage to the query. Common choices of the similarity function are dot product, cosine similarity, or the Max-Sum operator introduced in ColBERT (Khattab and Zaharia, 2020). \n\nGiven a labeled dataset of relevant passage-query pairs (q, d + ), dense retrievers are typically trained with a contrastive learning objective such as the InfoNCE loss (Oord et al., 2018): \n\n, \n\nwhere D \u2032 denotes the union of the positive and negative examples. Optimizing this objective promotes the similarity of the positive pair s(q, d + ) in contrast to the negative examples.",
                    "score": 0.6149566932443707,
                    "section_title": "Dense Retrieval",
                    "char_start_offset": 7520,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 137,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 492
                        },
                        {
                            "start": 493,
                            "end": 636
                        },
                        {
                            "start": 639,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 969
                        },
                        {
                            "start": 972,
                            "end": 1161
                        },
                        {
                            "start": 1164,
                            "end": 1165
                        },
                        {
                            "start": 1168,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1354
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 941,
                            "end": 968,
                            "matchedPaperCorpusId": "216553223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77197265625
                }
            ],
            "relevance_judgement": 0.77197265625,
            "relevance_judgment_input_expanded": "# Title: PairDistill: Pairwise Relevance Distillation for Dense Retrieval\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Chao-Wei Huang, Yun-Nung Chen\n## Abstract\nEffective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense retrieval have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent comparisons. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively. Our source code and trained models are released at https://github.com/MiuLab/PairDistill\n## Dense Retrieval\nThe goal of dense passage retrieval is to retrieve a subset of relevant passages, denoted as D + , from a large collection of passages \n\nIn order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval. \n\nThe relevance of a query q to a passage d i is measured using a similarity function: \n\nwhere a higher similarity score indicates a greater relevance of the passage to the query. Common choices of the similarity function are dot product, cosine similarity, or the Max-Sum operator introduced in ColBERT (Khattab and Zaharia, 2020). \n\nGiven a labeled dataset of relevant passage-query pairs (q, d + ), dense retrievers are typically trained with a contrastive learning objective such as the InfoNCE loss (Oord et al., 2018): \n\n, \n\nwhere D \u2032 denotes the union of the positive and negative examples. Optimizing this objective promotes the similarity of the positive pair s(q, d + ) in contrast to the negative examples.",
            "reference_string": "[273026177 | Huang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2304.13649",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.13649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2073044451",
                    "name": "Alireza Salemi"
                },
                {
                    "authorId": "2137765299",
                    "name": "Juan Altmayer Pizzorno"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                }
            ],
            "abstract": "Knowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retrieved passage separately and uses all passages jointly in its decoder. Compared to competitive baselines in the literature, this approach leads to 5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA and FVQA, respectively.",
            "corpus_id": 258331649,
            "sentences": [
                {
                    "corpus_id": "258331649",
                    "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
                    "text": "A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. [40] recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap. We first study two alternatives for developing symmetric dense retrieval models: 4 (1) producing a textual representation of the image and using a symmetric uni-modal bi-encoder architecture for dense retrieval, and (2) converting passages to a multi-modal input format and using a symmetric multi-modal bi-encoder architecture. We observe that both alternatives suffer from information loss, but also that they produce complementary representations. This observation motivates us to not only combine these two encodings, but also transfer knowledge between them. In more detail, we propose an iterative knowledge distillation approach to transfer knowledge between these two alternative symmetric dense retrieval models. The proposed symmetric dual encoding approach leads to 11.6% and 30.9% MRR improvements compared to the state-of-the-art baseline on OK-VQA [38] and FVQA [56] test sets, respectively. \n\nFor the second stage of the pipeline, unlike much prior work on answer span detection for KI-VQA [12,14,37,50,57] (i.e., answer extraction from the retrieved passages), we focus on retrievalaugmented autoregressive answer generation. We propose MM-FiD, a simple yet effective extension of the Fusion-in-Decoder (FiD) [20] architecture to multi-modal input. FiD is a retrieval-augmented text generation model that has recently shown effective performance in question answering tasks [20].",
                    "score": 0.5575220066638313,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1601
                        },
                        {
                            "start": 1604,
                            "end": 1837
                        },
                        {
                            "start": 1838,
                            "end": 1960
                        },
                        {
                            "start": 1961,
                            "end": 2091
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 245,
                            "matchedPaperCorpusId": "234337940"
                        },
                        {
                            "start": 1558,
                            "end": 1562,
                            "matchedPaperCorpusId": "173991173"
                        },
                        {
                            "start": 1572,
                            "end": 1576,
                            "matchedPaperCorpusId": "7483388"
                        },
                        {
                            "start": 1701,
                            "end": 1705,
                            "matchedPaperCorpusId": "226284018"
                        },
                        {
                            "start": 1705,
                            "end": 1708,
                            "matchedPaperCorpusId": "250144531"
                        },
                        {
                            "start": 1708,
                            "end": 1711,
                            "matchedPaperCorpusId": "229339845"
                        },
                        {
                            "start": 1711,
                            "end": 1714,
                            "matchedPaperCorpusId": "231627467"
                        },
                        {
                            "start": 1714,
                            "end": 1717,
                            "matchedPaperCorpusId": "232320648"
                        },
                        {
                            "start": 1921,
                            "end": 1925,
                            "matchedPaperCorpusId": "220302360"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7646484375
                },
                {
                    "corpus_id": "258331649",
                    "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
                    "text": "Figure 2 depicts a pipeline for knowledge-intensive visual question answering tasks. As shown in the pipeline, the input to the dense retrieval model is asymmetric -query encoder takes multi-modal input (i.e., a question and an image), while the passage encoder takes a uni-modal text input (i.e., a passage from C). This asymmetric property in the input modalities makes it challenging to design an effective symmetric dense retrieval model. This is why the current state-of-the-art dense retrieval model proposed by Qu et al. [40] uses an asymmetric architecture, where a pre-trained multi-modal language model (i.e., LXMERT [53]) is used for query encoding and a pre-trained uni-modal language model (i.e., BERT [8]) is used for document encoding. Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B [17], ColBERT [24,49], RocketQA [41,44], and CLDRD [62], use symmetric architectures. Motivated by this observation, our goal is to learn a symmetric dense retrieval model for KI-VQA tasks. \n\nTo this aim, we study two alternative solutions. First, we convert all model inputs to a uni-modal textual form and then use uni-modal language models for both query and document encoding (Section 4.1). Second, we convert all inputs to the same multi-modal (text and image) form and then use multi-modal language models for both encoders (Section 4.2). We hypothesize that these two models learn complementary representations for the following reasons: (1) they take different input formats, and (2) the pre-training process and data in uni-modal and multi-modal language models are different.",
                    "score": 0.518383762285851,
                    "section_title": "DEDR: DUAL ENCODING DENSE RETRIEVER FRAMEWORK",
                    "char_start_offset": 11555,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1358
                        },
                        {
                            "start": 1361,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1954
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 528,
                            "end": 532,
                            "matchedPaperCorpusId": "234337940"
                        },
                        {
                            "start": 627,
                            "end": 631,
                            "matchedPaperCorpusId": "201103729"
                        },
                        {
                            "start": 715,
                            "end": 718,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1169,
                            "end": 1173,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 1183,
                            "end": 1187,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1187,
                            "end": 1190,
                            "matchedPaperCorpusId": "244799249"
                        },
                        {
                            "start": 1201,
                            "end": 1205,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1205,
                            "end": 1208,
                            "matchedPaperCorpusId": "238857121"
                        },
                        {
                            "start": 1220,
                            "end": 1224,
                            "matchedPaperCorpusId": "248426770"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7080078125
                }
            ],
            "relevance_judgement": 0.7646484375,
            "relevance_judgment_input_expanded": "# Title: A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Alireza Salemi, Juan Altmayer Pizzorno, Hamed Zamani\n## Abstract\nKnowledge-Intensive Visual Question Answering (KI-VQA) refers to answering a question about an image whose answer does not lie in the image. This paper presents a new pipeline for KI-VQA tasks, consisting of a retriever and a reader. First, we introduce DEDR, a symmetric dual encoding dense retrieval framework in which documents and queries are encoded into a shared embedding space using uni-modal (textual) and multi-modal encoders. We introduce an iterative knowledge distillation approach that bridges the gap between the representation spaces in these two encoders. Extensive evaluation on two well-established KI-VQA datasets, i.e., OK-VQA and FVQA, suggests that DEDR outperforms state-of-the-art baselines by 11.6% and 30.9% on OK-VQA and FVQA, respectively. Utilizing the passages retrieved by DEDR, we further introduce MM-FiD, an encoder-decoder multi-modal fusion-in-decoder model, for generating a textual answer for KI-VQA tasks. MM-FiD encodes the question, the image, and each retrieved passage separately and uses all passages jointly in its decoder. Compared to competitive baselines in the literature, this approach leads to 5.5% and 8.5% improvements in terms of question answering accuracy on OK-VQA and FVQA, respectively.\n## INTRODUCTION\nA property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. [40] recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap. We first study two alternatives for developing symmetric dense retrieval models: 4 (1) producing a textual representation of the image and using a symmetric uni-modal bi-encoder architecture for dense retrieval, and (2) converting passages to a multi-modal input format and using a symmetric multi-modal bi-encoder architecture. We observe that both alternatives suffer from information loss, but also that they produce complementary representations. This observation motivates us to not only combine these two encodings, but also transfer knowledge between them. In more detail, we propose an iterative knowledge distillation approach to transfer knowledge between these two alternative symmetric dense retrieval models. The proposed symmetric dual encoding approach leads to 11.6% and 30.9% MRR improvements compared to the state-of-the-art baseline on OK-VQA [38] and FVQA [56] test sets, respectively. \n\nFor the second stage of the pipeline, unlike much prior work on answer span detection for KI-VQA [12,14,37,50,57] (i.e., answer extraction from the retrieved passages), we focus on retrievalaugmented autoregressive answer generation. We propose MM-FiD, a simple yet effective extension of the Fusion-in-Decoder (FiD) [20] architecture to multi-modal input. FiD is a retrieval-augmented text generation model that has recently shown effective performance in question answering tasks [20].\n\n## DEDR: DUAL ENCODING DENSE RETRIEVER FRAMEWORK\nFigure 2 depicts a pipeline for knowledge-intensive visual question answering tasks. As shown in the pipeline, the input to the dense retrieval model is asymmetric -query encoder takes multi-modal input (i.e., a question and an image), while the passage encoder takes a uni-modal text input (i.e., a passage from C). This asymmetric property in the input modalities makes it challenging to design an effective symmetric dense retrieval model. This is why the current state-of-the-art dense retrieval model proposed by Qu et al. [40] uses an asymmetric architecture, where a pre-trained multi-modal language model (i.e., LXMERT [53]) is used for query encoding and a pre-trained uni-modal language model (i.e., BERT [8]) is used for document encoding. Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B [17], ColBERT [24,49], RocketQA [41,44], and CLDRD [62], use symmetric architectures. Motivated by this observation, our goal is to learn a symmetric dense retrieval model for KI-VQA tasks. \n\nTo this aim, we study two alternative solutions. First, we convert all model inputs to a uni-modal textual form and then use uni-modal language models for both query and document encoding (Section 4.1). Second, we convert all inputs to the same multi-modal (text and image) form and then use multi-modal language models for both encoders (Section 4.2). We hypothesize that these two models learn complementary representations for the following reasons: (1) they take different input formats, and (2) the pre-training process and data in uni-modal and multi-modal language models are different.",
            "reference_string": "[258331649 | Salemi et al. | 2023 | Citations: 14]"
        },
        {
            "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 28,
            "citation_count": 49,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.07937",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.07937, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51583409",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "2162783506",
                    "name": "Kangmin Tan"
                },
                {
                    "authorId": "2111522090",
                    "name": "Chris Miller"
                },
                {
                    "authorId": "2143694337",
                    "name": "Beiwen Tian"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                }
            ],
            "abstract": "Humans can perform unseen tasks by recalling relevant skills acquired previously and then generalizing them to the target tasks, even if there is no supervision at all. In this paper, we aim to improve this kind of cross-task generalization ability of massive multi-task language models, such as T0 and FLAN, in an unsupervised setting. We propose a retrieval-augmentation method named ReCross that takes a few unlabelled examples as queries to retrieve a small subset of upstream data and uses them to update the multi-task model for better generalization. ReCross is a straightforward yet effective retrieval method that combines both efficient dense retrieval and effective pair-wise reranking. Our results and analysis show that it significantly outperforms both non-retrieval methods and other baseline methods.",
            "corpus_id": 248227479,
            "sentences": [
                {
                    "corpus_id": "248227479",
                    "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
                    "text": "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4). \n\nEncoding query-candidate pairs. The cross-encoder architecture has been widely used in sentence-pair classification tasks such as natural language inference and paraphrase detection. We here use a cross-encoder to encode the concatenation of a query example and a candidate example. Specifically, we fine-tune a RoBERTa (Liu et al., 2019) model to classify whether an example pair is a positive or negative match. The confidence of classifying such a pair to be positive can thus be used as the utility score of the candidate upstream example for this query example. On top of this, we then develop a reranking module for further improving retrieval performance as follows. \n\nScoring paired data. To re-rank the initially retrieved data by the dense retriever, we apply the cross-encoder on all pairs of query examples Q and candidate retrieved examples R , producing scores of all |Q| * |R| query-candidate pairs. For each candidate example r \u2208 R , we use the average of all cross-encoder scores involving r as its utility score. Finally, we take the top-K examples based on this new ranking of candidate examples in R as the final retrieved data R. We use upsampling ratio \u00b5 to denote the ratio between R and R, i.e., \u00b5 = |R |/|R|.",
                    "score": 0.5115860964264187,
                    "section_title": "Reranking Module",
                    "char_start_offset": 13098,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 537
                        },
                        {
                            "start": 538,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 698
                        },
                        {
                            "start": 701,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1374
                        },
                        {
                            "start": 1377,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1615
                        },
                        {
                            "start": 1616,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1851
                        },
                        {
                            "start": 1852,
                            "end": 1934
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76220703125
                }
            ],
            "relevance_judgement": 0.76220703125,
            "relevance_judgment_input_expanded": "# Title: Unsupervised Cross-Task Generalization via Retrieval Augmentation\n# Venue: Neural Information Processing Systems\n# Authors: Bill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen Tian, Xiang Ren\n## Abstract\nHumans can perform unseen tasks by recalling relevant skills acquired previously and then generalizing them to the target tasks, even if there is no supervision at all. In this paper, we aim to improve this kind of cross-task generalization ability of massive multi-task language models, such as T0 and FLAN, in an unsupervised setting. We propose a retrieval-augmentation method named ReCross that takes a few unlabelled examples as queries to retrieve a small subset of upstream data and uses them to update the multi-task model for better generalization. ReCross is a straightforward yet effective retrieval method that combines both efficient dense retrieval and effective pair-wise reranking. Our results and analysis show that it significantly outperforms both non-retrieval methods and other baseline methods.\n## Reranking Module\nWeakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4). \n\nEncoding query-candidate pairs. The cross-encoder architecture has been widely used in sentence-pair classification tasks such as natural language inference and paraphrase detection. We here use a cross-encoder to encode the concatenation of a query example and a candidate example. Specifically, we fine-tune a RoBERTa (Liu et al., 2019) model to classify whether an example pair is a positive or negative match. The confidence of classifying such a pair to be positive can thus be used as the utility score of the candidate upstream example for this query example. On top of this, we then develop a reranking module for further improving retrieval performance as follows. \n\nScoring paired data. To re-rank the initially retrieved data by the dense retriever, we apply the cross-encoder on all pairs of query examples Q and candidate retrieved examples R , producing scores of all |Q| * |R| query-candidate pairs. For each candidate example r \u2208 R , we use the average of all cross-encoder scores involving r as its utility score. Finally, we take the top-K examples based on this new ranking of candidate examples in R as the final retrieved data R. We use upsampling ratio \u00b5 to denote the ratio between R and R, i.e., \u00b5 = |R |/|R|.",
            "reference_string": "[248227479 | Lin et al. | 2022 | Citations: 49]"
        },
        {
            "title": "NL2KQL: From Natural Language to Kusto Query",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 93,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294879238",
                    "name": "Amir H. Abdi"
                },
                {
                    "authorId": "2295304070",
                    "name": "Xinye Tang"
                },
                {
                    "authorId": "50821288",
                    "name": "Jeremias Eichelbaum"
                },
                {
                    "authorId": "2294847106",
                    "name": "Mahan Das"
                },
                {
                    "authorId": "2294855455",
                    "name": "Alex Klein"
                },
                {
                    "authorId": "2294876696",
                    "name": "Nihal Irmak Pakis"
                },
                {
                    "authorId": "2294877213",
                    "name": "William Blum"
                },
                {
                    "authorId": "2294875530",
                    "name": "Daniel L Mace"
                },
                {
                    "authorId": "2294879377",
                    "name": "Tanvi Raja"
                },
                {
                    "authorId": "2294871555",
                    "name": "Namrata Padmanabhan"
                },
                {
                    "authorId": "2294870992",
                    "name": "Ye Xing"
                }
            ],
            "abstract": "Data is growing rapidly in volume and complexity. Proficiency in database query languages is pivotal for crafting effective queries. As coding assistants become more prevalent, there is significant opportunity to enhance database query languages. The Kusto Query Language (KQL) is a widely used query language for large semi-structured data such as logs, telemetries, and time-series for big data analytics platforms. This paper introduces NL2KQL an innovative framework that uses large language models (LLMs) to convert natural language queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several key components: Schema Refiner which narrows down the schema to its most pertinent elements; the Few-shot Selector which dynamically selects relevant examples from a few-shot dataset; and the Query Refiner which repairs syntactic and semantic errors in KQL queries. Additionally, this study outlines a method for generating large datasets of synthetic NLQ-KQL pairs which are valid within a specific database contexts. To validate NL2KQL's performance, we utilize an array of online (based on query execution) and offline (based on query parsing) metrics. Through ablation studies, the significance of each framework component is examined, and the datasets used for benchmarking are made publicly available. This work is the first of its kind and is compared with available baselines to demonstrate its effectiveness.",
            "corpus_id": 268889861,
            "sentences": [
                {
                    "corpus_id": "268889861",
                    "title": "NL2KQL: From Natural Language to Kusto Query",
                    "text": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
                    "score": 0.592371551984347,
                    "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
                    "char_start_offset": 3014,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1156
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 156,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 156,
                            "end": 159,
                            "matchedPaperCorpusId": "86611921"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "13403541"
                        },
                        {
                            "start": 201,
                            "end": 205,
                            "matchedPaperCorpusId": "53218829"
                        },
                        {
                            "start": 455,
                            "end": 458,
                            "matchedPaperCorpusId": "207210855"
                        },
                        {
                            "start": 458,
                            "end": 461,
                            "matchedPaperCorpusId": "1965270"
                        },
                        {
                            "start": 736,
                            "end": 740,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 927,
                            "end": 930,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 930,
                            "end": 932,
                            "matchedPaperCorpusId": "223637"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73681640625
                }
            ],
            "relevance_judgement": 0.73681640625,
            "relevance_judgment_input_expanded": "# Title: NL2KQL: From Natural Language to Kusto Query\n# Venue: arXiv.org\n# Authors: Amir H. Abdi, Xinye Tang, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing\n## Abstract\nData is growing rapidly in volume and complexity. Proficiency in database query languages is pivotal for crafting effective queries. As coding assistants become more prevalent, there is significant opportunity to enhance database query languages. The Kusto Query Language (KQL) is a widely used query language for large semi-structured data such as logs, telemetries, and time-series for big data analytics platforms. This paper introduces NL2KQL an innovative framework that uses large language models (LLMs) to convert natural language queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several key components: Schema Refiner which narrows down the schema to its most pertinent elements; the Few-shot Selector which dynamically selects relevant examples from a few-shot dataset; and the Query Refiner which repairs syntactic and semantic errors in KQL queries. Additionally, this study outlines a method for generating large datasets of synthetic NLQ-KQL pairs which are valid within a specific database contexts. To validate NL2KQL's performance, we utilize an array of online (based on query execution) and offline (based on query parsing) metrics. Through ablation studies, the significance of each framework component is examined, and the datasets used for benchmarking are made publicly available. This work is the first of its kind and is compared with available baselines to demonstrate its effectiveness.\n## RELATED WORKS 2.1 Retrieval-enhanced Generation\nHistorical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
            "reference_string": "[268889861 | Abdi et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.15564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51511290",
                    "name": "Fan Jiang"
                },
                {
                    "authorId": "2257020484",
                    "name": "Qiongkai Xu"
                },
                {
                    "authorId": "2268408815",
                    "name": "Tom Drummond"
                },
                {
                    "authorId": "2256997782",
                    "name": "Trevor Cohn"
                }
            ],
            "abstract": "Neural 'dense' retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present $\\texttt{ABEL}$, a simple but effective unsupervised method to enhance passage retrieval in zero-shot settings. Our technique follows a straightforward loop: a dense retriever learns from supervision signals provided by a reranker, and subsequently, the reranker is updated based on feedback from the improved retriever. By iterating this loop, the two components mutually enhance one another's performance. Experimental results demonstrate that our unsupervised $\\texttt{ABEL}$ model outperforms both leading supervised and unsupervised retrievers on the BEIR benchmark. Meanwhile, it exhibits strong adaptation abilities to tasks and domains that were unseen during training. By either fine-tuning $\\texttt{ABEL}$ on labelled data or integrating it with existing supervised dense retrievers, we achieve state-of-the-art results.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/BootSwitch}.}",
            "corpus_id": 265457188,
            "sentences": [
                {
                    "corpus_id": "265457188",
                    "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
                    "text": "Remarkable progress has been achieved in neural information retrieval through the adoption of the dual-encoder paradigm (Gillick et al., 2018), which enables efficient search over vast collections of passages by factorising the model such that the encoding of queries and passages are decoupled, and calculating the query-passage similarity using dot product. However, the efficacy of training dualencoders heavily relies on the quality of labelled data, and these models struggle to maintain competitive performance on retrieval tasks where dedicated training data is scarce (Thakur et al., 2021). \n\nVarious approaches have been proposed to enhance dense retrievers (Karpukhin et al., 2020) in zero-shot settings while maintaining the factorised dual-encoder structure, such as pre-training models on web-scale corpus (Izacard et al., 2022) and learning from cross-encoders through distillation (Qu et al., 2021). Other alternatives seek to trade efficiency for performance by using complex model architectures, such as fine-grained token interaction for more expressive representations (Santhanam et al., 2022) and scaling up the model size for better model capacity (Ni et al., 2022). Another line of work trains customised dense retrievers on target domains through query generation (Wang et al., 2022;Dai et al., 2023). This training paradigm is generally slow and expensive, as it employs large language models to synthesise a substantial number of high-quality queries. \n\nIn this paper, we present ABEL, an Alternating Bootstrapping training framework for unsupervised dense rEtrievaL. Our method alternates the distillation process between a dense retriever and a reranker by switching their roles as teachers and students in iterations. On the one hand, the dense retriever allows for efficient retrieval due to its factorised encoding, accompanied by a compromised model performance. On the other hand, a reranker has no factorisation constraint, allowing for more fine-grained and accurate scoring, but at the cost of intractable searches. Our work aims to take advantage of both schools by equipping the dense retriever with accurate scoring by the reranker while maintaining search efficiency.",
                    "score": 0.5656808119085912,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 598
                        },
                        {
                            "start": 601,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2050
                        },
                        {
                            "start": 2051,
                            "end": 2206
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 576,
                            "end": 597,
                            "matchedPaperCorpusId": "233296016"
                        },
                        {
                            "start": 667,
                            "end": 691,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 819,
                            "end": 841,
                            "matchedPaperCorpusId": "249097975"
                        },
                        {
                            "start": 896,
                            "end": 913,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1088,
                            "end": 1112,
                            "matchedPaperCorpusId": "244799249"
                        },
                        {
                            "start": 1169,
                            "end": 1186,
                            "matchedPaperCorpusId": "245144556"
                        },
                        {
                            "start": 1287,
                            "end": 1306,
                            "matchedPaperCorpusId": "245131402"
                        },
                        {
                            "start": 1306,
                            "end": 1323,
                            "matchedPaperCorpusId": "252519173"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.734375
                },
                {
                    "corpus_id": "265457188",
                    "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
                    "text": "The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2021) is employed to fetch top-k relevant passages.",
                    "score": 0.6400370357224477,
                    "section_title": "Dense Retrieval Model",
                    "char_start_offset": 5312,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 286
                        },
                        {
                            "start": 289,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 639
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 125,
                            "end": 149,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 571,
                            "end": 593,
                            "matchedPaperCorpusId": "926364"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71923828125
                }
            ],
            "relevance_judgement": 0.734375,
            "relevance_judgment_input_expanded": "# Title: Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Fan Jiang, Qiongkai Xu, Tom Drummond, Trevor Cohn\n## Abstract\nNeural 'dense' retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present $\\texttt{ABEL}$, a simple but effective unsupervised method to enhance passage retrieval in zero-shot settings. Our technique follows a straightforward loop: a dense retriever learns from supervision signals provided by a reranker, and subsequently, the reranker is updated based on feedback from the improved retriever. By iterating this loop, the two components mutually enhance one another's performance. Experimental results demonstrate that our unsupervised $\\texttt{ABEL}$ model outperforms both leading supervised and unsupervised retrievers on the BEIR benchmark. Meanwhile, it exhibits strong adaptation abilities to tasks and domains that were unseen during training. By either fine-tuning $\\texttt{ABEL}$ on labelled data or integrating it with existing supervised dense retrievers, we achieve state-of-the-art results.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/BootSwitch}.}\n## Introduction\nRemarkable progress has been achieved in neural information retrieval through the adoption of the dual-encoder paradigm (Gillick et al., 2018), which enables efficient search over vast collections of passages by factorising the model such that the encoding of queries and passages are decoupled, and calculating the query-passage similarity using dot product. However, the efficacy of training dualencoders heavily relies on the quality of labelled data, and these models struggle to maintain competitive performance on retrieval tasks where dedicated training data is scarce (Thakur et al., 2021). \n\nVarious approaches have been proposed to enhance dense retrievers (Karpukhin et al., 2020) in zero-shot settings while maintaining the factorised dual-encoder structure, such as pre-training models on web-scale corpus (Izacard et al., 2022) and learning from cross-encoders through distillation (Qu et al., 2021). Other alternatives seek to trade efficiency for performance by using complex model architectures, such as fine-grained token interaction for more expressive representations (Santhanam et al., 2022) and scaling up the model size for better model capacity (Ni et al., 2022). Another line of work trains customised dense retrievers on target domains through query generation (Wang et al., 2022;Dai et al., 2023). This training paradigm is generally slow and expensive, as it employs large language models to synthesise a substantial number of high-quality queries. \n\nIn this paper, we present ABEL, an Alternating Bootstrapping training framework for unsupervised dense rEtrievaL. Our method alternates the distillation process between a dense retriever and a reranker by switching their roles as teachers and students in iterations. On the one hand, the dense retriever allows for efficient retrieval due to its factorised encoding, accompanied by a compromised model performance. On the other hand, a reranker has no factorisation constraint, allowing for more fine-grained and accurate scoring, but at the cost of intractable searches. Our work aims to take advantage of both schools by equipping the dense retriever with accurate scoring by the reranker while maintaining search efficiency.\n\n## Dense Retrieval Model\nThe dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2021) is employed to fetch top-k relevant passages.",
            "reference_string": "[265457188 | Jiang et al. | 2023 | Citations: 2]"
        },
        {
            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 16,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.15838, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287336807",
                    "name": "Soyoung Yoon"
                },
                {
                    "authorId": "2287970016",
                    "name": "Eunbi Choi"
                },
                {
                    "authorId": "2287064006",
                    "name": "Jiyeon Kim"
                },
                {
                    "authorId": "2181032855",
                    "name": "Yireun Kim"
                },
                {
                    "authorId": "2286896884",
                    "name": "Hyeongu Yun"
                },
                {
                    "authorId": "2287694374",
                    "name": "Seung-won Hwang"
                }
            ],
            "abstract": "We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \\url{https://github.com/soyoung97/ListT5}.",
            "corpus_id": 267938301,
            "sentences": [
                {
                    "corpus_id": "267938301",
                    "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
                    "text": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
                    "score": 0.5136249182535588,
                    "section_title": "Generative Models for Reranking",
                    "char_start_offset": 4693,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 927
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 59,
                            "end": 83,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 239,
                            "end": 261,
                            "matchedPaperCorpusId": "212725651"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72216796875
                }
            ],
            "relevance_judgement": 0.72216796875,
            "relevance_judgment_input_expanded": "# Title: ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Yireun Kim, Hyeongu Yun, Seung-won Hwang\n## Abstract\nWe propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \\url{https://github.com/soyoung97/ListT5}.\n## Generative Models for Reranking\nIn the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
            "reference_string": "[267938301 | Yoon et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 33,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.15133",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.15133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8427191",
                    "name": "Dingkun Long"
                },
                {
                    "authorId": "2107949588",
                    "name": "Yanzhao Zhang"
                },
                {
                    "authorId": "2149131512",
                    "name": "Guangwei Xu"
                },
                {
                    "authorId": "35930962",
                    "name": "Pengjun Xie"
                }
            ],
            "abstract": "Pre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.",
            "corpus_id": 253157959,
            "sentences": [
                {
                    "corpus_id": "253157959",
                    "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                    "text": "Existing dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
                    "score": 0.6553620023168762,
                    "section_title": "Related Work",
                    "char_start_offset": 3117,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 87,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 759
                        },
                        {
                            "start": 762,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1253
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 94,
                            "end": 118,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 450,
                            "end": 470,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 470,
                            "end": 488,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 625,
                            "end": 642,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 661,
                            "end": 681,
                            "matchedPaperCorpusId": "211068995"
                        },
                        {
                            "start": 909,
                            "end": 927,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 927,
                            "end": 946,
                            "matchedPaperCorpusId": "211068995"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71728515625
                },
                {
                    "corpus_id": "253157959",
                    "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                    "text": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.",
                    "score": 0.5890499555128104,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 685
                        },
                        {
                            "start": 688,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1867
                        },
                        {
                            "start": 1868,
                            "end": 2050
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 164,
                            "end": 188,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 188,
                            "end": 204,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 246,
                            "end": 263,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 835,
                            "end": 855,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1025,
                            "end": 1049,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1049,
                            "end": 1068,
                            "matchedPaperCorpusId": "247476289"
                        },
                        {
                            "start": 1488,
                            "end": 1510,
                            "matchedPaperCorpusId": "220729621"
                        },
                        {
                            "start": 1510,
                            "end": 1526,
                            "matchedPaperCorpusId": "233307194"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6611328125
                }
            ],
            "relevance_judgement": 0.71728515625,
            "relevance_judgment_input_expanded": "# Title: Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval\n# Venue: arXiv.org\n# Authors: Dingkun Long, Yanzhao Zhang, Guangwei Xu, Pengjun Xie\n## Abstract\nPre-trained language model (PTM) has been shown to yield powerful text representations for dense passage retrieval task. The Masked Language Modeling (MLM) is a major sub-task of the pre-training process. However, we found that the conventional random masking strategy tend to select a large number of tokens that have limited effect on the passage retrieval task (e,g. stop-words and punctuation). By noticing the term importance weight can provide valuable information for passage retrieval, we hereby propose alternative retrieval oriented masking (dubbed as ROM) strategy where more important tokens will have a higher probability of being masked out, to capture this straightforward yet essential information to facilitate the language model pre-training process. Notably, the proposed new token masking method will not change the architecture and learning objective of original PTM. Our experiments verify that the proposed ROM enables term importance information to help language model pre-training thus achieving better performance on multiple passage retrieval benchmarks.\n## Introduction\nDense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.\n\n## Related Work\nExisting dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
            "reference_string": "[253157959 | Long et al. | 2022 | Citations: 4]"
        },
        {
            "title": "Generative Retrieval with Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185022832",
                    "name": "Ye Wang"
                },
                {
                    "authorId": "2290204960",
                    "name": "Xinrun Xu"
                },
                {
                    "authorId": "2143721734",
                    "name": "Rui Xie"
                },
                {
                    "authorId": "2288018918",
                    "name": "Wenxin Hu"
                },
                {
                    "authorId": "2052980435",
                    "name": "Wei Ye"
                }
            ],
            "abstract": "When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.",
            "corpus_id": 268031876,
            "sentences": [
                {
                    "corpus_id": "268031876",
                    "title": "Generative Retrieval with Large Language Models",
                    "text": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
                    "score": 0.7155243246749065,
                    "section_title": "Related Work",
                    "char_start_offset": 21339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 79,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 593
                        },
                        {
                            "start": 596,
                            "end": 788
                        },
                        {
                            "start": 789,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1243
                        },
                        {
                            "start": 1246,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1789
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 151,
                            "end": 175,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 175,
                            "end": 193,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 193,
                            "end": 211,
                            "matchedPaperCorpusId": "59604492"
                        },
                        {
                            "start": 911,
                            "end": 930,
                            "matchedPaperCorpusId": "258714822"
                        },
                        {
                            "start": 1410,
                            "end": 1428,
                            "matchedPaperCorpusId": "249395549"
                        },
                        {
                            "start": 1428,
                            "end": 1446,
                            "matchedPaperCorpusId": "239016601"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70654296875
                }
            ],
            "relevance_judgement": 0.70654296875,
            "relevance_judgment_input_expanded": "# Title: Generative Retrieval with Large Language Models\n# Venue: \n# Authors: Ye Wang, Xinrun Xu, Rui Xie, Wenxin Hu, Wei Ye\n## Abstract\nWhen completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.\n## Related Work\nTraditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
            "reference_string": "[268031876 | Wang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2022,
            "reference_count": 42,
            "citation_count": 53,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3477495.3531772",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.10641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "121875983",
                    "name": "Xinyu Ma"
                },
                {
                    "authorId": "1777025",
                    "name": "J. Guo"
                },
                {
                    "authorId": "2109960367",
                    "name": "Ruqing Zhang"
                },
                {
                    "authorId": "7888704",
                    "name": "Yixing Fan"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ],
            "abstract": "Dense retrieval has shown promising results in many information retrieval (IR) related tasks, whose foundation is high-quality text representation learning for effective search. Some recent studies have shown that autoencoder-based language models are able to boost the dense retrieval performance using a weak decoder. However, we argue that 1) it is not discriminative to decode all the input texts and, 2) even a weak decoder has the bypass effect on the encoder. Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder thoroughly. Comprehensive experiments over publicly available retrieval benchmark datasets show that our approach can outperform existing pre-training methods for dense retrieval significantly.",
            "corpus_id": 248366563,
            "sentences": [
                {
                    "corpus_id": "248366563",
                    "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                    "text": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
                    "score": 0.6061568020180226,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 477
                        },
                        {
                            "start": 478,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 753
                        },
                        {
                            "start": 756,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1523
                        },
                        {
                            "start": 1526,
                            "end": 1762
                        },
                        {
                            "start": 1763,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2091
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 177,
                            "end": 180,
                            "matchedPaperCorpusId": "232147859"
                        },
                        {
                            "start": 180,
                            "end": 183,
                            "matchedPaperCorpusId": "222310837"
                        },
                        {
                            "start": 204,
                            "end": 208,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 208,
                            "end": 211,
                            "matchedPaperCorpusId": "229363636"
                        },
                        {
                            "start": 211,
                            "end": 214,
                            "matchedPaperCorpusId": "237502990"
                        },
                        {
                            "start": 246,
                            "end": 249,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 457,
                            "end": 461,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 461,
                            "end": 464,
                            "matchedPaperCorpusId": "210063976"
                        },
                        {
                            "start": 464,
                            "end": 467,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 470,
                            "end": 473,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 882,
                            "end": 886,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 889,
                            "end": 892,
                            "matchedPaperCorpusId": "233289894"
                        },
                        {
                            "start": 1034,
                            "end": 1037,
                            "matchedPaperCorpusId": "211068995"
                        },
                        {
                            "start": 1037,
                            "end": 1040,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 1068,
                            "end": 1071,
                            "matchedPaperCorpusId": "211068995"
                        },
                        {
                            "start": 1689,
                            "end": 1693,
                            "matchedPaperCorpusId": "5541663"
                        },
                        {
                            "start": 1754,
                            "end": 1758,
                            "matchedPaperCorpusId": "215416349"
                        },
                        {
                            "start": 1758,
                            "end": 1761,
                            "matchedPaperCorpusId": "243865399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7001953125
                }
            ],
            "relevance_judgement": 0.7001953125,
            "relevance_judgment_input_expanded": "# Title: Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Xinyu Ma, J. Guo, Ruqing Zhang, Yixing Fan, Xueqi Cheng\n## Abstract\nDense retrieval has shown promising results in many information retrieval (IR) related tasks, whose foundation is high-quality text representation learning for effective search. Some recent studies have shown that autoencoder-based language models are able to boost the dense retrieval performance using a weak decoder. However, we argue that 1) it is not discriminative to decode all the input texts and, 2) even a weak decoder has the bypass effect on the encoder. Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder thoroughly. Comprehensive experiments over publicly available retrieval benchmark datasets show that our approach can outperform existing pre-training methods for dense retrieval significantly.\n## INTRODUCTION\nDense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
            "reference_string": "[248366563 | Ma et al. | 2022 | Citations: 53]"
        },
        {
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.01890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121348263",
                    "name": "Nicholas Monath"
                },
                {
                    "authorId": "2319130233",
                    "name": "Will Sussman Grathwohl"
                },
                {
                    "authorId": "51020741",
                    "name": "Michael Boratko"
                },
                {
                    "authorId": "2300098510",
                    "name": "Rob Fergus"
                },
                {
                    "authorId": "2286335051",
                    "name": "Andrew McCallum"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                }
            ],
            "abstract": "In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring\"hard negatives.\"We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.",
            "corpus_id": 272330251,
            "sentences": [
                {
                    "corpus_id": "272330251",
                    "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
                    "text": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
                    "score": 0.5642365360722125,
                    "section_title": "Latent Variables in Retrieval Augmented Models",
                    "char_start_offset": 12356,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 627
                        },
                        {
                            "start": 630,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1242
                        },
                        {
                            "start": 1245,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1317
                        },
                        {
                            "start": 1318,
                            "end": 1421
                        },
                        {
                            "start": 1424,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1722
                        },
                        {
                            "start": 1725,
                            "end": 1887
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 189,
                            "end": 207,
                            "matchedPaperCorpusId": "211204736"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6982421875
                }
            ],
            "relevance_judgement": 0.6982421875,
            "relevance_judgment_input_expanded": "# Title: A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks\n# Venue: International Conference on Machine Learning\n# Authors: Nicholas Monath, Will Sussman Grathwohl, Michael Boratko, Rob Fergus, Andrew McCallum, M. Zaheer\n## Abstract\nIn dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring\"hard negatives.\"We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.\n## Latent Variables in Retrieval Augmented Models\nRetrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
            "reference_string": "[272330251 | Monath et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 76,
            "citation_count": 165,
            "influential_citation_count": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.10628",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.10628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143802044",
                    "name": "Michele Bevilacqua"
                },
                {
                    "authorId": "3230478",
                    "name": "G. Ottaviano"
                },
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "144105277",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                },
                {
                    "authorId": "40052301",
                    "name": "F. Petroni"
                }
            ],
            "abstract": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",
            "corpus_id": 248366293,
            "sentences": [
                {
                    "corpus_id": "248366293",
                    "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
                    "text": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",
                    "score": 0.5914947918272457,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69677734375
                }
            ],
            "relevance_judgement": 0.69677734375,
            "relevance_judgment_input_expanded": "# Title: Autoregressive Search Engines: Generating Substrings as Document Identifiers\n# Venue: Neural Information Processing Systems\n# Authors: Michele Bevilacqua, G. Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, F. Petroni\n## Abstract\nKnowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.\n",
            "reference_string": "[248366293 | Bevilacqua et al. | 2022 | Citations: 165]"
        },
        {
            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13197",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13197, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109967721",
                    "name": "Zehan Li"
                },
                {
                    "authorId": "2107949588",
                    "name": "Yanzhao Zhang"
                },
                {
                    "authorId": "8427191",
                    "name": "Dingkun Long"
                },
                {
                    "authorId": "35930962",
                    "name": "Pengjun Xie"
                }
            ],
            "abstract": "Recently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability. To this end, we propose a novel token importance aware masking strategy based on pointwise mutual information to intensify the challenge of the decoder. Importantly, our approach can be implemented in an unsupervised manner, without adding additional expenses to the pre-training phase. Our experiments verify that the proposed method is both effective and robust on large-scale supervised passage retrieval datasets and out-of-domain zero-shot retrieval benchmarks.",
            "corpus_id": 258833383,
            "sentences": [
                {
                    "corpus_id": "258833383",
                    "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
                    "text": "Passage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
                    "score": 0.6135889598618036,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1766
                        },
                        {
                            "start": 1769,
                            "end": 1883
                        },
                        {
                            "start": 1884,
                            "end": 2198
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 112,
                            "end": 136,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 136,
                            "end": 153,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 153,
                            "end": 170,
                            "matchedPaperCorpusId": "237502990"
                        },
                        {
                            "start": 195,
                            "end": 212,
                            "matchedPaperCorpusId": "234343311"
                        },
                        {
                            "start": 263,
                            "end": 282,
                            "matchedPaperCorpusId": "247292113"
                        },
                        {
                            "start": 508,
                            "end": 532,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1314,
                            "end": 1330,
                            "matchedPaperCorpusId": "243865399"
                        },
                        {
                            "start": 1708,
                            "end": 1726,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 1747,
                            "end": 1765,
                            "matchedPaperCorpusId": "252917569"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6904296875
                }
            ],
            "relevance_judgement": 0.6904296875,
            "relevance_judgment_input_expanded": "# Title: Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval\n# Venue: arXiv.org\n# Authors: Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie\n## Abstract\nRecently, various studies have been directed towards exploring dense passage retrieval techniques employing pre-trained language models, among which the masked auto-encoder (MAE) pre-training architecture has emerged as the most promising. The conventional MAE framework relies on leveraging the passage reconstruction of decoder to bolster the text representation ability of encoder, thereby enhancing the performance of resulting dense retrieval systems. Within the context of building the representation ability of the encoder through passage reconstruction of decoder, it is reasonable to postulate that a ``more demanding'' decoder will necessitate a corresponding increase in the encoder's ability. To this end, we propose a novel token importance aware masking strategy based on pointwise mutual information to intensify the challenge of the decoder. Importantly, our approach can be implemented in an unsupervised manner, without adding additional expenses to the pre-training phase. Our experiments verify that the proposed method is both effective and robust on large-scale supervised passage retrieval datasets and out-of-domain zero-shot retrieval benchmarks.\n## Introduction\nPassage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
            "reference_string": "[258833383 | Li et al. | 2023 | Citations: 3]"
        },
        {
            "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149234508",
                    "name": "Zhen Yang"
                },
                {
                    "authorId": "2284682533",
                    "name": "Zhou Shao"
                },
                {
                    "authorId": "2243402027",
                    "name": "Yuxiao Dong"
                },
                {
                    "authorId": "2238207092",
                    "name": "Jie Tang"
                }
            ],
            "abstract": "Negative sampling stands as a pivotal technique in dense retrieval, essential for training effective retrieval models and significantly impacting retrieval performance. While existing negative sampling methods have made commendable progress by leveraging hard negatives, a comprehensive guiding principle for constructing negative candidates and designing negative sampling distributions is still lacking. To bridge this gap, we embark on a theoretical analysis of negative sampling in dense retrieval. This exploration culminates in the unveiling of the quasi-triangular principle, a novel framework that elucidates the triangular-like interplay between query, positive document, and negative document. Fueled by this guiding principle, we introduce TriSampler, a straightforward yet highly effective negative sampling method. The keypoint of TriSampler lies in its ability to selectively sample more informative negatives within a prescribed constrained region. Experimental evaluation show that TriSampler consistently attains superior retrieval performance across a diverse of representative retrieval models.",
            "corpus_id": 267751308,
            "sentences": [
                {
                    "corpus_id": "267751308",
                    "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
                    "text": "Dense retrieval. Dense retrieval (Lee, Chang, and Toutanova 2019;Karpukhin et al. 2020;Xiong et al. 2020;Khattab and Zaharia 2020) shows tremendous success in many downstream tasks (e.g. open-domain QA and web search) compared with the traditional sparse retrieval models (e.g. TF-IDF and BM25). The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dualencoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model finegrained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance. Besides, knowledge distillation has become a vital technique to enhance the capacity of the dual-encoder by distilling knowledge from a more powerful reader to a classical retriever (Qu et al. 2020;Ren et al. 2021b;Lin, Yang, and Lin 2020;Hofst\u00e4tter et al. 2021). \n\nRecently, massive works have investigated task-related pretraining methods for dense retrieval models (Gao and Callan 2021a,b;Wang, Reimers, and Gurevych 2021;Ren et al. 2021a;Oguz et al. 2021;Meng et al. 2021). Condenser (Gao and Callan 2021a) proposed the Condenser architecture to enforce the late backbone layers to aggregate the whole information. coCondenser (Gao and Callan 2021b) leveraged contrastive learning to incorporate a query-agnostic contrastive loss. PAIR (Ren et al. 2021a) and DPR-PAQ (Oguz et al. 2021) also designed special tasks in pre-training to enhance retrieval models. Additionally, jointly training retrieval models with the rerank model can bring about better performance.",
                    "score": 0.5689989780820396,
                    "section_title": "Related Work",
                    "char_start_offset": 6374,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 16
                        },
                        {
                            "start": 17,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1227
                        },
                        {
                            "start": 1230,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1698
                        },
                        {
                            "start": 1699,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 33,
                            "end": 65,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 65,
                            "end": 87,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 557,
                            "end": 589,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 868,
                            "end": 886,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 1146,
                            "end": 1162,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1203,
                            "end": 1226,
                            "matchedPaperCorpusId": "233231706"
                        },
                        {
                            "start": 1389,
                            "end": 1406,
                            "matchedPaperCorpusId": "231815627"
                        },
                        {
                            "start": 1704,
                            "end": 1721,
                            "matchedPaperCorpusId": "231815627"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67626953125
                }
            ],
            "relevance_judgement": 0.67626953125,
            "relevance_judgment_input_expanded": "# Title: TriSampler: A Better Negative Sampling Principle for Dense Retrieval\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zhen Yang, Zhou Shao, Yuxiao Dong, Jie Tang\n## Abstract\nNegative sampling stands as a pivotal technique in dense retrieval, essential for training effective retrieval models and significantly impacting retrieval performance. While existing negative sampling methods have made commendable progress by leveraging hard negatives, a comprehensive guiding principle for constructing negative candidates and designing negative sampling distributions is still lacking. To bridge this gap, we embark on a theoretical analysis of negative sampling in dense retrieval. This exploration culminates in the unveiling of the quasi-triangular principle, a novel framework that elucidates the triangular-like interplay between query, positive document, and negative document. Fueled by this guiding principle, we introduce TriSampler, a straightforward yet highly effective negative sampling method. The keypoint of TriSampler lies in its ability to selectively sample more informative negatives within a prescribed constrained region. Experimental evaluation show that TriSampler consistently attains superior retrieval performance across a diverse of representative retrieval models.\n## Related Work\nDense retrieval. Dense retrieval (Lee, Chang, and Toutanova 2019;Karpukhin et al. 2020;Xiong et al. 2020;Khattab and Zaharia 2020) shows tremendous success in many downstream tasks (e.g. open-domain QA and web search) compared with the traditional sparse retrieval models (e.g. TF-IDF and BM25). The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dualencoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model finegrained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance. Besides, knowledge distillation has become a vital technique to enhance the capacity of the dual-encoder by distilling knowledge from a more powerful reader to a classical retriever (Qu et al. 2020;Ren et al. 2021b;Lin, Yang, and Lin 2020;Hofst\u00e4tter et al. 2021). \n\nRecently, massive works have investigated task-related pretraining methods for dense retrieval models (Gao and Callan 2021a,b;Wang, Reimers, and Gurevych 2021;Ren et al. 2021a;Oguz et al. 2021;Meng et al. 2021). Condenser (Gao and Callan 2021a) proposed the Condenser architecture to enforce the late backbone layers to aggregate the whole information. coCondenser (Gao and Callan 2021b) leveraged contrastive learning to incorporate a query-agnostic contrastive loss. PAIR (Ren et al. 2021a) and DPR-PAQ (Oguz et al. 2021) also designed special tasks in pre-training to enhance retrieval models. Additionally, jointly training retrieval models with the rerank model can bring about better performance.",
            "reference_string": "[267751308 | Yang et al. | 2024 | Citations: 4]"
        },
        {
            "title": "MultiConIR: Towards multi-condition Information Retrieval",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349551985",
                    "name": "Xuan Lu"
                },
                {
                    "authorId": "2349802894",
                    "name": "Sifan Liu"
                },
                {
                    "authorId": "2349465013",
                    "name": "Bochao Yin"
                },
                {
                    "authorId": "2349749730",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2325183464",
                    "name": "Xinghao Chen"
                },
                {
                    "authorId": "2349762405",
                    "name": "Hui Su"
                },
                {
                    "authorId": "2349879907",
                    "name": "Yaohui Jin"
                },
                {
                    "authorId": "2349955598",
                    "name": "Wenjun Zeng"
                },
                {
                    "authorId": "2287612923",
                    "name": "Xiaoyu Shen"
                }
            ],
            "abstract": "In this paper, we introduce MultiConIR, the first benchmark designed to evaluate retrieval models in multi-condition scenarios. Unlike existing datasets that primarily focus on single-condition queries from search engines, MultiConIR captures real-world complexity by incorporating five diverse domains: books, movies, people, medical cases, and legal documents. We propose three tasks to systematically assess retrieval and reranking models on multi-condition robustness, monotonic relevance ranking, and query format sensitivity. Our findings reveal that existing retrieval and reranking models struggle with multi-condition retrieval, with rerankers suffering severe performance degradation as query complexity increases. We further investigate the performance gap between retrieval and reranking models, exploring potential reasons for these discrepancies, and analysis the impact of different pooling strategies on condition placement sensitivity. Finally, we highlight the strengths of GritLM and Nv-Embed, which demonstrate enhanced adaptability to multi-condition queries, offering insights for future retrieval models. The code and datasets are available at https://github.com/EIT-NLP/MultiConIR.",
            "corpus_id": 276928453,
            "sentences": [
                {
                    "corpus_id": "276928453",
                    "title": "MultiConIR: Towards multi-condition Information Retrieval",
                    "text": "Our experiments reveal notable differences between retrieval and reranking models across the three tasks. Specifically, retrieval models demonstrate greater robustness to query complexity, better preservation of relevance monotonicity, and stronger invariance to query format changes compared to rerankers. We analyze these differences through the lens of relevance computation and attention mechanisms. \n\nRetrieval models: robustness induced by bidirectional attention and dual-encoder. Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query. \n\nAnother critical factor is the training datasets. Many dense retrieval models are trained on a mixture of retrieval-specific and general textual datasets (Lee et al., 2024;BehnamGhader et al., 2024;Wang et al., 2024a), which enhances their generalization across diverse retrieval scenarios and query styles. This, in turn, improves their robustness against query complexity. \n\nReranking models: sensitivity to query complexity. Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format. \n\nOur experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models.",
                    "score": 0.6141489001985233,
                    "section_title": "Retrievers vs. Rankers",
                    "char_start_offset": 18881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 403
                        },
                        {
                            "start": 406,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 923
                        },
                        {
                            "start": 926,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1300
                        },
                        {
                            "start": 1303,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 1850
                        },
                        {
                            "start": 1853,
                            "end": 2078
                        },
                        {
                            "start": 2079,
                            "end": 2203
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1124,
                            "end": 1143,
                            "matchedPaperCorpusId": "266693831"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.673828125
                }
            ],
            "relevance_judgement": 0.673828125,
            "relevance_judgment_input_expanded": "# Title: MultiConIR: Towards multi-condition Information Retrieval\n# Venue: arXiv.org\n# Authors: Xuan Lu, Sifan Liu, Bochao Yin, Yongqi Li, Xinghao Chen, Hui Su, Yaohui Jin, Wenjun Zeng, Xiaoyu Shen\n## Abstract\nIn this paper, we introduce MultiConIR, the first benchmark designed to evaluate retrieval models in multi-condition scenarios. Unlike existing datasets that primarily focus on single-condition queries from search engines, MultiConIR captures real-world complexity by incorporating five diverse domains: books, movies, people, medical cases, and legal documents. We propose three tasks to systematically assess retrieval and reranking models on multi-condition robustness, monotonic relevance ranking, and query format sensitivity. Our findings reveal that existing retrieval and reranking models struggle with multi-condition retrieval, with rerankers suffering severe performance degradation as query complexity increases. We further investigate the performance gap between retrieval and reranking models, exploring potential reasons for these discrepancies, and analysis the impact of different pooling strategies on condition placement sensitivity. Finally, we highlight the strengths of GritLM and Nv-Embed, which demonstrate enhanced adaptability to multi-condition queries, offering insights for future retrieval models. The code and datasets are available at https://github.com/EIT-NLP/MultiConIR.\n## Retrievers vs. Rankers\nOur experiments reveal notable differences between retrieval and reranking models across the three tasks. Specifically, retrieval models demonstrate greater robustness to query complexity, better preservation of relevance monotonicity, and stronger invariance to query format changes compared to rerankers. We analyze these differences through the lens of relevance computation and attention mechanisms. \n\nRetrieval models: robustness induced by bidirectional attention and dual-encoder. Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query. \n\nAnother critical factor is the training datasets. Many dense retrieval models are trained on a mixture of retrieval-specific and general textual datasets (Lee et al., 2024;BehnamGhader et al., 2024;Wang et al., 2024a), which enhances their generalization across diverse retrieval scenarios and query styles. This, in turn, improves their robustness against query complexity. \n\nReranking models: sensitivity to query complexity. Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format. \n\nOur experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models.",
            "reference_string": "[276928453 | Lu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.03950",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.03950, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1630489015",
                    "name": "Shengyao Zhuang"
                },
                {
                    "authorId": "24962156",
                    "name": "Linjun Shou"
                },
                {
                    "authorId": "1692855",
                    "name": "G. Zuccon"
                }
            ],
            "abstract": "Effective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does not increase query latency at inference and can be combined with any cross-lingual dense retrieval method. Results from experiments on a benchmark cross-lingual information retrieval dataset show that our approach can improve the effectiveness of existing cross-lingual dense retrieval methods. Implementation of our methods, along with all generated query files are made publicly available at https://github.com/ielab/xQG4xDR.",
            "corpus_id": 258557604,
            "sentences": [
                {
                    "corpus_id": "258557604",
                    "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
                    "text": "Pre-trained language model-based (PLM) dense retrievers (DRs) have achieved remarkable success in the task of English-only passage retrieval [12-14, 20, 21, 26, 28, 29, 40, 43, 44]. These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33,45]. \n\nTo leverage the semantic modelling power of DRs, recent research has extended English-only DRs to support cross-lingual settings [1,2,19,22,27,31], i.e, where queries and passages are in different languages. This is achieved using multi-lingual PLMs, such as multilingual BERT [6], in place of the English-only PLMs. This approach is particularly important in this setting where traditional bag-of-words methods are ineffective due to the limited number of matching terms across languages. In contrast, cross-lingual DRs (xDRs) are able to encode queries and passages in different languages into a shared embedding space, enabling efficient ANN search across languages. However, such multi-lingual PLM-based xDRs usually are less effective on the cross-lingual passage retrieval task than DRs in the English-only setting [1]. The hypothesis to explain this result is that, in the English-only setting, a DR only needs to learn relevance matching between queries and passages. In contrast, a xDR not only needs to learn the relevance matching task, but also needs to learn how to align the embeddings of texts with similar semantic meaning but in different language [24,41]. It is this language gap that makes cross lingual retrieval a relatively harder task for xDRs. \n\nBased on this hypothesis, this paper proposes the use of crosslingual query generation (xQG) to bridge the language gap for xDRs. Our approach is illustrated in Figure 1.",
                    "score": 0.5388004361975152,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 564
                        },
                        {
                            "start": 567,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1392
                        },
                        {
                            "start": 1393,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1834
                        },
                        {
                            "start": 1837,
                            "end": 1966
                        },
                        {
                            "start": 1967,
                            "end": 2007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 696,
                            "end": 699,
                            "matchedPaperCorpusId": "225040672"
                        },
                        {
                            "start": 699,
                            "end": 701,
                            "matchedPaperCorpusId": "236428949"
                        },
                        {
                            "start": 701,
                            "end": 704,
                            "matchedPaperCorpusId": "245144717"
                        },
                        {
                            "start": 704,
                            "end": 707,
                            "matchedPaperCorpusId": "220871404"
                        },
                        {
                            "start": 707,
                            "end": 710,
                            "matchedPaperCorpusId": "256461230"
                        },
                        {
                            "start": 710,
                            "end": 713,
                            "matchedPaperCorpusId": "250390748"
                        },
                        {
                            "start": 844,
                            "end": 847,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1388,
                            "end": 1391,
                            "matchedPaperCorpusId": "225040672"
                        },
                        {
                            "start": 1732,
                            "end": 1736,
                            "matchedPaperCorpusId": "246210468"
                        },
                        {
                            "start": 1736,
                            "end": 1739,
                            "matchedPaperCorpusId": "248392078"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6689453125
                }
            ],
            "relevance_judgement": 0.6689453125,
            "relevance_judgment_input_expanded": "# Title: Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval\n# Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\n# Authors: Shengyao Zhuang, Linjun Shou, G. Zuccon\n## Abstract\nEffective cross-lingual dense retrieval methods that rely on multilingual pre-trained language models (PLMs) need to be trained to encompass both the relevance matching task and the cross-language alignment task. However, cross-lingual data for training is often scarcely available. In this paper, rather than using more cross-lingual data for training, we propose to use cross-lingual query generation to augment passage representations with queries in languages other than the original passage language. These augmented representations are used at inference time so that the representation can encode more information across the different target languages. Training of a cross-lingual query generator does not require additional training data to that used for the dense retriever. The query generator training is also effective because the pre-training task for the generator (T5 text-to-text training) is very similar to the fine-tuning task (generation of a query). The use of the generator does not increase query latency at inference and can be combined with any cross-lingual dense retrieval method. Results from experiments on a benchmark cross-lingual information retrieval dataset show that our approach can improve the effectiveness of existing cross-lingual dense retrieval methods. Implementation of our methods, along with all generated query files are made publicly available at https://github.com/ielab/xQG4xDR.\n## INTRODUCTION\nPre-trained language model-based (PLM) dense retrievers (DRs) have achieved remarkable success in the task of English-only passage retrieval [12-14, 20, 21, 26, 28, 29, 40, 43, 44]. These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33,45]. \n\nTo leverage the semantic modelling power of DRs, recent research has extended English-only DRs to support cross-lingual settings [1,2,19,22,27,31], i.e, where queries and passages are in different languages. This is achieved using multi-lingual PLMs, such as multilingual BERT [6], in place of the English-only PLMs. This approach is particularly important in this setting where traditional bag-of-words methods are ineffective due to the limited number of matching terms across languages. In contrast, cross-lingual DRs (xDRs) are able to encode queries and passages in different languages into a shared embedding space, enabling efficient ANN search across languages. However, such multi-lingual PLM-based xDRs usually are less effective on the cross-lingual passage retrieval task than DRs in the English-only setting [1]. The hypothesis to explain this result is that, in the English-only setting, a DR only needs to learn relevance matching between queries and passages. In contrast, a xDR not only needs to learn the relevance matching task, but also needs to learn how to align the embeddings of texts with similar semantic meaning but in different language [24,41]. It is this language gap that makes cross lingual retrieval a relatively harder task for xDRs. \n\nBased on this hypothesis, this paper proposes the use of crosslingual query generation (xQG) to bridge the language gap for xDRs. Our approach is illustrated in Figure 1.",
            "reference_string": "[258557604 | Zhuang et al. | 2023 | Citations: 8]"
        },
        {
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267237867",
                    "name": "Georgios Sidiropoulos"
                },
                {
                    "authorId": "2314138767",
                    "name": "Evangelos Kanoulas"
                }
            ],
            "abstract": "Speech-based open-domain question answering (QA over a large corpus of text passages with spoken questions) has emerged as an important task due to the increasing number of users interacting with QA systems via speech interfaces. Passage retrieval is a key task in speech-based open-domain QA. So far, previous works adopted pipelines consisting of an automatic speech recognition (ASR) model that transcribes the spoken question before feeding it to a dense text retriever. Such pipelines have several limitations. The need for an ASR model limits the applicability to low-resource languages and specialized domains with no annotated speech data. Furthermore, the ASR model propagates its errors to the retriever. In this work, we try to alleviate these limitations by proposing an ASR-free, end-to-end trained multimodal dense retriever that can work directly on spoken questions. Our experimental results showed that, on shorter questions, our retriever is a promising alternative to the \\textit{ASR and Retriever} pipeline, achieving better retrieval performance in cases where ASR would have mistranscribed important words in the question or have produced a transcription with a high word error rate.",
            "corpus_id": 272770506,
            "sentences": [
                {
                    "corpus_id": "272770506",
                    "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
                    "text": "To this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages. We followed the original dual-encoder training setting from Karpukhin et al. (2020) where, given a question q, a relevant passage p + and a set of irrelevant passages {p \u2212 1 , p \u2212 2 , . . . , p \u2212 n }, the model is fine-tuned via the minimization of the softmax cross-entropy: \n\ne s(q,p + ) e s(q,p + ) + p \u2212 e s(q,p \u2212 ) . (4) \n\nThe inference phase of our multimodal dense retriever remains the same as in traditional dualencoders for dense text retrieval. Specifically, we compute the similarity of a question-passage pair as the inner product of the respective question embedding and passage embedding. At query time, only the question needs to be encoded. In detail, we build a dense index of passage vectors (offline) by encoding the whole corpus and storing it in an index structure that supports efficient retrieval of the relevant passages via approximate nearest neighbor search (Johnson et al., 2021). At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings. On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers.",
                    "score": 0.6214049450752147,
                    "section_title": "Multimodal Dense Retriever",
                    "char_start_offset": 8050,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 454
                        },
                        {
                            "start": 457,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 504
                        },
                        {
                            "start": 507,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1438
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 239,
                            "end": 262,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1065,
                            "end": 1087,
                            "matchedPaperCorpusId": "926364"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66845703125
                }
            ],
            "relevance_judgement": 0.66845703125,
            "relevance_judgment_input_expanded": "# Title: A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering\n# Venue: arXiv.org\n# Authors: Georgios Sidiropoulos, Evangelos Kanoulas\n## Abstract\nSpeech-based open-domain question answering (QA over a large corpus of text passages with spoken questions) has emerged as an important task due to the increasing number of users interacting with QA systems via speech interfaces. Passage retrieval is a key task in speech-based open-domain QA. So far, previous works adopted pipelines consisting of an automatic speech recognition (ASR) model that transcribes the spoken question before feeding it to a dense text retriever. Such pipelines have several limitations. The need for an ASR model limits the applicability to low-resource languages and specialized domains with no annotated speech data. Furthermore, the ASR model propagates its errors to the retriever. In this work, we try to alleviate these limitations by proposing an ASR-free, end-to-end trained multimodal dense retriever that can work directly on spoken questions. Our experimental results showed that, on shorter questions, our retriever is a promising alternative to the \\textit{ASR and Retriever} pipeline, achieving better retrieval performance in cases where ASR would have mistranscribed important words in the question or have produced a transcription with a high word error rate.\n## Multimodal Dense Retriever\nTo this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages. We followed the original dual-encoder training setting from Karpukhin et al. (2020) where, given a question q, a relevant passage p + and a set of irrelevant passages {p \u2212 1 , p \u2212 2 , . . . , p \u2212 n }, the model is fine-tuned via the minimization of the softmax cross-entropy: \n\ne s(q,p + ) e s(q,p + ) + p \u2212 e s(q,p \u2212 ) . (4) \n\nThe inference phase of our multimodal dense retriever remains the same as in traditional dualencoders for dense text retrieval. Specifically, we compute the similarity of a question-passage pair as the inner product of the respective question embedding and passage embedding. At query time, only the question needs to be encoded. In detail, we build a dense index of passage vectors (offline) by encoding the whole corpus and storing it in an index structure that supports efficient retrieval of the relevant passages via approximate nearest neighbor search (Johnson et al., 2021). At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings. On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers.",
            "reference_string": "[272770506 | Sidiropoulos et al. | 2024 | Citations: 0]"
        },
        {
            "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
            "venue": "Web Search and Data Mining",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635774",
                "status": "HYBRID",
                "license": "other-oa",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.05225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118181226",
                    "name": "Hao-Lun Sun"
                },
                {
                    "authorId": "49544272",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2171182",
                    "name": "Yeyun Gong"
                },
                {
                    "authorId": "3300216",
                    "name": "Anlei Dong"
                },
                {
                    "authorId": "2143968416",
                    "name": "Jian Jiao"
                },
                {
                    "authorId": "2115404379",
                    "name": "Jing Lu"
                },
                {
                    "authorId": "2152822477",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "71790825",
                    "name": "Daxin Jiang"
                },
                {
                    "authorId": "7866194",
                    "name": "Linjun Yang"
                },
                {
                    "authorId": "32431940",
                    "name": "Rangan Majumder"
                },
                {
                    "authorId": "46429989",
                    "name": "Nan Duan"
                }
            ],
            "abstract": "Knowledge distillation is often used to transfer knowledge from a strong teacher model to a relatively weak student model. Traditional methods include response-based methods and feature-based methods. Response-based methods are widely used but suffer from lower upper limits of performance due to their ignorance of intermediate signals, while feature-based methods have constraints on vocabularies, tokenizers and model architectures. In this paper, we propose a liberal feature-based distillation method (LEAD). LEAD aligns the distribution between the intermediate layers of teacher model and student model, which is effective, extendable, portable and has no requirements on vocabularies, tokenizers, or model architectures. Extensive experiments show the effectiveness of LEAD on widely-used benchmarks, including MS MARCO Passage Ranking, TREC 2019 DL Track, MS MARCO Document Ranking and TREC 2020 DL Track. Our code is available in https://github.com/microsoft/SimXNS/tree/main/LEAD.",
            "corpus_id": 254564747,
            "sentences": [
                {
                    "corpus_id": "254564747",
                    "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
                    "text": "(1 \u2264  \u2264  ) to denote the encoding of the first  layers of  1 and  2 , respectively. The two major differences between those models are (1) the instantiations of  1 and  2 and (2) the similarity calculation function  (\u2022). To better elaborate the unified view, we show the following examples. \n\nDual Encoder (DE) [20] is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) [21] can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) [40] has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation.",
                    "score": 0.5330961284346933,
                    "section_title": "2",
                    "char_start_offset": 10356,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 290
                        },
                        {
                            "start": 293,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 655
                        },
                        {
                            "start": 658,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 844
                        },
                        {
                            "start": 847,
                            "end": 909
                        },
                        {
                            "start": 912,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1131
                        },
                        {
                            "start": 1134,
                            "end": 1282
                        },
                        {
                            "start": 1283,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1518
                        },
                        {
                            "start": 1521,
                            "end": 1563
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 311,
                            "end": 315,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 671,
                            "end": 675,
                            "matchedPaperCorpusId": "216553223"
                        },
                        {
                            "start": 1153,
                            "end": 1157,
                            "matchedPaperCorpusId": "231815627"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66748046875
                }
            ],
            "relevance_judgement": 0.66748046875,
            "relevance_judgment_input_expanded": "# Title: LEAD: Liberal Feature-based Distillation for Dense Retrieval\n# Venue: Web Search and Data Mining\n# Authors: Hao-Lun Sun, Xiao Liu, Yeyun Gong, Anlei Dong, Jian Jiao, Jing Lu, Yan Zhang, Daxin Jiang, Linjun Yang, Rangan Majumder, Nan Duan\n## Abstract\nKnowledge distillation is often used to transfer knowledge from a strong teacher model to a relatively weak student model. Traditional methods include response-based methods and feature-based methods. Response-based methods are widely used but suffer from lower upper limits of performance due to their ignorance of intermediate signals, while feature-based methods have constraints on vocabularies, tokenizers and model architectures. In this paper, we propose a liberal feature-based distillation method (LEAD). LEAD aligns the distribution between the intermediate layers of teacher model and student model, which is effective, extendable, portable and has no requirements on vocabularies, tokenizers, or model architectures. Extensive experiments show the effectiveness of LEAD on widely-used benchmarks, including MS MARCO Passage Ranking, TREC 2019 DL Track, MS MARCO Document Ranking and TREC 2020 DL Track. Our code is available in https://github.com/microsoft/SimXNS/tree/main/LEAD.\n## 2\n(1 \u2264  \u2264  ) to denote the encoding of the first  layers of  1 and  2 , respectively. The two major differences between those models are (1) the instantiations of  1 and  2 and (2) the similarity calculation function  (\u2022). To better elaborate the unified view, we show the following examples. \n\nDual Encoder (DE) [20] is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) [21] can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) [40] has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation.",
            "reference_string": "[254564747 | Sun et al. | 2022 | Citations: 2]"
        },
        {
            "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval",
            "venue": "Findings",
            "year": 2021,
            "reference_count": 36,
            "citation_count": 94,
            "influential_citation_count": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-acl.191.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.06027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1708171825",
                    "name": "Ruiyang Ren"
                },
                {
                    "authorId": "28128416",
                    "name": "Shangwen Lv"
                },
                {
                    "authorId": "51281403",
                    "name": "Yingqi Qu"
                },
                {
                    "authorId": "46700619",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "40430110",
                    "name": "Qiaoqiao She"
                },
                {
                    "authorId": "40354707",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "144270731",
                    "name": "Haifeng Wang"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ],
            "abstract": "Recently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets.",
            "corpus_id": 236477844,
            "sentences": [
                {
                    "corpus_id": "236477844",
                    "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval",
                    "text": "Recently, dense passage retrieval has demonstrated better performance than traditional sparse retrieval methods (e.g., TF-IDF and BM25). Different from sparse retrieval, dense passage retrieval represents queries and passages into lowdimensional vectors (Guu et al., 2020;Karpukhin et al., 2020), typically in a dual-encoder architecture, and uses dot product as the similarity measurement for retrieval. The existing approaches for dense passage retrieval can be divided into two categories: (1) unsupervised pre-training for retrieval (2) fine-tuning only on labeled data. In the first category, different pre-training tasks for retrieval were proposed.  proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly finetuned the retriever and a reader on labeled data. REALM (Guu et al., 2020) proposed a new pretraining approach, which jointly trained a masked language model and a neural retriever. Different from them, our proposed approach utilizes the pseudo-labeled data via knowledge distillation in the pre-training stage, and the quality of the generated data is high (see Section 4.6).\n\nIn the second category, the existing approaches fine-tuned pre-trained language models on labeled data (Karpukhin et al., 2020;Luan et al., 2021). Both DPR (Karpukhin et al., 2020) and ME-BERT (Luan et al., 2021) used in-batch random sampling and hard negative sampling by BM25, while ANCE (Xiong et al., 2020a), NPRINC (Lu et al., 2020) and RocketQA (Qu et al., 2020) explored more sophisticated hard negative sampling approach. Izacard and Grave (2020) and Yang et al. (2020) leveraged a reader and a crossencoder for knowledge distillation on labeled data, respectively. RocketQA found large batch size can significantly improve the retrieval performance of dual-encoders. ColBERT (Khattab and Zaharia, 2020) incorporated light-weight attention-based re-ranking while increasing the space complexity.\n\nThe existing studies mainly focus on",
                    "score": 0.578148247728193,
                    "section_title": "Related Work",
                    "char_start_offset": 5531,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 272,
                            "end": 295,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1278,
                            "end": 1302,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1331,
                            "end": 1355,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 1634,
                            "end": 1652,
                            "matchedPaperCorpusId": "165163737"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6630859375
                }
            ],
            "relevance_judgement": 0.6630859375,
            "relevance_judgment_input_expanded": "# Title: PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval\n# Venue: Findings\n# Authors: Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, Ji-rong Wen\n## Abstract\nRecently, dense passage retrieval has become a mainstream approach to finding relevant information in various natural language processing tasks. A number of studies have been devoted to improving the widely adopted dual-encoder architecture. However, most of the previous studies only consider query-centric similarity relation when learning the dual-encoder retriever. In order to capture more comprehensive similarity relations, we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval. To implement our approach, we make three major technical contributions by introducing formal formulations of the two kinds of similarity relations, generating high-quality pseudo labeled data via knowledge distillation, and designing an effective two-stage training procedure that incorporates passage-centric similarity relation constraint. Extensive experiments show that our approach significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions datasets.\n## Related Work\nRecently, dense passage retrieval has demonstrated better performance than traditional sparse retrieval methods (e.g., TF-IDF and BM25). Different from sparse retrieval, dense passage retrieval represents queries and passages into lowdimensional vectors (Guu et al., 2020;Karpukhin et al., 2020), typically in a dual-encoder architecture, and uses dot product as the similarity measurement for retrieval. The existing approaches for dense passage retrieval can be divided into two categories: (1) unsupervised pre-training for retrieval (2) fine-tuning only on labeled data. In the first category, different pre-training tasks for retrieval were proposed.  proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly finetuned the retriever and a reader on labeled data. REALM (Guu et al., 2020) proposed a new pretraining approach, which jointly trained a masked language model and a neural retriever. Different from them, our proposed approach utilizes the pseudo-labeled data via knowledge distillation in the pre-training stage, and the quality of the generated data is high (see Section 4.6).\n\nIn the second category, the existing approaches fine-tuned pre-trained language models on labeled data (Karpukhin et al., 2020;Luan et al., 2021). Both DPR (Karpukhin et al., 2020) and ME-BERT (Luan et al., 2021) used in-batch random sampling and hard negative sampling by BM25, while ANCE (Xiong et al., 2020a), NPRINC (Lu et al., 2020) and RocketQA (Qu et al., 2020) explored more sophisticated hard negative sampling approach. Izacard and Grave (2020) and Yang et al. (2020) leveraged a reader and a crossencoder for knowledge distillation on labeled data, respectively. RocketQA found large batch size can significantly improve the retrieval performance of dual-encoders. ColBERT (Khattab and Zaharia, 2020) incorporated light-weight attention-based re-ranking while increasing the space complexity.\n\nThe existing studies mainly focus on",
            "reference_string": "[236477844 | Ren et al. | 2021 | Citations: 94]"
        },
        {
            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2335861500",
                    "name": "Quang Hoang Trung"
                },
                {
                    "authorId": "2335861321",
                    "name": "Nguyen Van Hoang Phuc"
                },
                {
                    "authorId": "2335859619",
                    "name": "Le Trung Hoang"
                },
                {
                    "authorId": "2281214899",
                    "name": "Quang Huu Hieu"
                },
                {
                    "authorId": "113438599",
                    "name": "Vo Nguyen Le Duy"
                }
            ],
            "abstract": "Text Retrieval (TR) involves finding and retrieving text-based content relevant to a user's query from a large repository, with applications in real-world scenarios such as legal document retrieval. While most existing studies focus on English, limited work addresses Japanese contexts. In this paper, we introduce a new dataset specifically designed for Japanese legal contexts and propose a novel two-phase pipeline tailored to this domain. In the first phase, the model learns a broad understanding of global contexts, enhancing its generalization and adaptability to diverse queries. In the second phase, the model is fine-tuned to address complex queries specific to legal scenarios. Extensive experiments are conducted to demonstrate the superior performance of our method, which outperforms existing baselines. Furthermore, our pipeline proves effective in English contexts, surpassing comparable baselines on the MS MARCO dataset. We have made our code publicly available on GitHub, and the model checkpoints are accessible via HuggingFace.",
            "corpus_id": 274822530,
            "sentences": [
                {
                    "corpus_id": "274822530",
                    "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
                    "text": "The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023;Fu et al., 2023). When integrating LLMs, the model can leverage the EOS token embedding, which acts as a contextual representation of the entire sentence. This token's embedding encapsulates the semantic meaning of the full query or document, allowing for a richer, more accurate comparison of inputs within the shared vector space. The use of LLMs in dual-encoder architectures opens up new possibilities for improving the model's ability to capture nuanced semantic relationships. \n\nGenerative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2024), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search. Generative models learn to map queries to their relevant documents by generating unique docids or specific document content.",
                    "score": 0.537577452412439,
                    "section_title": "Dual-encoder approaches",
                    "char_start_offset": 15799,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1133
                        },
                        {
                            "start": 1136,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 2022
                        },
                        {
                            "start": 2023,
                            "end": 2147
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 630,
                            "end": 651,
                            "matchedPaperCorpusId": "257767171"
                        },
                        {
                            "start": 651,
                            "end": 667,
                            "matchedPaperCorpusId": "257970928"
                        },
                        {
                            "start": 1326,
                            "end": 1344,
                            "matchedPaperCorpusId": "258048596"
                        },
                        {
                            "start": 1350,
                            "end": 1368,
                            "matchedPaperCorpusId": "246863488"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65087890625
                }
            ],
            "relevance_judgement": 0.65087890625,
            "relevance_judgment_input_expanded": "# Title: Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval\n# Venue: arXiv.org\n# Authors: Quang Hoang Trung, Nguyen Van Hoang Phuc, Le Trung Hoang, Quang Huu Hieu, Vo Nguyen Le Duy\n## Abstract\nText Retrieval (TR) involves finding and retrieving text-based content relevant to a user's query from a large repository, with applications in real-world scenarios such as legal document retrieval. While most existing studies focus on English, limited work addresses Japanese contexts. In this paper, we introduce a new dataset specifically designed for Japanese legal contexts and propose a novel two-phase pipeline tailored to this domain. In the first phase, the model learns a broad understanding of global contexts, enhancing its generalization and adaptability to diverse queries. In the second phase, the model is fine-tuned to address complex queries specific to legal scenarios. Extensive experiments are conducted to demonstrate the superior performance of our method, which outperforms existing baselines. Furthermore, our pipeline proves effective in English contexts, surpassing comparable baselines on the MS MARCO dataset. We have made our code publicly available on GitHub, and the model checkpoints are accessible via HuggingFace.\n## Dual-encoder approaches\nThe dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023;Fu et al., 2023). When integrating LLMs, the model can leverage the EOS token embedding, which acts as a contextual representation of the entire sentence. This token's embedding encapsulates the semantic meaning of the full query or document, allowing for a richer, more accurate comparison of inputs within the shared vector space. The use of LLMs in dual-encoder architectures opens up new possibilities for improving the model's ability to capture nuanced semantic relationships. \n\nGenerative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2024), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search. Generative models learn to map queries to their relevant documents by generating unique docids or specific document content.",
            "reference_string": "[274822530 | Trung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14499",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14499, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "7353832",
                    "name": "Livio Baldini Soares"
                },
                {
                    "authorId": "2396669",
                    "name": "D. Gillick"
                },
                {
                    "authorId": "30859623",
                    "name": "Jeremy R. Cole"
                },
                {
                    "authorId": "15652489",
                    "name": "T. Kwiatkowski"
                }
            ],
            "abstract": "Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries.",
            "corpus_id": 258865354,
            "sentences": [
                {
                    "corpus_id": "258865354",
                    "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
                    "text": "Naive approaches are either computationally infeasible (scoring all possible queries), or rely on sampling a small, incomplete set of samples (such as in Lewis et al. [22]). \n\nTo overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14,26]. We call the retrieval system based on this proposed model nail (Non-Autoregressive Indexing with Language models). \n\nWe summarize our contributions as follows: \n\n(1) We advance prior work on learned sparse retrieval by leveraging pretrained encoder-decoder LMs with a novel nonautoregressive decoder. (2) We describe a range of experiments using the BEIR benchmark [39] that explore the performance and efficiency of our model as a reranker and as a retriever compared with a variety of existing systems. As a reranker, nail can recover 86% of the performance of a large cross-attention reranker [27], while requiring 10 \u22126 % of the inference-time FLOPS per query. As a retriever, nail has an extremely high upper bound for recall-exceeding the performance of all other retrievers in the zero-shot setting. Finally, by using BM25 as a retriever and nail as a reranker, we can match state-of-the-art dual-encoders [14,26] with 10 \u22124 % of the inference-time FLOPS. (3) We propose our model as a preferred solution when significant compute is available at indexing time, but not ondemand at serving time, and we provide a cost analysis that illustrates when our approach could be preferred to previous work that harnesses LLMs.",
                    "score": 0.5653598505408166,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2004,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 173
                        },
                        {
                            "start": 176,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 750
                        },
                        {
                            "start": 753,
                            "end": 795
                        },
                        {
                            "start": 798,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1860
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 167,
                            "end": 171,
                            "matchedPaperCorpusId": "231924957"
                        },
                        {
                            "start": 1001,
                            "end": 1005,
                            "matchedPaperCorpusId": "233296016"
                        },
                        {
                            "start": 1232,
                            "end": 1236,
                            "matchedPaperCorpusId": "212725651"
                        },
                        {
                            "start": 1599,
                            "end": 1602,
                            "matchedPaperCorpusId": "208229926"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64794921875
                }
            ],
            "relevance_judgement": 0.64794921875,
            "relevance_judgment_input_expanded": "# Title: NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Livio Baldini Soares, D. Gillick, Jeremy R. Cole, T. Kwiatkowski\n## Abstract\nNeural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries.\n## INTRODUCTION\nNaive approaches are either computationally infeasible (scoring all possible queries), or rely on sampling a small, incomplete set of samples (such as in Lewis et al. [22]). \n\nTo overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14,26]. We call the retrieval system based on this proposed model nail (Non-Autoregressive Indexing with Language models). \n\nWe summarize our contributions as follows: \n\n(1) We advance prior work on learned sparse retrieval by leveraging pretrained encoder-decoder LMs with a novel nonautoregressive decoder. (2) We describe a range of experiments using the BEIR benchmark [39] that explore the performance and efficiency of our model as a reranker and as a retriever compared with a variety of existing systems. As a reranker, nail can recover 86% of the performance of a large cross-attention reranker [27], while requiring 10 \u22126 % of the inference-time FLOPS per query. As a retriever, nail has an extremely high upper bound for recall-exceeding the performance of all other retrievers in the zero-shot setting. Finally, by using BM25 as a retriever and nail as a reranker, we can match state-of-the-art dual-encoders [14,26] with 10 \u22124 % of the inference-time FLOPS. (3) We propose our model as a preferred solution when significant compute is available at indexing time, but not ondemand at serving time, and we provide a cost analysis that illustrates when our approach could be preferred to previous work that harnesses LLMs.",
            "reference_string": "[258865354 | Soares et al. | 2023 | Citations: 1]"
        },
        {
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 71,
            "citation_count": 37,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2212.10380",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "73775461",
                    "name": "Ori Ram"
                },
                {
                    "authorId": "2096122406",
                    "name": "L. Bezalel"
                },
                {
                    "authorId": "2197529001",
                    "name": "Adi Zicher"
                },
                {
                    "authorId": "2083259",
                    "name": "Yonatan Belinkov"
                },
                {
                    "authorId": "1750652",
                    "name": "Jonathan Berant"
                },
                {
                    "authorId": "1786843",
                    "name": "A. Globerson"
                }
            ],
            "abstract": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model\u2019s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.",
            "corpus_id": 254877181,
            "sentences": [
                {
                    "corpus_id": "254877181",
                    "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
                    "text": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model\u2019s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.",
                    "score": 0.5407391912113412,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.646484375
                }
            ],
            "relevance_judgement": 0.646484375,
            "relevance_judgment_input_expanded": "# Title: What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ori Ram, L. Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, A. Globerson\n## Abstract\nDual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model\u2019s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.\n",
            "reference_string": "[254877181 | Ram et al. | 2022 | Citations: 37]"
        },
        {
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "venue": "International Conference on Information and Knowledge Management",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3583780.3614923",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.02371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203368550",
                    "name": "Qian Dong"
                },
                {
                    "authorId": "2108021633",
                    "name": "Yiding Liu"
                },
                {
                    "authorId": "144922928",
                    "name": "Qingyao Ai"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "1783406",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "8093158",
                    "name": "Shaoping Ma"
                }
            ],
            "abstract": "Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.",
            "corpus_id": 259076101,
            "sentences": [
                {
                    "corpus_id": "259076101",
                    "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
                    "text": "The interactor outputs query-aware passage vectors for each passage, which can be pre-computed and cached before deploying the model for online inference. The final query-passage relevance scores can be computed with simple dot-product operation, which gives our model the same high efficiency and low storage cost as dual-encoders. The superior balance between effectiveness and efficiency makes our model more attractive in real-world applications. We summarize our main contributions as follows: \n\n\u2022 We propose a novel PLM-based retrieval model, namely I 3 retriever, which incorporates implicit interaction in dual-encoders. \u2022 We introduce two modules in I 3 retriever that are jointly trained with query and passage encoders in an end-to-end manner, i.e., query reconstructor and query-passage interactor. The query reconstructor is able to generate pseudo-queries for the querypassage interactor, which subsequently encodes query-aware information in the final passage vectors. \u2022 We conduct comprehensive evaluation on large scale datasets. \n\nThe results show that I 3 is able to achieve superior performance w.r.t both effectiveness and efficiency for passage retrieval. We also conduct a thorough study to clarify the effects of implicit interaction.",
                    "score": 0.49935858834694447,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 4174,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 498
                        },
                        {
                            "start": 501,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1046
                        },
                        {
                            "start": 1049,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1258
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6416015625
                },
                {
                    "corpus_id": "259076101",
                    "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
                    "text": "Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.",
                    "score": 0.579278477654033,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63916015625
                }
            ],
            "relevance_judgement": 0.6416015625,
            "relevance_judgment_input_expanded": "# Title: I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval\n# Venue: International Conference on Information and Knowledge Management\n# Authors: Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, Shaoping Ma\n## Abstract\nPassage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.\n## INTRODUCTION\nThe interactor outputs query-aware passage vectors for each passage, which can be pre-computed and cached before deploying the model for online inference. The final query-passage relevance scores can be computed with simple dot-product operation, which gives our model the same high efficiency and low storage cost as dual-encoders. The superior balance between effectiveness and efficiency makes our model more attractive in real-world applications. We summarize our main contributions as follows: \n\n\u2022 We propose a novel PLM-based retrieval model, namely I 3 retriever, which incorporates implicit interaction in dual-encoders. \u2022 We introduce two modules in I 3 retriever that are jointly trained with query and passage encoders in an end-to-end manner, i.e., query reconstructor and query-passage interactor. The query reconstructor is able to generate pseudo-queries for the querypassage interactor, which subsequently encodes query-aware information in the final passage vectors. \u2022 We conduct comprehensive evaluation on large scale datasets. \n\nThe results show that I 3 is able to achieve superior performance w.r.t both effectiveness and efficiency for passage retrieval. We also conduct a thorough study to clarify the effects of implicit interaction.",
            "reference_string": "[259076101 | Dong et al. | 2023 | Citations: 12]"
        },
        {
            "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 55,
            "citation_count": 617,
            "influential_citation_count": 108,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.466.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.08191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51281403",
                    "name": "Yingqi Qu"
                },
                {
                    "authorId": "2111070044",
                    "name": "Yuchen Ding"
                },
                {
                    "authorId": "46700619",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2146384872",
                    "name": "Kai Liu"
                },
                {
                    "authorId": "1708171825",
                    "name": "Ruiyang Ren"
                },
                {
                    "authorId": "2145734278",
                    "name": "Xin Zhao"
                },
                {
                    "authorId": "9532787",
                    "name": "Daxiang Dong"
                },
                {
                    "authorId": "40354707",
                    "name": "Hua Wu"
                },
                {
                    "authorId": "144270731",
                    "name": "Haifeng Wang"
                }
            ],
            "abstract": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
            "corpus_id": 231815627,
            "sentences": [
                {
                    "corpus_id": "231815627",
                    "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
                    "text": "For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever. Nogueira et al. (2019c) first enriched document content by generating related queries and appending them to the documents, and then indexed the expanded documents for retrieval, which alleviate the term mismatch between questions and passages. Mao et al. (2020) used generation model to expand questions. DeepCT (Dai and Callan, 2019) utilized BERT to learn the term importance (i.e. term weighting) by considering the contexts, which was shown to achieve better retrieval performance than BM25. Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a). Existing approaches can be roughly divided into two categories: (1) pre-training and (2) fine-tuning only.  proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly fine-tuning the retriever and the reader on labeled data. Guu et al. (2020) proposed a new pre-training approach namely REALM, which jointly trained a masked language model and a neural retriever. In contrast, the second class of approaches only finetuned (existing) pre-trained language models (LMs) on labeled data. Our work follows the second class of approaches, which show better performance with less cost.\n\nAlthough the dual-encoder architecture enables the appealing paradigm of dense retrieval, it is difficult to effectively train such a retrieval architecture. As discussed in Section 1, it suffers from a number of challenges, including the training and inference discrepancy, a large number of unlabeled positives and limited training data. Several recent studies Luan et al., 2020;Henderson et al., 2017) tried to address the first challenge by designing complicated sampling mechanism to generate",
                    "score": 0.5621662779525336,
                    "section_title": "Passage Retrieval for Open-domain QA",
                    "char_start_offset": 6095,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 140,
                            "end": 159,
                            "matchedPaperCorpusId": "3618568"
                        },
                        {
                            "start": 677,
                            "end": 699,
                            "matchedPaperCorpusId": "162168864"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64111328125
                }
            ],
            "relevance_judgement": 0.64111328125,
            "relevance_judgment_input_expanded": "# Title: RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Xin Zhao, Daxiang Dong, Hua Wu, Haifeng Wang\n## Abstract\nIn open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.\n## Passage Retrieval for Open-domain QA\nFor open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever. Nogueira et al. (2019c) first enriched document content by generating related queries and appending them to the documents, and then indexed the expanded documents for retrieval, which alleviate the term mismatch between questions and passages. Mao et al. (2020) used generation model to expand questions. DeepCT (Dai and Callan, 2019) utilized BERT to learn the term importance (i.e. term weighting) by considering the contexts, which was shown to achieve better retrieval performance than BM25. Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a). Existing approaches can be roughly divided into two categories: (1) pre-training and (2) fine-tuning only.  proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly fine-tuning the retriever and the reader on labeled data. Guu et al. (2020) proposed a new pre-training approach namely REALM, which jointly trained a masked language model and a neural retriever. In contrast, the second class of approaches only finetuned (existing) pre-trained language models (LMs) on labeled data. Our work follows the second class of approaches, which show better performance with less cost.\n\nAlthough the dual-encoder architecture enables the appealing paradigm of dense retrieval, it is difficult to effectively train such a retrieval architecture. As discussed in Section 1, it suffers from a number of challenges, including the training and inference discrepancy, a large number of unlabeled positives and limited training data. Several recent studies Luan et al., 2020;Henderson et al., 2017) tried to address the first challenge by designing complicated sampling mechanism to generate",
            "reference_string": "[231815627 | Qu et al. | 2020 | Citations: 617]"
        },
        {
            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
            "venue": "SUSTAINLP",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.11550",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.11550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115829412",
                    "name": "Yuxuan Wang"
                },
                {
                    "authorId": "2220304036",
                    "name": "Hong Lyu"
                }
            ],
            "abstract": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings. However, many proposed procedures are often too complex or resource-intensive, which makes it difficult for practitioners to adopt them or identify sources of empirical gains. Therefore, in this work, we propose a trivially simple recipe to serve as a baseline method for boosting the efficiency of DE retrievers leveraging an asymmetric architecture. Our results demonstrate that even a 2-layer, BERT-based query encoder can still retain 92.5% of the full DE performance on the BEIR benchmark via unsupervised distillation and proper student initialization. We hope that our findings will encourage the community to re-evaluate the trade-offs between method complexity and performance improvements.",
            "corpus_id": 259203703,
            "sentences": [
                {
                    "corpus_id": "259203703",
                    "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
                    "text": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings. However, many proposed procedures are often too complex or resource-intensive, which makes it difficult for practitioners to adopt them or identify sources of empirical gains. Therefore, in this work, we propose a trivially simple recipe to serve as a baseline method for boosting the efficiency of DE retrievers leveraging an asymmetric architecture. Our results demonstrate that even a 2-layer, BERT-based query encoder can still retain 92.5% of the full DE performance on the BEIR benchmark via unsupervised distillation and proper student initialization. We hope that our findings will encourage the community to re-evaluate the trade-offs between method complexity and performance improvements.",
                    "score": 0.5512915186969122,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.640625
                }
            ],
            "relevance_judgement": 0.640625,
            "relevance_judgment_input_expanded": "# Title: Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency\n# Venue: SUSTAINLP\n# Authors: Yuxuan Wang, Hong Lyu\n## Abstract\nThe information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings. However, many proposed procedures are often too complex or resource-intensive, which makes it difficult for practitioners to adopt them or identify sources of empirical gains. Therefore, in this work, we propose a trivially simple recipe to serve as a baseline method for boosting the efficiency of DE retrievers leveraging an asymmetric architecture. Our results demonstrate that even a 2-layer, BERT-based query encoder can still retain 92.5% of the full DE performance on the BEIR benchmark via unsupervised distillation and proper student initialization. We hope that our findings will encourage the community to re-evaluate the trade-offs between method complexity and performance improvements.\n",
            "reference_string": "[259203703 | Wang et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder",
            "venue": "",
            "year": 2021,
            "reference_count": 34,
            "citation_count": 11,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.09206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1830381674",
                    "name": "Shuqi Lu"
                },
                {
                    "authorId": "1391126980",
                    "name": "Di He"
                },
                {
                    "authorId": "144628574",
                    "name": "Chenyan Xiong"
                },
                {
                    "authorId": "35286545",
                    "name": "Guolin Ke"
                },
                {
                    "authorId": "2060300532",
                    "name": "Waleed Malik"
                },
                {
                    "authorId": "47405877",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "144609235",
                    "name": "Paul N. Bennett"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                },
                {
                    "authorId": "2734525",
                    "name": "Arnold Overwijk"
                }
            ],
            "abstract": "Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a \\textit{weak} decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/.",
            "corpus_id": 237485691,
            "sentences": [
                {
                    "corpus_id": "237485691",
                    "title": "Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder",
                    "text": "Recently, Dense Retrieval (DR) has progressed to more important roles in many language systems, for example, web search (Xiong et al., 2021), question answering (Karpukhin et al., 2020), and news recommendation (Wu et al., 2020b). In the firststage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020). \n\nA popular choice of text encoders in DR is the Transformer network pre-trained by language modeling (e.g., BERT) (Reimers and Gurevych, 2019a). It is unexpected that, unlike in other language tasks where pre-trained models simply excel, directly fine-tuning BERT in DR often underperforms unsupervised sparse retrieval, e.g., BM25. Some complicated procedures are almost necessary to effectively fine-tune pre-trained Transformers in dense retrieval (Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2021). One observation is that the pre-trained language models are not effective at encoding the semantics of the entire text sequence in one embedding, especially in dense retrieval where text sequences are mostly longer than 128 tokens (Luan et al., 2021). \n\nIn some other modalities, autoencoders have been widely used to obtain high-quality data representations (Vincent et al., 2010;Kingma and Welling, 2013). They pair a decoder on top of the encoder, trains the decoder to reconstruct the data solely from the encoder's encodings, thus enforce an information bottleneck on the data encodings for better representation quality. Recently, autoencoders have been brought in language pre-training.",
                    "score": 0.6195289555133642,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 686
                        },
                        {
                            "start": 689,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1453
                        },
                        {
                            "start": 1456,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1895
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 140,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 211,
                            "end": 229,
                            "matchedPaperCorpusId": "220046458"
                        },
                        {
                            "start": 553,
                            "end": 571,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 668,
                            "end": 685,
                            "matchedPaperCorpusId": "218614141"
                        },
                        {
                            "start": 1163,
                            "end": 1181,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 1181,
                            "end": 1200,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1433,
                            "end": 1452,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 1561,
                            "end": 1583,
                            "matchedPaperCorpusId": "17804904"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63232421875
                }
            ],
            "relevance_judgement": 0.63232421875,
            "relevance_judgment_input_expanded": "# Title: Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder\n# Venue: \n# Authors: Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul N. Bennett, Tie-Yan Liu, Arnold Overwijk\n## Abstract\nDense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a \\textit{weak} decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/.\n## Introduction\nRecently, Dense Retrieval (DR) has progressed to more important roles in many language systems, for example, web search (Xiong et al., 2021), question answering (Karpukhin et al., 2020), and news recommendation (Wu et al., 2020b). In the firststage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020). \n\nA popular choice of text encoders in DR is the Transformer network pre-trained by language modeling (e.g., BERT) (Reimers and Gurevych, 2019a). It is unexpected that, unlike in other language tasks where pre-trained models simply excel, directly fine-tuning BERT in DR often underperforms unsupervised sparse retrieval, e.g., BM25. Some complicated procedures are almost necessary to effectively fine-tune pre-trained Transformers in dense retrieval (Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2021). One observation is that the pre-trained language models are not effective at encoding the semantics of the entire text sequence in one embedding, especially in dense retrieval where text sequences are mostly longer than 128 tokens (Luan et al., 2021). \n\nIn some other modalities, autoencoders have been widely used to obtain high-quality data representations (Vincent et al., 2010;Kingma and Welling, 2013). They pair a decoder on top of the encoder, trains the decoder to reconstruct the data solely from the encoder's encodings, thus enforce an information bottleneck on the data encodings for better representation quality. Recently, autoencoders have been brought in language pre-training.",
            "reference_string": "[237485691 | Lu et al. | 2021 | Citations: 11]"
        },
        {
            "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 45,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.17039, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291199148",
                    "name": "Minghan Li"
                },
                {
                    "authorId": "2288256282",
                    "name": "\u00c9. Gaussier"
                },
                {
                    "authorId": "2331371334",
                    "name": "Guodong Zhou"
                }
            ],
            "abstract": "In recent years, large language models (LLMs) have demonstrated exceptional power in various domains, including information retrieval. Most of the previous practices involve leveraging these models to create a single embedding for each query, each passage, or each document individually, a strategy exemplified and used by the Retrieval-Augmented Generation (RAG) framework. While this method has proven effective, we argue that it falls short in fully capturing the nuanced intricacies of document-level texts due to its reliance on a relatively coarse-grained representation. To address this limitation, we introduce a novel, fine-grained approach aimed at enhancing the accuracy of relevance scoring for long documents. Our methodology firstly segments a long document into blocks, each of which is embedded using an LLM, for matching with the query representation. When calculating the relevance score, we aggregate the query-block relevance scores through a weighted sum method, yielding a comprehensive score for the query with the entire document. Despite its apparent simplicity, our experimental findings reveal that this approach outperforms standard representation methods and achieves a significant reduction in embedding generation latency. Moreover, by carefully optimizing pairwise loss functions, superior performances have been achieved.",
            "corpus_id": 275931943,
            "sentences": [
                {
                    "corpus_id": "275931943",
                    "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models",
                    "text": "Karpukhin et al. [17] show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder. Later works in this direction mainly focused on optimizing the training process, e.g. through mining hard negatives [19], or on the domain shift issue [20,10]. \n\nIn the late-interaction based approach, ColBERT [21] is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs. \n\nThough the models based on pretrained language models show effectiveness in passage retrieval tasks, they face limitations with handling long documents.",
                    "score": 0.5535905228371338,
                    "section_title": "Representation-based Models for IR",
                    "char_start_offset": 4206,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 642
                        },
                        {
                            "start": 645,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1111
                        },
                        {
                            "start": 1112,
                            "end": 1293
                        },
                        {
                            "start": 1296,
                            "end": 1448
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 17,
                            "end": 21,
                            "matchedPaperCorpusId": "215737187"
                        },
                        {
                            "start": 634,
                            "end": 638,
                            "matchedPaperCorpusId": "245131402"
                        },
                        {
                            "start": 638,
                            "end": 641,
                            "matchedPaperCorpusId": "268384724"
                        },
                        {
                            "start": 693,
                            "end": 697,
                            "matchedPaperCorpusId": "216553223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6279296875
                }
            ],
            "relevance_judgement": 0.6279296875,
            "relevance_judgment_input_expanded": "# Title: Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models\n# Venue: arXiv.org\n# Authors: Minghan Li, \u00c9. Gaussier, Guodong Zhou\n## Abstract\nIn recent years, large language models (LLMs) have demonstrated exceptional power in various domains, including information retrieval. Most of the previous practices involve leveraging these models to create a single embedding for each query, each passage, or each document individually, a strategy exemplified and used by the Retrieval-Augmented Generation (RAG) framework. While this method has proven effective, we argue that it falls short in fully capturing the nuanced intricacies of document-level texts due to its reliance on a relatively coarse-grained representation. To address this limitation, we introduce a novel, fine-grained approach aimed at enhancing the accuracy of relevance scoring for long documents. Our methodology firstly segments a long document into blocks, each of which is embedded using an LLM, for matching with the query representation. When calculating the relevance score, we aggregate the query-block relevance scores through a weighted sum method, yielding a comprehensive score for the query with the entire document. Despite its apparent simplicity, our experimental findings reveal that this approach outperforms standard representation methods and achieves a significant reduction in embedding generation latency. Moreover, by carefully optimizing pairwise loss functions, superior performances have been achieved.\n## Representation-based Models for IR\nKarpukhin et al. [17] show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder. Later works in this direction mainly focused on optimizing the training process, e.g. through mining hard negatives [19], or on the domain shift issue [20,10]. \n\nIn the late-interaction based approach, ColBERT [21] is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs. \n\nThough the models based on pretrained language models show effectiveness in passage retrieval tasks, they face limitations with handling long documents.",
            "reference_string": "[275931943 | Li et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 34,
            "citation_count": 74,
            "influential_citation_count": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.220.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.emnlp-main.220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1830381674",
                    "name": "Shuqi Lu"
                },
                {
                    "authorId": "1391126980",
                    "name": "Di He"
                },
                {
                    "authorId": "2139787803",
                    "name": "Chenyan Xiong"
                },
                {
                    "authorId": "35286545",
                    "name": "Guolin Ke"
                },
                {
                    "authorId": "2060300532",
                    "name": "Waleed Malik"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "144609235",
                    "name": "Paul N. Bennett"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                },
                {
                    "authorId": "2734525",
                    "name": "Arnold Overwijk"
                }
            ],
            "abstract": "Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/.",
            "corpus_id": 243865399,
            "sentences": [
                {
                    "corpus_id": "243865399",
                    "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
                    "text": "Recently, Dense Retrieval (DR) has progressed to more important roles in many language systems, for example, web search (Xiong et al., 2021), question answering (Karpukhin et al., 2020), and news recommendation (Wu et al., 2020b). In the firststage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020). \n\nA popular choice of text encoders in DR is the Transformer network pre-trained by language modeling (e.g., BERT) (Reimers and Gurevych, 2019a). It is unexpected that, unlike in other language tasks where pre-trained models simply excel, directly fine-tuning BERT in DR often underperforms unsupervised sparse retrieval, e.g., BM25. Some complicated procedures are almost necessary to effectively fine-tune pre-trained Transformers in dense retrieval (Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2021). One observation is that the pre-trained language models are not effective at encoding the semantics of the entire text sequence in one embedding, especially in dense retrieval where text sequences are mostly longer than 128 tokens (Luan et al., 2021). \n\nIn some other modalities, autoencoders have been widely used to obtain high-quality data representations (Vincent et al., 2010;Kingma and Welling, 2013). They pair a decoder on top of the encoder, trains the decoder to reconstruct the data solely from the encoder's encodings, thus enforce an information bottleneck on the data encodings for better representation quality. Recently, autoencoders have been brought in language pre-training.",
                    "score": 0.6195628144023195,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 686
                        },
                        {
                            "start": 689,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1453
                        },
                        {
                            "start": 1456,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1895
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 140,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 211,
                            "end": 229,
                            "matchedPaperCorpusId": "220046458"
                        },
                        {
                            "start": 553,
                            "end": 571,
                            "matchedPaperCorpusId": "173990818"
                        },
                        {
                            "start": 668,
                            "end": 685,
                            "matchedPaperCorpusId": "218614141"
                        },
                        {
                            "start": 1163,
                            "end": 1181,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 1181,
                            "end": 1200,
                            "matchedPaperCorpusId": "220302524"
                        },
                        {
                            "start": 1433,
                            "end": 1452,
                            "matchedPaperCorpusId": "218470027"
                        },
                        {
                            "start": 1561,
                            "end": 1583,
                            "matchedPaperCorpusId": "17804904"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6240234375
                }
            ],
            "relevance_judgement": 0.6240234375,
            "relevance_judgment_input_expanded": "# Title: Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul N. Bennett, Tie-Yan Liu, Arnold Overwijk\n## Abstract\nDense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a weak decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/.\n## Introduction\nRecently, Dense Retrieval (DR) has progressed to more important roles in many language systems, for example, web search (Xiong et al., 2021), question answering (Karpukhin et al., 2020), and news recommendation (Wu et al., 2020b). In the firststage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020). \n\nA popular choice of text encoders in DR is the Transformer network pre-trained by language modeling (e.g., BERT) (Reimers and Gurevych, 2019a). It is unexpected that, unlike in other language tasks where pre-trained models simply excel, directly fine-tuning BERT in DR often underperforms unsupervised sparse retrieval, e.g., BM25. Some complicated procedures are almost necessary to effectively fine-tune pre-trained Transformers in dense retrieval (Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2021). One observation is that the pre-trained language models are not effective at encoding the semantics of the entire text sequence in one embedding, especially in dense retrieval where text sequences are mostly longer than 128 tokens (Luan et al., 2021). \n\nIn some other modalities, autoencoders have been widely used to obtain high-quality data representations (Vincent et al., 2010;Kingma and Welling, 2013). They pair a decoder on top of the encoder, trains the decoder to reconstruct the data solely from the encoder's encodings, thus enforce an information bottleneck on the data encodings for better representation quality. Recently, autoencoders have been brought in language pre-training.",
            "reference_string": "[243865399 | Lu et al. | 2021 | Citations: 74]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "268031876",
            "title": "Generative Retrieval with Large Language Models",
            "text": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009;Chen et al., 2017;Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models. For example, Cao et al. ( 2021 2023) introduced a two-stage approach that first generates passages, then URL identifiers. Ziems et al. (2023) utilized LLMs to first generate page URLs, followed by traditional retrieval techniques to obtain fine-grained passages. However, whether traditional or generative retrieval, they can only obtain predefined and segmented text chunks, making it difficult to naturally retrieve references from arbitrary positions. \n\nRecent research has found that relevant knowledge can be extracted from LLMs through prompting, especially in domains with insufficient coverage in knowledge bases (Liu et al., 2022;Fang et al., 2022). Enhancing model performance through the output of LLMs has also gained attention. 2023) propose using GPT-3 to generate relevant context as references, treating these contexts as additional inputs when answering questions. However, fully generating context through LLMs is still plagued by the phenomenon of hallucination (Li et al., 2023a).",
            "score": 0.7155243246749065,
            "section_title": "Related Work",
            "char_start_offset": 21339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1789
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 175,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 1410,
                    "end": 1428,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "239016601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "248524694",
            "title": "Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings",
            "text": "With the advances in neural language modeling [3], learning dense representations for text has become a vital component in many information retrieval (IR) tasks. In passage ranking and open-domain question answering, dense retrieval has become a new paradigm to retrieve relevant passages [6,7,9]. In contrast to traditional termbased IR models (TF-IDF and BM25) that fail to capture beyond lexical matching, dense retrieval learns dense representations of questions and passages for semantic matching. \n\nA typical approach for dense retrieval involves learning a dualencoder for embedding the questions and passages [6]. A dualencoder model consists of two separate neural networks optimized to score relevant (i.e., positive) question-passage pairs higher than irrelevant (i.e., negative) ones. At inference time, the score of a question-passage pair is computed as the inner product of the corresponding question and passage embeddings. Due to their high efficiency, dual-encoders are popular first-stage rankers in largescale settings (in contrast to cross-encoders where even though they can achieve higher performance, they are not indexable and therefore are used as re-rankers [4,15]). The whole corpus can be encoded and indexed offline, while at inference time, high-scoring passages with respect to a question can be found using efficient maximum inner product search [5]. \n\nSo far, dense retrieval models have been evaluated on clean and curated datasets. However, these models will encounter usergenerated noisy questions when deployed in real-life applications. Questions can include typos because of users mistyping words, such as keyboard typos (additional/missing character and character substitution), phonetic typing errors due to the close pronunciation, and misspellings. How these typos affect the encoding of questions and whether dense dual-encoder retrieval models are robust to them is not studied yet. \n\nWorks on text classification have shown that deep neural language models such as BERT are not robust against typos [13,16], even though they apply the WordPiece tokenization.",
            "score": 0.7010686310347543,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 49,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 292,
                    "end": 294,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 294,
                    "end": 296,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1185,
                    "end": 1188,
                    "matchedPaperCorpusId": "231662379"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "237592852"
                },
                {
                    "start": 1379,
                    "end": 1382,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "This model can capture more contextual information and generally performs better than the dual-encoder model. However, these dense retrieval methods still have 3 main problems: (1) The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert [15] further increases the latency to 458 milliseconds, nearly 13 times of the BM25. (3) Negative samples are typically randomly sampled from the current batch of data or the entire dataset, lacking true annotations. These samples may only be highly similar pseudo-negative ones to the query. Simply pushing away the query from them can impact the sentence representation and affect the overall model performance. \n\nCurrent research has focused on the combined use of crossencoder and dual-encoder models in dense passage retrieval. In this paper, we propose a multi-level distillation model called MD2PR, in which a dual-encoder model is utilized for large-scale candidate retrieval during the retrieval phase, while a cross-encoder model is employed for fine ranking of the retrieved results during the ranking phase. This method can not only improve the quality of the retrieval results but also decrease the computational cost. The main contributions are shown as follows. \n\n(1) We use a knowledge distillation method, by which the knowledge learned by the ranker is transferred to the retriever at both sentence and word levels.",
            "score": 0.682767924383774,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Large-scale query-passage retrieval is a core task in search systems, which aims to rank a collection of passages based on their relevance with regard to a query. To balance efficiency and effectiveness, existing work typically adopts a two-stage retrieval pipeline (Ren et al., 2021b;Zhu et al., 2021). The first-stage aims to retrieve a subset of candidate passages by a recall model from the entire corpus and the second stage aims to rerank the retrieved passages. In the first-stage retrieval, traditional approaches (Chen et al., 2017) implemented term-based retriever (e.g. TF-IDF and BM25) by weighting terms based on their frequency, which have limitations on representing semantics of text. Recently, dense passage retrieval is drawing more and more attention in the task of passage retrieval (Karpukhin et al., 2020). The underlying idea is to represent both queries and passages as embeddings, so that the semantic relevance can be measured via embeddings similarity. With the great success of pre-trained language models (PLMs) such as BERT/RoBERTa (Devlin et al., 2019;Liu et al., 2019) in natural language processing tasks, dense retrieval models parameterized by PLMs is emerging as the new state-of-the-art in a variety of passage retrieval tasks (Karpukhin et al., 2020;Xiong et al., 2020). \n\nTwo paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss.",
            "score": 0.6676279777217735,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 285,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 803,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1062,
                    "end": 1083,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1264,
                    "end": 1288,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1288,
                    "end": 1307,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "269457256",
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "text": "The emergence of deep learning techniques has significantly transformed the field of retrieval.There is a growing interest in using deep learning techniques to enhance retrieval accuracy, even if it means sacrificing some level of comprehensibility.The dual encoder architecture is a common design for dense retrieval models.(Hambarde and Proenca, 2023) The system comprises of two distinct networks that receive separate inputs, namely queries and documents, and independently generate dense embeddings for each input.Due to its high accuracy and dual-encoder structure, which is more suitable for RALM, most articles choose to use the dense indexing method to build their retrievers.This section classifies dense retrieval into three types: Word Embedding, Multimodal Retrieval, and Data Distillation, based on the characteristics of each retrieval method.",
            "score": 0.6612372329425643,
            "section_title": "Dense Retriever",
            "char_start_offset": 12411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 519
                },
                {
                    "start": 519,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Existing dense passage retrieval methods usually adopts a dual-encoder architecture. \n\nIn DPR (Karpukhin et al., 2020), they firstly presented that the passage retrieval performance of dense dual-encoder framework can remarkable outperform traditional term match based method like BM25. Based on the dual-encoder framework, studies explore to various strategies to enhance dense retrieval models, including mining hard negatives in fine-tuning stage (Xiong et al., 2021;Zhan et al., 2021), knowledge distillation from more powerful cross-encoder model (Ren et al., 2021;Zhang et al., 2021;Lu et al., 2022), data augmentation (Qu et al., 2021) and tailored PTMs (Chang et al., 2020;Gao andCallan, 2021, 2022;Ma et al., 2022;Liu and Shao, 2022;Wu et al., 2022). \n\nFor the pre-training of language model, previous research attend to design additional pre-training objectives tailored for dense passage retrieval (Lee et al., 2019;Chang et al., 2020) or adjust the Transformer encoder architecture (Gao andCallan, 2021, 2022) to obtain more practicable language models. In this paper, we seek to make simple transformations of the original MLM learning objective to improve the model performance, thereby reducing the complexity of the pre-training process.",
            "score": 0.6553620023168762,
            "section_title": "Related Work",
            "char_start_offset": 3117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 87,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 118,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 450,
                    "end": 470,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 625,
                    "end": 642,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 661,
                    "end": 681,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 927,
                    "end": 946,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Along with the success of deep learning, dualencoder based retrievers have become the dominant method for Web searching (Zhu et al., 2021;Zhao et al., 2022). For example, DPR (Karpukhin et al., 2020) employs two independent encoders to encode the question and the document respectively, then estimates their relevance by computing a single similarity score between two representations. However, these methods suffer from two major drawbacks. First, the representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021). Second, the question or document representation is embedded into a single dense vector, potentially missing fine-grained information when computing the similarity between the two vector representations (Khattab and Zaharia, 2020). \n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
            "score": 0.6548066316476866,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 579,
                    "end": 603,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 657,
                    "end": 679,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 883,
                    "end": 910,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1276,
                    "end": 1294,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "265457188",
            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
            "text": "The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2021) is employed to fetch top-k relevant passages.",
            "score": 0.6400370357224477,
            "section_title": "Dense Retrieval Model",
            "char_start_offset": 5312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 286
                },
                {
                    "start": 289,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 639
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 149,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 571,
                    "end": 593,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Baselines. Existing retrieval models can be broadly divided into sparse retrieval models and dense retrieval models. Sparse retrieval models focus on exact matching signals to design a relevance scoring function, with BM25 being the most prominent and widelyutilized baseline due to its promising performance. Additionally, dense retrieval models leverage deep neural networks to learn low-dimensional dense embeddings for queries and documents. Generally, most existing dense retrieval methods adhere to the cascade training paradigm [15,20,21]. Therefore, to facilitate easier comparison in future studies on our dataset, we simplify the training process as illustrated in Figure 5 as in [15,20]. Specifically, we utilize the dual-encoder (DE) as the architecture of dense retrieval models, which is illustrated in Figure 6(a). The following methods are employed as our baselines to evaluate the retrieval performance on T 2 Ranking. \n\n\u2022 QL (query likelihood) [19] is a representative statistical language model that measures the relevance of passages by modeling the generation of a query. \u2022 BM25 [23] is a widely-used sparse retrieval baseline. \n\n\u2022 DE w/ BM25 Neg is equivalent to DPR [12], which is the first work that uses the pre-trained language model as the backbone for the passage retrieval task. \u2022 DE w/ Mined Neg enhance the performance of DPR by sampling hard negatives globally from the entire corpus as in ANCE [28] and RocketQA [21]. \n\n\u2022 DPTDR [25] is the first work that employs prompt tuning for dense retrieval. Among them, QL and BM25 are sparse retrieval models, whereas the others are dense retrieval models Implementation details. BM25 is implemented by Pyserini [14] with default parameters. The dual-encoder models are implemented Metrics. The following evaluation metrics are used in our experiments to examine the retrieval performance of baselines on T 2 Ranking: (1) Mean Reciprocal Rank for the top 10 retrieved passages (MRR@10), ( 2) Recall for the top- retrieved passages (Recall@). Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages.",
            "score": 0.6343413943150928,
            "section_title": "Retrieval Performance",
            "char_start_offset": 19754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 962,
                    "end": 966,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1100,
                    "end": 1104,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "To this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages. We followed the original dual-encoder training setting from Karpukhin et al. (2020) where, given a question q, a relevant passage p + and a set of irrelevant passages {p \u2212 1 , p \u2212 2 , . . . , p \u2212 n }, the model is fine-tuned via the minimization of the softmax cross-entropy: \n\ne s(q,p + ) e s(q,p + ) + p \u2212 e s(q,p \u2212 ) . (4) \n\nThe inference phase of our multimodal dense retriever remains the same as in traditional dualencoders for dense text retrieval. Specifically, we compute the similarity of a question-passage pair as the inner product of the respective question embedding and passage embedding. At query time, only the question needs to be encoded. In detail, we build a dense index of passage vectors (offline) by encoding the whole corpus and storing it in an index structure that supports efficient retrieval of the relevant passages via approximate nearest neighbor search (Johnson et al., 2021). At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings. On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers.",
            "score": 0.6214049450752147,
            "section_title": "Multimodal Dense Retriever",
            "char_start_offset": 8050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 262,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1065,
                    "end": 1087,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66845703125
        },
        {
            "corpus_id": "243865399",
            "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
            "text": "Recently, Dense Retrieval (DR) has progressed to more important roles in many language systems, for example, web search (Xiong et al., 2021), question answering (Karpukhin et al., 2020), and news recommendation (Wu et al., 2020b). In the firststage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020). \n\nA popular choice of text encoders in DR is the Transformer network pre-trained by language modeling (e.g., BERT) (Reimers and Gurevych, 2019a). It is unexpected that, unlike in other language tasks where pre-trained models simply excel, directly fine-tuning BERT in DR often underperforms unsupervised sparse retrieval, e.g., BM25. Some complicated procedures are almost necessary to effectively fine-tune pre-trained Transformers in dense retrieval (Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2021). One observation is that the pre-trained language models are not effective at encoding the semantics of the entire text sequence in one embedding, especially in dense retrieval where text sequences are mostly longer than 128 tokens (Luan et al., 2021). \n\nIn some other modalities, autoencoders have been widely used to obtain high-quality data representations (Vincent et al., 2010;Kingma and Welling, 2013). They pair a decoder on top of the encoder, trains the decoder to reconstruct the data solely from the encoder's encodings, thus enforce an information bottleneck on the data encodings for better representation quality. Recently, autoencoders have been brought in language pre-training.",
            "score": 0.6195628144023195,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 140,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 211,
                    "end": 229,
                    "matchedPaperCorpusId": "220046458"
                },
                {
                    "start": 553,
                    "end": 571,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 668,
                    "end": 685,
                    "matchedPaperCorpusId": "218614141"
                },
                {
                    "start": 1163,
                    "end": 1181,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1181,
                    "end": 1200,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1433,
                    "end": 1452,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1561,
                    "end": 1583,
                    "matchedPaperCorpusId": "17804904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "237485691",
            "title": "Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder",
            "text": "Recently, Dense Retrieval (DR) has progressed to more important roles in many language systems, for example, web search (Xiong et al., 2021), question answering (Karpukhin et al., 2020), and news recommendation (Wu et al., 2020b). In the firststage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020). \n\nA popular choice of text encoders in DR is the Transformer network pre-trained by language modeling (e.g., BERT) (Reimers and Gurevych, 2019a). It is unexpected that, unlike in other language tasks where pre-trained models simply excel, directly fine-tuning BERT in DR often underperforms unsupervised sparse retrieval, e.g., BM25. Some complicated procedures are almost necessary to effectively fine-tune pre-trained Transformers in dense retrieval (Karpukhin et al., 2020;Luan et al., 2021;Xiong et al., 2021). One observation is that the pre-trained language models are not effective at encoding the semantics of the entire text sequence in one embedding, especially in dense retrieval where text sequences are mostly longer than 128 tokens (Luan et al., 2021). \n\nIn some other modalities, autoencoders have been widely used to obtain high-quality data representations (Vincent et al., 2010;Kingma and Welling, 2013). They pair a decoder on top of the encoder, trains the decoder to reconstruct the data solely from the encoder's encodings, thus enforce an information bottleneck on the data encodings for better representation quality. Recently, autoencoders have been brought in language pre-training.",
            "score": 0.6195289555133642,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 140,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 211,
                    "end": 229,
                    "matchedPaperCorpusId": "220046458"
                },
                {
                    "start": 553,
                    "end": 571,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 668,
                    "end": 685,
                    "matchedPaperCorpusId": "218614141"
                },
                {
                    "start": 1163,
                    "end": 1181,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1181,
                    "end": 1200,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1433,
                    "end": 1452,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1561,
                    "end": 1583,
                    "matchedPaperCorpusId": "17804904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "273026177",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "text": "The goal of dense passage retrieval is to retrieve a subset of relevant passages, denoted as D + , from a large collection of passages \n\nIn order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval. \n\nThe relevance of a query q to a passage d i is measured using a similarity function: \n\nwhere a higher similarity score indicates a greater relevance of the passage to the query. Common choices of the similarity function are dot product, cosine similarity, or the Max-Sum operator introduced in ColBERT (Khattab and Zaharia, 2020). \n\nGiven a labeled dataset of relevant passage-query pairs (q, d + ), dense retrievers are typically trained with a contrastive learning objective such as the InfoNCE loss (Oord et al., 2018): \n\n, \n\nwhere D \u2032 denotes the union of the positive and negative examples. Optimizing this objective promotes the similarity of the positive pair s(q, d + ) in contrast to the negative examples.",
            "score": 0.6149566932443707,
            "section_title": "Dense Retrieval",
            "char_start_offset": 7520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1354
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 968,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "276928453",
            "title": "MultiConIR: Towards multi-condition Information Retrieval",
            "text": "Our experiments reveal notable differences between retrieval and reranking models across the three tasks. Specifically, retrieval models demonstrate greater robustness to query complexity, better preservation of relevance monotonicity, and stronger invariance to query format changes compared to rerankers. We analyze these differences through the lens of relevance computation and attention mechanisms. \n\nRetrieval models: robustness induced by bidirectional attention and dual-encoder. Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query. \n\nAnother critical factor is the training datasets. Many dense retrieval models are trained on a mixture of retrieval-specific and general textual datasets (Lee et al., 2024;BehnamGhader et al., 2024;Wang et al., 2024a), which enhances their generalization across diverse retrieval scenarios and query styles. This, in turn, improves their robustness against query complexity. \n\nReranking models: sensitivity to query complexity. Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format. \n\nOur experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models.",
            "score": 0.6141489001985233,
            "section_title": "Retrievers vs. Rankers",
            "char_start_offset": 18881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 1124,
                    "end": 1143,
                    "matchedPaperCorpusId": "266693831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.673828125
        },
        {
            "corpus_id": "233295975",
            "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval",
            "text": "references. The autoregressive behavior of the model can capture the dependencies between passages in the output set. We employ a training method that encourages multiple ways to reach the multi-answer supervision, and introduce a new tree-decoding algorithm for the autoregressive model, enabling flexibility in handling the trade-off between diversity of the top-k passages and their relevance to the question.\n\nIn a set of experiments on three multi-answer datasets (Yih et al., 2016;Min et al., 2019;Baudi\u0161 and \u0160ediv\u1ef3, 2015), JPR achieves significantly improved recall over both a dense retrieval baseline and a state-of-the-art reranker that independently scores each passage . Improvements are particularly significant on questions with more than one answer, outperforming dense retrieval by up to 12% absolute and an independent reranker by up to 6% absolute.\n\nWe also showcase the impact of JPR in downstream question answering which takes the retrieved passages as input and generates the answers. Improved reranking leads to improved answer accuracy because we can supply fewer, higher-quality passages to a larger answer generation model that can fit on the same hardware. This practice leads to a new state-of-the-art on three multi-answer QA datasets and NQ (Kwiatkowski et al., 2019).\n\nTo summarize, our contributions are as follows:\n\n1. We study multi-answer retrieval, an underexplored problem that requires the top k retrieved passages to cover all distinct answers to a natural language question. 2. We propose JPR, a joint passage retrieval model that considers the dependencies between passages in the output set, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy.",
            "score": 0.6141140613821363,
            "section_title": "Introduction",
            "char_start_offset": 2007,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 469,
                    "end": 487,
                    "matchedPaperCorpusId": "13905064"
                },
                {
                    "start": 487,
                    "end": 504,
                    "matchedPaperCorpusId": "174801764"
                },
                {
                    "start": 504,
                    "end": 528,
                    "matchedPaperCorpusId": "17723267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "258833383",
            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
            "text": "Passage retrieval is a core sub-task in various downstream applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021a;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022a). Recently, a number of studies have demonstrated that dense passage retrieval systems based on pre-trained language models (PLMs) are significantly more effective compared to traditional sparse retrieval methods such as BM25 (Karpukhin et al., 2020). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair. 1 n the dual-encoder architecture, the text representation capability of the PLMs plays a crucial role as it shall encode all essential information into the low-dimensional dense vector. However, it has been observed that the progress of PLMs in general language understanding benchmarks does not necessarily lead to an improvement in text representation ability (Li et al., 2020;Lu et al., 2021;Wang et al., 2022) as the widely used masked language modeling (MLM) pre-training objective focuses more on representing individual tokens rather than the entire sentence. As a result, numerous recent studies have explored to enhance the base model's sentence representation ability via incorporating supplementary pre-training tasks or designing new pre-training architectures (Lee et al., 2019;Gao and Callan, 2021;Xiao et al., 2022). \n\nCurrently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
            "score": 0.6135889598618036,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 136,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 136,
                    "end": 153,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 195,
                    "end": 212,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 263,
                    "end": 282,
                    "matchedPaperCorpusId": "247292113"
                },
                {
                    "start": 508,
                    "end": 532,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1314,
                    "end": 1330,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1708,
                    "end": 1726,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1747,
                    "end": 1765,
                    "matchedPaperCorpusId": "252917569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "233295975",
            "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval",
            "text": "We propose JPR (Joint Passage Retrieval), which models the joint probability of P (p 1 ...p k |q) for multi-answer retrieval. Formally, a model is given a question q and outputs a set of k passages O = {o 1 ...o k } from a large text corpus C; O should have maximal coverage of the answer set to q.\n\nWe use an approach consisting of dense retrieval and reranking: dense retrieval retrieves candidate passages B from C, and a reranker processes B to output O = {o 1 ...o k } \u2282 B. Often k < |B| |C|; we use |B| = 100 in our experiments. In this work, we focus on the reranking component of the model, which allows (1) processing candidate passages with a more expressive model, and (2) efficiently modeling the joint probability of P (p 1 ...p k |q). We refer to Appendix A for the details of dense retrieval, and describe reranking here.\n\nThe overview of JPR is illustrated in Figure 2. Our reranking leverages the encoder-decoder architecture of T5 (Raffel et al., 2020) for an autoregressive generation of passage references (Section 3.1). We employ dynamic oracle training to successfully train our T5-based architecture (Section 3.2). Finally, we introduce an inference algorithm to decode a set of passage references (Section 3.3). These new training and inference methods are proposed in order to tackle the discrepancy between modeling a sequence and modeling a set.",
            "score": 0.6123434633718057,
            "section_title": "JPR: Joint Passage Retrieval",
            "char_start_offset": 8904,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 949,
                    "end": 970,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "231704318",
            "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation",
            "text": "In this section we describe our architecture for training a first-stage neural passage retriever. Our retrieval model belongs to the family of relevancebased dense retrieval 6 that encodes pairs of items in dense subspaces (Palangi et al., 2016). Let Q = (q 1 , . . . q n ) and P = (p 1 , . . . , p m ) be a question and passage of n and m tokens respectively. Our model consists of two encoders, {f Q (), f P ()} and a similarity function, sim(). An encoder is a function f that takes an item x as input and outputs a real valued vector as the encoding, The similarity function, sim(), takes two encodings, q, p \u2208 R N and calculates a real valued score, s = sim(q, p). For passage retrieval, the two encoders are responsible for computing dense vector representation of questions and passages.",
            "score": 0.6117727874861162,
            "section_title": "Neural First-stage Retrieval",
            "char_start_offset": 12853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 794
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 245,
                    "matchedPaperCorpusId": "3337266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "245218695",
            "title": "CODER: An efficient framework for improving retrieval through COntextual Document Embedding Reranking",
            "text": "Recent work on ad-hoc information retrieval has employed transformer-based architectures following two main approaches: the first approach (Nogueira and Cho, 2020;Khattab and Zaharia, 2020) allows direct interactions between query and document terms through attention, offering impressive retrieval performance, albeit at the expense of computational efficiency; it can practically be used only as part of a cascade system for reranking candidate documents retrieved by a first- While training loss is decreasing in all cases, only using a large number of candidates results in a significant improvement of performance over the base method on the validation and test sets (see Figure 2). \n\nstage method, and still introduces a significant endto-end processing delay, in the order of seconds per query. These powerful but slow \"cross-encoder\" models have also been used as teachers for \"dual encoder\" (a.k.a \"bi-encoder\") methods (Lin et al., 2020;Qu et al., 2021;Ren et al., 2021a;Hofst\u00e4tter et al., 2021). These methods constitute the second approach, described below. The dual encoder approach employs an architecture of two transformer encoders (optionally sharing weights, or implemented as the same encoder, distinguishing between queries and documents by adding special sequence type encodings) to separately encode the query and document sequences, without interactions between them. For inference, it relies on the efficient computation of the dot product through high-performing Approximate Nearest Neighbors libraries such as FAISS (Johnson et al., 2017) to evaluate the similarity between extracted query and document representations. This approach, called \"dense retrieval\", is highly effective, fast and single-stage, but still lags behind in terms of retrieval performance compared to the first approach. \n\nOur framework addresses the current dilemma of slow reranking versus fast but less effective singlestage retrieval: it efficiently fine-tunes the query encoder of an existing (base) dense retrieval dual encoder model through reranking a set of precomputed document embeddings, offering a substantial  performance improvement over the base model.",
            "score": 0.6089904573090194,
            "section_title": "A.8 Detailed comparison with related work",
            "char_start_offset": 45028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 189,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 947,
                    "end": 963,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 963,
                    "end": 981,
                    "matchedPaperCorpusId": "236477844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "253080873",
            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
            "text": "Recently, with the success of large pre-trained models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021) have surpassed the sparse vector space models, becoming the new de facto method. Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019;Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers. Recently, the retrieve-then-rank pipeline has also been applied to solve CommonGen (Wang et al., 2021;Li et al., 2021;Liu et al., 2022). Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus. To alleviate this, recent work, such as AR2 (Zhang et al., 2022), has focused on improving the retriever by distilling knowledge from the ranker. In this paper, we further extend this idea by distilling the order knowledge between the candidates and gold references to the ranker and retriever.",
            "score": 0.6078145015070715,
            "section_title": "Experiment on Keyword Generation",
            "char_start_offset": 25985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 90,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 146,
                    "end": 170,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 189,
                    "end": 205,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 855,
                    "end": 874,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 1039,
                    "end": 1058,
                    "matchedPaperCorpusId": "235166616"
                },
                {
                    "start": 1058,
                    "end": 1074,
                    "matchedPaperCorpusId": "244119606"
                },
                {
                    "start": 1074,
                    "end": 1091,
                    "matchedPaperCorpusId": "245144423"
                },
                {
                    "start": 1317,
                    "end": 1337,
                    "matchedPaperCorpusId": "238419331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "248366563",
            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
            "text": "Dense retrieval is receiving increasing interest in recent years from both industrial and academic communities due to its benefits to many IR related tasks, e.g., Web search [9,17,26], question answering [20,23,43] and conversational systems [10,39]. Without loss of generality, dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents. In essence, high-quality text representation is the foundation of dense retrieval to support effective search in the representation space. \n\nTaking the pre-trained representation models like BERT [8] and RoBERTa [28] as the text encoders have become a popular choice [21,38,40] in dense retrieval. Beyond these direct applications, there have been some works on the pre-training objectives tailored for dense retrieval [2,24]. For example, Chang et al. [2] presented three pre-training tasks that emphasize different aspects of semantics between queries and documents, including Inverse Cloze Task (ICT), Body First Selection (BFS), and Wiki Link Prediction (WLP). As we can see, some tasks even depend on certain special document structures, e.g., hyperlinks. When applying such pre-trained models to dense retrieval, marginal benefit could be observed on typical benchmark datasets as shown in Section 5.1. \n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25,29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
            "score": 0.6061568020180226,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 180,
                    "end": 183,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "237502990"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 464,
                    "end": 467,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 470,
                    "end": 473,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1689,
                    "end": 1693,
                    "matchedPaperCorpusId": "5541663"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "215416349"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "258041275",
            "title": "T2Ranking: A Large-scale Chinese Benchmark for Passage Ranking",
            "text": "Notably, for the retrieval task, we consider Level-2 and Level-3 passages as relevant passages, and all other passages are regarded as irrelevant passages. For a comprehensive comparison, we report Recall@50 and Recall@1K on the test queries. Following the evaluation settings of MS-MARCO and DuReader retrieval , MRR is defined as the average of the reciprocal ranks of the first relevant passage for a set of queries. The MRR is a value between 0 and 1, with a higher value indicating that the system is better at ranking the most relevant passage higher in the list. Meanwhile, Recall is defined as the fraction of relevant passages that are retrieved among all relevant passages, also with a value between 0 and 1, where a higher value indicates that the system is better at retrieving all relevant passages. MRR and Recall measure different aspects of retrieval performance. MRR@ and Recall@ can be depicted as: \n\n@ = \n\nwhere I(\u2022) is a indicator function. The  in Eq. 5 denotes the position of the first relevant passage in the retrieved candidates of query . The   and  K   represent the relevant passages of query  and the position of passage  in the candidate list K  . Retrieval performance. We report the retrieval performance of baselines in Table 5. Compared to the traditional sparse retrieval method BM25, dual-encoder models significantly boost the retrieval performance on our dataset. The improvement can be attributed to the integration of two distinct sources of knowledge, i.e., latent knowledge obtained through unsupervised pre-training of language models on a massive corpus and relevance knowledge acquired through supervised training on our large-scale annotated dataset. Equipped with the strategy of negative mining proposed in recent studies [28], the retrieval performance of dual-encoder models could be further improved on T 2 Ranking. It is worth noting that the Recall@ metrics observed in T 2 Ranking are lower than those reported in other benchmarks with coarse-grained annotations. For instance, the Recall@50 of BM25 is .601 and .700 on MS-MARCO-DEV Passage and DuReader retrieval , respectively, and 0.4918 on our dataset.",
            "score": 0.6038860567851406,
            "section_title": "Retrieval Performance",
            "char_start_offset": 21771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "Passage retrieval is fundamental in modern information retrieval (IR) systems, typically serving as a preceding stage of reranking. The aim of passage retrieval is to find relevant passages from a large corpus for a given query, which is crucial to the final ranking performance [3,24,26,62,68]. Conventional methods for passage retrieval (e.g., BM25 [50]) usually consider lexical matching between the terms of query and passage. In recent years, neural retrievers based on pre-trained language models (PLMs) have prospered and achieved the state-of-the-art performance. In particular, existing PLM-based IR models can be broadly categorized into cross-encoders [40], dual-encoders [24] and lateinteraction encoders [16,25], as shown in Figures 1(a), 1(b) and 1(c), respectively. Without considering the fine-grained interactions between the tokens of query and passage, the major merit of dualencoders is their high efficiency in inference. Yet, their effectiveness is usually considered sub-optimal compared with cross-encoders or other interaction-based models. Cross-encoders take the concatenation of query and passage as input to perform full interaction that effectively captures relevance features. As query-passage interactions are important factors in relevance modeling [17], cross-encoders usually have superior ranking performance. However, their applications are limited to small collections (e.g., the top passages retrieved by dual-encoders) due to their high inference latency. To combine the merits of both methods, late-interaction encoders adopt separate query/passage encoding and apply lightweight interaction schemes (i.e., late interactions) between the vectors of query and passage. They are usually more effective than dual-encoders for passage retrieval and less computationally expensive than cross-encoders. \n\nDespite their effectiveness, late-interaction models are still suboptimal for passage retrieval on large corpus, mainly due to two problems. First, effective late-interaction models usually relies on token-level representations of passages to allow subsequent tokenlevel interactions [25,52], where the storage cost of such multivector passage representation is enormous.",
            "score": 0.6030163022393003,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "252995741"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "5688521"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.537109375
        },
        {
            "corpus_id": "233295975",
            "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval",
            "text": "In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p 1 ...p k } from a large text corpus C. The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model.\n\nEvaluation Intrinsic evaluation directly evaluates the retrieved passages. The most commonly used metric is RECALL @ k which considers retrieval successful if the answer a is included in {p 1 ...p k }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2020) and evaluates final question answering performance.\n\nDense Retrieval State-of-the-art retrieval models use a dual-encoder architecture with supervised\n\nTask Single-answer Retrieval",
            "score": 0.6022050643202401,
            "section_title": "Review: Single-Answer Retrieval",
            "char_start_offset": 3907,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Passage retrieval aims to retrieve the relevant passages of a query from a large corpus, which benefits many downstream applications, such as web search (Fan et al. 2021;Guo et al. 2022;Lin, Nogueira, and Yates 2021), question answering (Karpukhin et al. 2020;Lee et al. 2020;Zhu et al. 2021) and dialogue systems (Gao et al. 2022a;Yu et al. 2021).\n\nFor a long time, sparse retrieval represented by BM25 (Robertson, Zaragoza et al. 2009) was the dominant retrieval method. Recently, dense retrieval has received increasing attention with the development of pre-trained language models (PLM) (Devlin et al. 2018;Liu et al. 2019). Dense retrieval models are generally based on pre-trained language models with a siamese or dual-encoder architecture to encode queries and documents into low-dimensional vector space for effective search (Hofst\u00e4tter et al. 2021;Humeau et al. 2019;Xiong et al. 2020;Zhan et al. 2021Zhan et al. , 2020. The relevances between queries and documents are calculated with cosine similarity or dot-product function in the vector space. Therefore, high-quality text representation based on PLM is crucial for dense passage retrieval.\n\nDPR (Karpukhin et al. 2020) successfully shows that dense retrieval models can outperform BM25 methods. Since then, some works have emerged to boost dense retrieval performance by improving the pre-training process tailored for dense retrieval. (Lu et al. 2021;Gao and Callan 2021a;Liu and Shao 2022) encourage the encoder to improve the text representation modeling ability through auxiliary self-supervised reconstruction tasks. Auxiliary tasks usually utilize a weak decoder to reconstruct the masked text with the assistance of the text's vector from the encoder, which forces the encoder to provide better text representations. Although these works have been shown to be very effective and achieved some improvements in dense retrieval, they mainly focus on single-text internal modeling without considering contextual information. (Chang et al. 2020;Gao and Callan 2021b;Ma et al. 2022) proposes multi-source and multi-granularity contrastive span prediction",
            "score": 0.5995810609120567,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 170,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 170,
                    "end": 186,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 186,
                    "end": 216,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 332,
                    "end": 347,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 404,
                    "end": 436,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 834,
                    "end": 858,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 895,
                    "end": 911,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1402,
                    "end": 1418,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "273185905",
            "title": "GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA",
            "text": "Retrieval Traditional retrieval techniques, such as TF-IDF (Jones, 1972) and BM25 (Robertson et al., 1995;Robertson & Zaragoza, 2009), retrieve information based on word terms. Subsequently, deep learning-based retrieval methods quickly became popular. REALM (Guu et al., 2020) augments the language model pre-training with a latent knowledge retriever using masked language modeling. DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) encodes queries and documents as dense embeddings, with similarity computed between them. ColBERT (Khattab & Zaharia, 2020;Santhanam et al., 2022) produces multi-vector representations at the token level. JPR (Joint Passage Retrieval) (Min et al., 2021) is a joint passage retrieval model with an autoregressive reranker that selects a sequence of passages. DHR (Dense Hierarchical Retrieval) (Liu et al., 2021) leverages both macroscopic document-level semantics and microscopic passage-level semantics. Fusion-in-Decoder (Izacard & Grave, 2021) employs both DPR and BM25 in a knowledge distillation manner, which does not require annotated query-document pairs. CPT-text (Neelakantan et al., 2022) utilizes contrastive pre-training on unsupervised data. NCI (Wang et al., 2022) directly generates relevant document identifiers for a given query. Atlas (Izacard et al., 2022) fine-tunes an encoder-decoder model with a retriever to address knowledge-intensive tasks with minimal training examples. RETRO (Borgeaud et al., 2022;Wang et al., 2023a) conditions on document chunks based on local similarity with preceding tokens. HHR (Hybrid Hierarchical Retrieval) (Arivazhagan et al., 2023) combines sparse and dense retrieval methods across both document and passage retrieval stages. SimLM (Wang et al., 2023b) proposes a new loss function to reduce the mismatch between pre-training and fine-tuning input distributions.",
            "score": 0.5987171444674837,
            "section_title": "RELATED WORK",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 106,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 106,
                    "end": 133,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 259,
                    "end": 277,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 415,
                    "end": 439,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 538,
                    "end": 563,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 563,
                    "end": 586,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 675,
                    "end": 692,
                    "matchedPaperCorpusId": "233295975"
                },
                {
                    "start": 833,
                    "end": 851,
                    "matchedPaperCorpusId": "240288895"
                },
                {
                    "start": 963,
                    "end": 986,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1200,
                    "end": 1219,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "258170263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "Our research questions and experiments have focused exclusively on interpolation-based re-ranking using dual-encoders and Fast-Forward indexes. However, the most common application of dualencoders in the field of IR is the use as dense retrieval models; a natural question that occurs is, whether the encoders proposed in Section 5 can be used for more efficient dense retrieval. \n\nIn Table 6, we present passage and document retrieval results on the MS MARCO corpus. Dense retrievers use a FAISS [32] vector index; no interpolation or re-ranking is performed. It is immediately obvious that our models do not achieve competitive results; on the contrary, the embedding-based encoder yields far worse performance than dense retrievers and even BM25, and even the attention-based encoder fails to improve over sparse retrieval. \n\nFrom these results, we infer that the models we trained are not suitable for dense retrieval. However, we assume that the main reason for this is not the architecture of the query encoder, but instead the following: \n\n\u2022 We use a simple in-batch negative sampling strategy [34], which has been shown to be less effective than more involved strategies [51,53,86,88]. \u2022 The hardware we use for training the models is limiting w.r.t. the batch size and thus the number of negative samples, i.e., we cannot use a batch size greater than 4. \u2022 We perform validation and early stopping based on re-ranking. \n\nConsidering the points above, we expect that our dual-encoder models, including ones with lightweight encoders, could also be used in retrieval settings if the shortcomings of the training setup are addressed, for example, by using more powerful hardware and state-of-the-art training approaches. On the other hand, we argue that the fact that our models perform well in the reranking setting (see Section 7) shows that it is both easier and more efficient (in terms of time and resources) to train models to be used with Fast-Forward indexes instead of for dense retrieval.",
            "score": 0.5985388455581879,
            "section_title": "Efficient Encoders for Dense Retrieval",
            "char_start_offset": 63161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1179,
                    "end": 1183,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 1186,
                    "end": 1189,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46728515625
        },
        {
            "corpus_id": "257622671",
            "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
            "text": "In the context of retrieval-augmented models, numerous works rerank retrieved passages by their relevance to the query and process only the highest scoring passages (Mao et al., 2021;Wang et al., 2018;Yu et al., 2022) and vary the number of processed passages depending on model confidence (Kratzwald and Feuerriegel, 2018;Varshney et al., 2022).Concurrent work CoDA (Lei et al., 2023) employs a related conditional computation mechanism, designed for efficient adaptation rather than modeling long documents.\n\nDevice utilization FLOPs do not tell the whole story, as modeling choices can influence the effective speed of operations achieved by accelerators.\n\nFor long text inputs, autoregressive decoder inference is very slow due to memory bandwidth constraints from repeatedly loading the long sequence of keys and values (Shazeer, 2019;de Jong et al., 2022).Shazeer (2019) introduces multi-query attention (MQA), sharing heads for keys and values to reduce memory bandwidth overhead.Pope et al. (2022) studies how to shard large models, especially in the context of MQA, to obtain optimal device utilization and therefore speed.\n\nTraining objectives T5 introduced the span corruption objective (Raffel et al., 2020), a modification of masked language modeling (Devlin et al., 2019).LONGT5 made use of the PEGA-SUS (Zhang et al., 2020) sentence reconstruc-tion objective for improved summarization performance.Tay et al. (2022) proposes UL2, a mixture of span corruption, prefix, and causal language modeling, and shows that it leads to strong performance on both short-output and generative tasks.",
            "score": 0.5969562536011754,
            "section_title": "Sparse attention",
            "char_start_offset": 5404,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 509
                },
                {
                    "start": 511,
                    "end": 658
                },
                {
                    "start": 660,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1132
                },
                {
                    "start": 1134,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1413
                },
                {
                    "start": 1413,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 183,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 201,
                    "end": 217,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 290,
                    "end": 323,
                    "matchedPaperCorpusId": "52054464"
                },
                {
                    "start": 367,
                    "end": 385,
                    "matchedPaperCorpusId": "258060039"
                },
                {
                    "start": 1198,
                    "end": 1219,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1264,
                    "end": 1285,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1318,
                    "end": 1338,
                    "matchedPaperCorpusId": "209405420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "258741194",
            "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
            "text": "Built on the robust infrastructure of memory storage, our memory retrieval mechanism operates akin to a knowledge retrieval task. In this context, we adopt a dual-tower dense retrieval model similar to Dense Passage Retrieval (Karpukhin et al., 2020). In this paradigm, every turn of conversations and event summaries is considered as a memory piece m, which is pre-encoded into a contextual representation h m using the encoder model E(\u2022). Consequently, the entire \n\nwhere each h m is a vector representation of a memory piece. These vector representations are then indexed using FAISS (Johnson et al., 2019) for efficient retrieval. Parallel to this, the current context of conversation c is encoded by E(\u2022) into h c , which serves as the query to search M for the most relevant memory. In practice, the encoder E(\u2022) can be interchanged to any suitable model.",
            "score": 0.5962285893901913,
            "section_title": "Memory Retrieval",
            "char_start_offset": 8393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 861
                }
            ],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 609,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23876953125
        },
        {
            "corpus_id": "267740545",
            "title": "Distillation Enhanced Generative Retrieval",
            "text": "Dense retrieval (Lee et al., 2019;Karpukhin et al., 2020) is currently the de facto implementation of text retrieval. This method benefits from the powerful representation abilities of encoder-based language models and the MIPS algorithm (Shrivastava and Li, 2014), allowing for efficient passage retrieval from a large-scale corpus. Dense retrieval has been constantly developed through knowledge distillation, hard negative sampling, and better pretraining design (Chang et al., 2019;Wang et al., 2022a). Compared to dense retrieval, which relies on the dual-encoder architecture, the recently arsing generative retrieval shows promise in overcoming the missing fine-grained interaction problem through the encoder-decoder paradigm. Despite the huge potential, generative retrieval still lags behind the state-of-the-art dense retrieval method and leaves much room for investigation.",
            "score": 0.5944694131651755,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 885
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 34,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 34,
                    "end": 57,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 238,
                    "end": 264,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 466,
                    "end": 486,
                    "matchedPaperCorpusId": "211068995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41650390625
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Recently, retrieval models based on dense representations are dominant in passage retrieval tasks, due to their outstanding ability in terms of capturing semantics of input text compared to the traditional sparse vector space models. A common practice of dense retrieval models is to exploit a dual-encoder architecture to represent a query and a passage independently. Though efficient, such a structure loses interaction between the query-passage pair, resulting in inferior accuracy. To enhance the performance of dense retrieval models without loss of efficiency, we propose a GNN-encoder model in which query (passage) information is fused into passage (query) representations via graph neural networks that are constructed by queries and their top retrieved passages. By this means, we maintain a dual-encoder structure, and retain some interaction information between query-passage pairs in their representations, which enables us to achieve both efficiency and efficacy in passage retrieval. Evaluation results indicate that our method significantly outperforms the existing models on MSMARCO, Natural Questions and TriviaQA datasets, and achieves the new state-of-the-art on these datasets.",
            "score": 0.5943362987542351,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "268889861",
            "title": "NL2KQL: From Natural Language to Kusto Query",
            "text": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering [7,17,30], fact-checking [28], dialogue systems [11], and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,24,39]. Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15,36]. Yates et al. have composed a comprehensive overview of dense retrieval techniques [38]. \n\nAfter retrieval, the pertinent documents undergo processing to generate the final output. In open-domain question answering, approaches vary from extracting text spans from documents [7,9,15,33] to leveraging seq2seq models that generate answers in freeform text based on the retrieved documents [14,15]. Notably, such architectures have also been instrumental in reducing hallucination in dialogue systems [25].",
            "score": 0.592371551984347,
            "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
            "char_start_offset": 3014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1156
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "13403541"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "207210855"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "1965270"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 930,
                    "end": 932,
                    "matchedPaperCorpusId": "223637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "277502276",
            "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval",
            "text": "The fundamental principle of dense retrieval is to transform documents and queries into vector representations. The introduction of pre-trained language models, particularly BERT [8], has revolutionized information retrieval, paving the way for dense retrieval methods such as Dense Passage Retrieval (DPR) [7], ColBERT [9], and GTR [13]. Techniques like SimCSE [5] leverage contrastive learning with models such as BERT and Roberta to optimize embeddings. Additionally, dense retrieval methods often employ Approximate Nearest Neighbor (ANN) search [6,25], Maximum Inner Product Search (MIPS) algorithms [21], and SimLM [24] to ensure efficient retrieval in sub-linear time. \n\nUnlike traditional sparse retrieval, these methods utilize transformer encoders to create dense vector representations for queries and documents, enhancing semantic understanding and retrieval accuracy. This combination of semantic depth and computational efficiency positions dense retrieval as a leading approach in modern information retrieval.",
            "score": 0.5920846340170571,
            "section_title": "Dense Retrieval",
            "char_start_offset": 6978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 307,
                    "end": 310,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27734375
        },
        {
            "corpus_id": "248366293",
            "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
            "text": "Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.",
            "score": 0.5914947918272457,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "247292113",
            "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval",
            "text": "Passage Retrieval Passage retrieval task aims to recall all potentially relevant passages from a large corpus given an informationseeking query. In practical, passage retrieval is often an important step in other information retrieval tasks [4]. Traditional passage retrieval systems usually rely on term-based retrieval models like BM25 [46]. Recently, with the rapid development in text representation learning research [3] and deep pre-trained language models [21,27,33,51], dense retrieval combined with pre-trained language models, has become a popular paradigm to improve retrieval performance [16,26,42]. In general, dense models significantly outperform traditional term-based retrieval models in terms of effectiveness and benefit downstream tasks. In a basic concept, the core problem of passage retrieval is how to form the text representation and then compute text similarity. Thus, based on the text representation type and corpus index mode, passage retrieval models can be roughly categorized into two main classes. Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]. \n\nFor the above two types of models, the current optimization directions are not the same. Specifically, Sparse retrieval models \n\n(Adults should not put their hands on the chest of a sleeping baby as this is not good for the baby's breathing.) \n\nfocus on improving retrieval performance by either enhancing the bag-of-words (BoW) representations in classical term-based methods or mapping input texts into latent space (e,g. doc2query [37], query expansion [7] and document expansion [39]). The sparse representation has attracted great attention as it can be easily integrated into the inverted index for efficient retrieval. Recently, With the development of deep neural networks, pre-trained language models have been widely employed to improve the capacity of sparse retrieval models, including term re-weighting [9,10], sparse representation learning [24,50], etc. The mainstream of existing studies on improving the performance of dense retrieval models can be roughly divided into three groups. 1) Designing more powerful pre-trained language model architectures for the passage retrieval task and then improving the quality of sentence representation.",
            "score": 0.5913093079316774,
            "section_title": "RELATED WORK",
            "char_start_offset": 4763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2380
                },
                {
                    "start": 2381,
                    "end": 2538
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "1058977"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 604,
                    "end": 607,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 607,
                    "end": 610,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "1815908"
                },
                {
                    "start": 2199,
                    "end": 2202,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 2239,
                    "end": 2242,
                    "matchedPaperCorpusId": "235293983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "218470027",
            "title": "Sparse, Dense, and Attentional Representations for Text Retrieval",
            "text": "The differences among first-pass retrieval models are therefore diminished. Both BM25-uni and dense retrieval models peak at document collection passage length 200 and their combination outperforms the best prior pipeline model (Min et al., 2019). It also matches the performance of the endto-end ORQA model (Lee et al., 2019), but uses two times more text at inference time. 13  current works (Guu et al., 2020) and (Karpukhin et al., 2020) brought significant improvements, reaching up to 41.5 short answer exact match, by better unsupervised model pretraining, and use of full supervision for passage relevance with careful selection of negatives, respectively. Our study shows a complementary analysis of the relative and combined strengths of sparse and dense dual encoder and multi-vector models, as the length of documents in the retrieval collection grows. \n\n9 Large Scale Supervised Passage Retrieval and Ranking \n\nThe previous two experimental sections focused on understanding the relationship of our theoretical analysis to the accuracy of compressive dense encoders on a memorization task in \u00a7 7 and weakly supervised open-domain question answering for information-seeking queries in \u00a7 8, relating representation dimensionality to document length. \n\nIn this section we evaluate whether our newly proposed efficient multi-vector dense retrieval model ME-BERT, its corresponding dual encoder baseline model DE-BERT, and sparse-dense hybrids compare favorably to state-of-the-art models focusing on large-scale supervised retrieval and ranking for IR benchmarks.",
            "score": 0.5907417296175237,
            "section_title": "Containing Passage ICT Task",
            "char_start_offset": 37789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1572
                }
            ],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 326,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.204833984375
        },
        {
            "corpus_id": "248524694",
            "title": "Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings",
            "text": "Dense retrieval is becoming one of the standard approaches for document and passage ranking. The dual-encoder architecture is widely adopted for scoring question-passage pairs due to its efficiency and high performance. Typically, dense retrieval models are evaluated on clean and curated datasets. However, when deployed in real-life applications, these models encounter noisy user-generated text. That said, the performance of state-of-the-art dense retrievers can substantially deteriorate when exposed to noisy text. In this work, we study the robustness of dense retrievers against typos in the user question. We observe a significant drop in the performance of the dual-encoder model when encountering typos and explore ways to improve its robustness by combining data augmentation with contrastive learning. Our experiments on two large-scale passage ranking and open-domain question answering datasets show that our proposed approach outperforms competing approaches. Additionally, we perform a thorough analysis on robustness. Finally, we provide insights on how different typos affect the robustness of embeddings differently and how our method alleviates the effect of some typos but not of others.",
            "score": 0.5905129530333314,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "253157959",
            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
            "text": "Dense passage retrieval has drown much attention recently due to its benefits to a wide range of downstreaming applications, such as open-domain question answering (Karpukhin et al., 2020;Qu et al., 2021;Zhu et al., 2021), conversational systems (Yu et al., 2021) and web search (Lin et al., 2021;Fan et al., 2021;Long et al., 2022). To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair. \n\nBased on the dual-encoder architecture, various optimization methods have been proposed recently, including hard negative training examples mining (Xiong et al., 2021), optimized PTMs specially designed for dense retrieval (Gao andCallan, 2021, 2022;Ma et al., 2022) and alternative text representation methods or fine-tuning strategies (Karpukhin et al., 2020;Zhang et al., 2022aZhang et al., , 2021)). In this paper, we focus on studying the part of pre-trained language model. We observe that the widely adopted random token masking MLM pre-training objective is sub-optimal for dense passage retrieval task. Referring to previous studies, introducing the weight of each term (or token) to assist in estimating the query-passage relevance is effective in both passage retrieval and ranking stages (Dai and Callan, 2020;Ma et al., 2021;Wu et al., 2022). However, the random masking strategy does not distinguish the term importance of tokens. Further, we find that about 40% of the masked tokens produced by the 15% random masking method are stop-words or punctuation 1 . Nonetheless, the effect of these tokens on passage retrieval is extremely limited (Fawcett et al., 2020). Therefore, we infer that LMs pre-trained with random token masking MLM objective is sub-optimal for dense passage retrieval due to its shortcoming in distinguishing token importance.",
            "score": 0.5890499555128104,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2050
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 188,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 188,
                    "end": 204,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 246,
                    "end": 263,
                    "matchedPaperCorpusId": "234343311"
                },
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1025,
                    "end": 1049,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1049,
                    "end": 1068,
                    "matchedPaperCorpusId": "247476289"
                },
                {
                    "start": 1488,
                    "end": 1510,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 1510,
                    "end": 1526,
                    "matchedPaperCorpusId": "233307194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "258107796",
            "title": "Generator-Retriever-Generator Approach for Open-Domain Question Answering",
            "text": "The Retriever-Reader approach is based on the idea of combining information retrieval (retriever) and machine reading comprehension (reader) techniques. Previous work in this area includes the use of document retrieval techniques such as TF-IDF, BM25, or neural ranking models [26,33] to select relevant documents from a large corpus. mNotable works include the Stanford Question Answering Dataset (SQuAD) and subsequent advancements in retriever-reader architectures like DrQA and BiDAF [35]. Dense Passage Retrieval (DPR) [13] focuses on dense representations for passage retrieval, utilizing a dual-encoder architecture to retrieve passages and a reader model to extract the answer. T5-RC [29], a variant of the T5 model, follows the Retriever-Reader approach by retrieving relevant passages and applying T5 as a reader for answer extraction.",
            "score": 0.5888945261823904,
            "section_title": "Retriever Reader",
            "char_start_offset": 5205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 845
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 281,
                    "matchedPaperCorpusId": "53702508"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 692,
                    "end": 696,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "258236093",
            "title": "CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval",
            "text": "Given a large passage collection {p 1 , p 2 , ..., p n } \u2208 P , passage retrieval tasks aim to find relevant passages of a given query q based on their similarities. \n\nDual-encoder is a typical retrieval architecture for passage retrieval. It often employs a PLMbased model as a backbone encoder for encoding representations. Formally, given a sequence of tokenized input queries and passages T. \n\nThe input texts are forwarded through L-layers (l \u2208 {1, ..., L}) Transformers Blocks of PLM encoder. We denote the output hidden states as follows. \n\nHidden states at [CLS] positions of the last layers are commonly used as the dense representations of Existing works, eg. SPLADE (Formal et al., 2021b,a) and LexMAE (Shen et al., 2022), also focus on learning PLM-based dual-encoders for capturing bag-of-words semantic sparse representations. Sparse representations are focusing on encoding semantics at the token level. The hidden states of the last layer h last are firstly projected to vocab spaces with the transposed embedding matrix E . Then ReLU and log saturation operations are performed to obtain the sparse representations. \n\n) here i \u2208 {1, ..., N } denotes the sequence length spaces. ReLU keeps non-negative hidden states as token frequencies produced by PLM. And log saturation suppresses the dominant frequencies for keeping sparsity. The above operations are common practices for pooling sparse representations.",
            "score": 0.5880966762405406,
            "section_title": "Passage Retrieval with Dual-Encoders",
            "char_start_offset": 5082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39453125
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "The dual-encoder architecture and lightweight dot product evaluation enable us to encode and index all passages in the collection C beforehand, so we only need to encode the given query for online retrieval, achieving more efficiency. \n\nLearning Framework for Retriever. To train the dual-encoder  , we utilize contrastive learning following previous works [15,49]. Specifically, with a given query , a labeled positive passage  + , and negative passages N , contrastive loss can be applied to optimize the dual-encoder  by maximizing the relevance of the  and  + while minimizing that of  and  \u2208 N , i.e., \n\nwhere negative passage set N can be generated from top-ranked non-answer passages in retrieval results of BM25 model [34] or a trained retrievers [52,53], i.e., \n\nwhere  is a probability distribution over C, which can be defined as non-parametric (e.g.,  samp = \u2298) or parametric (e.g.,  samp \u2260 \u2298). \n\nDense & Lexicon-Aware Retrievers. Both dense retriever ( den ) and lexicon-aware retriever ( lex ) follow dual-encoder architecture and the encoders are built upon PLMs like BERT [9]. Precisely, a PLM ( plm ) encodes a given text (i.e., query  or passage ),  = { 1 ,  2 , ...  }, to contextualized embeddings, i.e., \n\neventually \n\n[CLS] and [SEP] are special tokens designed for sentence representation and separation by recent PLMs [9,30]. Dense retriever [35,49] represents text by using the embedding of special token [CLS] (i.e.,   \n\n[CLS] ) as follows, \n\nwhere  den =  plm with no additional parameters. For lexicon-aware retriever, we adopt SPLADE [10] which learns to predict the weights of terms in PLM vocab for each token in the input  by the Masked Language Modeling (MLM) layer and sparse regularization, then max-pooling these weights into a discrete text representation after log-saturation.",
            "score": 0.5880774701934744,
            "section_title": "R (\ud835\udc5e, \ud835\udc5d",
            "char_start_offset": 10105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 759,
                    "end": 762,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1088,
                    "end": 1091,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1342,
                    "end": 1345,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1370,
                    "end": 1373,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41259765625
        },
        {
            "corpus_id": "248496043",
            "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers",
            "text": "Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels [26]. Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers [10,17,18,23,24,28,30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" [32], and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value [26]. \n\nOn the other hand, learned sparse models [2,5,6,8,9,14,19,22,31], also strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformerbased deep language model re-rankers [21]. They use transformerbased language models to learn term weights, and achieve comparable effectiveness to dense retrievers.",
            "score": 0.5878776025912427,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 307,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 974,
                    "end": 977,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 977,
                    "end": 980,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "237366133"
                },
                {
                    "start": 1812,
                    "end": 1814,
                    "matchedPaperCorpusId": "218521094"
                },
                {
                    "start": 1816,
                    "end": 1818,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1821,
                    "end": 1824,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5029296875
        },
        {
            "corpus_id": "265457282",
            "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
            "text": "In contrast to traditional IR methods, such as BM25 (Robertson and Zaragoza, 2009), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings: \n\nwhere E \u2022 (\u2022; \u03b8) is an encoder parameterised by \u03b8. \n\nThe adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2021) can be performed to fetch the top-k passages. Contrastive learning is applied to train the dualencoder. Given a query q, we have a positive passage p + and a set of n negative passages P \u2212 q = {p \u2212 i } n i=1 . The model is being optimised by minimising the negative log likelihood of the positive passage: \n\nP \u2212 q is the set of irrelevant passages constructed from in-batch negatives (Chen et al., 2020) (i.e. positive passages of other queries in the same minibatch) and mined hard negatives from existing retrievers (Karpukhin et al., 2020;Xiong et al., 2021).",
            "score": 0.5852308426801376,
            "section_title": "Dense Passage Retrieval",
            "char_start_offset": 5840,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 82,
                    "matchedPaperCorpusId": "202786778"
                },
                {
                    "start": 348,
                    "end": 372,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1264,
                    "end": 1283,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1398,
                    "end": 1422,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1422,
                    "end": 1441,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "258947148",
            "title": "Multiview Identifiers Enhanced Generative Retrieval",
            "text": "The retrieval performance on NQ and TriviaQA is summarized in Table 1. By jointly analyzing the results, we gained the following findings.\n\n(1) Among the generative retrieval methods, MINDER achieves the best performance. We found that SEAL which takes natural identifiers surpasses DSI based on numeric identifiers. This is because numeric identifiers lack semantic information and DSI requires the model to memorize the mapping from passages to their numeric IDs. As such, it becomes more challenging for DSI on the NQ and TriviaQA datasets with more than 20 million passages. Despite the superiority of SEAL, MINDER still outperforms it. Specifically, the improvements in terms of hits@5 are 4.5% and 1.6% on NQ and TriviaQA, respectively. This verifies the effectiveness of our proposed multiview identifiers, which could rank passages from different perspectives.\n\n(2) On NQ, MINDER achieves the best performance in terms of hits@100 and the second-best results in terms of hits@5, 20. However, generative retrieval methods, including MINDER, perform worse than dual-encoder approaches on TriviaQA. Generative retrieval methods rely on the identifiers to represent passages, and cannot \"see\" the content of the passage. Although the QG module in Methods Natural Questions TriviaQA @5 @20 @100 @5 @20 @100  Table 1: Retrieval performance on NQ and TriviaQA. We use hits@5, @20, and @100, to evaluate the retrieval performance. Inapplicable results are marked by \"-\". The best results in each group are marked in Bold, while the second-best ones are underlined. \u2020 denotes the best result in generative retrieval.  our work generates pseudo-queries based on a passage's content, the autoregressive language model AM still cannot directly \"see\" the original content of the passage. Besides, autoregressive generation has the error accumulation problem. These are the disadvantages of generative retrieval and why it may not perform as well as dense retrievers in some scenarios.",
            "score": 0.5847878492677512,
            "section_title": "Retrieval Results on QA",
            "char_start_offset": 18728,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "254564747",
            "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
            "text": "The goal of text retrieval task is to find relevant passages given a query. There are two categories of methods in text retrieval task, which include sparse retrieval methods and dense retrieval methods. Sparse retrieval methods aim to identify relevant passages based on text matching. For example, BM25 is a bag-of-words retrieval function that ranks a set of passages based on the query terms appearing in each passage. Though sparse retrieval has high efficiency, it cannot identify semantically relevant but text-independent querypassage pairs. As the contrary, dense retrieval methods [27,28,58,62] can identify semantically relevant but text-independent query-passage pairs. The query and passage representations in dense retrieval models are usually obtained using dual encoders [20]. [21] pushes performance higher by introducing the late interaction while requiring higher computation and storage costs. In spite of the model structure, recent work also focuses on utilizing training strategies to obtain better results, ranging from data-centric studies [41,46] and negative sampling [10,53,61] to distillation [16,30,57]. [41] proposes a dense retrieval approach that incorporates both querycentric and passage-centric similarity relations. [17] proposes an efficient topic-aware query and balanced margin sampling technique, for training dense retrieval models. [57] proposes a curriculum learning-based optimization framework, for enhancing dense retrieval models through knowledge distillation. [30] introduces on-the-fly distillation and cascade distillation processes for crossarchitecture knowledge distillation in dense retrieval. \n\nOur work is in line with the distillation methods, where LEAD can be a better alternative to the widely used response-based and feature-based methods.",
            "score": 0.5798546252131311,
            "section_title": "RELATED WORK 2.1 Dense Retrieval",
            "char_start_offset": 5417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1802
                }
            ],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 598,
                    "end": 601,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1099,
                    "end": 1102,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1129,
                    "end": 1132,
                    "matchedPaperCorpusId": "248426770"
                },
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1375,
                    "end": 1379,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.",
            "score": 0.579278477654033,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63916015625
        },
        {
            "corpus_id": "257970928",
            "title": "SS-BERT: A Semantic Information Selecting Approach for Open-Domain Question Answering",
            "text": "Retriever methods are mainly divided into sparse retrievers and dense retrievers. In recent work, dense retrievers (e.g., [10][11][12]) outperformed traditional sparse retrievers (e.g., TF-IDF and BM25). They usually represent queries and passages as vectors in lowdimensional vector space (compared with the dictionary size) and calculate their similarity in terms of the dot product. \n\nRetrieval models usually use dual encoders or cross-encoders, which are representation based and interaction based, respectively. Furthermore, the late interaction encoder is representation-interaction based (e.g., [13]). The three methods are shown in Figure 1. \n\nDual encoders use two encoders to represent queries and passages, respectively (e.g., [10]). Cross-encoders use one encoder to jointly represent queries and passages (e.g., [14][15][16]). The cross-encoder captures the semantic relationships between queries and passages more precisely than the dual encoder, but it requires much more computational resources than the dual encoder, and it is usually used as the passage reranker. Retriever Methods. Among the methods, dual-encoder is more efficient, cross-encoder is more accurate, and late-interaction looks for a balance of efficiency and accuracy.",
            "score": 0.5782885821540331,
            "section_title": "Retriever Methods",
            "char_start_offset": 4877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 126,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 130,
                    "end": 134,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 603,
                    "end": 607,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "61248440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49169921875
        },
        {
            "corpus_id": "236477844",
            "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval",
            "text": "Recently, dense passage retrieval has demonstrated better performance than traditional sparse retrieval methods (e.g., TF-IDF and BM25). Different from sparse retrieval, dense passage retrieval represents queries and passages into lowdimensional vectors (Guu et al., 2020;Karpukhin et al., 2020), typically in a dual-encoder architecture, and uses dot product as the similarity measurement for retrieval. The existing approaches for dense passage retrieval can be divided into two categories: (1) unsupervised pre-training for retrieval (2) fine-tuning only on labeled data. In the first category, different pre-training tasks for retrieval were proposed.  proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly finetuned the retriever and a reader on labeled data. REALM (Guu et al., 2020) proposed a new pretraining approach, which jointly trained a masked language model and a neural retriever. Different from them, our proposed approach utilizes the pseudo-labeled data via knowledge distillation in the pre-training stage, and the quality of the generated data is high (see Section 4.6).\n\nIn the second category, the existing approaches fine-tuned pre-trained language models on labeled data (Karpukhin et al., 2020;Luan et al., 2021). Both DPR (Karpukhin et al., 2020) and ME-BERT (Luan et al., 2021) used in-batch random sampling and hard negative sampling by BM25, while ANCE (Xiong et al., 2020a), NPRINC (Lu et al., 2020) and RocketQA (Qu et al., 2020) explored more sophisticated hard negative sampling approach. Izacard and Grave (2020) and Yang et al. (2020) leveraged a reader and a crossencoder for knowledge distillation on labeled data, respectively. RocketQA found large batch size can significantly improve the retrieval performance of dual-encoders. ColBERT (Khattab and Zaharia, 2020) incorporated light-weight attention-based re-ranking while increasing the space complexity.\n\nThe existing studies mainly focus on",
            "score": 0.578148247728193,
            "section_title": "Related Work",
            "char_start_offset": 5531,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 295,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1278,
                    "end": 1302,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1331,
                    "end": 1355,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1634,
                    "end": 1652,
                    "matchedPaperCorpusId": "165163737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "278165747",
            "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism",
            "text": "We previously showed that both Transformer-and SSM-based language models rely on a small number of heads to implement a two-stage Gather-and-Aggregate mechanism that supports in-context retrieval. This section extends that analysis to a broader range of benchmarks and model families, showing that differences in retrieval performance-and the gap between SSMs and Transformers in particular-often stem from specific algorithmic limitations, rather than from general deficiencies in language modeling ability. \n\nWe approach this in three stages: first, we isolate and characterize G&A heads across architectures; second, we quantify how architectural differences in G&A implementation affect task performance; and finally, we show how these differences are amplified or diminished by variations in task format.",
            "score": 0.5770476167402943,
            "section_title": "Transformer-SSM Gap Beyond MMLU",
            "char_start_offset": 47151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 809
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.195556640625
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Recent works have investigated the use of autoregressive language models to generate identifier strings for documents as an intermediate target for retrieval (Yu et al., 2022), such as Wikipedia page titles (De Cao et al., 2020), root-to-leaf paths in a hierarchical cluster tree (Tay et al., 2022), or distinctive n-grams that can be mapped to full passages (Bevilacqua et al., 2022)  index (DSI), and neural document indexers (NDI). \n\nCompared to traditional dense document retrievers, these methods leverage a generation model to produce the document indexes. By forcing the generation model to explain every token in the question and document using cross-attention, the generation abilities of the model significantly improve. \n\nOur work is closely related to these works, showing experimentally that properly prompting pretrained large language models can achieve better performance than traditional dense retrieval models (Ouyang et al., 2022;Yu et al., 2023) .",
            "score": 0.5768242728801356,
            "section_title": "Autoregressive Search Engines",
            "char_start_offset": 5642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 967
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 228,
                    "matchedPaperCorpusId": "222125277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34765625
        },
        {
            "corpus_id": "256461130",
            "title": "Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples",
            "text": "Passage retrieval, which matches relevant passages to queries, is an important task in Information Retrieval (IR). It can be also integrated as a core component to solve many Natural Language Processing (NLP) problems, e.g., open domain question answering (Chen et al., 2017), fact checking (Thorne et al., 2018), etc. Powered by large scale pretrained language models (e.g. BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2020)), dense passage retrieval, which explores dense vector representations to match relevant passages, has attracted growing interest (Gao et al., 2020;Khattab and Zaharia, 2020;Karpukhin et al., 2020;MacAvaney et al., 2020a;Qu et al., 2021;Zhan et al., 2021;Xu et al., 2022;Gao et al., 2021a). Dense passage retrieval models usually adopt a bi-encoder (also known as dual-encoder) architecture, one encoder for encoding queries, the other for learning passage representations, which can be computed offline. With dense representations of a query and document, passage ranking is recast as a nearest neighbor search problem that can be efficiently solved by similarity search toolkits tailored for dense vectors, such as Faiss (Johnson et al., 2019). \n\nDense passage retrieval models are usually trained via Contrastive Learning (CL), which encourages query representations to be close to positive (i.e., relevant) passages and away from negative (i.e., irrelevant) passages in the learned semantic space. Many previous studies demonstrate the effectiveness of CL in dense passage retrieval (Karpukhin et al., 2020;Qu et al., 2021;Xu et al., 2022). Under this CL-based dense passage retrieval setting, using more negative samples has proved beneficial to models (Wu et al., 2020;Chen et al., 2020;He et al., 2020;Giorgi et al., 2021;Gao et al., 2021c).",
            "score": 0.5767515383244858,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 380,
                    "end": 401,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 438,
                    "end": 459,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "222205492"
                },
                {
                    "start": 608,
                    "end": 634,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 634,
                    "end": 657,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 657,
                    "end": 681,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 681,
                    "end": 697,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 715,
                    "end": 731,
                    "matchedPaperCorpusId": "247411106"
                },
                {
                    "start": 731,
                    "end": 749,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1183,
                    "end": 1205,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1547,
                    "end": 1571,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1571,
                    "end": 1587,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1587,
                    "end": 1603,
                    "matchedPaperCorpusId": "247411106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "259991673",
            "title": "Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval",
            "text": "Dense retrieval has become a core component within several downstream NLP and web-search tasks such as question-answering [1,2], semantic similarity [3], conversational search [4], entity retrieval [5], fact-checking [6] and passage retrieval [7,8,9,10,11]. In dense retrieval, dual-encoder models encode semantically correlated queries and documents and represent them as spatially close embeddings, i.e., dense vector representations. Dense retrieval can be efficiently conducted via approximate nearest neighbor (ANN) search at inference [12]. \n\nDual-encoder models encode and store document embeddings within an index, which starts to become bulky, once the documents start to increase within the corpus. It becomes expensive to serve dense retrievers practically. For example, storing 21M (million) passages within a dense index requires about 65 GB of memory [13]. As nearest-neighbor (exact) search in large vector spaces is rather slow, ANN techniques such as HNSW [14] are popularly used, which results in even higher memory requirements such as 150 GB [13]. As a result, for searching on a very large corpus, we require a high-memory and expensive machine to host the bulky index, which hinders the practical application of dense retrieval. \n\nAware of the issue, there have been recent efforts to improve the efficiency of dense retrieval models by compressing the vector representation to lower memory, i.e., space requirements. Traditionally, out-of-the-box unsupervised strategies such as dimension reduction with principal component analysis (PCA) or product quantization (PQ) [15] have been popular for retrieval [16,17]. More recently, supervised learning-to-hash (LTH) techniques have been introduced such as BPR [13] and JPQ [18]. LTH techniques map the original dense embeddings into a Hamming space (binary space) and obtain the low-dimensional binary codes. Unfortunately, all existing vector compression methods are proposed under the single-domain retrieval assumption. However, in practice, many retrieval applications span across various domains with diverse distributions; often with scarce training data [19].",
            "score": 0.5747249607672064,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 125,
                    "end": 127,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "231779510"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "9896397"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 1632,
                    "end": 1635,
                    "matchedPaperCorpusId": "239993003"
                },
                {
                    "start": 1730,
                    "end": 1734,
                    "matchedPaperCorpusId": "235293983"
                },
                {
                    "start": 1743,
                    "end": 1747,
                    "matchedPaperCorpusId": "236772078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "268723791",
            "title": "Scaling Laws For Dense Retrieval",
            "text": "We now briefly revisit prior studies in the field of dense retrieval.The training data for dense retrieval tasks typically comprises annotated pairs, each consisting of a query and a human-labeled relevant passage.Early research primarily concentrated on effective negative sampling strategies used for dense retrieval training, such as employing random passages or the top irrelevant passages retrieved by BM25 as negative samples [23].ANCE [53] utilized self-mined hard negatives and substantially improved the retrieval performance.Furthermore, Zhan et al. [56] proposed dynamic hard negatives to further enhance both training efficiency and retrieval effectiveness.RocketQA [39] and TAS-B [19] introduced knowledge distillation, utilizing a well-trained cross-encoder model to generate soft labels for training pairs.Beyond the design of finetuning methods, researchers also explore other techniques, such as pretraining methods and multivector retrieval.(1) Pretraining studies design objectives that are similar to the retrieval tasks.For example, Condenser [13] and coCondenser [14] use the Sequence Contrastive Learning task to improve the representational capability.RetroMAE [51] leverages an encoder-decoder architecture, wherein a shallow decoder encourages the encoder to produce higher-quality representations.Contriever [20] pre-trains dense retrieval models with Inverse Cloze Task and the Independent Cropping Task.(2) Since the single vector representation in dense retrieval could become a limitation, various studies have explored more complex scoring techniques.ME-BERT [28] introduces multi-vector representations to enable more precise retrieval of long documents.ColBERT [24,45] investigates token-level vector representations and aggregates scores using a late-interaction mechanism.Other researchers attempt to expand the vector dimension to vocabulary size [11,12].This expansion allows dense retrieval models to directly generate term weights, facilitating retrieval similar to sparse models.\n\nPrior explorations of dense retrieval models mainly focus on techniques with a static setup, such as a certain model size, certain data size, etc.Instead, we employ a dynamic setup and explore how model perform when the model size and data size are scaled.",
            "score": 0.5720966429762143,
            "section_title": "Dense Retrieval",
            "char_start_offset": 8243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1432
                },
                {
                    "start": 1432,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1687
                },
                {
                    "start": 1687,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2020
                },
                {
                    "start": 2022,
                    "end": 2168
                },
                {
                    "start": 2168,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 436,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1335,
                    "end": 1339,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1699,
                    "end": 1702,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1888,
                    "end": 1891,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26513671875
        },
        {
            "corpus_id": "277780286",
            "title": "MURR: Model Updating with Regularized Replay for Searching a Document Stream",
            "text": "Neural retrieval models are generally based on a pretrained language model, such as BERT [4] or RoBERTa [24], and further fine-tuned with a retrieval objective, such as contrastive [2,31] or distillation loss [6,23,34], on a set of queries and documents, such as MS MARCO [28]. For dual encoders, because of their architecture, queries and documents can be processed separately and only require a lightweight interaction, such as vector dot product [7,18] or token level MaxSim [19]; documents can thus be preprocessed and indexed for supporting fast searches [17,33]. How well the language model is tuned for the specific query and document language distributions is crucial for designing an effective dense retrieval model [20]. \n\nSince proper training data is difficult to collect and the fine-tuning process is also computationally expensive, prior work in ad hoc retrieval typically aims for a one-size-fits-all general-purpose model and evaluates its robustness on various retrieval domains and tasks [6,29,38]. This perspective drove the creation of the BEIR benchmark [36]. Work on domain adaptation either aims for better zeroshot ability [3,35,41], which is the same as developing a general-purpose model, or it exploits additional unsupervised or semi-supervised fine-tuning [9,14,37]. However, with a stream of queries and documents arriving, even if one can afford to continuously fine-tune the retrieval model, the risk of catastrophic forgetting increases [12,25], which destroys the purpose of adaptation. \n\nAs mentioned in the previous section, statistical sparse retrieval models such as BM25 are capable of incrementally updating the index and related statistics with some careful lexical feature selection to account for the language drift over time [5,26]. Early work around TREC filtering task, which dates back to TREC 4 [13], also requires monitoring the lexical differences over time for serving standing queries [1]. In contrast, there is little work on neural dual encoders in such a streaming scenario.",
            "score": 0.5707829357811328,
            "section_title": "Background and Related Work",
            "char_start_offset": 4261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "252545066"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 478,
                    "end": 482,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "256231186"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "248665596"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1148,
                    "end": 1151,
                    "matchedPaperCorpusId": "246285831"
                },
                {
                    "start": 1154,
                    "end": 1157,
                    "matchedPaperCorpusId": "264146620"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "matchedPaperCorpusId": "235792454"
                },
                {
                    "start": 1289,
                    "end": 1292,
                    "matchedPaperCorpusId": "259360702"
                },
                {
                    "start": 1475,
                    "end": 1478,
                    "matchedPaperCorpusId": "231632962"
                },
                {
                    "start": 1770,
                    "end": 1773,
                    "matchedPaperCorpusId": "225684248"
                },
                {
                    "start": 1773,
                    "end": 1776,
                    "matchedPaperCorpusId": "53246679"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "14902533"
                },
                {
                    "start": 1938,
                    "end": 1941,
                    "matchedPaperCorpusId": "15643568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330078125
        },
        {
            "corpus_id": "233295975",
            "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval",
            "text": "There has been growing interest in passage retrieval where a model is given a natural language question and retrieves a set of relevant passages from a large text corpus (Ramos et al., 2003;Liu, 2011;. This is an important problem both as an end application where the retrieval results are presented to users (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002), and for downstream tasks such as question answering where the retrieval results are consumed by another model (Chen et al., 2017).\n\nQuestions posed by humans are often openended and ambiguous, leading to multiple valid answers . Given a question in Figure 1, \"What was Eli Whitney's job?\", an ideal retrieval system should provide passages covering all professions of Eli Whitney. This introduces * Work done while interning at Google.\n\nEli Whitney was an American inventor, widely known for inventing the cotton gin \u2026 Whitney worked as a farm laborer and school teacher to save money.\n\nQ: What was Eli Whitney's job?\n\nRetrieval System Figure 1: The problem of multi-answer retrieval. A retrieval system must retrieve a set of k passages (k = 5 in the figure) which has maximal coverage of the answers to the input question. This requires modeling the joint probability of the passages in the output set: P (p 1 ...p k |q). Our proposed model JPR achieves this by employing an autoregressive model. the problem of multi-answer retrieval-retrieval of multiple passages with maximal coverage of all distinct answers-which is a challenging, yet understudied problem. Most prior work focuses on single-answer retrieval where independent scoring of each passage is sufficient. Multi-answer retrieval intrinsically requires a different approach as it requires joint modeling of selected passages.\n\nIn this work, we study the multi-answer retrieval problem and introduce JPR, a new model for Joint Passage Retrieval. JPR consists of a sequence-tosequence reranker, receiving a set of candidate passages and autoregressively generating a sequence of passage references. The autoregressive behavior of the model can capture the dependencies between passages in the output set. We employ a training method that encourages multiple ways to reach the multi-answer supervision, and introduce a new tree-decoding algorithm for the autoregressive model, enabling flexibility in handling the",
            "score": 0.5700022335953131,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 190,
                    "end": 200,
                    "matchedPaperCorpusId": "28826624"
                },
                {
                    "start": 452,
                    "end": 471,
                    "matchedPaperCorpusId": "3618568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3125
        },
        {
            "corpus_id": "267751308",
            "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
            "text": "Dense retrieval. Dense retrieval (Lee, Chang, and Toutanova 2019;Karpukhin et al. 2020;Xiong et al. 2020;Khattab and Zaharia 2020) shows tremendous success in many downstream tasks (e.g. open-domain QA and web search) compared with the traditional sparse retrieval models (e.g. TF-IDF and BM25). The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dualencoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model finegrained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance. Besides, knowledge distillation has become a vital technique to enhance the capacity of the dual-encoder by distilling knowledge from a more powerful reader to a classical retriever (Qu et al. 2020;Ren et al. 2021b;Lin, Yang, and Lin 2020;Hofst\u00e4tter et al. 2021). \n\nRecently, massive works have investigated task-related pretraining methods for dense retrieval models (Gao and Callan 2021a,b;Wang, Reimers, and Gurevych 2021;Ren et al. 2021a;Oguz et al. 2021;Meng et al. 2021). Condenser (Gao and Callan 2021a) proposed the Condenser architecture to enforce the late backbone layers to aggregate the whole information. coCondenser (Gao and Callan 2021b) leveraged contrastive learning to incorporate a query-agnostic contrastive loss. PAIR (Ren et al. 2021a) and DPR-PAQ (Oguz et al. 2021) also designed special tasks in pre-training to enhance retrieval models. Additionally, jointly training retrieval models with the rerank model can bring about better performance.",
            "score": 0.5689989780820396,
            "section_title": "Related Work",
            "char_start_offset": 6374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 65,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 65,
                    "end": 87,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 557,
                    "end": 589,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 868,
                    "end": 886,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1146,
                    "end": 1162,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1203,
                    "end": 1226,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1389,
                    "end": 1406,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1704,
                    "end": 1721,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "265457188",
            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
            "text": "Remarkable progress has been achieved in neural information retrieval through the adoption of the dual-encoder paradigm (Gillick et al., 2018), which enables efficient search over vast collections of passages by factorising the model such that the encoding of queries and passages are decoupled, and calculating the query-passage similarity using dot product. However, the efficacy of training dualencoders heavily relies on the quality of labelled data, and these models struggle to maintain competitive performance on retrieval tasks where dedicated training data is scarce (Thakur et al., 2021). \n\nVarious approaches have been proposed to enhance dense retrievers (Karpukhin et al., 2020) in zero-shot settings while maintaining the factorised dual-encoder structure, such as pre-training models on web-scale corpus (Izacard et al., 2022) and learning from cross-encoders through distillation (Qu et al., 2021). Other alternatives seek to trade efficiency for performance by using complex model architectures, such as fine-grained token interaction for more expressive representations (Santhanam et al., 2022) and scaling up the model size for better model capacity (Ni et al., 2022). Another line of work trains customised dense retrievers on target domains through query generation (Wang et al., 2022;Dai et al., 2023). This training paradigm is generally slow and expensive, as it employs large language models to synthesise a substantial number of high-quality queries. \n\nIn this paper, we present ABEL, an Alternating Bootstrapping training framework for unsupervised dense rEtrievaL. Our method alternates the distillation process between a dense retriever and a reranker by switching their roles as teachers and students in iterations. On the one hand, the dense retriever allows for efficient retrieval due to its factorised encoding, accompanied by a compromised model performance. On the other hand, a reranker has no factorisation constraint, allowing for more fine-grained and accurate scoring, but at the cost of intractable searches. Our work aims to take advantage of both schools by equipping the dense retriever with accurate scoring by the reranker while maintaining search efficiency.",
            "score": 0.5656808119085912,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 576,
                    "end": 597,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 667,
                    "end": 691,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 819,
                    "end": 841,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 896,
                    "end": 913,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1088,
                    "end": 1112,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1169,
                    "end": 1186,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1287,
                    "end": 1306,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1306,
                    "end": 1323,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.734375
        },
        {
            "corpus_id": "258865354",
            "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
            "text": "Naive approaches are either computationally infeasible (scoring all possible queries), or rely on sampling a small, incomplete set of samples (such as in Lewis et al. [22]). \n\nTo overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14,26]. We call the retrieval system based on this proposed model nail (Non-Autoregressive Indexing with Language models). \n\nWe summarize our contributions as follows: \n\n(1) We advance prior work on learned sparse retrieval by leveraging pretrained encoder-decoder LMs with a novel nonautoregressive decoder. (2) We describe a range of experiments using the BEIR benchmark [39] that explore the performance and efficiency of our model as a reranker and as a retriever compared with a variety of existing systems. As a reranker, nail can recover 86% of the performance of a large cross-attention reranker [27], while requiring 10 \u22126 % of the inference-time FLOPS per query. As a retriever, nail has an extremely high upper bound for recall-exceeding the performance of all other retrievers in the zero-shot setting. Finally, by using BM25 as a retriever and nail as a reranker, we can match state-of-the-art dual-encoders [14,26] with 10 \u22124 % of the inference-time FLOPS. (3) We propose our model as a preferred solution when significant compute is available at indexing time, but not ondemand at serving time, and we provide a cost analysis that illustrates when our approach could be preferred to previous work that harnesses LLMs.",
            "score": 0.5653598505408166,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1599,
                    "end": 1602,
                    "matchedPaperCorpusId": "208229926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "However, we intend to investigate a different research question: Can we model query-passage interaction without any efficiency degeneration? Noting that the dual-encoders are efficient due to its offline pre-computation of passage vectors, the key to answer this question is how to pre-compute and model query-passage interactions offline. However, this is challenging because the actual queries issued by users are agnostic during the pre-computation, while we can only access to the passages in the corpus. In the next section, we propose a novel method, namely I 3 retriever, which tackles this challenge to achieve high effectiveness without hurting efficiency.",
            "score": 0.5645378155824028,
            "section_title": "PLM-based Retriever and Reranker",
            "char_start_offset": 17118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 665
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308349609375
        },
        {
            "corpus_id": "249926985",
            "title": "Questions Are All You Need to Train a Dense Passage Retriever",
            "text": "Our work is based on training a dense retriever using pre-trained language models (PLMs), which we have covered in previous sections. Here, we instead focus on other related approaches. \n\nA popular method to train the dual-encoder retriever is to optimize contrastive loss using inbatch negatives (Gillick et al., 2019) and hardnegatives (Karpukhin et al., 2020;Xiong et al., 2021). Alternatives to using hard-negatives such as sampling from cached evidence embeddings have also shown to work well in practice (Lindgren et al., 2021). Multi-vector encoders for questions and passages are more accurate than dualencoders, (Luan et al., 2021;Khattab and Zaharia, 2020;Humeau et al., 2020), although at the cost of an increased latency and storage requirements. \n\nPLMs have been shown to improve passage rankings as they can perform cross-attention between the question and the retrieved passages (Lin et al., 2021). Supervised approaches to rerank either finetune PLMs using question-passage pairs (Nogueira et al., 2020) or finetune PLMs to generate question conditioned on the passage (Nogueira dos Santos et al., 2020) while unsupervised re-rankers are based on zero-shot question scoring (Sachan et al., 2022). The re-ranking process is slow due to the cross-attention step and is bottlenecked by the accuracy of first-stage retrievers. To address these limitations, crossattention distillation approaches from the PLM to retriever have been proposed (Qu et al., 2021). Such distillation can be performed either in a single end-to-end training step (Guu et al., 2020;Sachan et al., 2021b) or in a multi-stage process (Khattab et al., 2021;Izacard and Grave, 2021). \n\nAn alternative approach to using PLMs is to generate data that can aid retrieval. The data can be either the title or an answer that provides more information about the question (Mao et al., 2021).",
            "score": 0.5644601783322727,
            "section_title": "Related Work",
            "char_start_offset": 29201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 319,
                    "matchedPaperCorpusId": "202718954"
                },
                {
                    "start": 338,
                    "end": 362,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 510,
                    "end": 533,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 621,
                    "end": 640,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 640,
                    "end": 666,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 666,
                    "end": 686,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 894,
                    "end": 912,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 996,
                    "end": 1019,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 1085,
                    "end": 1119,
                    "matchedPaperCorpusId": "222178252"
                },
                {
                    "start": 1190,
                    "end": 1211,
                    "matchedPaperCorpusId": "248218489"
                },
                {
                    "start": 1453,
                    "end": 1470,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1551,
                    "end": 1569,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1569,
                    "end": 1590,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1619,
                    "end": 1641,
                    "matchedPaperCorpusId": "220302658"
                },
                {
                    "start": 1641,
                    "end": 1665,
                    "matchedPaperCorpusId": "227746078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4306640625
        },
        {
            "corpus_id": "272330251",
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "text": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020;Izacard & Grave, 2021;Izacard et al., 2022). The input to a retrieval augmented language model is a natural language text sequence, x. This input text will be encoded using a dual-encoder retrieval model, f (x). Retrieval will be performed over a corpus of targets, Y, returning k targets relevant to x, denoted S x (Y). The reader model takes as input the retrieved targets, S x (Y), and the text x, and generates text. \n\nConcretely, in our experiments, the input text x is a question. The retrieval corpus contains targets y corresponding to passages in Wikipedia. The reader model takes as input the question and retrieved passages and generates a short answer to the question. We present the remainder of the section with this question-answering task in mind. \n\nRLMs can be formalized as latent variable models. The softmax function is used to parameterize the distribution over a discrete latent variable, which corresponds to the retrieved targets. We use a to refer to the generated sequence of text, i.e., the generated answer: \n\nP (a|y, x)P (y|x). \n\n(7) P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y). \n\nWhen training RLMs, we receive supervision in the form of question, answer pairs, e.g., x i , a i \u223c X. We do not receive supervision on which targets S x (Y) should be retrieved. We will learn the parameters of both the reader model and retriever model using these supervised question/answer pairs. \n\nTo train the reader and retriever model, we use perplexity distillation (Izacard et al., 2022) for retriever loss and negative log-likelihood for the reader loss.",
            "score": 0.5642365360722125,
            "section_title": "Latent Variables in Retrieval Augmented Models",
            "char_start_offset": 12356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1887
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 207,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "275119098",
            "title": "On the Robustness of Generative Information Retrieval Models",
            "text": "Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], \n\nto probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance. \n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [51,54], late interaction [20]. Recently, researchers have also investigated pre-training tasks for dense retrieval [12,33]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually come with considerable memory demands and computational overheads.",
            "score": 0.563473326662364,
            "section_title": "Sparse and dense retrieval models",
            "char_start_offset": 3413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1650
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "12255087"
                },
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "6473756"
                },
                {
                    "start": 412,
                    "end": 415,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 649,
                    "end": 652,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 652,
                    "end": 655,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1323,
                    "end": 1327,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1413,
                    "end": 1417,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1417,
                    "end": 1420,
                    "matchedPaperCorpusId": "251718954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.481201171875
        },
        {
            "corpus_id": "254853896",
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "text": "Text retrieval aims to find the relevant documents for a given query from a large collection of documents, playing an indispensable role in opendomain question answering (Chen et al., 2017), fact verification (Thorne et al., 2018) and retrievalaugmented generation (Lewis et al., 2020;He et al., 2022). At the early stage, sparse retrieval methods such as TF-IDF or BM25 dominated passage retrieval by relying mainly on lexical term matching to compute relevance between the query and document. Recently, there has been a surge of research interest in neural network-based dense retrieval (Karpukhin et al., 2020;Xiong et al., 2021). Different from sparse retrieval, dense retrieval resorts to neural encoders to compute the dense representations of the query and document. This enables dense retrieval to infer the relevance between them at the semantic level rather than the surface level, thus circumventing the term mismatch problem suffered by the sparse retrieval models. \n\nIn recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder. \n\nTo enhance the retrieval models' ability to capture interactions between queries and documents while maintaining retrieval efficiency, previous work mainly focuses on generating query-informed document representations. One approach, known as late interaction (Khattab and Zaharia, 2020), involves encoding the query and document independently in the early layers, while the later layers model their interactions. Late interaction combines dual-encoder and cross-encoder, making a trade-off between retrieval efficiency and performance. On the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder.",
            "score": 0.5632799152192681,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 189,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 209,
                    "end": 230,
                    "matchedPaperCorpusId": "53645946"
                },
                {
                    "start": 265,
                    "end": 285,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 589,
                    "end": 613,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 613,
                    "end": 632,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1936,
                    "end": 1963,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "277621440",
            "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
            "text": "Information retrieval (IR) contains two primary stages: retrieval and reranking [13,67]. Retrieval serves as the cornerstone of information retrieval and is critically important. Its objective is to retrieve relevant passages from a large-scale corpus in response to a query, thereby providing candidate passages for the subsequent reranking stage. In the early stages of retrieval, the focus was primarily on lexical matching between the terms of the query and the passage, with methods such as BM25 [34] and query likelihood (QL) model [46]. However, with the advent of pre-trained language models (PLMs) such as BERT [11], representing passages or queries as dense vectors has gradually become the mainstream approach. These methods typically employ two separate encoders to represent the query and passage and are referred to as dual-encoder models. \n\nLarge language models (LLMs) are being widely applied across various fields [27,54,61], and garnering increasing attention for their application in retrieval tasks [25,39,51]. Unlike bidirectional attention mechanisms in encoder-style pre-trained language models (PLMs) such as BERT, LLMs are typically decoder-style models that employ unidirectional attention. The task of next token prediction allows LLMs to ingest large amounts of various types of data and thus gain more powerful semantic understanding capability. However, the unidirectional attention during modeling may lead to insufficient representation of global semantics, which is inferior to encoder-style PLMs. Although leveraging the superior semantic understanding ability of LLMs for retrieval looks appealing, it is challenging to do so. Recent studies have attempted to repeat passages as input during encoding [51], to use a bidirectional attention mechanism for encoding during relevance matching fine-tuning [4], or pre-train LLMs to strengthen the global semantic representation capabilities [33]. \n\nSince LLMs are decoder-based language models (LMs), it is natural to think of adapting them to retrieval according to the classical language modeling approach to IR [46], i.e., modeling the query likelihood (QL) given the LM of a document.",
            "score": 0.562678292192217,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "259076101"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "235366815"
                },
                {
                    "start": 538,
                    "end": 542,
                    "matchedPaperCorpusId": "2759772"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 1922,
                    "end": 1926,
                    "matchedPaperCorpusId": "271885989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.546875
        },
        {
            "corpus_id": "273186680",
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "text": "Let Enc denote the retrieval language model. Given a query q and a document d, each with taskspecific instructions I q and I d , respectively, the embedding vectors are computed as follows: e q = Enc(I q \u2295 q) and e d = Enc(I d \u2295 d). The relevance score sim(q, d) is determined by the cosine similarity between these two embedding vectors. \n\nsim(q, d) = e q .e d \u2225e q \u2225\u2225e d \u2225 \n\n(1) \n\nIn this work, we use both autoregressive and bidirectional language models (Devlin et al., 2019) as our retrieval models. We add two special tokens [CLS] and [EOS] to the list of tokens representing textual input: \n\nwhere t 1 ...t n is the token representation of the input sequence. We use the embedding of the [CLS] token and [EOS] token from the last transformer layer as the vector representation of the input for the bidirectional language model and the autoregressive language model, respectively. \n\nA crucial problem in RAG is how to retrieve relevant documents given a query (Gao et al., 2024), especially in domain-specific tasks where retrieval models can lack information compared to their training data. We leverage a reward model to adapt the retrieval models for different tasks and user preferences effectively. The details of our approach will be introduced in the following sections.",
            "score": 0.5624423433005685,
            "section_title": "DENSE RETRIEVAL IN RAG",
            "char_start_offset": 8769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1283
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2247314453125
        },
        {
            "corpus_id": "231815627",
            "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
            "text": "For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever. Nogueira et al. (2019c) first enriched document content by generating related queries and appending them to the documents, and then indexed the expanded documents for retrieval, which alleviate the term mismatch between questions and passages. Mao et al. (2020) used generation model to expand questions. DeepCT (Dai and Callan, 2019) utilized BERT to learn the term importance (i.e. term weighting) by considering the contexts, which was shown to achieve better retrieval performance than BM25. Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a). Existing approaches can be roughly divided into two categories: (1) pre-training and (2) fine-tuning only.  proposed a specific approach to pre-training the retriever with an unsupervised task, namely Inverse Cloze Task (ICT), and then jointly fine-tuning the retriever and the reader on labeled data. Guu et al. (2020) proposed a new pre-training approach namely REALM, which jointly trained a masked language model and a neural retriever. In contrast, the second class of approaches only finetuned (existing) pre-trained language models (LMs) on labeled data. Our work follows the second class of approaches, which show better performance with less cost.\n\nAlthough the dual-encoder architecture enables the appealing paradigm of dense retrieval, it is difficult to effectively train such a retrieval architecture. As discussed in Section 1, it suffers from a number of challenges, including the training and inference discrepancy, a large number of unlabeled positives and limited training data. Several recent studies Luan et al., 2020;Henderson et al., 2017) tried to address the first challenge by designing complicated sampling mechanism to generate",
            "score": 0.5621662779525336,
            "section_title": "Passage Retrieval for Open-domain QA",
            "char_start_offset": 6095,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 159,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 677,
                    "end": 699,
                    "matchedPaperCorpusId": "162168864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "276724989",
            "title": "TempRetriever: Fusion-based Temporal Dense Passage Retrieval for Time-Sensitive Questions",
            "text": "The research on Temporal Information Retrieval (TIR) [3,4,16,20] emerged in the past as a specialized type of information retrieval, focusing on searching documents with respect to time dimension and temporal signals. While the previous research primarily addressed temporal representation and understanding of queries and documents, there has been limited exploration of integrating temporal information with modern neural network-based retrieval technologies and its application in the context of question answering. Our work seeks to fill that gap by incorporating temporal embeddings and alignment into dense retrieval architectures. Passage retrieval is a critical component in open-domain question answering, as the efficiency and precision of retrieving relevant passages directly impact the overall performance of the system [5,6,12,17]. Both sparse and dense vector models have been developed to enhance passage retrieval. Sparse retrieval methods represent queries and documents as sparse vectors, where each element corresponds to a term in the vocabulary. Dense retrieval methods, on the other hand, encode queries and documents into a continuous vector space, often leveraging pre-trained language models to capture richer semantic representations. \n\nNotable contributions to dense passage retrieval (DPR) include various architectures designed to improve the representation and retrieval of passages in QA tasks [18,21,24,32,45,48]. These models utilize deep learning techniques to encode queries and documents as dense vectors, enabling more effective semantic matching compared to traditional lexical methods. Despite these advancements, dense retrieval models often overlook temporal signals, a key aspect of many time-sensitive information retrieval tasks. \n\nIn the recent years, several studies [8,33,34,44,47,50] have explored the integration of temporal reasoning into language models to address the challenges posed by dynamic information needs. Cole et al. [9] proposed a novel pre-training technique called Temporal Span Masking (TSM), which enhances temporal reasoning by masking and predicting temporal spans such as dates and periods during training. This approach improves the model's ability to understand and reason about temporal relationships. Other works have focused on adapting language models to handle time-sensitive information. For example, Giulianelli et al. [13] analyzed contextualized word embeddings of BERT to track lexical semantic change over time, demonstrating how word meanings shift across different time periods.",
            "score": 0.5611687172417079,
            "section_title": "Related Work",
            "char_start_offset": 4170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2275
                },
                {
                    "start": 2276,
                    "end": 2366
                },
                {
                    "start": 2367,
                    "end": 2564
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 58,
                    "matchedPaperCorpusId": "14176218"
                },
                {
                    "start": 58,
                    "end": 61,
                    "matchedPaperCorpusId": "100776"
                },
                {
                    "start": 61,
                    "end": 64,
                    "matchedPaperCorpusId": "31909930"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 836,
                    "end": 838,
                    "matchedPaperCorpusId": "61248440"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "235669861"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "13865788"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "53035265"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "202558815"
                },
                {
                    "start": 1439,
                    "end": 1442,
                    "matchedPaperCorpusId": "211003735"
                },
                {
                    "start": 1442,
                    "end": 1445,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 1980,
                    "end": 1983,
                    "matchedPaperCorpusId": "257687447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419189453125
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "Pre-trained Language Models (PLMs). Through pre-training, language models can achieve higher performance when transferred to specific tasks. Transformer [27], based on self-attention mechanism, is a mainstream architecture of PLMs, including encoderonly [11,14] model and decoder-only [2,29] model. To address the quadratic time scaling of Transformer architecture, some architectures broadly regarded as state space models have been proposed [8,20], especially high-performing models like sub-quadratic architecture M2-BERT [6], linear architecture Mamba [7] and Mamba-2 [5]. \n\nDense Retrieval Models. Transformer PLMs have been proven effective for dense retrieval. Initially, encoder-only models are adopted for retrieval tasks due to the bi-directional attention mechanisms [10,19]. Later, decoder-only models are adopted for retrieval tasks due to their effectiveness on larger model size [15,17]. \n\nLong-text Dense Retrieval Models. In long-text retrieval, early works use chunking strategies [4] due to the small context window. In order to help the model better understand complete and coherent semantics, some studies explore Transformer-based long-text retrieval models [9,15,30]. \n\nFaced with the quadratic time scaling of Transformer-based longtext retrieval models, the sub-quadratic M2-BERT model [6] has been utilized for long-text retrieval tasks [23].",
            "score": 0.5601649483838567,
            "section_title": "Related Work",
            "char_start_offset": 4479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "264288993"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 1311,
                    "end": 1314,
                    "matchedPaperCorpusId": "264288993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "During training, the model is optimized based on a contrastive learning objective,\n\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question. Although the bi-encoder dense retrieval architecture has achieved impressive results in ODQA, few work has attempted to improve its parameter efficiency. Further, compared to the spare vector space model BM25 (Robertson and Walker, 1994), such bi-encoder dense retrievers sometimes suffer from inferior generalization performance, e.g., when the training data is extremely biased (Lebret et al., 2016; or when there is a distribution shift (Sciavolino et al., 2021). In this paper, we conjecture that the unstable generalization performance is partially related to the sheer number of learnable parameters in the model. Therefore, we develop a task-aware specialization architecture for dense retrieval with parameter sharing between the question and passage encoders, which turns out to improve both parameter efficiency and generalization performance.",
            "score": 0.5599007411128336,
            "section_title": "Background",
            "char_start_offset": 5876,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 861,
                    "end": 882,
                    "matchedPaperCorpusId": "1238927"
                },
                {
                    "start": 921,
                    "end": 946,
                    "matchedPaperCorpusId": "237562875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "Ranker and retriever are two important components in dense passage retrieval. The retriever typically adopts a dual-encoder model, where queries and documents are separately input into two pre-trained models, and the vectors generated by the models are used for similarity calculation. The ranker often uses a cross-encoder model, where the concatenated query-document pairs are input into a pre-trained model to obtain word similarities. However, the dual-encoder model lacks interaction between queries and documents due to its independent encoding, while the cross-encoder model requires substantial computational cost for attention calculation, making it difficult to obtain real-time retrieval results. In this paper, we propose a dense retrieval model called MD2PR based on multi-level distillation. In this model, we distill the knowledge learned from the cross-encoder to the dual-encoder at both the sentence level and word level. Sentence-level distillation enhances the dual-encoder on capturing the themes and emotions of sentences. Word-level distillation improves the dual-encoder in analysis of word semantics and relationships. As a result, the dual-encoder can be used independently for subsequent encoding and retrieval, avoiding the significant computational cost associated with the participation of the cross-encoder. Furthermore, we propose a simple dynamic filtering method, which updates the threshold during multiple training iterations to ensure the effective identification of false negatives and thus obtains a more comprehensive semantic representation space. The experimental results over two standard datasets show our MD2PR outperforms 11 baseline models in terms of MRR and Recall metrics.",
            "score": 0.5588311757638993,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "Dual-encoders learn dense vector representations for queries and documents using contextual models [34,35]. The dense vectors are then indexed in an offline phase [32], where retrieval is akin to performing an approximate nearest neighbor (ANN) search given a vectorized query. This allows dual-encoders to be used for both retrieval and re-ranking. Consequently, there has been a large number of follow-up works that boost the performance of dual-encoder models by improving pre-training [5,20,21,39,82], optimization [23], and negative sampling [68,86,88] techniques, or employing distillation approaches [51,54,90]. Lindgren et al. [53] propose a negative cache that allows for efficient training of dual-encoder models. LED [89] uses a SPLADE model to enrich a dense encoder with lexical information. Lin et al. [50] propose Aggretriever, a dual-encoder model which aggregates and exploits all token representations (instead of only the classification token). In this work, we use dual-encoders for computing semantic similarity between queries and passages. Some approaches have also proposed architectural modifications to the aggregations between the query and passage embeddings [6,27,31]. Nogueira et al. [67] propose a simple document expansion model. We use dual-encoder models to perform efficient semantic re-ranking in our work. \n\nEfficiency improvements of dual-encoder-based ranking and retrieval focus mostly on either inference efficiency of the encoders or memory footprint of the indexes. TILDE [92] and TILDEv2 [91] efficiently re-rank documents using a deep query and document likelihood model instead of a query encoder. The SpaDE model [7] employs a dual document encoder that has a term weighting and term expansion component; it improves inference efficiency by using a vastly simplified query representation. Li et al. [47] employ dynamic lexical routing in order to reduce the number of dot products in the late interaction step. Cohen et al. [8] use auto-encoders to compress document representations into fewer dimensions in order to reduce the overall size.",
            "score": 0.5581367099686657,
            "section_title": "Dual-Encoders",
            "char_start_offset": 9998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2088
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 492,
                    "end": 495,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "235176769"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 816,
                    "end": 820,
                    "matchedPaperCorpusId": "251224409"
                },
                {
                    "start": 1193,
                    "end": 1196,
                    "matchedPaperCorpusId": "239009411"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1660,
                    "end": 1663,
                    "matchedPaperCorpusId": "252212320"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61572265625
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. [40] recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap. We first study two alternatives for developing symmetric dense retrieval models: 4 (1) producing a textual representation of the image and using a symmetric uni-modal bi-encoder architecture for dense retrieval, and (2) converting passages to a multi-modal input format and using a symmetric multi-modal bi-encoder architecture. We observe that both alternatives suffer from information loss, but also that they produce complementary representations. This observation motivates us to not only combine these two encodings, but also transfer knowledge between them. In more detail, we propose an iterative knowledge distillation approach to transfer knowledge between these two alternative symmetric dense retrieval models. The proposed symmetric dual encoding approach leads to 11.6% and 30.9% MRR improvements compared to the state-of-the-art baseline on OK-VQA [38] and FVQA [56] test sets, respectively. \n\nFor the second stage of the pipeline, unlike much prior work on answer span detection for KI-VQA [12,14,37,50,57] (i.e., answer extraction from the retrieved passages), we focus on retrievalaugmented autoregressive answer generation. We propose MM-FiD, a simple yet effective extension of the Fusion-in-Decoder (FiD) [20] architecture to multi-modal input. FiD is a retrieval-augmented text generation model that has recently shown effective performance in question answering tasks [20].",
            "score": 0.5575220066638313,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "173991173"
                },
                {
                    "start": 1572,
                    "end": 1576,
                    "matchedPaperCorpusId": "7483388"
                },
                {
                    "start": 1701,
                    "end": 1705,
                    "matchedPaperCorpusId": "226284018"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "250144531"
                },
                {
                    "start": 1708,
                    "end": 1711,
                    "matchedPaperCorpusId": "229339845"
                },
                {
                    "start": 1711,
                    "end": 1714,
                    "matchedPaperCorpusId": "231627467"
                },
                {
                    "start": 1714,
                    "end": 1717,
                    "matchedPaperCorpusId": "232320648"
                },
                {
                    "start": 1921,
                    "end": 1925,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "218470027",
            "title": "Sparse, Dense, and Attentional Representations for Text Retrieval",
            "text": "Abstract Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.",
            "score": 0.5560617938634722,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44873046875
        },
        {
            "corpus_id": "278165747",
            "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism",
            "text": "Our work demonstrates that similar retrieval mechanisms also emerge within SSM-based language models. \n\nThe pronounced role of retrieval in MMLU performance suggests that a broader Transformer-SSM performance gap may primarily stem from retrieval capabilities concentrated in just a few specific heads, rather than fundamental differences in Performance is reported across 55 configurations with increasing numbers of key-value pairs, using the original (non-pruned) model. Each bar shows accuracy for a given dictionary size-for example, with 20 pairs, the original score is 85.6%, removing only the Gather Head (L16H22) drops accuracy to 76%, and removing only the Aggregate Head (L17H24) drops it to 62%. These results underscore two key points we build on later: (1) Retrieval is more sensitive to Aggregate Head removal than to Gather Head removal, underscoring its importance; \n\n(2) The Aggregate Head becomes increasingly important as task complexity grows. \n\nlanguage modeling capabilities. This insight fundamentally reframes our understanding of architectural tradeoffs, as the performance disparity appears to stem from a highly localized functional difference rather than a distributed architectural advantage. \n\nIn the next section, we will explore these two types of head and their distinct properties, revealing how they serve different roles.",
            "score": 0.5547789624828829,
            "section_title": "Retrieval Emerges as a Key Factor in MMLU",
            "char_start_offset": 28910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0582275390625
        },
        {
            "corpus_id": "275931943",
            "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models",
            "text": "Karpukhin et al. [17] show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder. Later works in this direction mainly focused on optimizing the training process, e.g. through mining hard negatives [19], or on the domain shift issue [20,10]. \n\nIn the late-interaction based approach, ColBERT [21] is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs. \n\nThough the models based on pretrained language models show effectiveness in passage retrieval tasks, they face limitations with handling long documents.",
            "score": 0.5535905228371338,
            "section_title": "Representation-based Models for IR",
            "char_start_offset": 4206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1448
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "268384724"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "267740545",
            "title": "Distillation Enhanced Generative Retrieval",
            "text": "We present MINDER (Li et al., 2023c), an advanced generative retrieval system, as the base model to demonstrate the generative retrieval scheme. MINDER leverages multiview identifiers, enabling robust performance across diverse search scenarios and achieving advanced performance. Identifiers. MINDER employs three types of identifiers to represent a passage: title, substring, and pseudo-query. The title generally reflects the main topic of the passage, while the substring is a randomly selected excerpt. The pseudo-query is generated based on the passage's content. These identifier types complement each other and adapt to different scenarios, contributing to MINDER's robustness across various search contexts. \n\nTraining. MINDER optimizes an autoregressive language model, denoted as AM, via typical sequence-to-sequence loss. The input text is the query text, and the output is the identifier of the corresponding passage that is relevant to the query. \n\nDuring training, the three identifiers of the samples are randomly shuffled to train the autoregressive model. For each training sample, the objective is to minimize the sum of the negative loglikelihoods of the tokens {i 1 , \u2022 \u2022 \u2022 , i l } in a target identifier I, whose length is l. The generation loss is formulated as, \n\nwhere I <j denotes the partial identifier sequence {i 0 , \u2022 \u2022 \u2022 , i j\u22121 }, q is the question text, and \u03b8 is the trainable parameters in the autoregessive model. \n\nInference. During the inference process, given a query text, the trained autoregressive language model AM could generate several predicted identifiers via beam search, denoted as I. A heuristic function is employed to transform the predicted identifiers I into a ranked list of passages. This function selects the predicted identifier i p \u2208 I p for a given passage p if i p appears at least once in the identifiers of that passage. The similarity score of the passage p corresponding to the query q is then calculated as the sum of the scores of its covered identifiers, as follows,",
            "score": 0.5532709069688999,
            "section_title": "Preliminary: Generative Retrieval",
            "char_start_offset": 7750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 36,
                    "matchedPaperCorpusId": "258947148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258544921875
        },
        {
            "corpus_id": "270380194",
            "title": "Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative Retrieval",
            "text": "Generative Retrieval (GR) has emerged as a promising approach within Information Retrieval, particularly for text retrieval tasks [3,40,42,21].This approach involves creating a set of document identifiers that represent documents from the original corpus.A generative model is then trained to generate document identifiers for an input query.The generated identifiers are subsequently mapped back to the corresponding documents in the corpus.GR methods typically utilize an autoregressive (AR) language model to generate the document identifier as a sequence of words or sub-words tokens from a predefined target vocabulary.By leveraging high-quality document identifiers and capturing complex dependencies between tokens through the autoregressive generation process, GR has achieved substantial improvements in retrieval performance in recent years [3,21,20].Despite these advancements, deploying GR models in low-latency applications, such as sponsored search, remains a significant challenge due to the high inference complexity of AR models [19,28].This stems from their sequential token-by-token generation mechanism [13].To address this challenge, our paper explores the use of non-autoregressive (NAR) language models for GR.These models significantly reduce inference costs by generating all tokens of the document identifier simultaneously.However, this parallel generation limits the model's ability to capture dependencies among tokens (words, sub-words) in the output identifier, leading to inferior retrieval performance compared to AR-based GR models.To enable NAR-based GR to leverage word and sub-word interactions during generation, we propose expanding the model's target vocabulary by incorporating phrases within the document identifiers as tokens.Intuitively, predicting high-probability phrases at each position in the output sequence allows the NAR model to better understand the intricate relationships between words and sub-words within each predicted phrase, potentially enhancing retrieval performance.This forms the basis of our first research question:\n\n(RQ1)-How does the retrieval accuracy of a NAR-based GR model (with a target vocabulary containing word/sub-word level tokens) change when the target vocabulary is expanded to include phrases from document identifiers as additional tokens?",
            "score": 0.5526989329833827,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 2030
                },
                {
                    "start": 2030,
                    "end": 2082
                },
                {
                    "start": 2084,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "259262395"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "259075708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39013671875
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Traditional methods such as TF-IDF and BM25 explore sparse retrieval strategies by matching the overlapping contents between questions and passages (Robertson and Zaragoza, 2009;Chen et al., 2017;Yang et al., 2019). DPR (Karpukhin et al., 2020) revolutionized the field by utilizing dense contextualized vectors for passage indexing. It is first initialized as a pretrained BERT model, then trained discriminatively using pairs of queries and relevant documents, with hard negatives from BM25. Recent research has improved DPR via better training strategies (Xiong et al., 2020;Qu et al., 2021;Zhang et al., 2023a) and passage reranking (Mao et al., 2021;Yu et al., 2021;Ju et al., 2022). However, representations of questions and documents are typically obtained independently in modern dual-encoder dense retrieval models (Karpukhin et al., 2020;Xiong et al., 2020), allowing for only shallow interactions between them (Khattab et al., 2021).",
            "score": 0.552239774014343,
            "section_title": "Traditional Document Retrievers",
            "char_start_offset": 4664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 944
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 178,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 178,
                    "end": 196,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 220,
                    "end": 244,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 558,
                    "end": 578,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 594,
                    "end": 614,
                    "matchedPaperCorpusId": "258865472"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "230435683"
                },
                {
                    "start": 824,
                    "end": 848,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 921,
                    "end": 943,
                    "matchedPaperCorpusId": "220302658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "266998785",
            "title": "Bridging the Preference Gap between Retrievers and LLMs",
            "text": "Retriever. Given an input x, the retriever aims to retrieve a ranked list of passages from a corpus D = {d i } m i=1 that are relevant to x. In this work, we assume a typical scenario of employing a frozen dense retriever. Typically, a dual encoder architecture is applied, where an encoder is used to encode both the input context x and the passage d. Specifically, the encoder maps each passage to an embedding E(d). The similarity between input and passage embedding is computed by their cosine similarity, \n\n(1) \n\nThe top-k passages that have the highest similarity scores when compared with the input x are retrieved in this step, \n\nBridge Model for RAG. The retrieved top-K passages provide richer information about the original input/query x to help the LLM to make a better prediction on downstream tasks. A bridge model B adapts the retrieved passages to a sequence of passages that is LLM-friendly. As mentioned in the Sec. 1, the bridge model is a seq2seq model. It takes all the retrieved passages (d retr. j ) k j=1 as well as the query x as input, and outputs the adapted passages (d bdr. j ) n j=1 , \n\nThis formulation is general enough as the seq2seq model automatically considers ranking by generating the next token based on the preceding one, selection by placing the end-of-sentence token in the appropriate position, and repetion by generating the same passage ID (as explained in the following paragraph). Note that n may be smaller or larger than k due to selection and repetition. \n\nBefore concatenating the query and passages as bridge model's input, we prepend each passage with a unique sentinel token as its passage ID, e.g., \n\n2 . In this way, the model only needs to generate the passage IDs instead of the actual passage content, which is much more efficient and avoids making unfaithful changes to the retrieved passages. We then convert the obtained passage IDs to the corresponding passages for downstream processing. \n\nRetrieval-augmented generation with bridge. We concatenate adapted passages from the bridge model, (d bdr.",
            "score": 0.5519923610462927,
            "section_title": "Problem Formulation",
            "char_start_offset": 11334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417724609375
        },
        {
            "corpus_id": "276647280",
            "title": "From Retrieval to Generation: Comparing Different Approaches",
            "text": "The increasing complexity of knowledge-intensive tasks, particularly open-domain question answering (ODQA) and retrieval-augmented applications, necessitates advanced approaches to efficiently retrieve and generate relevant information. Traditionally, retrieval-based methods have played a central role in these tasks, with models like BM25 (Robertson and Zaragoza, 2009) serving as foundational tools for extracting relevant documents. However, the limitations of keyword-based retrieval prompted the development of dense retrieval models such as Dense Passage Retrieval (DPR) (Karpukhin et al., 2020a) and Contriever (Izacard et al., 2021), which leverage transformer-based architectures to encode queries and documents into dense representations. While dense retrieval models improve over sparse methods, they introduce new challenges. First, retrieval corpora are typically divided into fixed chunks (Karpukhin et al., 2020a), which can lead to retrieving irrelevant content. Second, dualencoder architectures encode queries and documents separately, limiting direct interaction between them (Khattab et al., 2021). Finally, dense retrieval models require pre-encoding and storing document embeddings, which constrains scalability and hinders their ability to leverage large language models (LLMs) (Levine et al., 2022). \n\nTo address these limitations, generative models such as GPT-3.5 and InstructGPT (Brown et al., 2020;Ouyang et al., 2022) offer an alternative by directly generating contextualized responses instead of retrieving existing documents. Approaches like GenRead (Yu et al., 2022) first generate relevant text and then use it for answer prediction. However, generative models often struggle with factual consistency and may hallucinate information (Huang et al., 2023), making them less reliable for knowledge-intensive tasks. Given the trade-offs between retrieval and generation, hybrid models have emerged to integrate the strengths of both approaches. Merging Generator and Retriever (MGR) (Abdallah and Jatowt, 2023;Zhang et al., 2023) combines generated and retrieved documents, allowing models to refine answers while maintaining factual accuracy.",
            "score": 0.5513755278890012,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 371,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 578,
                    "end": 603,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 619,
                    "end": 641,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 904,
                    "end": 929,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1427,
                    "end": 1447,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "259203703",
            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
            "text": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings. However, many proposed procedures are often too complex or resource-intensive, which makes it difficult for practitioners to adopt them or identify sources of empirical gains. Therefore, in this work, we propose a trivially simple recipe to serve as a baseline method for boosting the efficiency of DE retrievers leveraging an asymmetric architecture. Our results demonstrate that even a 2-layer, BERT-based query encoder can still retain 92.5% of the full DE performance on the BEIR benchmark via unsupervised distillation and proper student initialization. We hope that our findings will encourage the community to re-evaluate the trade-offs between method complexity and performance improvements.",
            "score": 0.5512915186969122,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "276558404",
            "title": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs",
            "text": "Scaling large language models (LLMs) has shown great potential for improving retrieval model performance; however, previous studies have mainly focused on dense retrieval trained with contrastive loss (CL), neglecting the scaling behavior of other retrieval paradigms and optimization techniques, such as sparse retrieval and knowledge distillation (KD). In this work, we conduct a systematic comparative study on how different retrieval paradigms (sparse vs. dense) and fine-tuning objectives (CL vs. KD vs. their combination) affect retrieval performance across different model scales. Using MSMARCO passages as the training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a fixed compute budget, we evaluate various training configurations on both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key findings reveal that: (1) Scaling behaviors emerge clearly only with CL, where larger models achieve significant performance gains, whereas KD-trained models show minimal improvement, performing similarly across the 1B, 3B, and 8B scales. (2) Sparse retrieval models consistently outperform dense retrieval across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks, and they demonstrate greater robustness to imperfect supervised signals. (3) We successfully scale sparse retrieval models with the combination of CL and KD losses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation sets.",
            "score": 0.5506430091482228,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229736328125
        },
        {
            "corpus_id": "263605677",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
            "text": "We first describe the architecture of retrieval-augmented language models and then related efficiency techniques designed for them. Next, we summarize relevant methods to improve model efficiency, including quantization and binary representations. \n\nRetrieval-Augmented Language Models. Retrieval-augmented language models have shown strong performance in many tasks, including language modeling (Borgeaud et al., 2022;Min et al., 2023), open-domain question answering, and fact checking (Izacard et al., 2022b;Lewis et al., 2020;Guu et al., 2020;Shi et al., 2023). \n\nAs shown in Figure 1, a retrieval-augmented language model works by first using a retriever to retrieve many passages relevant to the input query and then using a reader component to extract or generate the answers. While the retriever is often fast enough, the reader causes a speed bottleneck in retrieval augmentation because it requires computing cross-attention between the input query and many passages. For example, Figure 1 illustrates a state-of-the-art encoder-decoder reader (Izacard et al., 2022b) architecture called Fusion-in-Decoder (FiD) (Izacard & Grave, 2021). FiD first concatenates each passage with the query and processes them in parallel (independently) in the encoder; then the decoder fuses information across all the concatenated passage-query representations and produces answers. In our experiments, we observe that the passage encoding takes over 60% of the reader's computation on commodity GPUs, and we save such computations by precomputing the passage representations, leading to significant speedup in the inference with reduced storage costs. \n\nEfficient methods for reader models. DensePhrase (Lee et al., 2021) builds contextualized phrase representations for the passage corpus, completely removes the reader component, and uses phrase retrieval to produce answers for the query. Despite its high inference throughput, the accuracy is much lower than similar size BTR models.",
            "score": 0.5499282234785999,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 4114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 419,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 511,
                    "end": 530,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 530,
                    "end": 547,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1122,
                    "end": 1145,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1697,
                    "end": 1715,
                    "matchedPaperCorpusId": "229363636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "236459960",
            "title": "xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering",
            "text": "Retrieving relevant passages given certain query from a large collection of documents is a crucial component in many information retrieval systems such as web search and open domain question answering (QA). Current QA systems often employ a two-stage pipeline: a retriever is firstly used to find relevant passages, and then a fine-grained reader tries to locate the answer in the retrieved passages. As recent advancement in machine reading comprehension (MRC) has demonstrated excellent results of finding answers given the correct passages (Wang et al., 2017), the performance of open-domain QA systems now relies heavily on the relevance of the selected passages of the retriever. \n\nTraditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 (Robertson and Zaragoza, 2009), which can be efficiently implemented with an inverted index. With the popularization of neural network in NLP, the dense passage retrieval approach has gained traction (Karpukhin et al., 2020). In this approach, a dual-encoder model is learned to encode questions and passages into a dense, low-dimensional vector space, where the relevance between questions and passages can be calculated by the inner product of their respective vectors. As the vectors of all passages can be pre-computed and indexed, dense passage retrieval can also be done efficiently with vector space search methods during inference time (Shrivastava and Li, 2014). \n\nDense retrieval models are usually trained with contrastive objectives between positive and negative question-passage pairs. As the positive pairs are often given by the training data, one challenge in contrastive learning is how to select negative examples to avoid mismatch between training and inference. During inference time, the model needs to find the correct passages from a very large set of pre-computed candidate vectors, but during training, both positive and negative examples need to be encoded from scratch, thus severely limiting the number of negative examples due to computational cost. One promising way to reduce the discrepancy is momentum constrastive learning (MoCo) proposed by He et al. (2020). In this method, a pair of fast/slow encoders are used to encode questions and passages, respectively.",
            "score": 0.5481589403504765,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 562,
                    "matchedPaperCorpusId": "12501880"
                },
                {
                    "start": 780,
                    "end": 810,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 980,
                    "end": 1004,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1424,
                    "end": 1450,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 2156,
                    "end": 2172,
                    "matchedPaperCorpusId": "207930212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "247922320",
            "title": "Multifaceted Improvements for Conversational Open-Domain Question Answering",
            "text": "Open-domain question answering (OpenQA) [28] is a task that uses a huge library of documents to answer factual queries. The two-stage design, which included a passage retriever to choose a subset of passages and a machine reader to exact answers, became popular after DrQA [3]. The passage retriever is an important component of OpenQA system since it searches relevant paragraphs for the next stage. Traditional sparse retrieval models, such as TF-IDF or BM25 [25], have been widely adopted as retriever in OpenQA systems [3,17,32]. While sparse retrieval cannot handle the case of high semantic correlation with little lexical overlap and it is untrainable, dense passage retrievers have lately gained popularity [8,11,15,24]. In general, the dense retrieval model is a dual-encoder architecture that encodes both the question and the passage individually. Both encoders are trained during the retriever pre-training process. When training with the reader for the QA task, only the question encoder is normally fine-tuned. In order to increase the retrieval impact of dense retrievers, some studies incorporate hard negatives. BM25 top passages which do not contain answers are utilized as hard negatives [7,11]. And dynamic hard negatives are employed in [8,30,33], which are the top-ranked irrelevant documents given by dense retriever during training. \n\nA contemporary OpenQA system also includes a reader as a key component. Its goal is to deduce the answer to a query from a collection of documents. Existing Readers may be divided into two types: (1) Extractive readers [3,11,32], which anticipate an answer span from the retrieved texts, (2) Generative readers [9,16], which produce natural language replies using sequence-tosequence (Seq2Seq) models.",
            "score": 0.5473146588677853,
            "section_title": "Open Domain Question Answering",
            "char_start_offset": 31237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1760
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "51875405"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 718,
                    "end": 721,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 724,
                    "end": 727,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1578,
                    "end": 1581,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1581,
                    "end": 1584,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 1670,
                    "end": 1673,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "254564419",
            "title": "In Defense of Cross-Encoders for Zero-Shot Retrieval",
            "text": "The recent success of pretrained language models such as BERT has sparkled the interest in bi-encoder models, generally used as dense retrievers, by the information retrieval community, as they promise to bridge the \"vocabulary mismatch\" problem that has plagued sparse retrievers (e.g., BM25) for decades. Compared to cross-encoders, which are mostly used as rerankers [32,33,27,22,16], dense retrievers are faster at retrieval time as vector representations of documents can be precomputed prior to retrieval. At retrieval time, only the query vector is computed, and a fast nearest-neighbor search infrastructure is used to retrieve the most similar document vectors to the query vector. Recent work on dense retrieval mostly compares these models with learned sparse representations [49,53,19,43,26,18,45], while neural rerankers are often ignored due to their computational costs at retrieval time. Dense retrievers also show great in-domain effectiveness, i.e., when training and test data are from the same domain. For example, on the MS MARCO leaderboard [2] or in the TREC Deep Learning track [5,6], many of the topscoring submissions use dense retrievers as part of their pipeline [4,37,38,20,14,58,52]. \n\nHowever, it is not clear how these retrieval methods compare in zero-shot tasks at different model scales. For instance, recent work shows that the zeroshot effectiveness of dense and sparse retrievers can be improved by distillation or IR-specific pretraining [43,11,21,15]. Due to the computational cost, these studies typically use models of the same size as the original BERT, published four years ago. Nevertheless, since hardware and algorithmic improvements reduce the costs of using larger models [51,54,9,10,55], it is imperative to understand how these different architectures behave at scale. In NLP tasks, for instance, Tay et al. [44] showed that architectures scale differently and that recently proposed ones do not scale as well as the original Transformer architecture [47].",
            "score": 0.5468829883641162,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "212725651"
                },
                {
                    "start": 377,
                    "end": 380,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 800,
                    "end": 803,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 1105,
                    "end": 1107,
                    "matchedPaperCorpusId": "261242374"
                },
                {
                    "start": 1191,
                    "end": 1194,
                    "matchedPaperCorpusId": "225094208"
                },
                {
                    "start": 1203,
                    "end": 1206,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1487,
                    "end": 1490,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1730,
                    "end": 1733,
                    "matchedPaperCorpusId": "238408308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.482177734375
        },
        {
            "corpus_id": "246285831",
            "title": "Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models",
            "text": "We adopt NPR [23], a neural passage retrieval model with improved negative contrast as the deep retrieval model in our framework. Note that our framework is flexible, and NPR can be replaced with any other deep model. Aligned with many popular deep retrievers [17,32,43], NPR adopts a dual encoder architecture, learning dense embedding vectors representations, computing the relevance using the dot product < q dense , d dense >. The training of this model is enhanced with several negative sampling strategies, aiming at obtaining hard and highquality negative (query, passage) pairs. This model is trained on MS-MARCO passage dataset (detailed in Section 4.1), and achieves a very competitive performance. To adapt NPR to document retrieval setting, we split documents into passages by applying sliding overlapping sentence windows. Following work by [8], we use the max passage retrieval score as the document level score.",
            "score": 0.5465114705005467,
            "section_title": "Deep retrieval model",
            "char_start_offset": 13822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 926
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "243865679"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "162168864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337646484375
        },
        {
            "corpus_id": "215737187",
            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
            "text": "In this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR), along with analysis on how its output differs from traditional retrieval methods, the effects of different training schemes and the run-time efficiency. \n\nThe DPR model used in our main experiments is trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional BM25 negative passage per question. We trained the question and passage encoders for up to 30 epochs for large datasets (NQ, Trivia, SQuAD) and 100 epochs for small datasets (TREC, WQ), with a learning rate of 10 \u22125 using Adam, linear scheduling with warm-up and dropout rate 0.1. \n\nWhile it is good to have the flexibility to adapt the retriever to each dataset, it would also be desirable to obtain a single retriever that works well across the board. To this end, we train a multidataset encoder by combining training data from all datasets excluding SQuAD. 7 In addition to DPR, we also present the results of BM25, the traditional retrieval method8 and BM25+DPR, using a linear combination of their scores as the new ranking function. Specifically, we re-rank top-2000 BM25 retrieved passages using BM25(q,p) + \u03bb \u2022 sim(q, p) as the ranking function. We used \u03bb = 1.1 based on the development set retrieval accuracy. Results can be improved further in some cases by combining DPR with BM25 in both single-and multi-dataset settings. We conjecture that the lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As a result, there is a high lexical overlap between passages and questions, which gives BM25 a clear advantage. Second, the data was collected from only 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as argued previously by Lee et al. (2019).",
            "score": 0.5463495161086,
            "section_title": "Experiments: Passage Retrieval",
            "char_start_offset": 16514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1857
                }
            ],
            "ref_mentions": [
                {
                    "start": 1839,
                    "end": 1856,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325927734375
        },
        {
            "corpus_id": "248377381",
            "title": "Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval",
            "text": "Neural information retrieval (IR) performs query-passage matching at a semantic level, often using a dual-encoder architecture that encodes the queries and the passages separately. Examples of such models include the Dense Passage Retriever (DPR) [9] and ANCE [23], which fine-tune transformer-based [20] pre-trained language models [4] to compute contextualized representations of queries and passages. Table 1: Retrieval scores from DPR for the passage in Figure 1, against both a gold-standard question from NQ and three synthetic questions. The important terms in the question, that are also in the passage, are shown in italic. \n\nIn this paper, we first uncover a shortcoming in the passage encoder of such a dual-encoder IR model, namely DPR, which stems from its sparse attention pattern. To illustrate, in Figure 1 we show a heatmap of the attention weights of DPR's passage encoder over different tokens of an example passage (taken from the Natural Questions (NQ) dataset [10]). We can see that the attention given to many potentially important words and phrases, e.g, academy of management and twentieth century, are rather low. \n\nWhat is the effect of such attention, or lack thereof, on retrieval performance? Table 1 shows DPR's retrieval scores for a goldstandard question (from the NQ dataset) and three automatically generated synthetic questions (details in Section 2) when paired with the passage of Figure 1. The gold-standard question, which overlaps highly with the well-attended first sentence of the passage, receives a relatively high retrieval score. Among the synthetic questions, the one that refers to the highest-attended entity (principles of scientific management) gets the highest score, whereas the ones about To further quantify this, we randomly sampled 20k passages from Wikipedia and identified named entities that received the highest and lowest attentions from the DPR passage encoder (using the process described in Section 2.1). We then generated synthetic questions corresponding to those entities (using the process of Section 2.2).",
            "score": 0.5458087845941064,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "272654178",
            "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
            "text": "Formalization and Challenge Given a query utterance u containing a rare word w, we aim to retrieve a relevant example (u e , y e ) from an example pool D = {(u 1 , y 1 ), . . . , (u m , y m )} with a retrieval model r, such that the rare word w is spoken in utterance u e . Here u i indicates the i-th utterance and y i its translation. As the query u is only in speech, we face additional complexities compared to text-based retrieval. First, speech is versatile, unlike text, which often has a standard writing system. The speaking condition for the same word varies in every recording, requiring a robust retriever that accounts for pronunciation variations. Second, speech sequences are magnitudes longer than text. The retriever must find fine-grained local features corresponding to the keywords in long sequences. Third, transcribing the query utterance first and then using text-based retrieval is suboptimal due to ASR errors, especially on rare words. \n\nArchitecture As the nature of our example retrieval task resembles information retrieval (IR) where relevant answers are retrieved given a question, we take inspiration from IR approaches for our retriever. In text-to-text IR, a prominent architecture is the Dense Passage Retriever (DPR) (Karpukhin et al., 2020). It has a dual-encoder architecture, where one encoder encodes the questions, and the other encodes the passages potentially containing answers to the questions. The retrieval model is trained with a contrastive objective, mapping question-passage (positive) pairs closer to each other in the latent space while pushing irrelevant (negative) pairs further apart. During inference, passages closer to the encoded question by the dot-product similarity are returned as answers. In our case, the utterances containing the same rare words are considered positive pairs, while those not sharing the same rare words are negative pairs. Speech-to-Speech/Text Retrieval We propose to extend the DPR model to support querying from speech.",
            "score": 0.5454095877283527,
            "section_title": "Example Retrieval",
            "char_start_offset": 6614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1253,
                    "end": 1277,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37939453125
        },
        {
            "corpus_id": "254877418",
            "title": "Adam: Dense Retrieval Distillation with Adaptive Dark Examples",
            "text": "Information retrieval (IR) that aims to identify relevant passages for a given query is an important topic for both academic and industrial areas, and has powered many downstream tasks such as opendomain QA (Chen et al., 2017) and knowledgegrounded conversation (Dinan et al., 2018). Typically, IR systems usually follow the retrieve-andre-rank paradigm (Hofst\u00e4tter et al., 2020;Huang et al., 2020;Zou et al., 2021) where a fast retriever -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.  (Zhou et al., 2023) over MS-MARCO. POS and NEG mean the distribution of positive and hard negatives respectively. The hard negatives are provided by RocketQAv2 (Ren et al., 2021c). \n\nfirst retrieved a bundle of relevant passages from a large-scale corpus through pre-built indices and then a more sophisticated ranker comes to re-rank these candidate passages to further obtain more accurate retrieval results. Under this paradigm, recent years have witnessed a growing number of works that utilize pre-trained language models (PLMs) (Qu et al., 2021;Gao and Callan, 2021b) as retrievers and rankers to build IR systems. Among these efforts, there are two commonly adopted architectures: cross-encoder (Devlin et al., 2019a) that measure the relevance of a query-passage pair through jointly modeling their deep interactions; dual-encoder (Karpukhin et al., 2020;Qu et al., 2021) that encodes queries and passages separately into dense representations and calculate the similarity. Although dual-encoders are efficient for billions of indices, they suffer from inferior performance compared with cross-encoders since they can't capture the fine-grained semantic relevance between the query and the passage due to the absence of their deep interactions (Luan et al., 2021a).",
            "score": 0.5424232012695394,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 379,
                    "end": 398,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "249712198"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1006,
                    "end": 1023,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1174,
                    "end": 1195,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1311,
                    "end": 1335,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1335,
                    "end": 1351,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "256105432",
            "title": "Information Retrieval: Recent Advances and Beyond",
            "text": "Another line of research focus on using contextual information to improve dense passage retrieval. For example, in [152], the authors propose a method called ConTextual Mask Auto-Encoder (CMAE), which uses contextual information to improve dense passage retrieval. Additionally, there are methods that use masked auto-encoder for pre-training retrieval-oriented transformers. For example, in [153], the authors propose a method called RetroMAE, which uses masked auto-encoder for pre-training retrieval-oriented transformers. Another line of research focus on using representation bottleneck for pretraining dense passage retrieval models. For example, in [154], the authors propose a method called SimLM, which uses representation bottleneck for pre-training dense passage retrieval models. Lastly, there are methods that use lexicon-bottlenecked pre-training for large-scale retrieval. For example, in [155], the authors propose a method called LexMAE, which uses lexicon-bottlenecked pre-training for large-scale retrieval. The last method is a method that uses a contrastive pre-training approach to learn discriminative autoencoder for dense retrieval. For example in [156]. Overall, these pre-training methods demonstrate promising results in improving dense passage retrieval performance. However, more research is needed to fully understand Zero-shot/Few-shot Learning Recent research in the field of zero-shot information retrieval has been focused on finding ways to improve the performance and generalizability of retrieval models. One common approach is the use of query generation, as seen in [157] and QGen [158]. These methods aim to improve zero-shot retrieval by embedding queries into a shared space, or by using synthetic question generation to improve passage retrieval. Another approach is the use of synthetic pre-training, as suggested by [159], to improve the robustness of neural retrieval models. Additionally, [160] presents a benchmark for evaluating the performance of zero-shot retrieval models. Another approach is the use of momentum adversarial domain-invariant representations as proposed by [161] for zeroshot dense retrieval. [162] proposes DTR, which uses large dual encoders for generalizable retrieval.",
            "score": 0.5421055588105731,
            "section_title": "Multi-vector Representation",
            "char_start_offset": 34647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 1173,
                    "end": 1178,
                    "matchedPaperCorpusId": "251718954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2705078125
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Task Definition. In the first-stage retrieval, given a query , a retriever is required to fetch top- relevant passages from a millioneven billion-scale passage collection C. Due to the efficiency requirement, dual-encoder architecture is widely applied in this task for its lightweight metric calculation. Formally, dual-encoder represents text  (could be query  or passage ) to -dimensional embeddings, i.e., \n\nwhere  could be dense retriever ( den ) or lexicon-aware retriever ( lex ). With separately encoded query  and passage , we could calculate the relevance score via dot product for retrieval, i.e.,",
            "score": 0.5413645655814028,
            "section_title": "Preliminary",
            "char_start_offset": 9485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4814453125
        },
        {
            "corpus_id": "253140807",
            "title": "Open-Domain Conversational Question Answering with Historical Answers",
            "text": "Following the prior work (Karpukhin et al., 2020;Xiong et al., 2021), we apply a dense retrieval method, which has shown dominant performance over sparse ones. Specifically, the model uses a dual-encoder architecture to map passages and questions to the same embedding space. The input of our question encoder is the concatenation of historical questions and answers: \n\nThe retrieval score is then defined as the dot product of the passage embedding and the question embedding: \n\nIn the training process, each question contains one gold passage p + and a set of negative passages P \u2212 , ConvADR-QA is then optimized using the negative log likelihood loss:",
            "score": 0.5409715251485819,
            "section_title": "Retriever",
            "char_start_offset": 6980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 654
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 49,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 49,
                    "end": 68,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483154296875
        },
        {
            "corpus_id": "277622120",
            "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
            "text": "Dense retrieval models primarily employ a two-tower architecture of pre-trained language models (PLMs), i.e.,R  (\u2022) and R  (\u2022), to encode query and passage, into fix-length dense vectors. The relevance between the query  and passage  is  (, ), i.e., \n\nwhere  < \u2022 > is usually implemented as a simple metric, e.g., dot product and cosine similarity. R  (\u2022) and R  (\u2022) are usually share the parameters. The traditional way for training dense retrievers uses contrastive loss, also referred to as InfoNCE [43] loss, i.e., \n\nwhere  + and  \u2212 represent the positive and negative instances for the query .",
            "score": 0.540837316668395,
            "section_title": "Typical Dense Retrieval Models",
            "char_start_offset": 11067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 598
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model\u2019s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.",
            "score": 0.5407391912113412,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "237532725",
            "title": "Phrase Retrieval Learns Passage Retrieval, Too",
            "text": "As shown in the previous section, phrase-based passage retrieval is able to achieve competitive passage retrieval accuracy, despite that the models were not explicitly trained for that.\n\nIn this section, we compare the training objectives of DPR and DensePhrases in detail and explain how DensePhrases learns passage retrieval.",
            "score": 0.5399650522374755,
            "section_title": "A Unified View of Dense Retrieval",
            "char_start_offset": 12163,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1949462890625
        },
        {
            "corpus_id": "225062341",
            "title": "Neural Passage Retrieval with Improved Negative Contrast",
            "text": "Automatic question answering is a very active area of research within natural language processing. One possible way to approach this task is to look for answers in the text passages of a collection of documents. Recent research has shown promising results (Yang et al., 2020;Guo et al., 2020;Karpukhin et al., 2020;Xiong et al., 2020) on developing neural models for passage retrieval tasks, including Retrieval Question Answering (Ahmad et al., 2019), Open Domain Question Answering (Chen et al., 2017), and MS MARCO (Nguyen Figure 1: Two-stages neural retrieval model with hard negatives in both stages. In Stage 1, the model is trained using synthetic question-passages pairs. In Stage 2, the model is fine tuned using gold training data. et al., 2016). The models in these systems are often trained using the dual encoder framework (Yang et al., 2018b;Gillick et al., 2018) where questions and passages are encoded separately. Training an effective neural retrieval model usually requires a large amount of high-quality data. To alleviate the need of high-quality data, training can be approached in two-stages: pre-training on noise data (Guo et al., 2018;Chang et al., 2020;Ma et al., 2020) and fine tuning on a smaller amount of highquality data, also regarded as \"gold\" data. \n\nWhen used for question answering, one advantage of the dual encoder is that training in batches allows to use, for each question, the passages that answer all the other questions in the batch as negatives (Gillick et al., 2018). Given that the training batches are randomly sampled from all the question-passage pairs, the negatives in the batch are random in nature. While effective in many retrieval tasks (Henderson et al., 2017;Yang et al., 2018b), random negatives have the limitation of not being targeted nor challenging enough to clearly separate the passage that answers a given question from any other passage. How to sample the negatives in a way that widens this separation and improves the contrast between the correct and incorrect passages remains an open question.",
            "score": 0.5395866156020734,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "195848217"
                },
                {
                    "start": 484,
                    "end": 503,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 836,
                    "end": 856,
                    "matchedPaperCorpusId": "5033484"
                },
                {
                    "start": 1143,
                    "end": 1161,
                    "matchedPaperCorpusId": "51890955"
                },
                {
                    "start": 1161,
                    "end": 1180,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1694,
                    "end": 1718,
                    "matchedPaperCorpusId": "2449317"
                },
                {
                    "start": 1718,
                    "end": 1737,
                    "matchedPaperCorpusId": "5033484"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.178466796875
        },
        {
            "corpus_id": "272330251",
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "text": "Setting & Metrics We evaluate training methods for supervised dense retrieval models. Each method is provided the same supervised data. All methods use a stale buffer of target representations and use this buffer to form the subset of targets, S(Y), used in computing the truncated softmax. All methods use the same loss (cross-entropy) and optimize parameters of the dual-encoders using gradient descent. The methods differ in their maintenance of the buffer, and, as such, differ in their computational requirements of maintaining this buffer. We measure the computational requirements in terms of how many targets are re-embedding during training1 . We measure re-embedding in terms of the number of targets encoded to indicate the computational expense (even if wall clock time is mitigated using a complicated asynchronous computation). Re-embedding every target even one additional time during training can be problematic if number of targets is large. Furthermore, the initial buffer, created using the initial parameters of the dual-encoder (e.g., a pre-trained language model) can be computed once and used for subsequent training jobs. \n\nData We evaluate on Natural Questions ( Models We initialize the dual encoder models with two publicly available pre-trained language models, GTR (Ni et al., 2022), and T5 (Raffel et al., 2020). GTR is an encoder model initialized from T5 and further pre-trained for dense retrieval on a large collection of corpora of question/answer pairs. For MSMARCO, we only use T5 since it is included in GTR's training data. We use the base size models, D = 768, and train separate parameters for f (x) and g(y). For the target corrector, we use a two layer MLP. We use 8192 hidden units, a ReLU non-linearity, and a residual connection. \n\nWe compare the following approaches: Target Corrector Networks (this paper): At the first training step, we initialize the buffer with vector representations of every target.",
            "score": 0.5395056636725762,
            "section_title": "Supervised Dense Retrieval",
            "char_start_offset": 21071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 1294,
                    "end": 1311,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1320,
                    "end": 1341,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5458984375
        },
        {
            "corpus_id": "258557604",
            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
            "text": "Pre-trained language model-based (PLM) dense retrievers (DRs) have achieved remarkable success in the task of English-only passage retrieval [12-14, 20, 21, 26, 28, 29, 40, 43, 44]. These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33,45]. \n\nTo leverage the semantic modelling power of DRs, recent research has extended English-only DRs to support cross-lingual settings [1,2,19,22,27,31], i.e, where queries and passages are in different languages. This is achieved using multi-lingual PLMs, such as multilingual BERT [6], in place of the English-only PLMs. This approach is particularly important in this setting where traditional bag-of-words methods are ineffective due to the limited number of matching terms across languages. In contrast, cross-lingual DRs (xDRs) are able to encode queries and passages in different languages into a shared embedding space, enabling efficient ANN search across languages. However, such multi-lingual PLM-based xDRs usually are less effective on the cross-lingual passage retrieval task than DRs in the English-only setting [1]. The hypothesis to explain this result is that, in the English-only setting, a DR only needs to learn relevance matching between queries and passages. In contrast, a xDR not only needs to learn the relevance matching task, but also needs to learn how to align the embeddings of texts with similar semantic meaning but in different language [24,41]. It is this language gap that makes cross lingual retrieval a relatively harder task for xDRs. \n\nBased on this hypothesis, this paper proposes the use of crosslingual query generation (xQG) to bridge the language gap for xDRs. Our approach is illustrated in Figure 1.",
            "score": 0.5388004361975152,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 699,
                    "end": 701,
                    "matchedPaperCorpusId": "236428949"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "245144717"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "220871404"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "256461230"
                },
                {
                    "start": 710,
                    "end": 713,
                    "matchedPaperCorpusId": "250390748"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1388,
                    "end": 1391,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 1732,
                    "end": 1736,
                    "matchedPaperCorpusId": "246210468"
                },
                {
                    "start": 1736,
                    "end": 1739,
                    "matchedPaperCorpusId": "248392078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6689453125
        },
        {
            "corpus_id": "236477844",
            "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval",
            "text": "With the recent advances of pre-trained language models, dense passage retrieval techniques (representing queries and passages in low-dimensional semantic space) have significantly outperformed traditional term-based techniques (Guu et al., 2020;Karpukhin et al., 2020). As the key step of finding the relevant information, it has been shown that dense passage retrieval can effectively improve the performance in a variety of tasks, includ- * Equal contribution.\n\n\u2020 The work was done when Ruiyang Ren was doing internship at Baidu. \u2021 Corresponding authors. 1 Our code is available at https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2021-PAIR  Figure 1: An illustrative case of a query q, its positive passage p + and negative passage p \u2212 : (a) Query-centric similarity relation enforces s(q, p + ) > s(q, p \u2212 ); (b) Passage-centric similarity relation further enforces s(p + , q) > s(p + , p \u2212 ), where s(p + , q) = s(q, p + ). We use the distance (i.e., dissimilarity) for visualization: the longer the distance is, the less similar it is.\n\ning question answering Xiong et al., 2020b), information retrieval (Luan et al., 2021;Khattab and Zaharia, 2020), dialogue (Ji et al., 2014;Henderson et al., 2017) and entity linking (Gillick et al., 2019;.\n\nTypically, the dual-encoder architecture is used to learn the dense representations of queries and passages, and the dot-product similarity between the representations of queries and passages becomes ranking measurement for retrieval. A number of studies have been devoted to improving this architecture (Guu et al., 2020;Karpukhin et al., 2020;Xiong et al., 2020a) for dense passage retrieval. Previous studies mainly consider learning query-centric similarity relation, where it tries to increase the similarity s(q, p + ) between a query and a positive (i.e., relevant) passage meanwhile decrease the similarity s(",
            "score": 0.5385033492585051,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 269,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1143,
                    "end": 1169,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1240,
                    "end": 1262,
                    "matchedPaperCorpusId": "202718954"
                },
                {
                    "start": 1587,
                    "end": 1610,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "267068650",
            "title": "Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense Passage Retrieval.",
            "text": "Passage Retrieval. Passage retrieval aims to find relevant information from large passage collections based on a given search query. Traditional techniques for ad-hoc retrieval, such as TF-IDF and BM25 [26], perform relevant searches based on lexicon terms and inverted document frequencies. Such methods have efficient retrieval abilities, but they suffer from vocabulary mismatch problems [12]. Although there are continuous techniques like query expansions [10,21,30] and query rewrite [9] to alleviate such mismatch issues, additional preprocess of queries or passage collections make the retrieval pipelines much more complicated and hard to maintain. \n\nDense retrieval [12,22] has become popular in mainstream retrieval applications recently, because it maps the textual information into learnable representations and naturally alleviates the lexicon mismatch problems by learning the similarity traits of queries and passages through end-to-end contrastive learning. Dense retrieval encodes queries and passage collections as condensed lowdimension sentence representations with pre-trained language models (PLMs), and performs retrieval based on the similarity of representations with efficient search algorithms like maximum innerproduct search (MIPS) [19]. Notably, there are continuous efforts [3,5,27,37] to adapt the PLMs to sparse retrieval. There are also extended studies about hybrid retrievals [15,33], which combines both dense and sparse retrievals to bring superior retrieval performances. However, our work primarily focuses on dense retrieval pre-training and is orthogonal to these works.",
            "score": 0.5383707312532541,
            "section_title": "RELATED WORKS",
            "char_start_offset": 5640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1612
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 395,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "1448737"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "16464483"
                },
                {
                    "start": 1308,
                    "end": 1310,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1310,
                    "end": 1313,
                    "matchedPaperCorpusId": "251953412"
                },
                {
                    "start": 1313,
                    "end": 1316,
                    "matchedPaperCorpusId": "266977305"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "253553743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.429443359375
        },
        {
            "corpus_id": "271909221",
            "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
            "text": "Throughout the paper, we utilize the dual-encoder architecture with shared parameters, which is commonly used for dense retrieval (DR; Ni et al., 2022). Contrastive learning is a method for training DR models by contrasting positive pairs against negatives. Specifically, given a batch of triplets, each of which consists of a query and its relevant and irrelevant documents: \n\nWe minimize the InfoNCE loss for each query q n : \n\n. \n\n(1) \n\nWe use cosine similarity as the scoring function: \n\n, where E \u03b8 is the encoder parametrized by \u03b8. Following Wang et al. (2022), we incorporate prefix identifiers \"Query:\" and \"Passage:\" for queries and passages, respectively. As shown in prior work (Hofst\u00e4tter et al., 2021;Lin et al., 2021), in-batch negatives mining, the second term of the denominator in Eq (1), plays a crucial role in dense retrieval training. In this work, we study different batch sampling approaches to control in-batch negative mining.",
            "score": 0.5383482272685053,
            "section_title": "Contrastive Learning",
            "char_start_offset": 4985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 951
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 151,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 689,
                    "end": 714,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 714,
                    "end": 731,
                    "matchedPaperCorpusId": "235720578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "274822530",
            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
            "text": "The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023;Fu et al., 2023). When integrating LLMs, the model can leverage the EOS token embedding, which acts as a contextual representation of the entire sentence. This token's embedding encapsulates the semantic meaning of the full query or document, allowing for a richer, more accurate comparison of inputs within the shared vector space. The use of LLMs in dual-encoder architectures opens up new possibilities for improving the model's ability to capture nuanced semantic relationships. \n\nGenerative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2024), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search. Generative models learn to map queries to their relevant documents by generating unique docids or specific document content.",
            "score": 0.537577452412439,
            "section_title": "Dual-encoder approaches",
            "char_start_offset": 15799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 630,
                    "end": 651,
                    "matchedPaperCorpusId": "257767171"
                },
                {
                    "start": 651,
                    "end": 667,
                    "matchedPaperCorpusId": "257970928"
                },
                {
                    "start": 1326,
                    "end": 1344,
                    "matchedPaperCorpusId": "258048596"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "246863488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65087890625
        },
        {
            "corpus_id": "243865659",
            "title": "Multi-Vector Attention Models for Deep Re-ranking",
            "text": "Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely independently and combined with a simple dot product operation. The second style is cross-attention models, where the query and document features are concatenated in the input layer and all computation is based on the joint query-document representation. Dual encoder models are typically used for retrieval and deep re-ranking, while cross-attention models are typically used for shallow re-ranking. In this paper, we present a lightweight architecture that explores this joint cost vs. accuracy trade-off based on multi-vector attention (MVA). We thoroughly evaluate our method on the MS-MARCO passage retrieval dataset and show how to efficiently trade off retrieval accuracy with joint computation and offline document storage cost. We show that a highly compressed document representation and inexpensive joint computation can be achieved through a combination of learned pooling tokens and aggressive downprojection. Our code and model checkpoints are open-source and available on GitHub.",
            "score": 0.5374849284710584,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "Information retrieval (IR) aims to retrieve information objects that are relevant to users' queries from a large-scale collection. Dense retrieval (DR) models [10] are proposed to assess the relevance between query and passage by encoding them into embeddings and calculating the similarity in the embedding space. Then, using Approximate Nearest Neighbor Search algorithms [24] on embeddings, we can retrieve the top-k relevant passages to the query. \n\nDR model needs to balance both effectiveness and efficiency. Regarding effectiveness, DR models focus on improving retrieval performance, which is influenced by factors including model's inherent ability in semantic understanding and summarization. Regarding efficiency, this paper focuses on the passage inference time instead of query, which has greater potential for improvement. \n\nPre-trained Language Models (PLMs) have demonstrated their effectiveness on downstream tasks, thanks to the sufficient world knowledge and semantic knowledge they gain during pre-training. Especially, the Transformer-based PLMs [11,26,27] represent the advantage of capturing long-range dependencies by the self-attention mechanism and allowing parallel training. Many studies [10,15,17,19] have proposed to adopt Transformer-based PLMs as the encoders of DR models and have observed their effectiveness. \n\nHowever, despite the effectiveness of Transformer-based DR models, the efficiency is limited by their inherent model architectures. In detail, the self-attention component in Transformer-based PLMs results in a computational complexity that grows quadratically with sequence length. Thus, for retrieval tasks with long passages, such as legal case retrieval task [13], Transformer-based DR models exhibit slow inference speeds. Some non-Transformer PLMs [6,7,20] are proposed to improve efficiency without sacrificing effectiveness. In particular, Mambabased PLMs present comparable performance to Transformer-based PLMs on generative language tasks and achieve linear time scaling in sequence length based on a selective state space model mechanism. Recently, Xu proposed the RankMamba model [28], which leverages Mamba for re-ranking tasks. Our work differs in two key aspects: (1) The proposed Mamba Retriever is a bi-encoder model designed for effectiveness and efficiency in first-stage retrieval tasks.",
            "score": 0.5373504192482853,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 1067,
                    "end": 1071,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1226,
                    "end": 1229,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1800,
                    "end": 1803,
                    "matchedPaperCorpusId": "264288993"
                },
                {
                    "start": 1805,
                    "end": 1808,
                    "matchedPaperCorpusId": "258832459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "245144556",
            "title": "Large Dual Encoders Are Generalizable Retrievers",
            "text": "Neural information retrieval. Document retrieval is an important task in the NLP and information retrieval (IR) communities. Traditionally, lexical based approaches trying to match the queries and documents based on term overlap, such as TF-IDF and BM25 (Robertson and Zaragoza, 2009), have achieved great success in this task. Recently, neural based approaches, which go beyond the simple term matching, are being quickly adopted by the community and achieving state-of-the-art performance on multiple retrieval tasks, such as passage retrieval (Karpukhin et al., 2020), question answering (Ahmad et al., 2019), conversational question answering (Qu et al., 2020) and bitext retrieval (Feng et al., 2020). \n\nDual encoders for neural retrieval. In the line of neural retrievers, dual encoders have demonstrated great performance compared to traditional sparse models, such as BM25, on a wide range of retrieval tasks (Karpukhin et al., 2020;Gillick et al., 2018). A key to the success of dual encoders is pre-trained language models, from which the dual encoders are initialized. Other techniques, such as negative mining (Xiong et al., 2020;Lu et al., 2021;Sachan et al., 2021) and having large training batch sizes (Qu et al., 2021), are also highly effective. However, little work has been done on discussing the effect of the capacity of the backbone models. \n\nZero-shot neural retrieval. Recent works have shown great improvement under the zero-shot setting for dual encoders by leveraging distillation and synthetic data generation (Thakur et al., 2021;Hofst\u00e4tter et al., 2021;Ma et al., 2020). Both these techniques, and scaling up backbone models, are effective ways to close the gap between dual encoders and the upper bound of the singleproduct approaches with fixed-dimension embeddings. On the other hand, multi-vector approaches introduce more interactions between dense embeddings, which could also benefit from scaling up the backbone encoders.",
            "score": 0.5359878530664605,
            "section_title": "Related Work",
            "char_start_offset": 21409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 284,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 546,
                    "end": 570,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 591,
                    "end": 611,
                    "matchedPaperCorpusId": "195873973"
                },
                {
                    "start": 647,
                    "end": 664,
                    "matchedPaperCorpusId": "218869571"
                },
                {
                    "start": 917,
                    "end": 941,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1142,
                    "end": 1158,
                    "matchedPaperCorpusId": "243865679"
                },
                {
                    "start": 1158,
                    "end": 1178,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1217,
                    "end": 1234,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1538,
                    "end": 1559,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348388671875
        },
        {
            "corpus_id": "259075735",
            "title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives",
            "text": "The dual encoder architecture applied to information retrieval has shown excellent performance in a wide range of tasks (Gillick et al., 2018;Karpukhin et al., 2020;Ni et al., 2021Ni et al., , 2022)). \n\nRecently, the Information Retrieval community has transitioned towards Deep Learning models that leverage large unsupervised corpus pretraining (Devlin et al., 2019;Raffel et al., 2020), which offers more powerful semantic and contextual representation for queries and documents. These models can be successfully applied to scoring tasks, e.g. Dehghani et al. (2017), or retrieval tasks, e.g. Gillick et al. (2018). In contrast, classic SearchQA show that sharing a projection layer in Asymmetric Dual Encoders (ADE-SPL) (Dong et al., 2022) may not guarantee that the embeddings from the two encoder towers are in coinciding parameter spaces. However SamToNe can effectively achieve that. \n\nretrieval models, such as BM25 (Robertson and Zaragoza, 2009), rely on bag-of-words lexical overlap, term frequency heuristics, inverse document frequency and document length. This type of retrieval models does not require any training and can generalize reasonably well, but they fall short of finding documents that have low term overlap but high semantic similarity. \n\nA dual encoder (Gillick et al., 2018;Yang et al., 2020;Karpukhin et al., 2020;Reimers and Gurevych, 2019) consists of two encoding towers that map queries and documents, respectively, into a shared low-dimensional dense representation, namely, the embedding space. The model is usually optimized by a contrastive loss (Chopra et al., 2005), which moves the embeddings of the queries and documents from the same positive examples closer to each other, and the embeddings from negative examples farther away. Training the dual encoder in batches allows to use, for each question, the passages that answer all the other questions within the batch as negatives (Gillick et al., 2018), namely \"in-batch negatives\".",
            "score": 0.5356600323999665,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 165,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 347,
                    "end": 368,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 547,
                    "end": 569,
                    "matchedPaperCorpusId": "3666085"
                },
                {
                    "start": 724,
                    "end": 743,
                    "matchedPaperCorpusId": "248177840"
                },
                {
                    "start": 925,
                    "end": 955,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1303,
                    "end": 1321,
                    "matchedPaperCorpusId": "195848217"
                },
                {
                    "start": 1321,
                    "end": 1344,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1344,
                    "end": 1371,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1584,
                    "end": 1605,
                    "matchedPaperCorpusId": "5555257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3642578125
        },
        {
            "corpus_id": "277043051",
            "title": "Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach",
            "text": "RAG has become an influential approach for tasks requiring knowledge-intensive natural language processing, particularly in open-domain question answering [4]. The foundational work by Lewis et al. [4] demonstrated that combining dense retrieval systems with generative models significantly improved accuracy in generating factually correct responses. In such setups, a dense embedding index of textual content, created using transformer-based encoders, supports efficient retrieval by measuring the semantic similarity between the query and stored documents. \n\nAt the core of RAG systems lies the embedding model, usually based on transformer architectures like BERT, which encode queries and documents into dense vector representations. Dense Passage Retrieval (DPR), introduced by Karpukhin et al. [5], exemplifies this approach by training dual-encoder models to produce embeddings that closely align queries with relevant document passages. The key idea behind DPR is to use contrastive learning methods, ensuring queries are embedded in close proximity to their corresponding answer-containing passages while distancing unrelated texts. However, despite extensive training, discrepancies between the query and document embedding spaces can still occur, particularly if the query formulation significantly differs from the format of the indexed documents. These differences often result in reduced retrieval effectiveness, especially when generic embeddings or pre-trained models without fine-tuning are employed. \n\nRecent advancements have aimed at reducing these embedding mismatches through innovative strategies. One such method is the Hypothetical Document Embedding (HyDE), proposed by Gao et al. [6]. In HyDE, an instruction-driven language model like GPT-3 generates a plausible but hypothetical document in response to a query, transforming the original query into a form more akin to a assertive statement. Embedding this hypothetical text, rather than the original question, results in vectors more closely aligned with document embeddings, thereby improving retrieval accuracy. This generative step effectively addresses the inherent mismatch between interrogative queries and assertive knowledge statements, resulting in enhanced zeroshot retrieval capabilities. By reframing the query as a factual-like representation, HyDE achieves substantial performance gains without requiring additional annotated data for training.",
            "score": 0.535294998613473,
            "section_title": "Retrieval-Augmented Generation (RAG) and Vector Embeddings",
            "char_start_offset": 8347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2280
                },
                {
                    "start": 2281,
                    "end": 2439
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "253581551",
            "title": "Data-Efficient Auto-Regressive Document Retrieval for Fact Verification",
            "text": "In contrast to the previous approaches, where the content of a passage is scored for a query using it's content, autoregressive document retrieval (De Cao et al., 2020) uses a seq2seq model that is trained to predict a relevant document title, such as a Wikipedia page. Tokens are decoded incre-mentally left to right where the and scored with p(y|x) = i=1 p(y i |y <i , x) where the decoded document title \u0177 \u2208 E exists in a corpus E. To ensure this constraint is satisfied, constrained decoding sets p(y i |y <i,x , x) = 0 for token sequences (y 1 , . . . y i ) that do not occur in the index. In practice this works well in the Wikipedia domain where document titles are simple canonical descriptors of an entity or concept. An extension, mGENRE (Cao et al., 2021), has been trained for multi-lingual entity linking using Wikipedia hyperlinks and internationalized versions of the pages from the Wikidata graph as supervision targets in other languages. This has not been applied to an entity linking task, but not evaluated for document retrieval. \n\nSimilarly, GENRE did use hyperlink-based information by incorporating data from BLINK during training. However, its contribution to system performance appears low (De Cao et al., 2020, Table 8), warranting further investigation. While the use of pre-training with hyperlink information in retrieval has shown promise (Ma et al., 2021) in other formulations, the use of distant-supervision in autoregressive retrieval, using the article titles and hyperlinks in training is emerging and has been studied in contemporaneous work (Chen et al., 2022). Lee et al. (2022) train autoregressive models for multi-hop retrieval, with a data augmentation strategy. Alternative autoregressive retrieval formulations are designed to predict document sub-strings (Bevilacqua et al., 2022): this obviates the need to have unique document identifiers.",
            "score": 0.5352389505477528,
            "section_title": "Autoregressive Document Retrieval",
            "char_start_offset": 4111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 167,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1370,
                    "end": 1387,
                    "matchedPaperCorpusId": "237266405"
                },
                {
                    "start": 1580,
                    "end": 1599,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23583984375
        },
        {
            "corpus_id": "257585074",
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "text": "In this section, we introduce a conceptual framework consisting of three components (sparse encoder, sparse regularizer, supervision) that captures the key differences we observe between existing learned sparse retrieval methods. Later, we describe how LSR methods in the literature can be fit into this framework. Sparse (Lexical) Encoders. A sparse or lexical encoder encodes queries and passages into weight vectors of equal dimension. This is the main component that determines the effectiveness of a learned sparse retrieval method. There are three distinct characteristics that make sparse encoders different from dense encoders. The first and most straightforward difference is that sparse encoders produce sparse vectors (i.e., most term weights are zero). This sparsity is controlled by sparse regularizers, which we will discuss in the next section. Second, dimensions in sparse weight vectors are usually tied to terms in a vocabulary that contains tens of thousands of terms. Therefore, the size of the vectors is large, equal to the size of the vocabulary; each dimension represents a term (typically a BERT word piece). On the contrary, (single-vector) dense retrieval methods produce condensed vectors (usually fewer than 1000 dimensions) that encode the semantics of the input text without a clear correspondence between terms and dimensions. Term-level dense retrieval methods like ColBERT [13] do preserve this correspondence. \n\nThe third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies. \n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8,35]. There are also recent works proposing hybrid retrieval systems that combine the strength of both dense and sparse representations [3,18,19], which can bring benefits for both in-domain and out-of-domain effectiveness [19].",
            "score": 0.5350943474381061,
            "section_title": "Unified learned sparse retrieval framework",
            "char_start_offset": 5623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2353
                }
            ],
            "ref_mentions": [
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 2126,
                    "end": 2129,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17041015625
        },
        {
            "corpus_id": "267028109",
            "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
            "text": "The authors studied various types of architectures with multiple components of a broader category of RAG models: retrieval-augmented neural architectures. The components include retrievers, rankers, and encoder-decoders.When a user has a prompt or a query. The retriever component takes this prompt and searches for relevant documents from a large corpus. Once a set of potentially relevant documents is retrieved, the ranker sorts these documents based on their relevance to the user's prompt. This step ensures that the most pertinent information is prioritized. The encoder processes the user's prompt along with the top-ranked documents retrieved and produces an intermediate representation. This representation captures the essential information from both the user's input and the external knowledge sourced from the database. The decoder then uses this representation to generate a response. The encoder-decoder enables the system to provide responses that are grounded in factual information. \n\nThe authors discussed various retrieval augmentation mechanisms.Dense Passage Retrieval (DPR) [19] is a retrieval strategy that employs a dual-encoder architecture to score dialogue context-document pairs. Given a dialogue context, DPR retrieves relevant documents by computing a relevance score between the context and each document in a corpus. It encodes the queries and the documents separately. The term dense opposes sparse retrieval method, as it maps both the query and the documents into continuous vector spaces. Poly-encoders [16] produce a set of vectors (codes) to attentively process the context token outputs of a transformer encoder. These codes capture different distinct semantic aspects or features of the context. Each code interacts with a candidate's vector to compute a score. These scores from all codes are then combined for a final score for that candidate.The Fusion-in-Decoder (FiD) [17] mechanism fuses independent encoder outputs before decoding the final generation. While DPR and Poly-encoder are retrieval strategies used to find relevant documents or passages given the dialogue context, FiD is for integrating the information from these retrieved documents into the final response generated by the model. FiD then separately encodes the dialogue context and each retrieved document using a transformer-based encoder. The",
            "score": 0.5348875235044184,
            "section_title": "NEURAL PATH HUNTER: REDUCING HALLUCINATION IN DIALOGUE SYSTEMS VIA",
            "char_start_offset": 52956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383544921875
        },
        {
            "corpus_id": "276580326",
            "title": "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers",
            "text": "Dense Passage Retrieval (Karpukhin et al., 2020) utilizes a pre-trained language model such as BERT (Devlin et al., 2019), to encode text into dense vectors and conduct passage retrieval as a nearest neighbor search. This approach has shown strong in-domain effectiveness compared to traditional lexical retrievers such as BM25 (Robertson and Zaragoza, 2009). However, dense retrievers have been found to struggle with generalization when applied to out-of-domain retrieval tasks (Thakur et al., 2021). To address this issue, various works have aimed to improve the generalization of dense retrievers through continuous pre-training tailored for retrieval tasks. Works such as Condenser (Gao and Callan, 2021), RetroMAE (Xiao et al., 2022), and SimLM (Wang et al., 2023) have enhanced the dense representation of BERT via customized architectures during language modeling. Other works, including Contriever (Izacard et al., 2022), GTE (Li et al., 2023), E5 (Wang et al., 2024a) have further adapted two-stage contrastive learning. These models are first trained with unsupervised or weakly supervised large-scale contrastive learning, followed by supervised contrastive learning with available relevance-judged data (Nussbaum et al., 2024;Yu et al., 2024). CDE (Morris and Rush, 2024) further proposes a two-stage model architecture that integrates corpus-level information into document embeddings. In this work, we propose a data augmentation approach based on large language models, enabling the creation of high-quality augmentation data for smaller retrievers.",
            "score": 0.534320557069011,
            "section_title": "Robust Dense Retrieval",
            "char_start_offset": 2248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 48,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 100,
                    "end": 121,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 328,
                    "end": 358,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 687,
                    "end": 709,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "250311114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.401123046875
        },
        {
            "corpus_id": "238743940",
            "title": "Exploring Dense Retrieval for Dialogue Response Selection",
            "text": "The dense retrieval technique [14] has been widely applied in lots of downstream research, such as open-domain question answer and passage retrieval. Many works have proved the effectiveness and efficiency of the dense retrieval models, such as ColBERT [15,29] and RocketQA [25,26]. However, the dense retrieval model has not been well explored in the dialogue response selection to the best of our knowledge. Whether the dense retrieval models can achieve better performance than the state-of-the-art cross-encoder models remains an open problem in dialogue response selection. \n\nTo boost the performance of the dense retrieval models, previous works in dense passage retrieval [26,29] propose some effective training strategies to optimize the dense retrieval model: (1) crossbatch negative sampling; (2) denoised hard negative sampling; (3) data augmentation brought from better cross-encoder models; (4) joint training for cross-encoder and dense retrieval models. However, we notice that these strategies are limited in the dialogue response selection task. This paper employs another three simple yet effective training strategies to optimize the dialogue dense retrieval model, which helps our DR-BERT model significantly outperform the stateof-the-art cross-encoder dialogue response selection models.",
            "score": 0.5342908312803937,
            "section_title": "Dense Retrieval",
            "char_start_offset": 7767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39111328125
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "In passage retrieval for speech-based open-domain QA, the ASR-Retriever pipelines that were used so far suffer from propagating ASR errors to the retriever. Such approaches are not trained end-to-end; thus, the quality of the ASR transcriptions bounds the retrieval performance. Furthermore, the requirement for an ASR model limits the applicability of such pipelines to scenarios where annotated speech is available for training the ASR model. To alleviate the above-mentioned problems, we propose an ASR-free multimodal dense retriever that can support speech, and can be trained end-to-end. Specifically, we modify the dual-encoder architecture for dense text retrieval in traditional opendomain QA (Karpukhin et al., 2020) to account for spoken questions. We replace the BERT-based question encoder with HuBERT (Hsu et al., 2021) Figure 1: Overview of our multimodal dense retriever. and leave the BERT-based passage encoder as is. \n\nHuBERT is a self-supervised model for speech representation learning, which leverages a BERT-like masked prediction loss. It utilizes offline clustering to provide target labels for masked language model pertaining. Figure 1 illustrates the architecture of our ASR-free, multimodal dense retriever. Suppose a pair of question q, in speech form, and a passage p, in text form. The speech and language encoders produce the output representations: \n\nHuBERT (q) = (q 1 , ..., q n ), BERT (p) = (p 1 , ..., p m ). \n\n(1) We use the first token embedding output from the speech and language modules to encode questions and passages into a single vector each: \n\nThe relevance between a question and a passage is computed as the dot product of their vectors: \n\nTo this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages.",
            "score": 0.5335104780214535,
            "section_title": "Multimodal Dense Retriever",
            "char_start_offset": 6360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 726,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 815,
                    "end": 833,
                    "matchedPaperCorpusId": "235421619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "224803983",
            "title": "Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently",
            "text": "Recent studies use neural networks to improve first-stage retrieval. We classify them into three categories, namely sparse retrieval, dense retrieval, and end-to-end retrieval. \n\nSparse Retrieval: Several works use neural networks to improve sparse retrieval performance. doc2query [10] and docTTTTTquery [21] are proposed to alleviate the vocabulary mismatch problem where the query terms are different from those in the relevant documents. They use deep language models [22,23] to predict possible query terms for each document. They expand the documents with these predicted query terms. DeepCT [24] replaced the term frequency field in BM25 with term weights predicted by BERT [22]. Thus, the bag-of-words retrievers use term weights based on semantic importance rather than term frequency. \n\nDense Retrieval: Dense Retrieval is a representation-based first-stage retriever. It relies on neural models to represent text as embeddings, i.e., real-valued vectors in low-dimensional space. Similarity search [25], such as maximum inner product search, is then used to efficiently retrieve the vectors that are similar to the query vector. Section 3 introduces its architecture and inference procedure in details. Though early research [26] found that representation-based models usually underperform interaction-based models, recent language modeling advances prompt researchers to revisit the Dense Retrieval approach. With pretrained deep language models, several works [11,13,14,16] demonstrated that DR can significantly outperform traditional methods. We present the details of these works in Section 3.3. \n\nEnd-to-End Retrieval: Several works [20,27,28] focused on improving the inference-time efficiency of the BERT ranker [4]. Khattab et al. [20] further applied their proposed ColBERT for end-to-end retrieval. Its retrieval performance almost matches existing BERT-based models and outperforms many non-BERT baselines. For each document, they precompute contextualized word representations offline. During online stage, they utilize light-weight interactions between query and document terms to judge relevance.",
            "score": 0.533268733370455,
            "section_title": "Related Work",
            "char_start_offset": 4864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "81977235"
                },
                {
                    "start": 1477,
                    "end": 1480,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1650,
                    "end": 1654,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1654,
                    "end": 1657,
                    "matchedPaperCorpusId": "211572791"
                },
                {
                    "start": 1657,
                    "end": 1660,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 1751,
                    "end": 1755,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318115234375
        },
        {
            "corpus_id": "254564747",
            "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
            "text": "(1 \u2264  \u2264  ) to denote the encoding of the first  layers of  1 and  2 , respectively. The two major differences between those models are (1) the instantiations of  1 and  2 and (2) the similarity calculation function  (\u2022). To better elaborate the unified view, we show the following examples. \n\nDual Encoder (DE) [20] is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) [21] can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) [40] has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation.",
            "score": 0.5330961284346933,
            "section_title": "2",
            "char_start_offset": 10356,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1563
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66748046875
        },
        {
            "corpus_id": "256630726",
            "title": "Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval",
            "text": "Information Retrieval (IR) aims to retrieve relevant passages for a given query, which plays a critical role in many industry scenarios such as Open-Domain Question Answering (QA) (Lee et al., 2019) and Web Search (Nguyen et al., 2016). Traditionally, bag-of-words (BOW) retrieval systems such as TF-IDF and BM25 (Robertson and Zaragoza, 2009) were widely used, which mainly depend on keyword matching between queries and passages. With the development of large-scale pretrained language models (PLMs) (Vaswani et al., 2017;Devlin et al., 2019) such as BERT, dense retrieval methods (Lee et al., 2019;Karpukhin et al., 2020) show quite effective performance. These methods usually employed a dual-encoder architecture to encode both queries and passages into dense embeddings and then perform approximate nearest neighbor searching (Johnson et al., 2021). \n\nRecently, some works found that dense retrievers perform poorly in the zero-shot multilingual settings (Zhang et al., 2021b) due to the distributional shift. To boost the performance of dense retrievers, some previous methods for cross-domain retrieval can be directly adopted to unsupervised multilingual dense retrieval. There are two important kinds: 1) generating training data in target languages. For example, Kulshreshtha et al. applied self-training to generate labeled data and further proposed back-training (Kulshreshtha et al., 2021) to obtain more high-quality data. QGen (Ma et al., 2021a) proposed to use a query generator to generate in-domain queries. 2) leveraging sparse retrievers, which is more effective in the unsupervised setting, to enhance dense retrievers. For example, SPAR (Chen et al., 2021) proposed to distill knowledge from BM25 to the dense model and LaPraDoR (Xu et al., 2022) proposed to enhance the dense model by multiplying the similarity with the BM25 score. \n\nHowever, there are three major problems when directly adopting these methods to multilingual dense retrieval.",
            "score": 0.5328605849316256,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 198,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 214,
                    "end": 235,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 313,
                    "end": 343,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 502,
                    "end": 524,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 601,
                    "end": 624,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 832,
                    "end": 854,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1443,
                    "end": 1461,
                    "matchedPaperCorpusId": "231704318"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "254853896",
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "text": "In recent years, with the development of largescale pre-trained language models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), we witness the research interest in information retrieval shifting from the traditional sparse retrieval to neural dense retrieval. Karpukhin et al. (2020); Xiong et al. (2021) proposed dense passage retriever (DPR), which uses two neural encoders initialized with BERT to model the query and document, independently. The subsequent works follow this dual-encoder framework. One line of work improves dense retrieval by mining hard negatives, where they select the top-ranked documents retrieved by the recent retriever as hard negatives and then re-train the retriever with the newly mined hard negatives (Xiong et al., 2021;Qu et al., 2021). However, the mined hard negatives are highly likely to contain false negatives, harming the performance. To mitigate this, following studies denoise the mined hard negatives with re-rankers (Qu et al., 2021;Ren et al., 2021;Zhang et al., 2022). Another kind of work focuses on pre-training to make the pre-trained models more suitable for dense retrieval, such as Condenser (Gao and Callan, 2021), coCondenser (Gao and Callan, 2022) and SIMLM (Wang et al., 2022). \n\nDense retrieval models typically depend on extensive supervised data, comprising pairs of queries and positive documents. To address the challenge of limited training data, Ma et al. (2021); Sun et al. (2021) proposed to train a query generator on highresource information retrieval data, and then used the query generator to generate synthetic queries for low-source target domains. Additionally, Dai et al. (2023) harnessed large language models in zero/few settings to produce synthetic queries for documents in target domains, eliminating the need for training a general query generator. In contrast to these approaches, our work leverages synthetic queries for document expansion.",
            "score": 0.5325422081546054,
            "section_title": "Dense Retrieval",
            "char_start_offset": 24814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 115,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 280,
                    "end": 303,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 305,
                    "end": 324,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 754,
                    "end": 774,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 774,
                    "end": 790,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 982,
                    "end": 999,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 999,
                    "end": 1016,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1016,
                    "end": 1035,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1202,
                    "end": 1224,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1431,
                    "end": 1447,
                    "matchedPaperCorpusId": "231704318"
                },
                {
                    "start": 1449,
                    "end": 1466,
                    "matchedPaperCorpusId": "235303662"
                },
                {
                    "start": 1656,
                    "end": 1673,
                    "matchedPaperCorpusId": "252519173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53125
        },
        {
            "corpus_id": "236772688",
            "title": "Self-supervised Answer Retrieval on Clinical Notes",
            "text": "Traditionally, passage retrieval is based on overlapping terms in queries and passages (O'Connor, 1980;Salton et al., 1993) with term weighting strategies such as TF-IDF (Jones, 1972) or BM25 (Robertson et al., 1995). This exact match strategy often suffers from low recall. To overcome this problem, neural approaches match between query 1 Code and evaluation data available at [ANONYMIZED] and passage with a similarity function learned on contextual features, such as feed-forward networks (Huang et al., 2013), convolutional neural networks (Hu et al., 2014) or word count histograms (Guo et al., 2016). More recently, Zhu et al. (2019) (HAR) used attention and GloVe embeddings (Pennington et al., 2014) to capture local semantic features between a given passage and query. \n\nTransformer-based passage retrieval. Humeau et al. (2020) apply three different Transformer-based architectures to the passage retrieval task, which offer a different level of coupling between query and passage representations. Bi-encoders initially learn individual networks for query or passage and later a matching function for these independent representations. Cross-encoders apply full self-attention between query and passage, allowing for direct interaction between words or phrases. Humeau et al. (2020) propose Poly-encoder as a compromise between the high computation cost of Cross-encoder and the efficiency of the Bi-encoder to gain better performance. Furthermore, Khattab and Zaharia (2020) show that a late interaction in the form of a fine-grained similarity matching between queryand document representation can improve over regular Bi-encoders. \n\nContextual discourse representation. Arnold et al. (2020) propose a dual-encoder architecture that embeds discourse information from the entire document into passage representations. Their Contextual Discourse Vectors (CDV) model utilises a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to encode a sequence of sentence representations generated by either GloVe, FastText (Bojanowski et al., 2017) or BioBERT embeddings (Lee et al., 2019).",
            "score": 0.5318452215761009,
            "section_title": "Related Work",
            "char_start_offset": 3731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 103,
                    "matchedPaperCorpusId": "25240758"
                },
                {
                    "start": 103,
                    "end": 123,
                    "matchedPaperCorpusId": "15801148"
                },
                {
                    "start": 192,
                    "end": 216,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 493,
                    "end": 513,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 545,
                    "end": 562,
                    "matchedPaperCorpusId": "4497054"
                },
                {
                    "start": 588,
                    "end": 606,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 623,
                    "end": 640,
                    "matchedPaperCorpusId": "86601362"
                },
                {
                    "start": 683,
                    "end": 708,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 818,
                    "end": 838,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1273,
                    "end": 1293,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1460,
                    "end": 1486,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1684,
                    "end": 1704,
                    "matchedPaperCorpusId": "211010432"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490966796875
        },
        {
            "corpus_id": "248366293",
            "title": "Autoregressive Search Engines: Generating Substrings as Document Identifiers",
            "text": "Surfacing knowledge from large corpora is a crucial step when dealing with knowledge intensive language tasks (Levy et al., 2017;Dinan et al., 2019;Elsahar et al., 2018;Petroni et al., 2021), such as open-domain question answering (Voorhees et al., 1999;Joshi et al., 2017;Yang et al., 2018;Kwiatkowski et al., 2019) and fact checking (Thorne et al., 2018). A popular paradigm to approach such tasks is to combine a search engine with a machine reader component. The former retrieves relevant context, usually in the form of short passages, which the latter then examines to produce answers (Chen et al., 2017;Lewis et al., 2020;Izacard and Grave, 2021). \n\nIn recent years we have witnessed a surge of research and development in autoregressive language models (Radford et al., 2019;Lewis et al., 2019;Raffel et al., 2019;Brown et al., 2020;Rae et al., 2021;Artetxe et al., 2021;Smith et al., 2022), with ever increasing size and natural language understanding (NLU) capabilities. Such models are currently the de-facto implementation of the machine reader component in retrieval-reader architectures, and have contributed to rapid progress on a wide range of benchmarks (Joshi et al., 2017;Kwiatkowski et al., 2019;Petroni et al., 2021). However, these tremendous advances in aggressive modelling has yet to bring similar transformational changes in how retrieval is approached. \n\nTransferring the NLU capabilities of modern autoregressive models to retrieval is non-trivial. Some works have demonstrated that knowledge stored in the parameters of these models can be retrieved to some extend by directly generating evidence given a query (Petroni et al., 2019(Petroni et al., , 2020;;Roberts et al., 2020). However, such approaches have been shown to be unreliable because of their tendency to hallucinate non-factual content (Massarelli et al., 2019;Metzler et al., 2021;Ji et al., 2022).",
            "score": 0.5316947337653112,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 148,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 169,
                    "end": 190,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 231,
                    "end": 254,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 254,
                    "end": 273,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 335,
                    "end": 356,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 629,
                    "end": 653,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 761,
                    "end": 783,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1171,
                    "end": 1191,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1216,
                    "end": 1237,
                    "matchedPaperCorpusId": "221507798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2117919921875
        },
        {
            "corpus_id": "248965423",
            "title": "Table Retrieval May Not Necessitate Table-specific Model Design",
            "text": "Given the previous analysis, we hypothesize that general-purpose text-based retrievers without tablespecific designs might not be necessarily worse than special-purpose table-based retrievers, contradictory to what most previous work has assumed (Herzig et al., 2021(Herzig et al., , 2020;;Yin et al., 2020;Wang et al., 2021b). Properly trained text-based retrievers might even outperform table-based retrievers because the strong content matching ability learned on text retrieval datasets can transfer to the table retrieval task. \n\nTo validate these assumptions, we examine two representative retrieval systems: the text-based Dense Passage Retriever (DPR) and the table-based Dense Table Retriever (DTR). We first briefly introduce their input formats and model architectures ( \u00a7 3.1, \u00a7 3.2), then conduct experiments in both zeroshot and fine-tuning settings and compare their table retrieval performance ( \u00a7 3.3).",
            "score": 0.531678158781036,
            "section_title": "Text Retrieval vs Table Retrieval",
            "char_start_offset": 7026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 919
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 290,
                    "matchedPaperCorpusId": "214802901"
                },
                {
                    "start": 290,
                    "end": 307,
                    "matchedPaperCorpusId": "218674345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17236328125
        },
        {
            "corpus_id": "277272101",
            "title": "Dense Passage Retrieval in Conversational Search",
            "text": "Dense Passage Retrieval (DPR) is a first stage retriever having an architecture of dual-encoders where each encoder is an instantiation of BERT [7]. These encoders encode the query and passage into a dense vector space where relevant pairs are placed closer together than irrelevant ones. DPR uses the following similarity function to measure the relevance of a query q and passage p: \n\nThe model is then trained using the negative log likelihood as seen in the following equation: L(\u20d7 e q,i , \u20d7 e + p,i , \u20d7 e \u2212 p,i,1 , ..., \u20d7 e \u2212 p,i,n ) = \u2212log e sim(\u20d7 eq,i,\u20d7 e + p,i ) \n\ne sim(\u20d7 eq,i,\u20d7 e + p,i ) + n j=1 e sim(\u20d7 eq,i,\u20d7 e \u2212 q,i,j ) \n\nAs seen the conditional probability P r(\u20d7 e + p,i |\u20d7 e q,i ) is estimated using a sampled SoftMax for practicality. Additionally, DPR uses in-batch negatives for memory efficiency and one additional gold or hard negative that comes from a negative within the top-1000 retrieved passages from a BM25. Additionally, DPR can be indexes efficiently since the dense embeddings are in a lower dimensional continuous space. As previously mentioned in the introduction section we were unable to use hard-negatives due to lack of annotation, in the case of MSMARCO 70% [8] of the negative candidates in the top-1000 BM25 pool are false negatives. Therefore we used in-batch negatives for fine-tuning DPR and used DPR's [11] model weights to instantiate our base DPR model. To resolve this issue Approximate nearest neighbor Constrastive Estimation (ANCE) [17] is a computationally intensive technique for training a dense retriever. This technique asynchronously creates embeddings for the entire corpus from a checkpoint during training. Next the ANN index is updated so that negatives which are closest to the queries can be utilized for negative samples. As these samples are more informative during training and improve training convergence by creating large gradient norms.",
            "score": 0.53138766558736,
            "section_title": "B. Dense Retrieval",
            "char_start_offset": 9560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "Given a query q, dense retriever is required to retrieve k most relevant passages {p i } k i=1 from a large corpus consisting of hundreds of thousands of passages. For the sake of retrieval efficiency, the dual-encoder architecture is widely adopted, where query encoder E Q (\u2022) and passage encoder E P (\u2022) are used to embed query q and passage p into ddimensional vectors, respectively. The similarity between query q and passage p can be computed as the dot product of their vectors: \n\n(1) \n\nThe training objective of the dual-encoder is to learn embeddings of queries and passages to make positive query-passage pairs have higher similarity than the negative query-passage pairs in training data. Hence, the contrastive-learning loss function is adopted for the dual-encoder: \n\nwhere q and p + represent query and positive passage, respectively, and {p \u2212 } represents the set of negative passages. \n\nIn practical retrieval systems, passage embeddings are usually pre-computed offline, while query embeddings are computed by the query encoder in an ad hoc manner. Therefore we can obtain better passage embeddings through a complicated encoder as long as it does not increase the online inference latency.",
            "score": 0.531257349238572,
            "section_title": "Preliminary",
            "char_start_offset": 7943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1207
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "251402929",
            "title": "Learning Diverse Document Representations with Deep Query Interactions for Dense Retrieval",
            "text": "Document retrieval plays an important role in information retrieval (IR) tasks such as web search and open domain question answering (Chen et al., 2017). Early works such as BM25-based retriever (Robertson and Zaragoza, 2009) rely on lexical term matching to calculate the relevance of a pair of texts. Recently, neural network based dense retrieval (Karpukhin et al., 2020) has gained traction in research community. Dense retrieval learns a neural encoder to map queries and documents into a dense, low-dimensional vector space, and is less vulnerable to term mismatch problem compared to lexical match-based approaches. \n\nThere are two architectures to model the relevance between queries and documents. Dual encoder architecture encodes query and document separately into fixed-dimensional vectors (Karpukhin et al., 2020), where the similarity between query and document is usually instantiated as a dot product or cosine similarity between their vectors. As there are no interactions between query and document, dual encoder approach permits efficient inference with vector space search on precomputed document vectors. Cross encoder architecture feeds the concatenation of a query and document pair into one encoder to calculate its relevance score (Nogueira and Cho, 2019). Compared to dual encoder, cross encoder is more accurate due to the deep interaction between query and document, but comes with computation costs infeasible for first-stage retrieval. It is highly desirable to design a retrieval model which can match the performance of the cross encoder approach while maintaining the inference efficiency of the dual encoder approach. \n\nTo this end, previous works mainly focus on two directions: late-interaction and distillation. The first solution is to design a hybrid architecture, where the early layers act as a dual encoder while the late layers work like a cross encoder (MacAvaney et al., 2020;Khattab and Zaharia, 2020;Humeau et al., 2020). Its effectiveness comes with the cost of retrieval latency due to the computation involved with late layers.",
            "score": 0.5311545260467202,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 152,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 195,
                    "end": 225,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 350,
                    "end": 374,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 802,
                    "end": 826,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1897,
                    "end": 1921,
                    "matchedPaperCorpusId": "216641996"
                },
                {
                    "start": 1921,
                    "end": 1947,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56591796875
        },
        {
            "corpus_id": "237605196",
            "title": "Towards Universal Dense Retrieval for Open-domain Question Answering",
            "text": "Most modern retrievers today rely on dense representations to encode queries and documents into a low-dimensional vector space describing semantic meaning. Dense models are built on top of innovations in other areas of the natural language processing community like large pre-trained language models. \n\nBERT Dense retrievers today use Bidirectional Encoder Representations from Transformers, commonly referred to as BERT (Devlin et al., 2019), as the backbone to obtain dense representations. \n\nBERT consists of a stack of encoders based on the Transformer architecture (Vaswani et al., 2017) which use multi-head self-attention to learn powerful representations over the input sequence. \n\nBERT performs large-scale pre-training using the Masked Language Modeling (MLM) objective. \n\nThe task replaces 15% of tokens in the input sequence with a special retrievers use BERT to obtain corresponding hidden representations h \n\nRather than use all hidden representations to represent the sequence, most retrievers will compress the information using a reduction function reduce that outputs a single hidden vector, usually the representation of the [CLS] token. A general dense encoding function can be implemented as follows: \n\nwhere x is an input token sequence. Most models follow a dual encoder architecture, where one BERT model encodes the query and a separate BERT model encodes the passage. For a particular query, we can calculate the dense retrieval score S de for a query q and passage p as follows: \n\nwhere the superscript (Q) and (P ) denote distinct BERT encoder models. Note that in most dense models, the passage is tokenized as the concatenation of the title of the article it comes from and its contents, separated by an [SEP] token.",
            "score": 0.5307325859210702,
            "section_title": "Dense Retrieval",
            "char_start_offset": 9201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1746
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 442,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 570,
                    "end": 592,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "240288895",
            "title": "Dense Hierarchical Retrieval for Open-Domain Question Answering",
            "text": "In the retrieval process, a commonly used approach referred as a dual encoder model (Bromley et al., 1993) consists of a question encoder E Q and a context encoder E P , which encodes the question and the passage to l dimensional vectors, respectively. Unlike sparse term-based retrievers that rely on term frequency and inverse document frequency, dense neural retrievers formulate a scoring function between question q and passage p by the similarity of their embeddings, formalized as\n\nwhere E \u03b8 Q (q) \u2208 R l and E \u03b8 P (p) \u2208 R l are the embeddings, and \u00b7, \u00b7 represents a similarity function such as doc product and cosine similarity. Typically, E \u03b8 Q and E \u03b8 P are two large pre-trained models, e.g., BERT . We use different subscripts and same superscript \u03b8 to emphasize that these are two language models and fine tuned jointly. DPR  is one of the representative models in this model family. Contrastive Learning. Given a training set S = {(q 1 , y 1 ), \u00b7 \u00b7 \u00b7 , (q m , y m )}, we can create a training set T = {(q 1 , p + 1 , P \u2212 1 ), \u00b7 \u00b7 \u00b7 , (q m , p + m , P \u2212 m )} for the retrieval, where q i , y i , p + i , P \u2212 i are a question,   its answer, its positive passage and a set of negative passages, respectively. All the selections of positive passages or documents in this paper are described in Appendix 4.3. For the set of negative passages P \u2212 i , it is constructed in two ways: 1) BM25 negatives: top BM25-based passages not containing the answer; 2) In-batch negatives: passages paired with other questions appearing in the same mini batch.\n\nTraining. The objective of training is to learn an embedding function such that relevant pairs of questions and passages will have higher similarity than the irrelevant ones. For each training instance (q i , p + i , P \u2212 i ) \u2208 T , we contrastively optimize the negative log-likelihood of each positive passages against their negative passages,\n\nInference. During inference, we encode the given question q and conduct the",
            "score": 0.5306950518162542,
            "section_title": "Dual Encoder Retrieval Model",
            "char_start_offset": 5139,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 106,
                    "matchedPaperCorpusId": "16394033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "To better understand how different model training schemes affect retrieval performance (RQ4), we perform an ablation on our multimodal dense retriever and discuss our findings below. \n\nLearning Rate In traditional dense text retrieval, the same learning rate is used to update all the weights in the retriever (Karpukhin et al., 2020;Qu et al., 2021). However, in our multimodal dense retrieval setup, the HuBERT (question encoder) and BERT (passage encoder) models are pre-trained independently to allow the usage of available largescale unsupervised data. Therefore, there can be disparities between the two modalities that can hurt performance. To alleviate this problem we follow an alternative setting where the two encoders have different learning rates. In particular, since the language model contains more information than the speech model, we increase the learning rate of the passage encoder by a factor of 10. Comparing the values in the first block of Table 5, we find that the choice of learning rate is important for effectively training our multimodal dense retriever. \n\nPooling Learning Rate AR@20 AR@100 first p: 2e-5, q: 2e-5 50.77 64.48 first p: 2e-5, q: 2e-4 57.25 70.11 mean p: 2e-5, q: 2e-4 56.59 69.97 max p: 2e-5, q: 2e-4 53.57 67.45 Pooling The next ablation involves different pooling methods for encoding the spoken question into a single vector. Following previous works on dense retrieval, we use the [CLS] token embedding output from BERT to encode the text passage. In contrast, this decision is not that straightforward in the case of the spoken question. The HuBERT speech transformer we use for encoding the spoken questions does not have a next-sentence prediction pre-training task as in BERT. Thus, there is no [CLS] token available. To this extent, we asses dif-ferent pooling strategies for encoding the spoken question, namely, max and mean pooling or taking the first embedding of the sequence as a pooling strategy.",
            "score": 0.5306437238298919,
            "section_title": "Ablation Study on Model Training",
            "char_start_offset": 24121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1958
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 334,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 334,
                    "end": 350,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33984375
        },
        {
            "corpus_id": "252739471",
            "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
            "text": "Dense retrieval aims to learn effective representations of queries and documents by making representations of relevant query-document pairs to be similar (Chopra et al., 2005;Van den Oord et al., 2018). With the success of dense passage retrieval for open-domain question answering (QA) Karpukhin et al., 2020), recent studies build an index for a finer granularity such as dense phrase retrieval (Lee et al., 2021a), which largely improves the computational efficiency of open-domain QA * JL currently works at Google Research. \u2020 YZ currently works at Apple. by replacing the retriever-reader model (Chen et al., 2017) with a retriever-only model (Seo et al., 2019;. Also, phrase retrieval provides a unifying solution for multi-granularity retrieval ranging from open-domain QA (formulated as retrieving phrases) to document retrieval (Lee et al., 2021b), which makes it particularly attractive. Building a dense retrieval system involves multiple steps (Figure 1) including training a dual encoder ( \u00a74), selecting the best model with validation ( \u00a73), and constructing an index (often with filtering) for an efficient search ( \u00a75). However, these components are somewhat loosely connected to each other. For example, model training is not directly optimizing the retrieval performance using the full corpus on which models should be evaluated. In this paper, we aim to minimize the gap between training and inference of dense retrievers to achieve better retrieval performance.\n\nHowever, developing a better dense retriever requires validation, which requires building large indexes from a full corpus (e.g., the entire Wikipedia for open-domain QA) for inference with a huge amount of computational resources and time. To tackle this problem, we first propose an efficient way of validating dense retrievers without building large-scale indexes. Analysis of using a smaller random corpus with different sizes for the validation reveals that the accuracy from small indexes does not necessarily correlate well with the retrieval accuracy on the full index. As an alternative, we construct a compact corpus using a pre-trained dense retriever so that validation on this corpus better correlates well with the retrieval on the full scale while keeping the size of the corpus as small as",
            "score": 0.5306403502978667,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "5555257"
                },
                {
                    "start": 287,
                    "end": 310,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 397,
                    "end": 416,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 600,
                    "end": 619,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 837,
                    "end": 856,
                    "matchedPaperCorpusId": "237532725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.452880859375
        },
        {
            "corpus_id": "259949948",
            "title": "Offline Pseudo Relevance Feedback for Efficient and Effective Single-pass Dense Retrieval",
            "text": "Dense retrieval. The retrieval models based on PLMs [12] have been developed into a new trend of information retrieval. There are two typical architectures for dense retrieval, viz. cross-encoder and bi-encoder. The crossencoder architecture [13,14,15] takes the concatenation of the query and the document as input and utilizes PLMs to catch the semantic interaction. The bi-encoder [3,16,4] architecture adopts the two-tower architecture and transforms query and document into single-or multi-representation [3,2] allowing the pre-computation of document representation for efficiency. \n\nPseudo relevance feedback was first proposed to ease the term mismatch problem [17] and now increasingly raises attention due to its effectiveness in dense retrieval. Many [8,18,9] have put efforts to employ PRF techniques in dense retrieval and reach a satisfactory result. Though developed, current PRF approaches may be impeded by the increased computational requirements, which could restrict their widespread use for very large datasets. \n\nPLM-based reranker. Pre-trained language model [19] has been explored to construct re-ranker [20,21] to further re-order the candidate passages returned by the firstpass retriever. The cross-encoder architecture is typically employed for the re-ranker [13,22,23] with substantial improvement while burdening the online latency. The contextualized relevance offline weighting (CROW) [11] approach is proposed following the idea of splitting the pipeline into the online and the offline stages [24,25]. Despite the gain in efficiency, the heavy offline computational cost hinders further enhancing effectiveness with techniques like PRF, which is attempted to address in this paper through lightweight integration of PRF.",
            "score": 0.5300060607737017,
            "section_title": "Related Work",
            "char_start_offset": 2302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1754
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "5688521"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "5878197"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 387,
                    "end": 390,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "3002280"
                },
                {
                    "start": 762,
                    "end": 765,
                    "matchedPaperCorpusId": "53081945"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "213861855"
                },
                {
                    "start": 768,
                    "end": 770,
                    "matchedPaperCorpusId": "231648324"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1291,
                    "end": 1294,
                    "matchedPaperCorpusId": "211481850"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "235792532"
                },
                {
                    "start": 1527,
                    "end": 1531,
                    "matchedPaperCorpusId": "16346772"
                },
                {
                    "start": 1531,
                    "end": 1534,
                    "matchedPaperCorpusId": "53429039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "278165747",
            "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism",
            "text": "SSMs offer efficient processing of long sequences with fixed state sizes, but struggle with algorithmic tasks like retrieving past context. In this work, we examine how such in-context retrieval operates within Transformer- and SSM-based language models. We find that both architectures develop the same fundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first identifies and extracts relevant information from the context, which an Aggregate Head then integrates into a final representation. Across both model types, G&A concentrates in just a few heads, making them critical bottlenecks even for benchmarks that require a basic form of retrieval. For example, disabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades its ability to retrieve the correct answer letter in MMLU, reducing accuracy from 66% to 25%. This finding suggests that in-context retrieval can obscure the limited knowledge demands of certain tasks. Despite strong MMLU performance with retrieval intact, the pruned model fails on other knowledge tests. Similar G&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the significance of G&A in performance, we show that retrieval challenges in SSMs manifest in how they implement G&A, leading to smoother attention patterns rather than the sharp token transitions that effective G&A relies on. Thus, while a gap exists between Transformers and SSMs in implementing in-context retrieval, it is confined to a few heads, not the entire model. This insight suggests a unified explanation for performance differences between Transformers and SSMs while also highlighting ways to combine their strengths. For example, in pretrained hybrid models, attention components naturally take on the role of Aggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A head with an attention-based variant significantly improves retrieval.",
            "score": 0.529998456705089,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1922607421875
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "This paper proposes a novel pre-training method SIMLM for dense passage retrieval. It follows an encoder-decoder architecture with a representation bottleneck in between. The encoder learns to compress all the semantic information into a dense vector and passes it to the decoder to perform well on the replaced language modeling task. When used as initialization in a dense retriever training pipeline, our model achieves competitive results on several large-scale passage retrieval datasets.\n\nFor future work, we would like to increase the model size and the corpus size to examine the scaling effects. It is also interesting to explore other pre-training mechanisms to support unsupervised dense retrieval and multilingual retrieval.",
            "score": 0.5295977920955736,
            "section_title": "Conclusion",
            "char_start_offset": 20697,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4990234375
        },
        {
            "corpus_id": "246035178",
            "title": "Improving Biomedical Information Retrieval with Neural Retrievers",
            "text": "Neural Retrievers aim to retrieve relevant context from a large corpus given a query. NRs can be clubbed into two architectural families -cross-attention models (Nogueira and Cho 2019;MacAvaney et al. 2019;Yang et al. 2019), and dual-encoder models which employ separate encoders to encode the query and context (Karpukhin et al. 2020;. The cross-attention model requires heavy computation and can not be directly used in a large corpus setting, while dual-models can allow pre-computation of context representations and the application of efficient search methods such as MIPS (Shrivastava and Li 2014) during inference. To take advantage of both models, Poly-Encoder (Humeau et al. 2020) uses K representations for each query and an attention mechanism to get context-specific query representations. Col-BERT (Khattab and Zaharia 2020) extends the dual-encoder architecture by performing a token-level interaction step over the query and context representations, but requires significant memory for large corpora (Thakur et al. 2021).\n\nPre-training Tasks for NR. Masked language modeling (MLM) and next-sentence prediction introduced in BERT ( Devlin et al. 2019) have led to a paradigm shift in the training of neural network models for multiple NLP tasks. For text retrieval, pre-training tasks that are more aligned with the retrieval task have been developed.  propose Body First Selection (BFS), and Wiki Link Prediction (WLP) for document retrieval. Lee, Chang, and Toutanova (2019) propose an Inverse Cloze Task (ICT) task in which a random sentence drawn from a passage acts as a query and the remaining passage as a relevant answer. Guu et al. (2020) show that ICT effectively avoids the cold-start problem.\n\nQuestion Generation (QG) methods have become sophisticated due to the advances in sequence-to-sequence modeling (Sutskever, Vinyals, and Le 2014); QG is considered an auxiliary pre-training task for question answering models . One set of QG methods can be categorized as 'Answer-Aware' QG (Du and Cardie 2018;Zhao et al. 2018;Dong et al. 2019), in which an answer extraction model",
            "score": 0.5295652246586073,
            "section_title": "Related Work",
            "char_start_offset": 5683,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 206,
                    "matchedPaperCorpusId": "131776819"
                },
                {
                    "start": 206,
                    "end": 223,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 312,
                    "end": 335,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 669,
                    "end": 689,
                    "matchedPaperCorpusId": "210063976"
                },
                {
                    "start": 1015,
                    "end": 1034,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1146,
                    "end": 1164,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1458,
                    "end": 1490,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1832,
                    "end": 1865,
                    "matchedPaperCorpusId": "7961699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "250390910",
            "title": "An Encoder Attribution Analysis for Dense Passage Retriever in Open-Domain Question Answering",
            "text": "In the previous sections, we manage to identify the importance of the passage encoder and its affecting factors such as positive passage overlap and corpus coverage of the training data. We find that our discoveries are consistent with some previous work's conclusions. For example, Zhan et al. (2021Zhan et al. ( , 2020a)); Sciavolino et al. (2021) all find that it is sufficient to achieve reasonable retrieval accuracy by just fine-tuning the question encoder with a fixed passage encoder, which demonstrates the importance of a robust passage encoder in domain adaptation and hard-negative mining. \n\nHowever, how to learn such a robust passage encoder is challenging as pre-training DPR on a single QA dataset will introduce biases. Multi-task dense retrieval (Maillard et al., 2021;Li et al., 2021;Metzler et al., 2021) uses multiple experts learned in different domains to solve this problem. These solutions are effective but not efficient as they build multiple indexes and perform searches for each expert, requiring a lot of resources and storage space. \n\nAnother solution is to build a question-agnostic passage encoder so that the model is not biased towards particular QA tasks. DensePhrases (Lee et al., 2021a,b) pioneers this direction by building indexes using phrases instead of chunks of passages for multi-granularity retrieval. By breaking passages into finer-grained units, DensePhrases indeed improve the generalization of dense retrieval in different domains with query-side fine-tuning. However, similar to multi-task learning, it is not efficient as the phrase index can be enormous for a corpus like Wikipedia. Although techniques such as product quantization (Gray and Neuhoff, 1998) can be applied to improve efficiency, it comes at the cost of effectiveness. \n\nOverall, it is desirable to have a robust passage encoder for efficient dense retrieval according to previous work and our analysis, but challenges still remain in the effectiveness-efficiency trade-off.",
            "score": 0.5291809996830469,
            "section_title": "Impact of Passage Encoders",
            "char_start_offset": 22516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 300,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 764,
                    "end": 787,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 787,
                    "end": 803,
                    "matchedPaperCorpusId": "240286557"
                },
                {
                    "start": 1686,
                    "end": 1710,
                    "matchedPaperCorpusId": "119156912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "248496840",
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "text": "Passage retrieval has been studied for decades, and many recent approaches of passage retrieval are using dense embeddings generated from deep neural models, called \"dense passage retrieval\". The state-of-the-art end-to-end dense passage retrieval systems normally deploy a deep neural model followed by an approximate nearest neighbor (ANN) search module. The model generates embeddings of the corpus and queries, which are then indexed and searched by the high-performance ANN module. With the increasing data scale, the ANN module unavoidably becomes the bottleneck on efficiency. An alternative is the learned index, which achieves significantly high search efficiency by learning the data distribution and predicting the target data location. But most of the existing learned indexes are designed for low dimensional data, which are not suitable for dense passage retrieval with high-dimensional dense embeddings.\n \n In this paper, we propose\n LIDER\n , an efficient high-dimensional\n L\n earned\n I\n ndex for large-scale\n DE\n nse passage\n R\n etrieval. LIDER has a clustering-based hierarchical architecture formed by two layers of core models. As the basic unit of LIDER to index and search data, a\n core model\n includes an adapted recursive model index (RMI) and a dimension reduction component which consists of an extended SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction component reduces the high-dimensional dense embeddings into one-dimensional keys and sorts them in a specific order, which are then used by the RMI to make fast prediction. Experiments show that LIDER has a higher search speed with high retrieval quality comparing to the state-of-the-art ANN indexes on passage retrieval tasks, e.g., on large-scale data it achieves 1.2x search speed and significantly higher retrieval quality than the fastest baseline in our evaluation. Furthermore, LIDER has a better capability of speed-quality trade-off.",
            "score": 0.5290430255453245,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31103515625
        },
        {
            "corpus_id": "271534382",
            "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
            "text": "Dense retrieval has become an important research area following the development of pre-trained Transformer language models (PLMs) [2], [19]- [22]. To enhance text retrieval performance, dense retrieval leverages PLM-based text embeddings to encode queries and documents into a shared semantic vector space, focusing on matching semantic contents beyond mere keywords. This text embedding application in retrieval is fundamental to Retrieval-Augmented Generation (RAG) [23], which reduces the hallucinations in LLMs. Recent advancements in dense retrieval include architectural innovations, optimized training methodologies, and efficient indexing techniques, all of which contribute to improved retrieval accuracy and efficiency [4]- [8], [24]. Since the introduction of Sentence-BERT [25] and Dense Passage Retrieval (DPR) [2], numerous sentence embedding models have been proposed to enhance dense passage retrieval. Advanced sentence embedding models, which have been highlighted in the retrieval task of massive text embedding benchmark (MTEB) [26], include Contriever [27], M3E 2 , BGE [9], etc. Our text augmentation method serves as a preprocessing module for dense retrieval and is compatible with various embedding models mentioned above.",
            "score": 0.5288368778628375,
            "section_title": "A. Dense Retrieval",
            "char_start_offset": 10070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "236772078"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "149698624"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "252907685"
                },
                {
                    "start": 1073,
                    "end": 1077,
                    "matchedPaperCorpusId": "249097975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36181640625
        },
        {
            "corpus_id": "238857121",
            "title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking",
            "text": "Recently, dense passage retrieval has become an important approach in the task of passage retrieval (Karpukhin et al., 2020) to identify relevant contents from a large corpus. The underlying idea is to represent both queries and passages as low-dimensional vectors (a.k.a., embeddings), so that the relevance can be measured via embedding similarity. Additionally, a subsequent procedure of passage re-ranking is widely adopted to further improve the retrieval results by incorporating a reranker Luan et al., 2021). Such a two-stage procedure is particularly useful in a variety of natural language processing tasks, including question answering ; Xiong * Equal contribution. The work was done when Ruiyang Ren was doing internship at Baidu. \u2020 Corresponding authors. et al., 2020b), dialogue system (Ji et al., 2014;Henderson et al., 2017) and entity linking (Gillick et al., 2019;. Following a retrieve-then-rerank way, the dense retriever in passage retrieval and the re-ranker in passage re-ranking jointly contribute to the final performance. Despite the fact that the two modules work as a pipeline during the inference stage, it has been found useful to train them in a correlated manner. For example, the retriever with a dual-encoder can be improved by distilling from the re-ranker with a more capable cross-encoder architecture , and the re-ranker can be improved with training instances generated from the retriever Huang et al., 2020;Gao et al., 2021b). Therefore, there is increasing attention on correlating the training of the retriever and re-ranker in order to achieve mutual improvement (Metzler et al., 2021;Huang et al., 2020;. Typically, these attempts train the two modules in an alternative way: fixing one module and then optimizing another module. It will be more ideal to mutually improve the two modules in a joint training approach.\n\nHowever, the two modules are usually optimized in different ways, so that the joint learning cannot be trivially implemented. Specially, the retriever is usually trained by sampling a number of in-batch negatives to maximize the probabilities of positive passages and minimize the probabilities of the",
            "score": 0.5280514540928724,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 124,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 860,
                    "end": 882,
                    "matchedPaperCorpusId": "202718954"
                },
                {
                    "start": 1428,
                    "end": 1447,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 1447,
                    "end": 1465,
                    "matchedPaperCorpusId": "231662379"
                },
                {
                    "start": 1628,
                    "end": 1647,
                    "matchedPaperCorpusId": "219965935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.401123046875
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020). More recently, Chen et al. (2021) attempted to train a dense retriever (called as dense Lexical Model \u039b) to imitate BM25 prediction. Though \u039b model underperforms BM25 by itself on EQ dataset, when combined with DPR trained on multiple QA datasets (DPRmulti), the combined model (SPAR) outperforms BM25. SPAR requires two individually trained bi-encoders (thus four BERT models) with tangled architecture, whereas our motivation is to investigate if dense retrievers could generalize with a single language model without ensemble. We also investigate whether dense retrieval has any strengths in areas where sparse retrieval is lacking. We experimentally demonstrate the differences between dense and sparse retrieval by using the IDF value to quantify the generality and rarity of an entity name in a question. \n\nMultiple Keys for Passage Retrieval. Several methods have been proposed for calculating fine-grained interactions between a question and passages using multiple retrieval keys. SPARTA (Sparse Transformer Matching, (Zhao, Lu, and Lee 2021)) and COIL (Contextualized Inverted List, (Gao, Dai, and Callan 2021)) produce passage representations for each token embedded by fine-tuned language models and store them in the inverted index. ColBERT (Khattab, Potts, and Zaharia 2021) leverages the embedding of all subword tokens in each passage to form a large collection of keys. However, all of the methods listed here require the encoders for questions and passages to be fine-tuned. During this fine-tuning for retrieval, multiple index updates are required, which is computationally expensive. We seek a retrieval method that works in zero-shot setting, so we propose a method that only requires the inference operation of a pretrained language model together with named-entity recognition (NER). \n\nLUKE model (Yamada et al. 2020) is the state-of-the-art language model for tasks related to named entities, including NER.",
            "score": 0.5277472259167612,
            "section_title": "Related Work",
            "char_start_offset": 6953,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 152,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1182,
                    "end": 1206,
                    "matchedPaperCorpusId": "221971009"
                },
                {
                    "start": 1248,
                    "end": 1275,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "266573365",
            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
            "text": "In information retrieval, the main task is to match the query and the documents. Generally, a retriever is used to retrieve a set of passages from a massive collection of documents that have relatively high relevance to a given query; these passages are then further ranked by a ranker based on their relevance scores, to obtain the most relevant passages to the query. Traditional methods employ bag-of-words models such as TF-IDF [24] and BM25 [26] to represent the similarity between the query and the documents. With the development of deep learning, word embedding has been widely used as a new representation. Both bag-of-words model and word embedding are sparse passage retrieval that rely on one-hot encoding or independent vector based on word frequencies. Sparse retrieval models have limitations such as vocabulary mismatch, semantic mismatch, and lack of contextual interaction, which make it challenged to meet the increasingly complex and diverse business needs and application scenarios. \n\nIn recent years, deep models like Transformer [30] and Bert [4] have been applied to information retrieval. They represent words and sentences as dense vectors, capturing contextual information and semantic relevance between query and documents, known as the dense passage retrieval. The advantage lies in scenarios where there is minimal overlap between the query and the documents. However, it requires labeled data to optimize model and thus performs worse than sparse retrieval model in zero-shot conditions. \n\nThe main dense retrieval models include the dual-encoder and cross-encoder models. In a dual-encoder model, two Transformerliked models, such as Bert, are separately trained for the query and the documents, and the semantic representation of the entire sentence is obtained using a CLS token, which is used to calculate the relevance. The cross-encoder model, on the other hand, trains the query and documents together by concatenating them, enabling interaction between words in the query and passage. This model can capture more contextual information and generally performs better than the dual-encoder model.",
            "score": 0.5275800287462235,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 436,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "207178704"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "261531457",
            "title": "AVATAR: Robust Voice Search Engine Leveraging Autoregressive Document Retrieval and Contrastive Learning",
            "text": "At the inference time, the score of a query-document pair can be efficiently computed as the inner product of the corresponding query and document embeddings. \n\nRecently, another novel autoregressive retrieval approach orthogonal to the dual encoder models dubbed Differentiable Search Index (DSI) [9] has been proposed. DSI first encodes all the information about the corpus into a single transformer model's parameters space, on top of which DSI can generate the relevant document identities (docids) in an autoregressive manner in response to a user query. DSI has many advantages over the dual encoder models: 1) It avoids using only dot products, which could miss the fine-grained interaction between the query and the document meta information. 2) It lowers memory requirements; storing dense vectors for the whole corpus requires a large memory footprint. Although  with the benefits mentioned above, DSI suffers serious data distribution problems during the model training and inference phase. [10]. Specifically, DSI learns to build connections between long document texts and their docids, but at inference time, relatively short queries are input into the model to retrieve their relevant docids. Differentiable search index with query generation (DSI-QG) [10] migrates this problem using another query generation model to generate relevant pseudo queries from documents. It uses these short pseudo queries instead of long text documents to build the connection with their docids in the training phase. After incorporating these pseudo queries [11], the retrieval model has become effective while requiring less memory has potential to be deployed on mobile or edge device, making a more secure and personal retrieval system. \n\nNevertheless, as far as we are concerned, the autoregressive retrieval model has not been sufficiently and systematically studied in neither ad-hoc information retrieval (IR) nor VS, and its retrieval effectiveness is mostly unknown. Based on this background, we present an empirical VS evaluation that sheds light on the efficacy of the autoregressive retrieval model in this paper. Further, we address one of the critical problems with VS, viz.",
            "score": 0.5271445272254153,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1736
                },
                {
                    "start": 1739,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "249890267"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "249890267"
                },
                {
                    "start": 1555,
                    "end": 1559,
                    "matchedPaperCorpusId": "251710261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412841796875
        },
        {
            "corpus_id": "278129624",
            "title": "Leveraging Decoder Architectures for Learned Sparse Retrieval",
            "text": "In this section, we outline the experimental design, including the models, training configurations, and evaluation methods, to investigate learned sparse retrieval (LSR) performance across various transformer architectures. \n\nRetrieval and Indexing: We apply the same architecture type with shared weights for both the query and document encoders. For indexing and retrieval, we use the Anserini toolkit [32,33] to index the encoded documents and retrieve with the encoded queries. \n\nDataset and Evaluation: We trained our models on the MS MARCO passage dataset [3], which includes 8.8 million passages and 40 million training triplets. All models are trained on those triplets and hard negatives generated by the cross-encoder teacher model in [14]. We report the MRR@10, NDCG@10 and Recall@1000 of all models on the MS MARCO Dev and TREC Deep Learning (TREC-DL) 2019/2020 [6,7]. In addition of the effectiveness metrics, we report the FLOPs metric (the average number of term overlap between a pair of query and document) as a proxy of efficiency. \n\nBackbones and baselines We selected three different pretrained transformer language model as the backbone of our study, including encoder-only (Distilbert-base-uncased [29]), decoder-only (OPT [35]) and encoder-decoder (T5 [5]). Distilbert-base-uncased is widely recognized and extensively employed within the LSR literature [9,12,15]. The OPT [35] and Flan-T5 [5] offers a wide range of parameter sizes. This variability enables us to explore how decoder-only architectures scale and perform in generating sparse representations, providing insights into their effectiveness across different model sizes. In terms of baselines, we select based on query expansion and term weighting strategies and compare against the unsupervised sparse model BM25 and the supervised sparse model, including EPIC [20], DeepCT [8], DocT5Query [26], DeepImpact [22], Splademax [12] and DistilSplade-max [12].",
            "score": 0.5266751909487773,
            "section_title": "Experimental Setup",
            "char_start_offset": 11527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1413,
                    "end": 1416,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1279296875
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at \\url{https://github.com/Ziems/llm-url}.",
            "score": 0.5265511560929534,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "272770506",
            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
            "text": "We compare our proposed multimodal method against pipeline approaches that consist of (i) an ASR model for transcribing the spoken question and (ii) a retriever. In detail, similar to Sidiropoulos et al. (2022) we use wav2vec 2.0 (Baevski et al., 2020) pretrained and fine-tuned on 960 hours of annotated speech data (Panayotov et al., 2015) as the ASR model. Concerning the retrievers, we experiment with popular lexical and dense retrievers. We further experiment with dense retrievers explicitly trained to improve robustness against questions with typos since they improve robustness against ASR noise as well (Sidiropoulos et al., 2022). \n\nWe need to underline that contrary to Sidiropoulos et al. ( 2022), which assumed that the clean textual version of the spoken question is provided for training the retrievers, we follow a real-world scenario where only spoken questions are available; thus, we need to transcribe them for training the retrievers. Our experimental results showed that we build stronger baselines by training on the transcriptions of spoken questions. In particular, we experiment with the following retrievers: \n\nBM25 is a traditional retriever based on termmatching. Question and retrieved passages have lexical overlap. \n\nDense Retriever (DR) is used for scoring question-passage pairs and consists of two separate neural networks (dual-encoder), each repre-senting a question and a passage. Given a question, a positive, and a set of negative passages, the learning task trains the two encoders by minimizing a loss function, typically softmax cross-entropy, to encourage positive question-passage pairs to have smaller distances than the negative ones. To train the DR model, we use the training scheme and hyperparameters described in (Karpukhin et al., 2020) with a batch size of 64 which is the largest we can fit in our GPU setup. \n\nDense retriever with data augmentation (DR+Augm.) alternates the training scheme of classic dense retrieval via the addition of data augmentation.",
            "score": 0.526024002213769,
            "section_title": "Baselines and Implementation",
            "char_start_offset": 9541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 252,
                    "matchedPaperCorpusId": "219966759"
                },
                {
                    "start": 317,
                    "end": 341,
                    "matchedPaperCorpusId": "2191379"
                },
                {
                    "start": 1767,
                    "end": 1791,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "In the information retrieval (IR) area, dense retrieval (DR) models use deep learning techniques to encode queries and passages into embedding space to compute their semantic relations. It is important for DR models to balance both efficiency and effectiveness. Pre-trained language models (PLMs), especially Transformer-based PLMs, have been proven to be effective encoders of DR models. However, the self-attention component in Transformer-based PLM results in a computational complexity that grows quadratically with sequence length, and thus exhibits a slow inference speed for long-text retrieval. Some recently proposed non-Transformer PLMs, especially the Mamba architecture PLMs, have demonstrated not only comparable effectiveness to Transformer-based PLMs on generative language tasks but also better efficiency due to linear time scaling in sequence length. This paper implements the Mamba Retriever to explore whether Mamba can serve as an effective and efficient encoder of DR model for IR tasks. We fine-tune the Mamba Retriever on the classic short-text MS MARCO passage ranking dataset and the long-text LoCoV0 dataset. Experimental results show that (1) on the MS MARCO passage ranking dataset and BEIR, the Mamba Retriever achieves comparable or better effectiveness compared to Transformer-based retrieval models, and the effectiveness grows with the size of the Mamba model; (2) on the long-text LoCoV0 dataset, the Mamba Retriever can extend to longer text length than its pre-trained length after fine-tuning on retrieval task, and it has comparable or better effectiveness compared to other long-text retrieval models; (3) the Mamba Retriever has superior inference speed for long-text retrieval. In conclusion, Mamba Retriever is both effective and efficient, making it a practical model, especially for long-text retrieval.",
            "score": 0.5256826130429293,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "247476289",
            "title": "Multi-View Document Representation Learning for Open-Domain Dense Retrieval",
            "text": "In addition to the aforementioned work focusing on the architecture design, there exist loads of work that proposes to improve the effectiveness of dense retrieval. Existing approaches of learning dense passage retriever can be divided into two categories:\n\n(1) pre-training for retrieval (Chang et al., 2020;Guu et al., 2020) and (2) finetuning pre-trained language models (PLMs) on labeled data (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021).\n\nIn the first category,  and Chang et al. (2020) propose different pre-training task and demonstrate the effectiveness of pre-training in dense retrievers. Recently, DPR-PAQ (Oguz et al., 2021) proposes domain matched pre-training, while Condenser (Gao and Callan, 2021a,b) enforces the model to produce an information-rich CLS representation with continual pre-training.\n\nAs for the second class, recent work (Karpukhin et al., 2020;Xiong et al., 2020;Qu et al., 2021;Zhan et al., 2021) shows the key of fine-tuning an effective dense retriever revolves around hard nega-tives. DPR (Karpukhin et al., 2020) adopts in-batch negatives and BM25 hard negatives. ANCE (Xiong et al., 2020) proposes to construct hard negatives dynamically during training. RocketQA (Qu et al., 2021;Ren et al., 2021b) shows the cross-encoder can filter and mine higher-quality hard negatives. Li et al. (2021) and Ren et al. (2021a) demonstrate that passage-centric and query-centric negatives can make the training more robust. It is worth mentioning that distilling the knowledge from crossencoder-based re-ranker into bi-encoder-based retriever (Sachan et al., 2021;Izacard and Grave, 2021;Ren et al., 2021a,b; can improve the bi-encoder's performance. Most of these works are built upon bi-encoder and naturally inherit its limit of a single vector representation",
            "score": 0.5249658268528037,
            "section_title": "Effective Dense Retrieval",
            "char_start_offset": 8857,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 309,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 397,
                    "end": 421,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 440,
                    "end": 456,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 868,
                    "end": 892,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 911,
                    "end": 927,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 927,
                    "end": 945,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1041,
                    "end": 1065,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1218,
                    "end": 1235,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1235,
                    "end": 1253,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1329,
                    "end": 1345,
                    "matchedPaperCorpusId": "236034464"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "236477844"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433349609375
        },
        {
            "corpus_id": "256461230",
            "title": "Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval",
            "text": "Information Retrieval (IR) aims to retrieve pieces of evidence for a given query. Traditional methods mainly use sparse retrieval systems such as BM25 (Robertson and Zaragoza, 2009), which depend on keyword matching between queries and passages. With the development of large-scale pretrained language models (PLMs) (Vaswani et al., 2017;Devlin et al., 2019) such as BERT, dense retrieval methods (Lee et al., 2019;Karpukhin et al., 2020) show quite effective performance. These methods usually employed a dual-encoder architecture to encode both queries and passages into dense embeddings and then perform approximate nearest neighbor searching (Johnson et al., 2021). \n\nRecently, leveraging a cross-encoder re-ranker as the teacher model to distill knowledge to a dual-  Figure 1: The performance of cross-encoder and query generator when varying the number of training samples and retrievers. We use BM25 and DPR as retrievers, respectively. For the cross-encoder (BERT-Large), we use retrieved top-100 passages which do not contain the answer as negative and contrastive loss for training. For the query generator (T5-Base), we firstly train it with the query generation task and then fine-tune the model with the same setting as BERT-Large. The reported performance is the top-5 score of re-ranked top 500 passages on the NQ test set. \n\nencoder has shown quite effective to boost the dualencoder performance. Specifically, these methods first train a warm-up dual-encoder and a warm-up cross-encoder. Then, they perform knowledge distillation from the cross-encoder to the dual-encoder by KL-Divergence or specially designed methods. For example, RocketQAv2 (Qu et al., 2021) proposed dynamic distillation, and AR2 (Zhang et al., 2021) proposed adversarial training. \n\nHowever, there are two major problems when scaling the method to the cross-lingual dense retrieval setting.",
            "score": 0.5242564007322794,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1771
                },
                {
                    "start": 1774,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 181,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 316,
                    "end": 338,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 338,
                    "end": 358,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 397,
                    "end": 415,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 415,
                    "end": 438,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 646,
                    "end": 668,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1663,
                    "end": 1680,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "Dense retrieval, a novel paradigm in Information Retrieval (IR), has emerged with the advancement of deep neural networks. Unlike traditional IR methods, dense retrieval encodes both queries and documents as embeddings within a shared latent space, capturing their semantic relationships through embedding similarities. Dense retrieval models have become the predominant choice in recent neural retrieval approaches and are widely applied in various downstream tasks such as web search, question answering, and sentence similarity (Karpukhin et al., 2020;Xiong et al., 2020;Muennighoff et al., 2022). \n\nIn the past few years, dense retrieval models intensively adopted pre-trained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020), as their backbone encoders. These models excel in identifying semantic similarities between queries and documents. However, they still face significant challenges in becoming versatile enough to handle a wide range of retrieval tasks (Muennighoff et al., 2022). Their in-domain retrieval accuracy is often constrained by the capacity of their backbone encoders, such as the number of parameters (Ni et al., 2021). Additionally, dense retrieval models typically struggle to generalize to unseen data, necessitating fine-tuning with a large amount of labeled data to perform well in the target domain. Finally, achieving versatility in dense retrieval models requires training on multiple retrieval tasks simultaneously, which demands sufficient capacity from the backbone encoder (Zhang et al., 2023;Xiao et al., 2023). \n\nRecently Large Language Models (LLMs) have been prompted or fine-tuned as dense retrieval models and achieved improved performance across a wide range of retrieval tasks, thanks to their superior capability for semantic understanding and rich world knowledge (Li et al., 2023;Wang et al., 2023;Zhuang et al., 2024;Muennighoff et al., 2024). These models vary in parameters from 2 billion to 56 billion, with pre-training sufficiency ranging from hundreds of billions to tens of trillions of tokens, and include both base models and human preference aligned chat models.",
            "score": 0.524206394819671,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "259949733",
            "title": "Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion",
            "text": "After converting the knowledge graph into text passages, we proceed to conduct retrieval given an incomplete triplet. To achieve this, we first verbalize the triplet using Equation 1. We then employ the widely-used retrieval method BM25 [17], which is based on TF-IDF scores of sparse word matches between input queries and passages. We have opted for sparse retrieval as opposed to dense retrieval [10], which involves computing passage embeddings and query embeddings using pre-trained language models. The reason for this is that we place value on the generalization and efficiency of sparse representation. Dense retrieval methods typically require additional model training and consume significant amounts of memory in order to save the passage embeddings. Through retrieval, we are able to obtain  passages [  ] =1,\u2022\u2022\u2022 , that are potentially relevant to the input question.",
            "score": 0.5239994945352042,
            "section_title": "Retrieval",
            "char_start_offset": 5918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 879
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2139892578125
        },
        {
            "corpus_id": "259262395",
            "title": "Learning to Rank in Generative Retrieval",
            "text": "As previously mentioned, it is insufficient for generative retrieval to only learn how to generate identifiers. Therefore, we develop a framework to enable generative retrieval to learn how to rank passages directly. To accomplish this, we continue training the autoregressive model AM using a passage rank loss. \n\nTo begin, we retrieve passages for all queries in the training set using the trained autoregressive language model AM after the learning-to-generate phase. For a given query q, we obtain a passage rank list \n\nwhere n is the number of retrieved passages. Each passage p j is assigned a relevant score s(q, p j ) via Eq. 3, which is calculated as the sum of the language model scores of a set of predicted identifiers. It is important to note that the passage rank list includes both positive passages that are relevant to the query and negative passages that are not. A reliable retrieval system should assign a higher score to positive passages than to negative passages, which is the goal of the learning-to-rank paradigm. To achieve this objective in generative retrieval, we utilize a margin-based rank loss, which is formulated as follows: \n\nwhere p p and p n represent a positive and negative passage in the list P, respectively, and m is the margin. It is noted that the gradients could be propagated to the autoregressive model AM via the language model score s ip , which is the logits of the neural network. \n\nIn practice, we take two rank losses based on different sampling strategies for positive and negative passages. In L rank1 , the positive and negative passages are the ones with the highest rank scores, respectively. In L rank2 , both the positive and negative passages are randomly sampled from the passage rank list. While the rank loss optimizes the autoregressive model toward passage ranking, the generation of identifiers is also crucial for successful passage ranking. Therefore, we also incorporate the generation loss into the learning-to-rank stage. The final loss is formulated as a multi-task format: \n\nwhere \u03bb is the weight to balance the rank losses and generation loss. \n\nWe continue training the autoregressive model AM via Eq. 5.",
            "score": 0.5238881490756739,
            "section_title": "Learning to Rank",
            "char_start_offset": 11834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2046
                },
                {
                    "start": 2049,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354248046875
        },
        {
            "corpus_id": "259858866",
            "title": "Retrieval-augmented Video Encoding for Instructional Captioning",
            "text": "As TDR conducts retrieval for each frame in V S , efficiency should be prioritized, and we mainly consider the dense retrieval architecture. First architecture, the cross retrieval requires the exhaustive computation of O(|V S | \u00d7 |R S |) as the v j i and r l interact within a single neural network. However, the dense retrieval conducts the retrieval with little computation cost, at O(|V S | + |R S |), by reusing the encoding of the v j i and r l . Specifically, the dense retriever consists of two distinct encoders \u2126 V and \u2126 R , which encode the query frame v j i and the procedural sentence r l independently. Then, the interaction between v j i and r l is modeled as a simple dot product operation, resulting in retrieval as follows: \n\nFor training, we adopt the contrastive learning objective (Mnih and Kavukcuoglu, 2013), denoted by L TDR , that guides the retriever to assign larger relevance for the gold procedural sentence r + than that of negative procedural sentences r \u2212 : \n\n, \n\n(5) We utilize the caption c j as the gold procedural sentence r + , as there is no available gold procedural sentence, and this approach is reported to be effective in previous work (Gur et al., 2021). We also utilize in-batch negatives, treating all other gold procedural sentences representing different procedures from the identical instructional video, as negative procedural sentences.",
            "score": 0.5229484118155286,
            "section_title": "Temporal Description Retrieval (TDR)",
            "char_start_offset": 13652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1387
                }
            ],
            "ref_mentions": [
                {
                    "start": 802,
                    "end": 830,
                    "matchedPaperCorpusId": "14992849"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222412109375
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss. To improve the performance of dual-encoders, some efforts have been made to incorporate more complicated structures (i.e., late interaction) such as attention layers (Humeau et al., 2019;Tang et al., 2021), the sum of maximum similarity computations (Khattab and Zaharia, 2020), and the transformer layers (Cao et al., 2020;Chen et al., 2020) into encoding. These late interaction strategies bring considerable improvements on retrieval performance but also increase computational overhead. Moreover, interaction information is still neglected in earlier encoding of query and passage. \n\nIn this work, we aim to achieve both efficiency and effectiveness in passage retrieval. The key idea is to maintain two independent encoders, and keep as much interaction information as possible in the meanwhile. To this end, we propose a novel approach that explicitly fuses query (passage) information into passage (query) embeddings through a graph neural network (GNN), and name the model GNN-encoder. Our model is built upon the dualencoder, and learns query-interactive passage representations and passage-interactive query representations through a graph neural network. Specifically, given a query set, we retrieve top passages for each query, and form a graph whose nodes are the queries and the passages, and edges reflect correspondence between query-passage pairs (i.e., if a passage is retrieved by the query). Then, we initialize the GNN model with the representations of the pre-trained dual-encoder and cross-encoder, and then perform information propagation on the graph. To avoid information leakage, we further design a new training algorithm and name it Masked Graph Training (MGT), in which the query set used for training GNN is no longer used to construct the query-passage graph in each training epoch. Finally, the passage embeddings could be pre-computed offline corresponding to the GNN. Thus our model holds the efficiency advantage inherited from the dual structure, and at the same time takes query-passage interaction into account. \n\nOur contributions can be summarized as follows:",
            "score": 0.5225999704407291,
            "section_title": "Introduction",
            "char_start_offset": 1969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2180
                },
                {
                    "start": 2183,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 335,
                    "matchedPaperCorpusId": "234338073"
                },
                {
                    "start": 380,
                    "end": 407,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 436,
                    "end": 454,
                    "matchedPaperCorpusId": "218487151"
                },
                {
                    "start": 454,
                    "end": 472,
                    "matchedPaperCorpusId": "222177208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "273026177",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "text": "Dense Passage Retrieval Dense retrieval has garnered attention for its efficacy in semantic space exploration. A notable technique in this domain is DPR (Karpukhin et al., 2020), employing both query and passage encoders for efficient retrieval. \n\nVarious studies have delved into enhancing dense retrieval, including negative example mining techniques like RocketQA (Qu et al., 2021), and diverse data augmentation methods such as DRAGON (Lin et al., 2023a). ColBERT (Khattab and Zaharia, 2020;Santhanam et al., 2022b) introduced the lateinteraction mechanism, offering an alternative architecture for dense retrieval. \n\nAnother line of research is pre-training strategies for dense retrieval. Approaches like Contriever (Izacard et al., 2021), coCondenser (Gao and Callan, 2022), and COCO-DR (Yu et al., 2022) have proposed contrastive pre-training techniques tailored for retrieval tasks. Concurrently, CoT-MAE (Wu et al., 2023) and RetroMAE (Xiao et al., 2022) have focused on masked auto-encoding for pre-training. \n\nAs large language models (LLMs) continue to evolve, their integration into dense retrieval has become increasingly widespread. GTR (Ni et al., 2022) utilized LLM encoders, showcasing performance gains with increased model size. Similarly, Promptagator (Dai et al., 2023) and InPars (Bonifacio et al., 2022) employed LLMs to generate synthetic query-document pairs, effectively enhancing the training of dense retrievers. Building on a similar concept, (Huang et al., 2024) extended the approach to multilingual settings, enabling broader applicability. \n\nOur contribution is orthogonal to these studies as we concentrate on refining training signals for knowledge distillation. This suggests that our approach holds potential for integration with other methods to achieve further improvements. \n\nKnowledge Distillation for Dense Retrieval Enhancing the performance of dense retrievers often involves employing knowledge distillation techniques. Izacard and Grave (2020) pioneered the distillation of knowledge from the reader to the retriever, resulting in improved performance in open-domain question answering.",
            "score": 0.5222194003232138,
            "section_title": "Related Work",
            "char_start_offset": 2741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 177,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 367,
                    "end": 384,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 468,
                    "end": 495,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 495,
                    "end": 519,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 758,
                    "end": 780,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 794,
                    "end": 811,
                    "matchedPaperCorpusId": "253157773"
                },
                {
                    "start": 914,
                    "end": 931,
                    "matchedPaperCorpusId": "251594591"
                },
                {
                    "start": 945,
                    "end": 964,
                    "matchedPaperCorpusId": "252917569"
                },
                {
                    "start": 1153,
                    "end": 1170,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 1274,
                    "end": 1292,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 1304,
                    "end": 1328,
                    "matchedPaperCorpusId": "250340449"
                },
                {
                    "start": 1474,
                    "end": 1494,
                    "matchedPaperCorpusId": "268681628"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.341796875
        },
        {
            "corpus_id": "249395549",
            "title": "A Neural Corpus Indexer for Document Retrieval",
            "text": "The other way to approach retrieval is utilizing an end-to-end autoregressive model. Firstly, several efforts have been done on entity linking [13; 12; 11], which can be regarded as a special type of retrieval task, e.g., using an entity to ask the posed question. Recently, different from the entity linking task, Tay et al. [50] proposed the DSI (differentiable search index) model to generate relevant document identifiers directly corresponding to the query. Bevilacqua et al. [4] employed the autoregressive model to generate relevant words for a query and utilize the generated string to retrieve relevant documents. Besides, the Deep Retrieval (DR) [20] approach for recommendation is also related to this category, which learns a deep retrievable network with user-item clicks and gets rid of the ANN algorithms based on the Euclidean space assumption. \n\nPre-trained language models. Recently, pre-trained Language Models (LMs), such as BERT [14] and RoBERTa [35], have led to a revolution in web search techniques. The representation vectors for all documents can be calculated and indexed offline. In the online serving stage, it calculates the representation vector for the input query, and applies a crossing layer to calculate the relevance score between each query and document pair. The crossing layer usually adopts simple operators such as cosine similarity or a single feed-forward layer to retain a high efficiency. Gao et al. [16] found that a standard LMs' internal attention structure is not ready-to-use for dense encoders and proposed the Condenser to improve the performance of dense retrieval. Moreover, ANCE [54] leverages hard negatives to improve the effectiveness of contrastive learning, which generates better text representations for the retrieval tasks.",
            "score": 0.5215255396119445,
            "section_title": "Related work",
            "char_start_offset": 9574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 1446,
                    "end": 1450,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1635,
                    "end": 1639,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27685546875
        },
        {
            "corpus_id": "273901352",
            "title": "Language Bias in Multilingual Information Retrieval: The Nature of the Beast and Mitigation Methods",
            "text": "Our benchmark addresses this limitation by introducing semantically parallel queries across multiple languages, enabling comprehensive analysis of language fairness in MLIR. \n\nDPR Dense Passage Retrieval (DPR: Karpukhin et al. (2020)) is a neural retrieval framework initially proposed for monolingual supervised finetuning. DPR uses dual encoders: one for encoding queries and another for encoding passages (documents), both based on the BERT architecture (Devlin et al., 2019). The primary advantage of DPR over traditional retrieval models like BM25 is its ability to embed both queries and documents into a shared dense vector space, enabling efficient nearest-neighbor search for retrieval. The relevance of a document to a query is determined by the similarity between their embeddings, typically using the dot product as a similarity measure. \n\nIn our work, we employ mDPR using mBERT and XLM-R to handle multilingual queries and documents. These models are fine-tuned on parallel query-document pairs from multiple languages, allowing the system to generalize across different languages. The use of mDPR allows us to explore how multilingual language models handle language biases, which often favor high-resource languages over low-resource ones. Furthermore, we investigate the performance of these models on the Mul-tiEuP dataset, assessing their ability to ensure fair and equitable retrieval across 24 languages, thus promoting fairness in multilingual IR.",
            "score": 0.5210898005819187,
            "section_title": "Background Knowledge",
            "char_start_offset": 23243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1469
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 233,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 457,
                    "end": 478,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "249395549",
            "title": "A Neural Corpus Indexer for Document Retrieval",
            "text": "In this section, we briefly introduce the related works and leave more discussions in Appendix A. \n\nSparse retrieval. Traditional document retrieval methods are based on Sparse Retrieval, which is built upon inverted index with term matching metrics such as TF-IDF [45], query likelihood [33] or BM25 [44]. In industry-scale web search, BM25 is a difficult-to-beat baseline owing to its outstanding trade-off between accuracy and efficiency. In recent years, there are some attempts to incorporate the power of neural networks into inverted index. The Standalone Neural Ranking Model (SNRM) [57] learns high-dimensional sparse representations for query and documents, which enables the construction of inverted index for efficient document retrieval. Doc2Query [41] predicts relevant queries to augment the content of each document before building the BM25 index, and DocT5Query [40] improves the performance of query generation by the pre-trained language model T5 [5]. Furthermore, DeepCT [9] calculates context-aware term importance through neural networks to improve the term matching metrics of BM25. \n\nDense retrieval. Another line of research lies in Dense Retrieval, which presents query and documents in dense vectors and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain dense representations for queries and documents. At inference time, efficient Approximate Nearest Neighbor (ANN) search algorithms, such as k-dimensional trees [3], localitysensitive hashing [10], and graph-based indexes (e.g., HNSW [38], DiskANN [27] and SPANN [7]) can be utilized to retrieve relevant documents within a sublinear time. Besides, Luan et al. [37] analyze the limited capacity of dual encoders, and propose a combination of sparse and dense retrieval methods with multi-vector encoding to achieve better search quality. \n\nAutoregressive retrieval. The other way to approach retrieval is utilizing an end-to-end autoregressive model.",
            "score": 0.5210484754124747,
            "section_title": "Related work",
            "char_start_offset": 7589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "16829071"
                },
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "1183147"
                },
                {
                    "start": 591,
                    "end": 595,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "13091446"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "747083"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1667,
                    "end": 1671,
                    "matchedPaperCorpusId": "209392043"
                },
                {
                    "start": 1780,
                    "end": 1784,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264404296875
        },
        {
            "corpus_id": "264451831",
            "title": "1-PAGER: One Pass Answer Generation and Evidence Retrieval",
            "text": "\"Retrieve-and-read\" Question Answering Question answering approaches in NLP are dominated by the \"retrieve-and-read\" paradigm where a retriever first fetches hundreds of relevant documents from a corpus, followed by a language model that reranks and extracts the answer (Harabagiu et al., 2003;Chen et al., 2017;Zhu et al., 2021). Sparse retrievers such as BM25 (Robertson et al., 2009) build a highdimensional lexical index over text corpus. Dense retrievers (Karpukhin et al., 2020) use a dual encoder architecture to embed the query and document and perform an approximate nearest neighbor search. Various modifications to dense retrieval have been proposed over the years includ-ing hard negative training (Xiong et al., 2020), late interaction (Khattab and Zaharia, 2020;Santhanam et al., 2022), few-shot learning (Izacard et al., 2022), joint retriever and reader training (Jiang et al., 2022). \n\nA particular variant of interest is the Iterative Retrieval process where the query is reformulated incrementally (Das et al., 2019;Lee et al., 2022) leading to an interactive search process (Jiang et al., 2023;Adolphs et al., 2021). This query augmentation scheme has similarities with our use of search paths. However, we use the paths to iteratively partition the corpus while prior works have used it for refining the query. \n\nTo perform well, retrieve-and-read systems will typically retrieve 10s to 100s of passages that must be processed by a language model. In constrast, 1P retrieves and extracts an answer in a single pass of language model generation. \n\nClosed Book Question Answering With data and parameter scale, LLMs in a closed-book setting (CBQA) have shown competitive performance (OpenAI, 2023;Anil et al., 2023;Yu et al., 2023) to retrieval pipelines (ODQA), however without producing any attributed passages (Rashkin et al., 2021;Bohnet et al., 2022).",
            "score": 0.5210091571926351,
            "section_title": "Related Work",
            "char_start_offset": 3806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 294,
                    "matchedPaperCorpusId": "1717175"
                },
                {
                    "start": 362,
                    "end": 386,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 460,
                    "end": 484,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 749,
                    "end": 776,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1035,
                    "end": 1052,
                    "matchedPaperCorpusId": "249049410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2076416015625
        },
        {
            "corpus_id": "231815627",
            "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
            "text": "We develop our approach based on the typical dualencoder architecture. Figure 1a illustrates a dualencoder. First, a dense passage retriever uses an encoder E p (\u00b7) to obtain the d-dimensional real-valued vectors (a.k.a., embedding) of passages. Then, an index of passage embeddings is built for retrieval. At query time, another encoder E q (\u00b7) is applied to embed the input question to a d-dimensional real-valued vector, and k passages that are the closest to the question in terms of the similarity in the embedding space will be retrieved. The similarity between the question q and a candidate passage p can be computed as the dot product of their vectors:\n\n(1)\n\nIn practice, the separation of question encoding and passage encoding is desirable, so as the dense representations of all passages can be precomputed for efficient retrieval. Here, we adopt two independent neural networks initialized from pre-trained LMs and take the representations at the first token (e.g., [CLS] symbol in BERT) as the output for encoding.\n\nTraining The training objective is to learn dense representations of questions and passages so that question-positive passage pairs have higher similarity than the question-negative passage pairs in training data. Formally, given a question q i together with its positive passage p + i and m negative passages {p \u2212 i,j } m j=1 , we minimize the following loss function:\n\nwhere we aim to optimize the negative log likelihood of the positive passage against a set of m negative passages. Ideally, we can take all the negative samples into consideration in Equation 2. However, it is computationally infeasible to consider a large number of negative samples for a question, and m is practically set to a small number that is far less than M . As will be discussed later, both the number and the quality of negatives affect the final performance of passage retrieval. Inference Since there is usually a large number of candidate passages at inference stage, a commonly adopted acceleration method is the exact maximum inner product search (Shrivastava and Li, 2014), and we use the implementation of FAISS (Johnson et al., 2019). The basic idea is to offline construct the index for the dense representations of all passages, and",
            "score": 0.5209393908455392,
            "section_title": "The Dual-Encoder Architecture",
            "char_start_offset": 11266,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "253116993",
            "title": "Autoregressive Structured Prediction with Language Models",
            "text": "Autoregressive Modeling Assumption. The decoder model, which is autoregressive, introduces an inductive bias on the structured prediction approach. Specifically, the left-to-right approach requires the model to model dependencies in a specific order. This could account for some of the reduction in performance compared to task-specific discriminative models. Understanding the implications of the autoregressive decision is indeed an interesting question, but one that we felt was out of scope for this short paper.\n\nEfficiency. In our experiments, we reduce the burden of finding many mention spans in two-stage approaches. On sentence-level tasks, e.g., entity and relation extraction, the number of decoding steps is relatively small. For instance, the average number of words in an input sentence is \u224820. Our system has a lighter memory trace as opposed to discriminative models. This extra time cost can be partially compensated with larger batch sizes. However, on document-level tasks, e.g., coreference resolution, the number of decoding steps is too large to be compensated with parallelism. More efficient methods for inference such as non-autoregressive decoding (Gu et al., 2018) remain to be explored in future work.\n\nDecoding Algorithms. In this work, we use greedy decoding in all the experiments. Alternative decoding algorithms might further improve the quality of the generated sequences, e.g., beam search (Zhang and Clark, 2008;Goldberg et al., 2013).",
            "score": 0.5204356573180369,
            "section_title": "Limitations",
            "char_start_offset": 16674,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "247628023",
            "title": "Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction",
            "text": "We want to emphasize that it becomes increasingly hard to contrast neural retrieval models and make conclusive statements about \"SOTA\" (State-of-the-art). This is because there are numerous factors at play for the effectiveness including training data sampling, distillation, generational training, and even hardware setups. At the same time it is highly important to compare systems not only by their effectiveness, but factor in the efficiency as well, to avoid misleading claims. We believe it is important to show that we do not observe substantial differences in effectiveness compared to other systems of similar efficiency and that small deviations of effectiveness should not strongly impact our overall assessmenteven if those small differences come out in our favor. With that in mind, we study: RQ4 How does the fully optimized ColBERTer system compare to other end-to-end retrieval approaches? In Table 5 we group models by our main efficiency focus: the storage requirements, measured as the factor of the indexed plaintext size. \n\nLow Storage Systems. We find that ColBERTer does improve on the existing Pareto frontier compared to other approaches, especially for cases with low storage footprint. Our Uni-ColBERTer (line 7) especially outperforms previous single-dimension token encoding approaches, while at the same time offering improved transparency and making it easier to showcase model scores with mappings to whole words. However, we also observe that we could further improve the dense retrieval component with a technique similar to JPQ [58] (line 2) to further reduce our storage footprint. \n\nHigher Storage Systems. At first glance in this section, we see that while 32 dimensions per token does not sound much, the resulting increase of the total storage requirement is staggering. Col-BERTer outperforms similarly sized architectures as well, however a fair comparison becomes more difficult than in the low storage systems, as the absolute size differences become much larger. Another curious observation is that larger ColBERTer models (lines 13 & 14) seem to be slightly faster than our smaller instances (lines 7 & 8).",
            "score": 0.520421581108662,
            "section_title": "Comparing to Related Work",
            "char_start_offset": 39344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "236772078"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04217529296875
        },
        {
            "corpus_id": "273026177",
            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
            "text": "Information retrieval (IR) is the process of retrieving relevant information from vast datasets, such as web pages or documents, based on user queries. Recently, deep learning methods, notably the dense passage retriever (DPR) (Karpukhin et al., 2020), have attracted attention for their superior performance compared to traditional sparse retrieval techniques like BM25. These methods, often termed dual-encoder models, encode both queries and documents into high-dimensional representations, facilitating efficient similarity computation and retrieval via nearest neighbor search (Douze et al., 2024). 1 Our source code and trained models are released at https: //github.com/MiuLab/PairDistill Despite the effectiveness of dense retrievers, their modeling capacity is limited. To enhance retrieval performance, knowledge distillation is commonly employed (Izacard and Grave, 2020). Typically, knowledge from a robust cross-encoder reranker is distilled to train the dense retriever, achieving state-of-the-art results on retrieval benchmarks (Santhanam et al., 2022b). The efficacy of knowledge distillation largely relies on the performance of the reranker, which serves as the upper bound for the distilled retriever's performance. \n\nHowever, existing studies primarily utilized pointwise rerankers for knowledge distillation, which an absolute relevance score is assigned for each document. Such scores are not trivial to compare due to inconsistent baselines. In contrast, pairwise reranking, an advanced technique comparing pairs of documents to assess their relative relevance to a query, has demonstrated superior reranking performance (Pradeep et al., 2021). By emphasizing relative comparison, pairwise rerankers can distinguish more finely between similarly relevant documents, yielding more precise relevance scores conducive to better distillation. \n\nIn this paper, we introduce Pairwise Relevance Distillation (PAIRDISTILL), a novel method leveraging the fine-grained training signals provided by pairwise rerankers. PAIRDISTILL enriches the training of dense retrieval models by distilling knowledge from pairwise comparisons, enabling the model to learn more nuanced distinctions between closely ranked passages.",
            "score": 0.5202858637848183,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 251,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1044,
                    "end": 1069,
                    "matchedPaperCorpusId": "244799249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "278165747",
            "title": "Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism",
            "text": "These insights enable us to view all mentioned sequence models as structured matrices operating along the time dimension. For that reason, we generalize token-mixing mechanisms beyond traditional self-attention, and use throughout this work the term Temporal Mixing Heads (or simply Mixing Heads). \n\nRetrieval Tradeoffs Between Transformers and SSMs. SSM-based language models rely on compressed state representations, where information is continually updated in a low-dimensional hidden state-unlike Transformers, which cache their full context. These inherent differences have been observed across multiple benchmarks. Waleffe et al. [2024] found that Mamba-based models require extensive training to perform well on MMLU, while Wen et al. [2024] identified weaknesses in in-context retrieval tasks, such as associative recall. Jelassi et al. [2024] theoretically and empirically confirmed that SSMs have difficulty with precise copying. Arora et al. [2023] further demonstrated this gap, finding that associative recall capabilities account for majority of the performance difference between attention-based and gated-convolution models. Park et al. [2024] benchmarked Mamba and Transformer models across a range of in-context learning tasks, showing that SSMs underperform on tasks requiring non-standard retrieval, and proposed a hybrid architecture to address this limitation. Arora et al. [2025] analyzed the recall-throughput tradeoff across architectures, showing that attention-based models occupy a different region of the tradeoff frontier than SSMs, with stronger recall but higher computational cost. Blouir et al. [2025] complemented this view by enhancing retrieval in SSMs through training improvements without architectural changes. Unlike previous work, our study reveals that while these limitations exist, they originate from a small subset of heads responsible for retrieval rather than the entire language model. \n\nMechanistic Interpretability of Mixing Heads.",
            "score": 0.5202772015184791,
            "section_title": "Related Work",
            "char_start_offset": 11637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 1615,
                    "end": 1635,
                    "matchedPaperCorpusId": "273811349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12274169921875
        },
        {
            "corpus_id": "258865354",
            "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
            "text": "A major goal of this work is to investigate retrieval methods that forego neural computation and the need for specialized accelerator hardware at query time. As such, we focus on a method that uses a large neural model to precompute the required representations of the retrieval items (documents) ahead of time. Then, at retrieval time, the method performs only basic featurization (e.g., tokenization) of the queries. \n\nSpecifically, we investigate query-document scoring functions that score the compatibility of a query-document pair with the inner-product of separate featurizations of the query   () and document   (). score(, ) = \u27e8  (),   ()\u27e9 \n\nThis form is familiar from both traditional lexicalized retrieval and from more recent work on dense retrieval. In lexicalized retrieval, (e.g., TF-IDF and BM25) [36,37],   and   assign non-zero scores to sub-strings of  and . On the other hand, in dense retrieval [14,16,26],   and   are neural networks that map  and  to dense vectors. Note that this formulation does not allow for deeper interactions between  and , such as typical cross-encoder scorers, as these cannot be computed efficiently and without an accelerator at query time. \n\nWe investigate an alternative formulation of Equation 1 than either traditional lexicalized retrieval or dense retrieval. In this formulation,   can be an arbitrarily complex neural network, but   must be a sparse featurization that can be quickly computed on commodity CPUs. This way, it is possible to push all costly neural network inference to indexing time, and avoid the need for accelerators at serving-time. For this paper, we choose   to be a Figure 1: Our model adapts the T5 encoder-decoder architecture to predict query token scores given an input passage. The encoder (a) reads an input passage prepended with a static prompt. The decoder (b) can be initialized from a pretrained T5 checkpoint, but the architecture is modified in a few ways to be non-autoregressive: the only inputs are the standard position embeddings, the decoding is parallelized for efficiency, and the output at each position is the full distribution over the vocabulary.",
            "score": 0.5202196214658175,
            "section_title": "NAIL MODEL",
            "char_start_offset": 7958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 817,
                    "end": 820,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 920,
                    "end": 923,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29833984375
        },
        {
            "corpus_id": "268031876",
            "title": "Generative Retrieval with Large Language Models",
            "text": "Knowledge-intensive tasks, including open-domain question answering, dialogues, and fact verification, require access to a vast amount of world or domain-specific knowledge (Petroni et al., 2021). Common methods involve utilizing external knowledge sources such as Wikipedia and employing additional sparse or dense retrieval models to first retrieve relevant passage from Wikipedia as reference, then predict answers under the condition of the reference (Karpukhin et al., 2020;Lewis et al., 2020;Izacard and Grave, 2021). However, traditional sparse lexical retrieval lacks deep semantic understanding, and in dual-tower dense retrieval models, the representations of questions and passages are usually obtained independently (Karpukhin et al., 2020), leading to only shallow interactions being captured (Khattab et al., 2021). \n\nRecently, generative retrieval methods that leverage the generative capabilities of models (Cao et al., 2021;Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022;Lee et al., 2022) have gained increasing attention. They generate passage identifiers through autoregressive generative models, performing deep token-level cross-attention, and interacting with the entire parameter space of models trained on the target corpus to overcome bottlenecks. These retrieval methods have proven effective across various domains and tasks. However, both dense retrieval and previous generative retrieval methods can only obtain predefined and segmented passages (e.g., 100 words), making it difficult to freely choose the starting point of the reference. This somewhat affects the flexibility of obtaining references and the naturalness of reading. Humans, on the other hand, can recall any seen passage from memory and naturally recall it from any starting point. \n\nLarge language models (LLMs) have proven to possess strong generative understanding capabilities that can generalize to multiple tasks through prompting, and they have parameterized memory of extensive document knowledge during the pretraining phase. We explore whether LLMs can independently recall seen passage as reference from any starting position through prompting. To this end, we mimic the human process of recalling references: first recalling related coarse-grained documents or pages, then recalling fine-grained reference passage, as shown in Figure 1.",
            "score": 0.5197787890693325,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 195,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 498,
                    "end": 522,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 923,
                    "end": 941,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 941,
                    "end": 958,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 958,
                    "end": 982,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "249395549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "237485691",
            "title": "Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder",
            "text": "One would expect the autoencoder to provide good representations if the decoder can well recover the input. However, we found that a typical model stacking a standard autoregressive decoder on a standard BERT-style encoder doesn't work well in dense retrieval tasks. For example, we finetune the pre-trained checkpoint of Optimus, which stacks GPT-2 on top of BERT on MS MARCO and compare it with BERT. We use Mean Reciprocal Rank(mrr) and recall as evaluation metrics. The detailed experimental setting can be found in Section 4.3, and the results are shown in Figure 1(a). The performance of Optimus on dense retrieval tasks is worse than standard BERT, a sharp contrast with Optimus's effectiveness on other language tasks, e.g., in GLUE benchmarks. Note that one difference between data in GLUE and MS MARCO is the sequence length. In most GLUE tasks, the sequence length is short, e.g., average 14 tokens in SST-2, while the average passage length in MS MARCO is more than 450. Also, recent research shows that long sentences are hard to represent via single embedding vectors from pre-trained models (Luan et al., 2021). \n\nTo confirm this, We randomly select sequence pairs of different lengths and calculate the cosine similarity of their CLS embeddings provided by Optimus. The results are shown in Figure 1(b). The representations of long sequences (256 or 512 tokens) from Optimus are quite similar; the cosine similarities of random long sequence pairs are around 0.8. The model yields cluttered representations for long text sequences. When fine-tuned for dense retrieval in MS MARCO, it does not separate relevant documents for a query from those irrelevant ones. All of those representations might be similar to each other and require dedicated finetuning to realign their encodings.",
            "score": 0.5197040647526997,
            "section_title": "Effects of Using a Strong Decoder",
            "char_start_offset": 7001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 1106,
                    "end": 1125,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "243865399",
            "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
            "text": "One would expect the autoencoder to provide good representations if the decoder can well recover the input. However, we found that a typical model stacking a standard autoregressive decoder on a standard BERT-style encoder doesn't work well in dense retrieval tasks. For example, we finetune the pre-trained checkpoint of Optimus, which stacks GPT-2 on top of BERT on MS MARCO and compare it with BERT. We use Mean Reciprocal Rank(mrr) and recall as evaluation metrics. The detailed experimental setting can be found in Section 4.3, and the results are shown in Figure 1(a). The performance of Optimus on dense retrieval tasks is worse than standard BERT, a sharp contrast with Optimus's effectiveness on other language tasks, e.g., in GLUE benchmarks. Note that one difference between data in GLUE and MS MARCO is the sequence length. In most GLUE tasks, the sequence length is short, e.g., average 14 tokens in SST-2, while the average passage length in MS MARCO is more than 450. Also, recent research shows that long sentences are hard to represent via single embedding vectors from pre-trained models (Luan et al., 2021). \n\nTo confirm this, We randomly select sequence pairs of different lengths and calculate the cosine similarity of their CLS embeddings provided by Optimus. The results are shown in Figure 1(b). The representations of long sequences (256 or 512 tokens) from Optimus are quite similar; the cosine similarities of random long sequence pairs are around 0.8. The model yields cluttered representations for long text sequences. When fine-tuned for dense retrieval in MS MARCO, it does not separate relevant documents for a query from those irrelevant ones. All of those representations might be similar to each other and require dedicated finetuning to realign their encodings.",
            "score": 0.5196912983140174,
            "section_title": "Effects of Using a Strong Decoder",
            "char_start_offset": 7001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 1106,
                    "end": 1125,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "256461130",
            "title": "Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples",
            "text": "Monolingual Dense Passage Retrieval The past few years have witnessed growing interest in monolingual dense passage retrieval. DPR (Karpukhin et al., 2020) is built on a bi-encoder architecture, which is initialized with BERT (Devlin et al., 2019) and outperforms early dense retrieval methods. \n\nRocketQA (Qu et al., 2021) first mines hard negative samples with a trained retrieval model and then uses the mined negative samples to re-train the model. However, RocketQA requires a pretrained cross-encoder to filter out false negative samples, which is not efficient and must be trained in advance. TAS-B (Hofst\u00e4tter et al., 2021) is also a retrieval model using the bi-encoder architecture. \n\nIt utilizes topic-aware sampling to improve training with in-batch negative samples, and applies a dual-teacher supervision paradigm to achieve better knowledge distillation from both a crossencoder and a ColBERT (Khattab and Zaharia, 2020) teacher model simultaneously. Other studies further apply hard negative sample mining to train dense passage retrieval models. Gao et al. (2021b) and Karpukhin et al. (2020) use BM25 (Robertson et al., 2009) top passages as hard negative samples. ANCE (Xiong et al., 2021) enhances hard negative sampling by dynamically mining hard negative samples in the training phase. However, it requires periodically rebuilding the index and refreshing hard negative samples, which greatly increases computational cost. Zhan et al. (2021) combine static BM25 hard negative samples with dynamic hard negative samples retrieved from the entire corpus by the model at the current training step. \n\nMultilingual and Cross-lingual Dense Passage Retrieval Researchers have been utilizing crosslingual knowledge transfer to enhance monolingual retrieval for low-resource languages since the advent of multilingual pretrained language models. Both MacAvaney et al. (2020b) and Shi et al. (2020) investigate zero-shot transfer using a cross-encoder architecture, in which they first fine-tune mBERT on the source language, then apply the model to the target language directly. However, the crossencoder architecture they use is slow in practice.",
            "score": 0.5191556746025225,
            "section_title": "Related Work",
            "char_start_offset": 6663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 155,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 306,
                    "end": 323,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 606,
                    "end": 631,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 908,
                    "end": 934,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1063,
                    "end": 1081,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 1086,
                    "end": 1109,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1119,
                    "end": 1143,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1445,
                    "end": 1463,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1864,
                    "end": 1888,
                    "matchedPaperCorpusId": "209515542"
                },
                {
                    "start": 1893,
                    "end": 1910,
                    "matchedPaperCorpusId": "226284008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "258331649",
            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
            "text": "Figure 2 depicts a pipeline for knowledge-intensive visual question answering tasks. As shown in the pipeline, the input to the dense retrieval model is asymmetric -query encoder takes multi-modal input (i.e., a question and an image), while the passage encoder takes a uni-modal text input (i.e., a passage from C). This asymmetric property in the input modalities makes it challenging to design an effective symmetric dense retrieval model. This is why the current state-of-the-art dense retrieval model proposed by Qu et al. [40] uses an asymmetric architecture, where a pre-trained multi-modal language model (i.e., LXMERT [53]) is used for query encoding and a pre-trained uni-modal language model (i.e., BERT [8]) is used for document encoding. Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B [17], ColBERT [24,49], RocketQA [41,44], and CLDRD [62], use symmetric architectures. Motivated by this observation, our goal is to learn a symmetric dense retrieval model for KI-VQA tasks. \n\nTo this aim, we study two alternative solutions. First, we convert all model inputs to a uni-modal textual form and then use uni-modal language models for both query and document encoding (Section 4.1). Second, we convert all inputs to the same multi-modal (text and image) form and then use multi-modal language models for both encoders (Section 4.2). We hypothesize that these two models learn complementary representations for the following reasons: (1) they take different input formats, and (2) the pre-training process and data in uni-modal and multi-modal language models are different.",
            "score": 0.518383762285851,
            "section_title": "DEDR: DUAL ENCODING DENSE RETRIEVER FRAMEWORK",
            "char_start_offset": 11555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 528,
                    "end": 532,
                    "matchedPaperCorpusId": "234337940"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1187,
                    "end": 1190,
                    "matchedPaperCorpusId": "244799249"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "238857121"
                },
                {
                    "start": 1220,
                    "end": 1224,
                    "matchedPaperCorpusId": "248426770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "271855703",
            "title": "FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection",
            "text": "The first stage is retrieval efficiency. Given the potentially massive number of passages, dense retrieval can be extremely slow. Instead of relying solely on brute force search methods, alternative algorithms such as Approximate Nearest Neighbor (ANN) (Johnson et al., 2021) and Hierarchical Navigable Small World (HNSW) (Malkov and Yashunin, 2020) can be employed to expedite the retrieval process. \n\nThe second efficiency challenge lies in the reading process, which involves handling multiple passages for each query. To address this, Hofst\u00e4tter et al. (2023) propose FiD-Light, which limits the decoder's attention to the first k tokens of each passage to reduce the context length. FiDO (de Jong et al., 2023) explores reducing the number of cross attention layers in FiD's decoder to increase efficiency, but this comes at the cost of re-pretraining the base model. Other complementary strategies explore to identify and stop processing less relevant passages early on by utilizing adaptive computation (Wu et al., 2020(Wu et al., , 2021) ) or knowledge graph with GNN network (Yu et al., 2022). Additionally, some research has focused on directly retrieving answers to questions without the need for passage processing (Seo et al., 2019;Lee et al., 2021;Lewis et al., 2021), or using language models to generate answers directly by finetuning and fewshot prompting (Roberts et al., 2020;Brown et al., 2020). \n\nAnswer Sentence Selection Answer Sentence Selection (AS2) is a long-standing task that has been extensively explored. Dense Neural Networks (DNNs) have been widely employed in this task (Severyn and Moschitti, 2015;Shen et al., 2017). Garg et al. (2020) further advanced the field by utilizing transformer-based pre-trained language models (PLMs) to achieve better results. Recent studies have investigated methods such as generating answer sentences (Hsu et al., 2021) and implementing complex ranking pipelines (Matsubara et al., 2020). Unlike these approaches, our work aims to predict the exact answer span from retrieved passages, using answer sentence selection only for enhancing inference speed.",
            "score": 0.5179619618317913,
            "section_title": "Related Work",
            "char_start_offset": 6959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 275,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 322,
                    "end": 349,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 539,
                    "end": 563,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1010,
                    "end": 1026,
                    "matchedPaperCorpusId": "226262308"
                },
                {
                    "start": 1026,
                    "end": 1047,
                    "matchedPaperCorpusId": "235731757"
                },
                {
                    "start": 1084,
                    "end": 1101,
                    "matchedPaperCorpusId": "238583601"
                },
                {
                    "start": 1227,
                    "end": 1245,
                    "matchedPaperCorpusId": "189762341"
                },
                {
                    "start": 1245,
                    "end": 1262,
                    "matchedPaperCorpusId": "229363636"
                },
                {
                    "start": 1262,
                    "end": 1281,
                    "matchedPaperCorpusId": "231924957"
                },
                {
                    "start": 1373,
                    "end": 1395,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 1604,
                    "end": 1633,
                    "matchedPaperCorpusId": "3356807"
                },
                {
                    "start": 1633,
                    "end": 1651,
                    "matchedPaperCorpusId": "43289219"
                },
                {
                    "start": 1653,
                    "end": 1671,
                    "matchedPaperCorpusId": "207853043"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1697998046875
        },
        {
            "corpus_id": "238857121",
            "title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking",
            "text": "Recently, dense passage retrieval has demonstrated better performance than traditional sparse retrieval methods (e.g., TF-IDF and BM25) on the task of passage retrieval. Existing approaches of learning dense passage retriever can be di-vided into two categories: (1) self-supervised pretraining for retrieval Guu et al., 2020) and (2) fine-tuning pre-trained language models (PLMs) on labeled data (Lu et al., 2020;Karpukhin et al., 2020;Xiong et al., 2020a;Luan et al., 2021; . Our work follows the second class of approaches, which show better performance with less cost. There are two important tricks to train an effective dense retriever: (1) incorporating hard negatives during training (Karpukhin et al., 2020;Xiong et al., 2020a; and (2) distilling the knowledge from cross-encoder-based reranker into dual-encoder-based retriever (Izacard and Grave, 2020; Yang and Seo, 2020;. Based on the retrieved passages from a retriever, PLM-based rerankers with the cross-encoder architecture have recently been applied on passage re-ranking to improve the retrieval results (Qiao et al., 2019;Nogueira and Cho, 2019;Yan et al., 2019), and yield substantial improvements over the traditional methods. Apart from separately considering the above two tasks, it has been proved that passage retrieval and passage re-ranking are actually highly related and dependent (Huang et al., 2020;Khattab and Zaharia, 2020). The retriever needs to capture the relevance knowledge from the re-ranker, and the re-ranker should be specially optimized according to the preceding results of the retriever. Some efforts studied the possibility of leveraging the dependency of retriever and re-ranker, and try to enhance the connection between them in an alternative way Huang et al., 2020). Furthermore, several studies attempted to jointly train the retriever and the reader for Open-domain Question Answering (Guu et al., 2020;Sachan et al., 2021;Karpukhin et al., 2020).",
            "score": 0.5178407776598838,
            "section_title": "Related Work",
            "char_start_offset": 4460,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 438,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 693,
                    "end": 717,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1116,
                    "end": 1133,
                    "matchedPaperCorpusId": "211481850"
                },
                {
                    "start": 1362,
                    "end": 1382,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 1382,
                    "end": 1408,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "250311114",
            "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
            "text": "Passage retrieval is an important component in applications like ad-hoc information retrieval, opendomain question answering (Karpukhin et al., 2020), retrieval-augmented generation (Lewis et al., 2020) and fact verification (Thorne et al., 2018). Sparse retrieval methods such as BM25 were the dominant approach for several decades, and still play a vital role nowadays. With the emergence of large-scale pre-trained language models (PLM) (Devlin et al., 2019), increasing attention is being paid to neural dense retrieval methods (Yates et al., 2021). Dense retrieval methods map both queries and passages into a low-dimensional vector space, where the relevance between the queries and passages are measured by the dot product or cosine similarity between their respective vectors. Like other NLP tasks, dense retrieval benefits greatly from a strong general-purpose pre-trained language model. However, general-purpose pretraining does not solve all the problems. As shown in Table 1, improved pre-training techniques that are verified by benchmarks like GLUE  do not result in consistent performance gain for retrieval tasks. Similar observations are also made by Lu et al. (2021). We hypothesize that, to perform robust retrieval, the [CLS] vector used for computing matching scores should encode all the essential information in the passage. The next-sentence prediction (NSP) task in BERT introduces some supervision signals for the [CLS] token, while RoBERTa  and ELECTRA do not have such sequence-level tasks.\n\nIn this paper, we propose SimLM to pre-train a representation bottleneck with replaced language modeling objective. SimLM consists of a deep encoder and a shallow decoder connected with a representation bottleneck, which is the [CLS] vector in our implementation. Given a randomly masked text segment, we first employ a generator to sample replaced tokens for masked positions, then use both the deep encoder and shallow decoder to predict the original tokens at all positions. Since the decoder only has limited modeling capacity, it must rely on the representation bottleneck to perform well on this pre-training task. As a result, the encoder will learn to compress important semantic information into the bottleneck, which would help train biencoder-based 1 dense retrievers. Our pretraining objective works",
            "score": 0.5175805196766298,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 149,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 182,
                    "end": 202,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 225,
                    "end": 246,
                    "matchedPaperCorpusId": "53645946"
                },
                {
                    "start": 440,
                    "end": 461,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "243865399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2305908203125
        },
        {
            "corpus_id": "230435546",
            "title": "Multi-Task Retrieval for Knowledge-Intensive Tasks",
            "text": "Given a large collection of unstructured text passages, information retrieval (IR) can be broadly defined as finding a small set of passages that satisfies an information need, often presented in the form of a short text query (Manning et al., 2008). Traditional IR methods, such as tf-idf and BM25 (Robertson and Zaragoza, 2009), match keywords efficiently with an inverted index. Such methods can be seen as representing queries and passages in high-dimensional, sparse vectors, where each dimension corresponds to a term in the vocabulary and the weight indicates its importance. In contrast to tf-idf and BM25, dense retrieval methods encode text as a latent semantic vector of a fixed, much smaller dimensionality. Whether a passage is relevant to a given query is determined by the distance of their vectors (Deerwester et al., 1990). Although dense representations do not encode tokens explicitly and can potentially map paraphrases of completely different tokens to close vectors, performance of early dense retrieval methods was often inferior to term-matching approaches, except when large labelled data is available Gao et al., 2011;Huang et al., 2013). Thanks to success of large pre-trained models Liu et al., 2019b), however, recent dense retrieval methods have shown to outperform the sparse counterparts, when fine-tuned on a small set of in-domain labelled data Lewis et al., 2020b;Xiong et al., 2020). Efficient index and search of dense vectors are made possible by maximum inner product search (MIPS) algorithms (e.g., Shrivastava and Li, 2014;Guo et al., 2016), as well as tools like FAISS (Johnson et al., 2019).\n\nOur work is built upon the Dense Passage Retriever (DPR) architecture of , which was initially proposed for the task of open-domain question answering. DPR is a neural bi-encoder model which embeds queries with an encoder f (\u00b7) and passages with a separate encoder g (\u00b7). Given an input query x and a target passage y, we have\n\nwhere the similarity score sim (x, y) is defined as the inner product of the embeddings of its arguments",
            "score": 0.5163608385991986,
            "section_title": "Retrieval methods",
            "char_start_offset": 4314,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 329,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1127,
                    "end": 1144,
                    "matchedPaperCorpusId": "16213872"
                },
                {
                    "start": 1144,
                    "end": 1163,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 1379,
                    "end": 1399,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1399,
                    "end": 1418,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1539,
                    "end": 1564,
                    "matchedPaperCorpusId": "1653222"
                },
                {
                    "start": 1564,
                    "end": 1581,
                    "matchedPaperCorpusId": "14163648"
                },
                {
                    "start": 1611,
                    "end": 1633,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58642578125
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Large-scale passage retrieval [2] aims to fetch relevant passages from a million-or billion-scale collection for a given query to meet users' information needs, serving as an important role in many downstream applications including open domain question answering [20], search engine [56], and recommendation system [54], etc. Recent years have witnessed an upsurge of interest and remarkable performance of dense passage retrievers on first-stage retrieval. Built upon powerful pre-trained language models (PLM) [9,30,36], dense retrievers [20,35,49] encode queries and passages into a joint low-dimensional semantic space in a Siamese manner (i.e. dual-encoder), so that the passages could be offline pre-indexed and query could be encoded online and searched via approximate nearest neighbor [19], reaching an efficiency-effectiveness trade-off. \n\nAlthough dense retrieval becomes indispensable in modern systems, a long-term challenge is that the dense representations in a latent semantic space are abstractive and condensed, exposing the systems to a risk that pivot phrases and mentions may be overlooked and thus leading to sub-optimal efficacy. For example, DPR [20] didn't regard \"Thoros of Myr\" as an entity mention in the query \"Who plays Thoros of Myr in Game of Thrones?\". Analogously, given the query \"What is an active margin\", ANCE [49] overlooked the \"active margin\" as an entire local salient phrase and hence retrieved passages related to the financial term \"margin\". As a remedy, prior works resort to either coupling a dense retriever with the term matching scores (e.g., TF-IDF, BM25) [7,14,26,33] or learning BM25 ranking into a dense model as additional features to complement the original one [4]. But, these approaches are limited by superficial combinations and almost unlearnable BM25 scoring.",
            "score": 0.5162964545844555,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 544,
                    "end": 547,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1610,
                    "end": 1613,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1616,
                    "end": 1619,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45361328125
        },
        {
            "corpus_id": "257585074",
            "title": "A Unified Framework for Learned Sparse Retrieval",
            "text": "Neural information retrieval has becoming increasingly common and effective with the introduction of transformers-based pre-trained language models [17]. Due to latency constraints, a pipeline is often split into two stages: first-stage retrieval and re-ranking. The former focuses on efficiently retrieving a set of candidates to re-rank, whereas the latter focuses on re-ranking using highly effective but inefficient methods. Neural first-stage retrieval approaches can be grouped into two categories: dense retrieval (e.g., [12,13,38]) and learned sparse retrieval (e.g., [7,40,44]). Learned sparse retrieval (LSR) methods transform an input text (i.e., a query or document) into sparse lexical vectors, with each dimension containing a term score analogous to TF. The sparsity of these vectors allows LSR methods to leverage an inverted index. Compared with dense retrieval, LSR has several attractive properties. Each dimension in the learned sparse vectors is usually tied to a term in vocabulary, which facilitates transparency. We can, for example, examine biases encoded by models by looking at the generated terms. Furthermore, LSR methods can re-use the inverted indexing infrastructure built and optimized for traditional lexical methods over decades. \n\nThe idea of using neural methods to learn weights for sparse retrieval predates transformers [40,42], but approaches' effectiveness with pre-BERT methods is limited. With the emergence of retrieval powered by transformer-based pre-trained language models [6,17,36], many LSR methods [5,7,8,16,20,23,41] have been introduced that leverage transformer architectures to substantially improve effectiveness. Among them, the Splade [7] family is a recent prominent approach that shows strong performance on the MSMarco [26] and BEIR benchmarks [35]. \n\nDespite their architectural similarities, different learned sparse retrieval methods exhibit very different behaviors regarding effectiveness and efficiency. The underlying reasons for these differences are often unclear. \n\nIn this work, we conceptually analyze existing LSR methods and identify key components in order to establish a comparative framework that unifies all methods under the same perspective. Under this framework, the key differences between existing LSR methods become apparent.",
            "score": 0.5162801998795126,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2223
                },
                {
                    "start": 2224,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "235792476"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "52229883"
                },
                {
                    "start": 1364,
                    "end": 1367,
                    "matchedPaperCorpusId": "155300"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1525,
                    "end": 1528,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 1550,
                    "end": 1553,
                    "matchedPaperCorpusId": "220729621"
                },
                {
                    "start": 1560,
                    "end": 1563,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 1563,
                    "end": 1566,
                    "matchedPaperCorpusId": "233394068"
                },
                {
                    "start": 1781,
                    "end": 1785,
                    "matchedPaperCorpusId": "1289517"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1513671875
        },
        {
            "corpus_id": "274822530",
            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
            "text": "Specifically, we explore the limitations of sparse retrieval, dense retrieval, and generative retrieval on Japanese datasets. \n\nSparse retrieval, which relies on term-matching methods like TF-IDF (Salton and Buckley, 1988), BM25 (Robertson et al., 2004), BM25+ (Robertson and Walker, 1994), struggles with Japanese due to its complex morphology, flexible syntax, and the nuances of word segmentation, which complicate accurate term matching. \n\nDense retrieval, which relies on embeddings for semantic similarity, has potential in Japanese but requires large, specialized datasets, a resource often lacking. The integration of large language models (LLMs) has propelled dense retrieval by improving both in-domain accuracy and cross-domain generalization, as well as enabling new capabilities like instruction-following and in-context learning. Early advances with models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) laid the groundwork, but recent LLMs such as LLaMA (Touvron et al., 2023a,b) and RepLLaMA (Ma et al., 2024) have set new benchmarks, outperforming traditional multi-stage methods on tasks like MS-MARCO (Bajaj et al., 2016) and BEIR (Thakur et al., 2021). Efforts such as Llama2Vec (Li et al., 2023) and NV-Embed (Lee et al., 2024) have further optimized these models, enhancing adaptability and performance. LLM-based dense retrieval models now offer advanced functionalities, handling diverse queries with better adaptability, flexibility, and efficiency than earlier methods. The overview of the multi-stage retrieval applies a two-phase approach for optimal performance. During the Preprocessing stage, unnecessary elements such as special characters, undesired symbols, and stop words are filtered out, ensuring the input text is cleaner and more accurate before passing through BM25+. This process allows for the retrieval of top-a 1 relevant documents related to the query.",
            "score": 0.5160435199081836,
            "section_title": "Introduction",
            "char_start_offset": 1993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 253,
                    "matchedPaperCorpusId": "16628332"
                },
                {
                    "start": 261,
                    "end": 289,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 910,
                    "end": 931,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1022,
                    "end": 1039,
                    "matchedPaperCorpusId": "263908865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158203125
        },
        {
            "corpus_id": "254853896",
            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
            "text": "On the other hand, Li et al. (2022) proposed a promising retrieval architecture, dual-cross-encoder. As shown in Figure 1 (c), this architecture computes the query-related document representation by expanding the document with a real or pseudo query. \n\nCompared with late interaction, dual-crossencoder (i.e., dense retrieval with document expansion) gets the query-related document representation without sacrificing the retrieval efficiency at inference. However, there exists a discrepancy between training and inference in the current dualcross-encoder retriever. Specifically, during training, the document is expanded using a real query, whereas during inference, the document is enriched with a generated query. This discrepancy causes the learned retriever overly focus on the query, yet neglect the document, when computing the document representation. During inference, if the generated query q\u2032 significantly differs from the user-input query q, the query-related document representation will be misled by q\u2032, thus degrading the performance. That is why the dual-cross-encoder even underperforms the vanilla dual-encoder. To address this issue, Li et al. (2022) proposed a solution by computing multiview document representations using different generated queries for each document. While multiview document representations improve retrieval performance, they come at the cost of significantly increased retrieval latency, which scales linearly with the number of views. \n\nIn this paper, we propose CAPSTONE, a curriculum sampling for dense retrieval with document expansion, to bridge the gap between training and inference for dual-cross-encoder. Our motivation is to expect the dual-cross-encoder retrieval model can utilize both the document d and pseudo query q\u2032 to compute the query-informed document representation. To achieve this, we train the dual-cross-encoder retriever by gradually increasing the relevance of the pseudo query q\u2032 to the gold query q. Specifically, at the early training stage, a pseudo query q\u2032 irrelevant to q is selected, causing the retriever to solely rely on the document. As we progress to the late training stage, a highly related pseudo query q\u2032 is chosen, allowing the retriever to learn to augment the document representation with the pseudo query.",
            "score": 0.5160281926643,
            "section_title": "Introduction",
            "char_start_offset": 2228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "276928532",
            "title": "GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval",
            "text": "While decomposition-based approaches require many auto-regressive steps to decompose questions, extract answers, and a different model for retrieval, our encoder-only approach only requires a single forward pass per hop to compute the next dense vector. Example is from (Trivedi et al., 2022). \n\nperformance by leveraging cross-encoder architectures. Unlike bi-encoders, which independently encode questions and passages to compute similarity, cross-encoders process both as a single sequence, resulting in linear scaling with respect to the number of passages. This makes them only suited as a retriever for a few hundred passages but not open book retrieval with up to billions of passages. Despite its strengths, it shares MDR's generalization issues while introducing scalability challenges due to its computational overhead, making it impractical for large-scale open retrieval tasks. These limitations underscore the need for a scalable and generalizable multi-hop retrieval framework that can perform well on both in-distribution and out-ofdistribution benchmarks in open-domain retrieval scenarios. \n\nTo address these challenges, we introduce GRITHopper-7B, the first decoder-based end-toend multi-hop dense retrieval model trained on an unprecedented scale of multi-hop datasets spanning both question-answering and fact-checking tasks. GRITHopper-7B achieves state-of-theart performance across out-of-distribution benchmarks (see Figure 1) while preserving the simplicity and scalability of encoder-only paradigms like MDR (see Figure 2). The foundation of GRITHopper lies in GRITLM (Muennighoff et al., 2025), a Mistral-7B-based model that integrates causal language modeling with dense retrieval training. GRITLM's design sparked a critical debate in the field: Does joint optimization of generative and retrieval tasks enhance dense embedding quality?",
            "score": 0.5159994115367557,
            "section_title": "Introduction",
            "char_start_offset": 1794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 292,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 1593,
                    "end": 1619,
                    "matchedPaperCorpusId": "267681873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "236493551",
            "title": "Domain-matched Pre-training Tasks for Dense Retrieval",
            "text": "We have investigated domain-matched pre-training tasks for bi-encoder dense retrieval models. We found that the proposed approach is more effective than previously proposed artificial pre-training tasks. We demonstrated the generality of our conclusions, by evaluating on a large and varied set of passage retrieval and dialogue retrieval benchmarks.\n\nOur work should be considered as a new statement in the ongoing dialogue of how to best train dense retrieval models. We believe we have addressed some important open questions, such as whether and when pre-training can be useful. However we have also raised new questions, in addition to the many which remain open. For instance, many different ways of leveraging reader models for better retrieval have been recently proposed, including end-to-end training, distillation, data filtering and data augmentation. What is the relationship between these approaches? Are they complementary? Which ones are more efficient, and more performant? We believe these questions deserve a more thorough investigation.\n\nWe have focused mostly on dense retrieval when full supervision is available, and showed that for k = 100 retrieval candidates, the performance is already approaching a ceiling. There is more room for improvement for smaller k. In this regime, however, re-ranking models also become feasible and separable architecture is not a strict requirement. Therefore, further improvements to retrieval will likely need to be discussed with more emphasis on the computation-accuracy trade-off. Few-shot and zero-shot retrieval will also be of increasing importance, and there are already works investigating this direction (Maillard et al., 2021;Thakur et al., 2021).",
            "score": 0.5157953730607742,
            "section_title": "Conclusion",
            "char_start_offset": 23840,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "3.1 Text-Based RAG Models: Text-based RAG models represent the most mature and widely researched category. These models leverage textual data for both retrieval and generation tasks, enabling applications such as question-answering, summarization, and conversational agents. \n\nTransformer architectures, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), are foundational in text-based RAG models. These models utilize self-attention mechanisms to capture contextual relationships within text, which enhances both retrieval accuracy and generation fluency. Dense retrieval models, such as those using dense embeddings from BERT, offer superior performance compared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage dense representations to retrieve relevant documents more effectively. Recent advancements focus on integrating retrieval and generation into a single training pipeline. REALM (Guu et al., 2020) is an example of such an end-to-end model that jointly optimizes retrieval and generation processes, improving overall task performance.",
            "score": 0.5156896975062694,
            "section_title": "Retrieval-Augmented Generation Models Across Different Modalities",
            "char_start_offset": 19845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1104
                }
            ],
            "ref_mentions": [
                {
                    "start": 317,
                    "end": 338,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3310546875
        },
        {
            "corpus_id": "243865679",
            "title": "Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval",
            "text": "Given the above analysis, this subsection describes several negative sampling strategies to address the drawbacks of in-batch negative sampling. \n\nRandom sampling samples negatives passages from Y with equal chance, i.e., treats P N (y) as a uniform distribution. Despite its simplicity, uniform negative/noise has been shown effective in training language models (Mnih and Teh, 2012). \n\nContext negatives samples negative passages from those occurred in the same document as y i , assuming these negatives are less relevant to the question than y i , but more relevant than rest of the passage collection. Documents that contain only one passage are split in half, and the half that does not contain the answer span is picked as negative. \n\nBM25 negatives samples negatives from top passages returned by a BM25 model. Previous work (Karpukhin et al., 2020;Luan et al., 2021) have shown that such negatives are crucial to building high accuracy dense retrieval models. \n\nNeural retrieval negatives employs neural retrieval models to sample negative passages. We do this by running the models on the questions in the training set and then sampling negatives from the top K predictions. As analyzed by Luan et al. (2021), encoding dimension and model size are crucial factors affecting the dense retrieval model accuracy. Varying encoding dimension and model capacity allows us to control the relatedness of the negative passages. In particular, the coarse negatives are sampled from a dual encoder model with 3 Transformer layers, and just 25 dimensions in the encoding outputs; the fine and super fine negatives are sampled from dual encoders with 12 Transformer layers with encoding dimension 512 and 768, respectively. \n\nTo illustrate our sampling strategies, Section 7.4 includes examples of all six hard negative types.",
            "score": 0.5152326584432004,
            "section_title": "Negative Sampling Strategies",
            "char_start_offset": 9666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 384,
                    "matchedPaperCorpusId": "6633369"
                },
                {
                    "start": 833,
                    "end": 857,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299560546875
        },
        {
            "corpus_id": "243986045",
            "title": "Recent Advances in Automated Question Answering In Biomedical Domain",
            "text": "Neural networks can also be used for the initial retrieval without sacrificing on the time needed. These models can be composed of dual BERT based encoders (also referred to as two-tower retrieval model) [42,126] where one encoder is used for embedding the questions and the other one for the passages into a low dimensional continuous embedding space. The architecture has been schematically explained in figure 3. These models are trained so that the embeddings of the questions and passages are similar (in terms of dot product or cosine distance) in the embedding space if the question-passage pair are relevant, and dissimilar otherwise. The document embeddings are precomputed and stored, such that during inference, only the question embeddings need to be computed using the question encoder. The problem of retrieval can then be reduced to finding the K Nearest Neighbour document/passage embeddings to the question embeddings. Using specialized data structures like FAISS [98] and Maximum Inner Product Search (MIPS), fast and efficient retrieval of relevant documents from a large collection is possible, making the method practically feasible, as was shown in the Dense Passage Retrieval (DPR) system [101]. DPR has been used as the retriever in several state of the art QA systems [86,87,116]. BERT models typically need a lot of labelled training data (question and relevant document pairs). In absence of such large datasets, the models can be pretrained using inverse close task [113], in which, instead of question and document pairs, sentences and its contexts (nearby passages) are used. \n\nQuery modification, especially query expansion in document retrieval step has been shown to give rise to significant improvements in QA performance. The GAR [130] system uses a sequence to sequence generative transformer model, BART [114], to generate query expansion terms. Using the expanded query in a simple BM25 model can cause significant improvement in the performance of the retriever. It was also shown that the GAR query expansion model used in conjugation with the DPR model can lead to even better performance.",
            "score": 0.5146996101579215,
            "section_title": "Method Retriver Reader",
            "char_start_offset": 37617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432373046875
        },
        {
            "corpus_id": "218470027",
            "title": "Sparse, Dense, and Attentional Representations for Text Retrieval",
            "text": "Specifically, we assess the capacity of compressive dual encoder retrieval to mimic the retrieval results of sparse bag-of-words models. We address the following questions: \n\n\u2022 Under what conditions does a dual encoder have the capacity to match the retrieval decisions of a sparse bag-of-words model such as BM25? \n\n\u2022 How does the capacity of the dual encoder vary with document length and vocabulary size? \n\nWe first establish theoretical links between the error with which a compressive dual encoder preserves distances and its ability to replicate the rankings of a sparse boolean inner product retrieval model, and show that a dual encoder with embedding size that grows with the square of the number of unique terms in the longest document can preserve all pairwise rankings of boolean sparse inner product. We then derive an upper bound on embedding size for more general sparse retrieval models such as BM25, which implies that random projections achieve suitably low error when the encoding size grows with a measure of query-dependent normalized margin between documents. While this is an upper boundand could be pessimistic-we show empirically that the normalized margin is highly indicative of the random projection dimension required to preserve a given ranking between two documents for a given query. Attention-based architectures enable an upper bound on embedding size using far more compact token encodings; however, such architectures have a significantly higher computational cost. Empirical evaluation of learned neural models shows how the performance of compressive encoders degrades with document length, and demonstrates that significant improvements can be obtained from relatively simple scalable hybrids of dual encoding, attention, and sparse retrieval.",
            "score": 0.5140462096838447,
            "section_title": "Introduction",
            "char_start_offset": 3876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1782
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302978515625
        },
        {
            "corpus_id": "259224595",
            "title": "On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective",
            "text": "Dense retrieval models. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and utilize embedding similarities as estimated relevance scores for effective retrieval [9]. Karpukhin et al. [12] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25. Subsequently, researchers began exploring various fine-tuning techniques to enhance dense retrieval models, such as mining hard negatives [32,35], late interaction [14]. Recently, researchers have also investigated pre-training tasks for dense retrieval [8,19]. Although these methods greatly improve the performance of dense retrieval models, they follow the same bi-encoder architecture represented by the DPR and usually need considerable memory consumption and computational overhead. Generative retrieval models. Generative retrieval has recently garnered increasing interest [1,4,20,30], which retrieves documents by directly generating their identifiers based on the given query. It offers an end-to-end solution for document retrieval tasks [3,20] and allows for better exploitation of the capabilities of large generative language models. For example, De Cao et al. [7] proposed an autoregressive entity retrieval model and Tay et al. [28] introduced a differentiable search index (DSI) and represent documents as atomic ids, naive string, or semantic strings. Chen et al. [5] proposed a pre-trained generative retrieval model called Corpus-Brain to encode all information of the corpus within its parameters in a general way. However, the robustness of generative retrieval models has been overlooked by the community. Out-of-distribution in IR. Current studies on OOD robustness in IR have their own limitations. For example, Wu et al. [31] only explored the OOD generalization performance of neural ranking models. Some works have been devoted to alleviating the poor performance of dense retrieval in the scenarios of query variants [6,23,27,36] or zero/few-shot of corpus [17,29,34]. In this work, we focus on the OOD generalizability of generative retrieval models.",
            "score": 0.5139347095688399,
            "section_title": "RELATED WORK",
            "char_start_offset": 3158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 567,
                    "end": 570,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 682,
                    "end": 685,
                    "matchedPaperCorpusId": "251718954"
                },
                {
                    "start": 1006,
                    "end": 1009,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1009,
                    "end": 1011,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 1174,
                    "end": 1177,
                    "matchedPaperCorpusId": "258418300"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1507,
                    "end": 1510,
                    "matchedPaperCorpusId": "251594672"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "236976200"
                },
                {
                    "start": 2071,
                    "end": 2074,
                    "matchedPaperCorpusId": "250638661"
                },
                {
                    "start": 2074,
                    "end": 2077,
                    "matchedPaperCorpusId": "244709375"
                },
                {
                    "start": 2077,
                    "end": 2080,
                    "matchedPaperCorpusId": "248524694"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "237347134"
                },
                {
                    "start": 2115,
                    "end": 2118,
                    "matchedPaperCorpusId": "233296016"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60693359375
        },
        {
            "corpus_id": "267938301",
            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
            "text": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities. Specifically, a series of studies that use the encoder-decoder architecture of T5 (Sec. 2.2), and applying zero-shot reranking with LLMs (Sec. 2.3), or viewing reranking as autoregressive text generation problem (Wang et al., 2024) has been successful.",
            "score": 0.5136249182535588,
            "section_title": "Generative Models for Reranking",
            "char_start_offset": 4693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 927
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 83,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 239,
                    "end": 261,
                    "matchedPaperCorpusId": "212725651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "233231706",
            "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling",
            "text": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
            "score": 0.513540351879853,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "264147039",
            "title": "Unsupervised Domain Adaption for Neural Information Retrieval",
            "text": "In the field of IR, documents and queries are typically represented as sparse vectors, with each  element corresponding to a term in the vocabulary. BM25 (Robertson and Zaragoza, 2009) is a well-known ranking function that ranks documents based on query terms within a document, without considering the relationship between query terms. BM25 is a family of scoring functions, with different components and parameters. On the other hand, Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) are retrieval methods that use a two-tower model architecture. The first encoder builds an index of all text passages, while the second encoder maps the input question to a vector and retrieves the top k passages with the closest vectors. The similarity of the vectors is calculated by using the dot product or cosine similarity. Moreover, they optimize the negative log-likelihood loss function to create a vector space where relevant pairs of questions and passages have higher similarity than irrelevant ones, using in-batch negatives as negative passages. The two most important differences are: (i) SBERT uses tied encoders (shared weights), whereas DPR uses two independent encoders; (ii) SBERT uses mean pooling to obtain the final vector, while DPR makes use of the [CLS] token. Modern IR models allow fine-grained token-level interaction to improve the performance but with higher inference cost. Two of such models are ColBERT (Khattab and Zaharia, 2020) and SPLADE (Formal et al., 2021). The main difference between DPR and ColBERT is in their approach to encode the document and query representations. ColBERT uses a joint space approach and a late interaction strategy, while DPR uses a dual-encoder architec-ture and a dense retrieval approach.",
            "score": 0.5134758373832486,
            "section_title": "Retrieval models",
            "char_start_offset": 5638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 183,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 467,
                    "end": 491,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 518,
                    "end": 546,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1523,
                    "end": 1544,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "252739471",
            "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
            "text": "In this study, we aim to bridge the gap between training and inference of phrase retrieval. We first develop an efficient validation metric that measures retrieval accuracy on the index from a small corpus with hard passages using a pre-trained retriever. Based on this validation, we show that the improvements in dense phrase retrieval training with unified loss and hard negatives are effective. As a result, we achieve state-of-the-art phrase retrieval and pas-sage retrieval accuracy in open-domain question answering among retrieval-only approaches.\n\nOur work demonstrates that thorough validation is crucial for the accurate and efficient development of phrase retrieval with a large corpus. Also, we prove that modeling and training methods should be designed closely to retrieval in inference time. Despite its remarkable efficiency and flexibility, phrase retrieval has been relatively less studied than passage retrieval. We believe that our work can encourage more study on phrase retrieval with an efficient development cycle. Furthermore, we hope our findings could be extended to dense retrieval in general to help a wide variety of applications. Moreover, it could be especially beneficial in real applications where the corpus size is much larger than benchmark datasets.",
            "score": 0.5126477161512477,
            "section_title": "Conclusion",
            "char_start_offset": 29437,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "251253350",
            "title": "Active entailment encoding for explanation tree construction using parsimonious generation of hard negatives",
            "text": "For the extrinsic evaluation, we use several baseline models to show the impact of similarity bias on the retrieval-based premise-selection task: \u2022 TF-IDF creates vector representations from tfidf scores. \u2022 DPR (Dense Passage Retrieval) (Karpukhin et al., 2020) uses two separate encoders, for query and premises, to perform passage retrieval at scale. \u2022 Simple Contrastive Learning of Sentence Embeddings (SimCSE) (Gao et al., 2021) is a simple BERT-based bi-encoder trained on both dropout-based augmented samples and NLI samples. \u2022 Sentence-T5 (Ni et al., 2022) is a T5 (Raffel et al., 2020) -based sentence encoder originally proposed for generative tasks. We use it with mean pooling. \u2022 Sentence-BERT (SBERT) in various settings: as a simple Siamese encoder (i.e., query and premise encoders share weights), as a Dual encoder (i.e., query and premise encoders are both updated during fine-tuning), as a Single encoder (only the query encoder is updated during fine-tuning) . \n\nWe use the above baseline models in various settings defined by the loss function used: SCL vs. TML, or by the architecture type used: Siamese vs. Single vs. Dual encoders. We use the following models pre-trained without any fine-tuning: BERT-base-uncased, DPR, SimCSE, SBERT, T5. In the results, we denote their fine-tuned versions by specifying the loss type: SCL or TML. For each of the baselines, we generate embeddings for each hypothesis/premise in the EB corpus and index them using FAISS for fast retrieval.",
            "score": 0.5122907550791825,
            "section_title": "Baselines",
            "char_start_offset": 19666,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1497
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 563,
                    "matchedPaperCorpusId": "237260023"
                },
                {
                    "start": 573,
                    "end": 594,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404296875
        },
        {
            "corpus_id": "253420284",
            "title": "Distribution-Aligned Fine-Tuning for Efficient Neural Retrieval",
            "text": "Dense retrievers employ neural self-attention-based language models to compute vector representations of queries and documents. The original DPR approach by Karpukhin et al. [16] paved the way for a number of improved dense retrieval models, with most improvements being made with respect to the training method. ANCE [31] is trained using hard negatives obtained from an ANN index that is updated asynchronously during the training. TCT-ColBERT [21] employs knowledge distillation from the ColBERT model [17]. Lindgren et al. [22] maintain a negative cache to efficiently obtain the most impactful negative examples during training. Zhan et al. [32] start by training the model with random negatives in order to improve its stability and subsequently use dynamic hard negatives for better ranking performance. Gao and Callan [8] argue that existing dense language models are not optimal for retrieval and propose the Condenser architecture, which, in combination with LM pre-training, allows for better aggregation of information in a single vector representation. This architecture is further improved with coCondenser [9]. \n\nThe above-mentioned implementations base their encoders on BERT-base or similar transformer models, where the query and document encoder use shared weights, i.e. in a Siamese fashion. Jung et al. [15] use a semi-Siamese setting, where the encoders do share parameters as well, but they are adapted to their specific role (query or document encoding) using light fine-tuning methods. We are not aware of any approaches that employ heterogeneous models, where the two encoders do not share the same model architecture and initial weights. \n\nHybrid retrievers combine sparse term-matching-based models [25] with dense neural models. Typically, two sets of documents are retrieved and subsequently combined. CLEAR [11] takes the sparse component into account in the loss margin during the training phase of the dense model. COIL [10] builds dense representations of terms using a deep language model in a pre-processing step. \n\nWork on efficiency for dual-encoder-based models is limited to reranking so far. TILDE [35] replaces the query encoder with a probabilistic model.",
            "score": 0.51219964075734,
            "section_title": "Neural Retrieval and Ranking",
            "char_start_offset": 5839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "245018271"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 826,
                    "end": 829,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "240070656"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "11592162"
                },
                {
                    "start": 1838,
                    "end": 1842,
                    "matchedPaperCorpusId": "232423090"
                },
                {
                    "start": 1953,
                    "end": 1957,
                    "matchedPaperCorpusId": "233241070"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "This allows them to perform query-document attention in order to compute the corresponding relevance score. These models are typically used as re-rankers. Dual-encoder models employ two language models to independently encode queries and documents as fixed-size vector representations. Usually, a similarity metric between query and document vector determines their relevance. As a result, dual-encoders are mostly used for dense retrieval, but also, less commonly, for re-ranking. \n\nWe divide the remainder of the related work section into subcategories for cross-attention models, dual-encoder models, and hybrid models, which employ both lexical and semantic rankers. Finally, we briefly cover inference efficiency for BERT-based models.",
            "score": 0.5117902078067856,
            "section_title": "RELATED WORK",
            "char_start_offset": 7783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 740
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "254685782",
            "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
            "text": "In this section, we introduce the task definition of this work and present the typical fine-tuning process of dense retrieval. \n\nTask Definition. Given a query q, the dense passage retrieval task aims to retrieve the most relevant top-k passages {p i } k i=1 from a large candidate pool P. To achieve this goal, the dual-encoder architecture is widely used. It consists of a query encoder E q and a passage encoder E p , mapping the query q and passage p into k-dimensional dense vectors h q and h p , respectively. Then, the semantic relevance score of q and p will be computed using dot product as \n\n(1) \n\nExisting work mostly adopts pre-trained Transformers (e.g., BERT (Devlin et al., 2019)) as the two encoders, using the representations of the [CLS] token as the dense vectors. In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations. Formally, given a pre-training corpus and a Transformer encoder, we focus on devising several tasks to pre-train the parameters of it. Then, the pre-trained Transformer will be used as the backbone of the query encoder E q and passage encoder E p , and can be fine-tuned on downstream dense retrieval tasks. \n\nFine-tuning Dense Retrievers. In the fine-tuning stage, the learning objective is to pull the representations of a query q and its relevant passages P + together (as positives), while pushing apart irrelevant ones P \u2212 = P \\ P + (as negatives). Therefore, high-quality negatives are critical to the effectiveness of dense retrievers. Existing work commonly leverages the BM25 negatives (Karpukhin et al., 2020) or the top-k ranked negatives mined by a well-trained dense retriever (Qu et al., 2021;Xiong et al., 2021), denoted as D\u2212 . Then, the optimization objective can be formulated as:",
            "score": 0.5115930547226488,
            "section_title": "PRELIMINARY",
            "char_start_offset": 8942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 129,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 673,
                    "end": 694,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1683,
                    "end": 1707,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1778,
                    "end": 1795,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1795,
                    "end": 1814,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "248227479",
            "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
            "text": "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4). \n\nEncoding query-candidate pairs. The cross-encoder architecture has been widely used in sentence-pair classification tasks such as natural language inference and paraphrase detection. We here use a cross-encoder to encode the concatenation of a query example and a candidate example. Specifically, we fine-tune a RoBERTa (Liu et al., 2019) model to classify whether an example pair is a positive or negative match. The confidence of classifying such a pair to be positive can thus be used as the utility score of the candidate upstream example for this query example. On top of this, we then develop a reranking module for further improving retrieval performance as follows. \n\nScoring paired data. To re-rank the initially retrieved data by the dense retriever, we apply the cross-encoder on all pairs of query examples Q and candidate retrieved examples R , producing scores of all |Q| * |R| query-candidate pairs. For each candidate example r \u2208 R , we use the average of all cross-encoder scores involving r as its utility score. Finally, we take the top-K examples based on this new ranking of candidate examples in R as the final retrieved data R. We use upsampling ratio \u00b5 to denote the ratio between R and R, i.e., \u00b5 = |R |/|R|.",
            "score": 0.5115860964264187,
            "section_title": "Reranking Module",
            "char_start_offset": 13098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "A.1 Retrieval Before retrieval, external documents must first be processed from raw data into a list of small, noticeable chunks that can be efficiently handled by language models. Since external data sources may vary significantly in format, it is necessary to align these sources into uniform, context-rich chunks. Following this, an embedding model is employed to encode the chunks, creating embeddings that facilitate the indexing [56]. From the perspective of encoding mechanisms, retrieval methods can be broadly categorized into two types: sparse and dense, depending on how the information is encoded [5]. Sparse methods rely on explicit term matching, while dense methods leverage learned embeddings to capture deeper semantic relationships within the data. Sparse retrieval is primarily word-based and widely employed in text retrieval tasks. Classical approaches such as TF-IDF and BM25 [57] rely on inverted index matching to identify relevant documents. BM25, in particular, is often applied from a macro perspective, where entire passages are treated as singular retrieval units [58,59,60], [61]. However, a key limitation of sparse retrieval in the context of RAG is its untrained nature, leading to retrieval performance highly dependent on both the quality of the data source and the specificity of the query. In contrast, dense retrieval encodes user queries and external knowledge into vector representations, enabling application across a wide range of data formats [62]. Simple dense retrieval methods [63] compute similarity scores between the query vector and the vectors of indexed chunks, retrieving the top K similar chunks to the query. These retrieved chunks are then incorporated as an extended context within the prompt, facilitating more accurate and contextually relevant responses. \n\nEmbedding models are a crucial component of dense retrieval systems. A straightforward approach involves utilizing off-the-shelf NLP models. BERT-based architectures [64] are commonly employed in retrieval models. A prevalent design within RAG frameworks involves constructing bi-encoders with the BERT structure-one encoder dedicated to processing queries and the other for documents [65,66]. Further advancements in RAG models are achieved through large-scale specialized pre-training, which enhances their performance on knowledge-intensive tasks.",
            "score": 0.511260626140259,
            "section_title": "A More details of Retrieval and Generation",
            "char_start_offset": 32867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "244714119"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "258714822",
            "title": "Large Language Models are Built-in Autoregressive Search Engines",
            "text": "Baselines: Contriever (Izacard et al., 2021) and BM25 (Robertson and Zaragoza, 2009) are usually used for passage retrieval. Contriever is a dual encoder which uses a dot product between dense representations of a question and passage to calculate relevance. BM25 is a sparse retriever which uses the overlapping contents between question and passage to calculate relevance. Because we use the same passage size to chunk Wikipedia documents, we were able to map their retrieved passages back to the original documents. We use Google API (Brin and Page, 1998) restricted to Wikipedia as a third baseline to retrieve relevant documents given a question. \n\nExisting works such as DSI and SEAL have investigated the use of autoregressive language models to generate identifier strings for documents as an intermediate target for retrieval. DSI is a Trans- former which has been trained to map directly from question to document identifiers by memorizing the contents of the entire corpus (Tay et al., 2022). SEAL is a variant of DSI which uses ngrams as document ids to improve retrieval performance (Bevilacqua et al., 2022). Neither DSI nor SEAL report retrieval results on full documents and do not have publicly available implementations, so they are left out and discussed in Table 3 and Section 4.1.2 on passage retrieval. \n\nUnlike the baselines, our LLM-URL employs an LLM. It has two settings: zero-shot and fewshot. In the zero-shot setting, no in-context demonstrations are given whereas in the few-shot setting a few demonstrations are appended to the prompt.",
            "score": 0.5108050106408994,
            "section_title": "Document Retrieval",
            "char_start_offset": 13809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1566
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 84,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 537,
                    "end": 558,
                    "matchedPaperCorpusId": "7587743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4189453125
        },
        {
            "corpus_id": "252545318",
            "title": "On the Impact of Speech Recognition Errors in Passage Retrieval for Spoken Question Answering",
            "text": "BM25 is a standard retrieval model based on best match; there is lexical overlap between the query and every retrieved passage. We use the Anserini IR toolkit [14] to compute BM25 scores. \n\nDense Retriever (DR) [6] is a dual-encoder BERT-based model used for scoring question-passage pairs. Given a question , a positive (i.e., relevant) passage + and a set of negatives (i.e., irrelevant) passages { \u2212 1 , \u2212 2 , . . . , \u2212 }, the model learns to rank the positive question-passage pair higher than the negative ones. The two seperate encoders of the model are fine-tuned via the minimization of the softmax cross-entropy: \n\nDuring inference time, the similarity of a question-passage pair is calculated as the inner product of the respective question embedding and passage embedding. In detail, the whole corpus is encoded into an index of passage vectors offline, and the retrieval with respect to a question is implemented with efficient maximum inner product search [5] over the index. We follow the dualencoder architecture compared to a cross-encoder one (that jointly encodes question and passage) due to its high efficiency as a firststage ranker in large-scale settings. While the latter can achieve higher performance, the former makes the whole corpus indexable. \n\nDense retriever with data augmentation (DR+Data augm.) builds on the standard practice for improving the robustness of neural models by augmenting the training data with noisy data. We explore two cases of data augmentation, namely, augmentation with synthetic keyboard noise and augmentation with synthetic ASR noise. For the former, we augment each question on the training set with keyboard noise following the approach presented in [13], while for the latter, we augment with ASR noise as shown in Section 2.2. In contrast with ASR noise, keyboard noise will rarely alter the original word into a different correctly spelled word.",
            "score": 0.5106931635987237,
            "section_title": "Models",
            "char_start_offset": 9727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1711,
                    "end": 1715,
                    "matchedPaperCorpusId": "248524694"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "score": 0.5103475337374797,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1829833984375
        },
        {
            "corpus_id": "252739471",
            "title": "Bridging the Training-Inference Gap for Dense Phrase Retrieval",
            "text": "Building dense retrievers requires a series of standard procedures, including training and validating neural models and creating indexes for efficient search. However, these procedures are often misaligned in that training objectives do not exactly reflect the retrieval scenario at inference time. In this paper, we explore how the gap between training and inference in dense retrieval can be reduced, focusing on dense phrase retrieval (Lee et al., 2021) where billions of representations are indexed at inference. Since validating every dense retriever with a large-scale index is practically infeasible, we propose an efficient way of validating dense retrievers using a small subset of the entire corpus. This allows us to validate various training strategies including unifying contrastive loss terms and using hard negatives for phrase retrieval, which largely reduces the training-inference discrepancy. As a result, we improve top-1 phrase retrieval accuracy by 2~3 points and top-20 passage retrieval accuracy by 2~4 points for open-domain question answering. Our work urges modeling dense retrievers with careful consideration of training and inference via efficient validation while advancing phrase retrieval as a general solution for dense retrieval.",
            "score": 0.510280229732941,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388916015625
        },
        {
            "corpus_id": "264935522",
            "title": "Efficient Neural Ranking Using Forward Indexes and Lightweight Encoders",
            "text": "Dual-encoder-based dense retrieval models have become the standard in IR. They employ large Transformer-based language models, which are notoriously inefficient in terms of resources and latency. We propose Fast-Forward indexes\u2014vector forward indexes which exploit the semantic matching capabilities of dual-encoder models for efficient and effective re-ranking. Our framework enables re-ranking at very high retrieval depths and combines the merits of both lexical and semantic matching via score interpolation. Furthermore, in order to mitigate the limitations of dual-encoders, we tackle two main challenges: Firstly, we improve computational efficiency by either pre-computing representations, avoiding unnecessary computations altogether, or reducing the complexity of encoders. This allows us to considerably improve ranking efficiency and latency. Secondly, we optimize the memory footprint and maintenance cost of indexes; we propose two complementary techniques to reduce the index size and show that, by dynamically dropping irrelevant document tokens, the index maintenance efficiency can be improved substantially. We perform an evaluation to show the effectiveness and efficiency of Fast-Forward indexes\u2014our method has low latency and achieves competitive results without the need for hardware acceleration, such as GPUs.",
            "score": 0.5100694302349865,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "247154785",
            "title": "Asyncval: A Toolkit for Asynchronously Validating Dense Retriever Checkpoints During Training",
            "text": "Pre-trained language models (PLMs) based neural ranking models have shown promising effectiveness on information retrieval tasks [12] -in this paper we specifically focus on passage retrieval, although methods, tool and findings can be extended to documents. Among these models, a great effort has been devoted to developing effective dense retrievers (DRs) [1-3, 5-9, 16-18, 20-25]. DRs exploit a PLM-based bi-encoder architecture to separately encode queries and passages into low-dimensional dense vectors. Then the passage vectors will be indexed and the retrieval is performed by estimating the similarity of the query vector against all the passage vectors in the index. \n\nAt first sight, the task a DR is trained for is similar to the sentence similarity task in natural language processing in that the inputs of the model are two pieces of text and the goal of the model is to estimate the similarity of the two texts [10]. One key difference, however, is that DRs are to be exploited in a retrieval pipeline and thus they are required to encode all the passages in the corpus into dense vectors. It is important to highlight this difference because, as we shall see below, it is this difference that poses a key efficiency challenge when validating DR model checkpoints during trainingand this challenge is not present in tasks such that of training (with validation) a model for the sentence similarity task. With validation of a DR model checkpoint during training we refer to the process of evaluating the effectiveness of a checkpoint training as a way to guide decisions about model convergence and training termination. In this demo paper we present a method and its corresponding implementation into a usable tool for validating DRs in an efficient manner. \n\nSo, how are deep learning models commonly validated during training? The most common (and straightforward) validation approach is the use, during training, of a held-out validation set to evaluate some of the checkpoints' performance on the task (e.g., classification accuracy for the sentence classification task). Checkpoints to validate are typically selected according to a fix step schedule (e.g., a validation every  thousand steps) or at the end of each training epoch.",
            "score": 0.5099764943439561,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "222310837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "257985191",
            "title": "CoT-MAE v2: Contextual Masked Auto-Encoder with Multi-view Modeling for Passage Retrieval",
            "text": "Passage retrieval involves searching a large corpus for passages that are relevant to a particular query, and is essential for various applications such as web search (Fan et al., 2021;Guo et al., 2022;Lin et al., 2021a), question answering (Karpukhin et al., 2020;Lee et al., 2020;Zhu et al., 2021), and dialogue systems (Gao et al., 2022;Yu et al., 2021). Sparse retrieval methods, such as BM25, are practical and dominant approaches. However, retrieval methods based on pre-trained language models (PLMs) (Devlin et al., 2018;Liu et al., 2019) have gained popularity in recent years. PLMs are utilized to improve (1) sparse retrieval models that rely on lexical match; (2) dense retrieval models that model the semantic interaction between queries and passages in latent semantic space. \n\nTo improve sparse retrieval, PLM mitigates vocabulary mismatch by projecting each term in the query and passage to a vocabulary-sized weight vector. Each dimension in the weight vector represents the weight of a term in the PLM vocabulary, and the correlation between query and passage depends on lexical matching (Formal et al., 2021b,a;Shen et al., 2022). PLM-based sparse vectors model the features of passage on every lexical term (denoted by PLM logits output), and thus naturally capture the semantic at the lexicon level. SPLADE (Formal et al., 2021b,a) uses learnable PLM, e.g., BERT, to produce sparse vectors and fine-tune the retriever with contrastive learning loss. PLM-based SPLADE outperforms traditional sparse retrieval like BM25. LexMAE (Shen et al., 2022) further expands on SPLADE by proposing to pre-train a lexical-bottlenecked masked autoencoder to learn importance-aware sparse representations. It learns the lexicon importance distribution in an unsupervised way, through a continuous bag-of-words representation bottleneck.",
            "score": 0.5097823803951153,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "232147859"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "222310837"
                },
                {
                    "start": 340,
                    "end": 356,
                    "matchedPaperCorpusId": "234343311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3349609375
        },
        {
            "corpus_id": "258236093",
            "title": "CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval",
            "text": "To fully explore the performance differences of CoT-MoTE with Siamese and fully separate architectures, we pre-train and fine-tune the above architectures with the following settings: 1) Siamese dual-encoder is pre-trained and fine-tuned by removing qry-FFN from CoT-MoTE. 2) Fully separated dual-encoder is pre-trained and fine-tuned with completely separated encoders. We keep other pipelines the same as our main result settings. \n\nA1. We compare the dense, sparse, and hybrid retrieval results of CoT-MoTE with Siamese and fully separate dual-encoders in  that 1) Fully separated dual-encoders achieve lower performances than the Siamese encoder. 2) CoT-MoTE achieves a significant boost on all retrieval pipelines compared with other dual-encoders. This verifies that our proposed MoTE significantly contributes to the retrieval baseline with respect to traditional Siamese and fully separated dual-encoders. Please note that CoT-MoTE has the same retrieval speed as other dual-encoders, as discussed in Section 4.4. Despite using more model parameters, we believe that the additional parameter size we used is not the decisive factor for better retrieval in our scenario. Individual textual modeling with MoTE and joint attention should be the key to the performance increase. In contrast, the fully separated dual-encoder employs more parameters than CoT-MoTE and Siamese encoders but performs the worst.",
            "score": 0.5097319955228995,
            "section_title": "Comparision of Different Dual-encoders",
            "char_start_offset": 17804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "257427642",
            "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?",
            "text": "To produce queries and keys for retrieval, we use dense representations of the span-level embeddings of entity names in questions and passages. We used the pretrained LUKE The macro average of the top-20 retrieval accuracies (denoted as Recall@20) on the 24 relations of EQ test set. # LM refers to the total number of language models used in each dense retriever with different weight parameters. Retrieval training denotes fine-tuning methods for retrieval of each dense retriever. See text for details of the baseline models. \n\nmodel 3 to encode an entity span to a contextualized representation. The LUKE model is based on the RoBERTa model (Liu et al. 2019) and has an extension called entityaware self-attention. This mechanism allows our system to embed a span-level contextualized representation of the entity name. An entity embedding is output as a single vector from the model, even if it consists of multiple tokens. We also use the embedding of the entire span of a title of a passage as a retrieval key.",
            "score": 0.5094636852052373,
            "section_title": "Contextualized Representations of Named Entities",
            "char_start_offset": 12451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1017
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181640625
        },
        {
            "corpus_id": "266163052",
            "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
            "text": "This suggests that propositions, being both compact and rich in context, enable dense retrievers to access precise information while maintaining adequate context. The average improvement over passagebased retrieval of Recall@20 is +10.1 on unsupervised dense retrievers and +2.7 on supervised retrievers, even though these retrievers were directly trained on passage-level retrieval. Furthermore, we observe a distinct advantage of propositionbased retrieval in downstream QA performance when using retrieval-augmented language models, as elaborated in \u00a76. Retrieval by finer-grained units inherently provides a higher density of questionrelevant information. This finding implies using finer-grained units in the prompts achieves the same performance with a shorter input length, and hence, a faster inference time. \n\nOur main contributions are: \n\n\u2022 We provide a systemic study on how retrieval granularity impacts retrieval and downstream task performance. We observe that the retrieval units have a significant impact on performance. \n\n\u2022 We introduce FACTOIDWIKI, a processed English Wikipedia dump, where each page is segmented into multiple granularities: passages, sentences, and our proposed propositions. \n\n\u2022 We propose retrieval by proposition as an alternative strategy, which achieves better retrieval and QA accuracy and generalization performance (with unsupervised retriever), compared to passage or sentence as retrieval unit.",
            "score": 0.5089718815461283,
            "section_title": "Passage",
            "char_start_offset": 5259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 846
                },
                {
                    "start": 849,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1422119140625
        },
        {
            "corpus_id": "258840876",
            "title": "On the Risk of Misinformation Pollution with Large Language Models",
            "text": "We conduct experiments on four distinctive types of retrieve-and-read ODQA systems, classified based on the choice of the retrievers and the readers. \n\nRetrievers For retrievers, we use BM25 (Robertson and Zaragoza, 2009) and Dense Passage Retriever (DPR) (Karpukhin et al., 2020), representing sparse and dense retrieval mechanisms respectively, which are the mainstream of the current ODQA models. BM25 is a traditional probabilistic model for information retrieval that remains a robust baseline in retrieval tasks. Although sparse retrievers may fall short in capturing complex semantics, they excel at handling simple queries, thus forming the backbone of several contemporary retrieval systems (Formal et al., 2021). Conversely, DPR leverage learned embeddings to discern implicit semantics within sentences, outpacing sparse retrievers in most retrieval tasks. \n\nReaders For readers, we use Fusion-in-Decoder (FiD) (Izacard and Grave, 2021) and GPT-3.5 (text-davinci-003). FiD is a T5-based (Raffel et al., 2020) reader, which features utilizing multiple passages at once to predict answers compared to concurrent models, yielding outstanding performance. Considering that answering questions with conflicting information might diverge from the training objectives of current MRC models, we also experimented with GPT-3.5 as a reader to leverage its extensive training set and flexibility. Additional model configurations are in Appendix A.",
            "score": 0.5088513557353591,
            "section_title": "ODQA Systems",
            "char_start_offset": 15127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1447
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 221,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 256,
                    "end": 280,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "270688320",
            "title": "Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models",
            "text": "Passage ranking, which aims to rank each passage in a large corpus according to its relevance to the user's information need expressed in a short query, is an important task in information retrieval and natural language processing and plays a crucial role in many applications such as web search and retrieval-augmented generation. To achieve both effectiveness and efficiency, current mainstream approaches usually follow a two-stage paradigm known as \"retrievalthen-rerank\", which involves efficiently retrieving a set of candidates first, and further reranking them with a reranker to boost the effectiveness [18,25]. \n\nIn the first retrieval stage, dense retrieval models based on a bi-encoder architecture are widely used [12]. Trained on large-scale datasets of text pairs through contrastive learning, these models can encode text into a low-dimensional dense embedding and capture the relevance between query and passage using vector similarity. \n\nIn the second reranking stage, we can employ more sophisticated models for better ranking performance. A common reranking model is a supervised model based on the cross-encoder design [25]. With the emergence of large language models (LLMs), such as , a series of studies have tried to leverage LLMs' text comprehension and reasoning abilities for zero-shot reranking. Typically, there are three main prompting approaches: pointwise [13,32], pairwise [30], and listwise [28,33]. Among these methods, listwise approaches like RankGPT [33] have achieved state-of-the-art performance by directly producing a final ranking list for multiple passages, rather than merely assessing the relevance of a single passage or the relative position between two passages. \n\nWhile the listwise approaches demonstrate good performance in the reranking task, they are limited by two challenges. Firstly, some LLMs are limited by context length and cannot rank multiple passages simultaneously, necessitating techniques such as a sliding window strategy to complete the ranking process [33]. Secondly, incorporating entire passages into prompts significantly increases inference costs, resulting in high latency in practice [2], which is untenable in the ranking scenario.",
            "score": 0.508463861188367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "9346130"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 2023,
                    "end": 2027,
                    "matchedPaperCorpusId": "258212638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447998046875
        },
        {
            "corpus_id": "231815627",
            "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
            "text": "Open-domain question answering (QA) aims to find the answers to questions expressed in natural language from a large collection of documents. Early QA systems (Brill et al., 2002;Dang et al., 2007;Ferrucci et al., 2010) constructed complicated pipelines consisting of multiple components, including question understanding, document retrieval, passage ranking and answer extraction.\n\nIn recent years, inspired by the advancements of machine reading comprehension (MRC), Chen et al. (2017) proposed a simplified two-stage approach, where a traditional IR retriever (e.g., TF-IDF or BM25) first selects a few relevant passages as contexts, and then a neural reader reads the contexts and extracts the answers. As the recall [CLS] q (1) q (k) [CLS] ...\n\np (1)   component, the first-stage retriever significantly affects the final QA performance. Though efficient with the support of an inverted index, traditional IR retrievers with term-based sparse representations have limited capabilities in matching questions and passages, e.g., term mismatch.\n\nTo deal with term mismatch, the dual-encoder architecture (as shown in Figure 1a) has been widely explored Guu et al., 2020;Luan et al., 2020;Henderson et al., 2017) to learn dense representations of questions and passages in an endto-end manner, which provides a more powerful representation way for semantic match. These studies first separately encode questions and passages, and then compute the similarity between the dense representations using similarity functions such as cosine or dot product. Typically, the dual-encoder is trained by using in-batch random negatives: for each question-positive passage pair in a training batch, the positive passages for the other questions in the batch would be used as negatives. However, it is still difficult to effectively train a dual-encoder for dense passage retrieval due to the following three major challenges.\n\nFirst, there exists discrepancy between training and inference for the dual-encoder retriever. During inference, the retriever needs to identify positive (or relevant) passages for each question from a large collection containing millions of candidates. However, during training, the model is learned to optimize the probabilities",
            "score": 0.5082158350277506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 179,
                    "matchedPaperCorpusId": "5541486"
                },
                {
                    "start": 179,
                    "end": 197,
                    "matchedPaperCorpusId": "262787428"
                },
                {
                    "start": 197,
                    "end": 219,
                    "matchedPaperCorpusId": "1831060"
                },
                {
                    "start": 469,
                    "end": 487,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1190,
                    "end": 1213,
                    "matchedPaperCorpusId": "2449317"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "259858975",
            "title": "Hybrid Hierarchical Retrieval for Open-Domain Question Answering",
            "text": "trieved documents. This helps prune passages that look relevant but are from irrelevant documents to improve answer accuracy, meanwhile greatly reducing the candidate set for passage retrieval and improve the inference speed of ODQA systems. \n\nDespite its success, Liu et al.'s ( 2021) approach (dense hierarchical retrieval, DHR) relies on dense neural retrievers (Lee et al., 2019;Karpukhin et al., 2020) for both document retrieval and passage retrieval, which suffers from two key weaknesses. First, neural encoders used in retrieval are often limited in context length for effectiveness and efficiency, which is too short to encompass most documents. As a result, DHR needs to make use of the structure of Wikipedia documents, and represents the documents succinctly with title, abstract and table of contents, which is not always available for non-Wikipedia text. Second, dense retrievers have been shown to suffer from poor generalization on out-of-domain data (Thakur et al., 2021), whereas sparse retrievers like BM25 (Robertson et al., 2009) excel with lexical matches (Sciavolino et al., 2021). \n\nIn this work, we propose a hybrid hierarchical retrieval (HHR) framework to alleviate these issues. Specifically, we investigate the tradeoffs and complementary strengths of sparse retrievers and dense retrievers at both the document retrieval and passage retrieval stages for ODQA (see Figure 2). We find, among other things, that sparse retrievers can complement dense retrievers at both retrieval stages with a simple approach to aggregate results from both. Besides in-domain evaluation on the dataset that neural models are trained on, we also perform zero-shot evaluation on unseen datasets to compare the generalization of these retriever architectures. We find that sparse retrievers can help HHR generalize better to unseen data and potentially replace dense retrievers in document retrieval. \n\nIn addition, we also study the accuracy, storage cost, and latency tradeoff for these architectures under the HHR framework, and offer practical insights to real-world ODQA systems that often need to factor these into consideration. \n\nOur main contributions are as follows.",
            "score": 0.5076769978584361,
            "section_title": "Introduction",
            "char_start_offset": 2109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 2144
                },
                {
                    "start": 2147,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 406,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2481689453125
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "highlight the high coverage of tokens shared by the query and the passage in the top-k of their projections. This obersvation suggests that the lexical overlap between query and passages plays an important role in the retrieval mechanism. Second, we show that vocabulary projections of passages they are likely to contain words that appear in queries about the given passage. Thus, they can be viewed as predicting the questions one would ask about the passage. Last, we show that the model implicitly implements query expansion (Rocchio, 1971). For example, in Figure 2 the query is \"How many judges currently serve on the Supreme court?\", and the words in the query projection Q include \"justices\" (the common way to refer to them) and \"nine\" (the correct answer).\n\nThe above findings are especially surprising due to the fact that these retrieval models are fine-tuned in a contrastive fashion, and thus do not perform any prediction over the vocabulary or make any use of their language modeling head during finetuning. In addition, these representations are the result of running a deep transformer network that can implement highly complex functions. Nonetheless, model outputs remain \"faithful\" to the original lexical space learned during pretraining.\n\nWe further show that our approach is able to shed light on the reasons for which dense retrievers struggle with simple entity-centric questions (Sciavolino et al., 2021). Through the lens of vocabulary projections, we identify an interesting phenomenon: dense retrievers tend to \"ignore\" some of the tokens appearing in a given passage. This is reflected in the ranking assigned to such tokens in the passage projection. For example, the word \"michael\" in the bottom example of Figure 2 is ranked relatively low (even though it appears in the passage title), thereby hindering the model from retrieving this passage. We refer to this syndrome as token amnesia ( \u00a76).\n\nWe leverage this insight and suggest a simple inference-time fix that enriches dense representations with lexical information, addressing token amnesia. We show that lexical enrichment significantly improves performance compared to vanilla models on the challenging BEIR benchmark (Thakur et al., 2021) and additional datasets. For example, we boost the performance of the strong MPNet model on BEIR from 43.1% to 44.",
            "score": 0.5076575886808722,
            "section_title": "Introduction",
            "char_start_offset": 2098,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1807861328125
        },
        {
            "corpus_id": "265457188",
            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
            "text": "Neural Information Retrieval Neural retrievers adopt pre-trained language models and follow a dual-encoder architecture (Karpukhin et al., 2020) to generate the semantic representations of queries and passages and then calculate their semantic similarities. Some effective techniques have been proposed to advance the neural retrieval models, such as hard negative mining (Xiong et al., 2021), retrieval-oriented pre-training objectives (Izacard et al., 2022), and multi-vector representations (Khattab and Zaharia, 2020). All of these approaches require supervised training data and suffer from performance degradation on outof-domain datasets. Our work demonstrates the possibility that an unsupervised dense retriever can outperform a diverse range of state-of-the-art supervised methods in zero-shot settings.",
            "score": 0.5069360927251999,
            "section_title": "Related Work",
            "char_start_offset": 25398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 813
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 144,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 372,
                    "end": 392,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 437,
                    "end": 459,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 494,
                    "end": 521,
                    "matchedPaperCorpusId": "216553223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "259262395",
            "title": "Learning to Rank in Generative Retrieval",
            "text": "Generative retrieval stands out as a promising new paradigm in text retrieval that aims to generate identifier strings of relevant passages as the retrieval target. This generative paradigm taps into powerful generative language models, distinct from traditional sparse or dense retrieval methods. However, only learning to generate is insufficient for generative retrieval. Generative retrieval learns to generate identifiers of relevant passages as an intermediate goal and then converts predicted identifiers into the final passage rank list. The disconnect between the learning objective of autoregressive models and the desired passage ranking target leads to a learning gap. To bridge this gap, we propose a learning-to-rank framework for generative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn to rank passages directly, optimizing the autoregressive model toward the final passage ranking target via a rank loss. This framework only requires an additional learning-to-rank training phase to enhance current generative retrieval systems and does not add any burden to the inference stage. We conducted experiments on three public benchmarks, and the results demonstrate that LTRGR achieves state-of-the-art performance among generative retrieval methods. The code and checkpoints are released at https://github.com/liyongqi67/LTRGR.",
            "score": 0.5066417869230481,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "227746078",
            "title": "Distilling Knowledge from Reader to Retriever for Question Answering",
            "text": "usually returned by a traditional IR systems. In the context of ad-hoc document retrieval, Gillick et al. (2018) showed that bi-encoder models could be competitive with traditional IR systems. For open domain question-answering, Karpukhin et al. (2020) introduced dense passage retrieval (DPR), which uses dense embeddings and nearest neighbors search. More precisely, question and passage embeddings are obtained using a BERT-based biencoder model, which is trained on a small dataset of question and passage pairs. Then, the full knowledge source (Wikipedia) is encoded using this model, and passages are queried by computing the k-nearest neighbors of the embedding of the question. Jointly embedding the query and documents makes the application of cross-encoder models intractable to large database. To address this limitation, Humeau et al. (2019) introduced the poly-encoder architecture, in which each documents is represented by multiple vectors instead of one. Similarly, Khattab et al. (2020) proposed a scoring function where each term of the query and documents is represented by a single vector. To make the method tractable, their system retrieves documents with an approximate score, which are then re-ranked with the exact one. Finally, Luan et al. (2020) conducts a theoretical and empirical study of sparse, dense and cross-attention information retrieval systems.\n\nUnsupervised learning. Closest to our work, there is growing body of work trying to learn information retrieval systems from unsupervised data.  introduced the inverse cloze task for pre-training retrievers, which can then be fine-tuned end-to-end on question-answering tasks. This pre-training scheme was later evaluated for ad-hoc document retrieval by . Guu et al. (2020) proposed to augment language model pre-training with a retriever module, which is trained using the masked language modeling objective. Similarly, Lewis et al. (2020a) introduced a sequence-to-sequence model that is pre-trained by generating a target text, after retrieving a set of related texts. Lewis et al. (2020b) further train the retriever obtained in Karpukhin et al. (2020) by backpro",
            "score": 0.5062506814242156,
            "section_title": "RELATED WORK",
            "char_start_offset": 5514,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "276928532",
            "title": "GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning (Huang and Chang, 2023), reflection, and decomposition, making them indispensable tools for a wide range of natural language processing tasks. Their generative abilities have been successfully leveraged to solve open-domain multi-hop problems, where complex questions are broken into  (Tang and Yang, 2024). GRITHopper substantially outperforms previous state-of-the-art multi-hop retrieval models on out-of-distribution benchmarks with more hops. smaller sub-questions to retrieve supporting evidence and reflect on them (Asai et al., 2024;Shao et al., 2023;Guan et al., 2024) in a step-by-step manner. However, such decomposition-based approaches (left side Figure 2) require multiple autoregressive steps and discrete intermediate outputs, which breaks the end-to-end differentiability of the retrieval pipeline and increases computational overhead. \n\nDecomposition-free approaches, such as Multi-Hop Dense Retrieval (MDR) (Xiong et al., 2021), and cross-encoder-based methods like Beam Retriever (Zhang et al., 2024), enable end-to-end differentiability by not requiring discrete decompositions, but both suffer from significant limitations. MDR offers an efficient and scalable dense retrieval framework by concatenating the query with passages and encoding them into a single vector representation in one model call per iteration. However, it struggles with more complex datasets like MuSiQue (Trivedi et al., 2022), more hops than 2, and performs poorly on out-ofdistribution benchmarks. On the other hand, Beam Retriever achieves state-of-the-art in-distribution arXiv:2503.07519v1 [cs.IR] 10 Mar 2025  (Guan et al., 2024;Shao et al., 2023) to our encoder-only approach with GRITHopper. While decomposition-based approaches require many auto-regressive steps to decompose questions, extract answers, and a different model for retrieval, our encoder-only approach only requires a single forward pass per hop to compute the next dense vector.",
            "score": 0.5060318296638244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 107,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 369,
                    "end": 390,
                    "matchedPaperCorpusId": "267312593"
                },
                {
                    "start": 606,
                    "end": 625,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 643,
                    "end": 661,
                    "matchedPaperCorpusId": "267406772"
                },
                {
                    "start": 1010,
                    "end": 1030,
                    "matchedPaperCorpusId": "221970302"
                },
                {
                    "start": 1084,
                    "end": 1104,
                    "matchedPaperCorpusId": "261030563"
                },
                {
                    "start": 1483,
                    "end": 1505,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 1695,
                    "end": 1714,
                    "matchedPaperCorpusId": "267406772"
                },
                {
                    "start": 1714,
                    "end": 1732,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466796875
        },
        {
            "corpus_id": "268723791",
            "title": "Scaling Laws For Dense Retrieval",
            "text": "As discussed previously, in this paper, we construct dense retrieval models from the pre-trained language model checkpoints and perform fine-tuning with the annotated query-document pairs in each dataset.One of the most important parts of dense retrieval model training is the negative sampling strategy.Previous work has shown that mining hard negative samples in the training process can significantly improve the retrieval performance.However, the primary objective of this work is to investigate the scaling effects of dense retrieval models.As a result, we do not focus on sophisticated training strategies.For simplicity, we adopt the most straightforward approaches, namely random negative sampling and in-batch negative techniques, for the training of all dense retrieval models in this paper.These methods are employed to minimize the influence of sampling strategies.\n\nFormally, for each query-passage pair (  ,  +  ), we randomly select a set of unlabeled passages from the corpus as the negative.Then we can optimize the following contrastive ranking loss:\n\nwhere  denotes the training batch size, { \u2212  } is the set of negative passages and  (, ;  ) is the scoring function of query and passage:\n\nHere, \u27e8\u2022\u27e9 denotes inner product and  denotes the parameters of the text encoder.We fine-tune the models for a fixed 10,000 steps and random sample 256 negatives at each step.",
            "score": 0.5058011284995338,
            "section_title": "Training Setting",
            "char_start_offset": 16270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1207
                },
                {
                    "start": 1209,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2578125
        },
        {
            "corpus_id": "277621440",
            "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling",
            "text": "Large language models (LLMs), e.g., LlaMa [14,54], Mistral [24], Qwen [2] and GPT-4 [1], have achieved excellent performance in various fields, especially in various subtasks of natural language generation (NLG). In the era of Pre-trained Language Models (PLMs), pre-trained language models have largely surpassed traditional sparse vector space models and neural network-based retrievers. Moreover, many studies [15,23,44,59] found that with the expansion of the model and training scale, the performance and generality of the PLM-based dense retrieval can be further improved. It is not difficult to infer whether LLM-based retrievers also have the potential to surpass PLM-based retrievers. Recently, many studies [4,31,33,39,41,51] have applied decoder-only architecture LLMs to dense retrieval to encode query and document embeddings as representation vectors. For example, RepLLaMA [39] directly replaces the PLMs with LLMs to encode the representation of query and document, and empirical evidence shows that LLMbased retrievers trained using a simple fine-tuning strategy outperform LM-based retrievers trained using complex training strategies. LLM2VEC [4,31] replaced the decoder-only unidirectional attention with bidirectional attention, further improving retrieval performance. Llama2Vec [33] introduced two pre-training tasks (i.e., EBAE Embedding-based Auto-Encoding and Embedding-based Auto-Regression) for LLMs to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. Echo [51] repeated the input twice in context and extracted embeddings from the second occurrence to address an architectural limitation of autoregressive models. Unlike these works, in this work, we propose a novel approach that simply introduces generative training before contrastive learning can significantly enhance the retrieval performance of LLMs.",
            "score": 0.5055071406333915,
            "section_title": "Large Language Models-Based IR",
            "char_start_offset": 8558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 723,
                    "end": 726,
                    "matchedPaperCorpusId": "271885989"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 1301,
                    "end": 1305,
                    "matchedPaperCorpusId": "271885989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "237562875",
            "title": "Simple Entity-Centric Questions Challenge Dense Retrievers",
            "text": "Recent dense passage retrievers outperform traditional sparse retrieval methods like TF-IDF and BM25 (Robertson and Zaragoza, 2009) by a large margin on popular question answering datasets (Lee et al. 2019, Guu et al. 2020, Karpukhin et al. 2020, Xiong et al. 2021. These dense models are trained using supervised datasets and the dense passage retriever (DPR) model (Karpukhin et al., 2020)  In this work, we argue that dense retrieval models are not yet robust enough to replace sparse methods, and investigate some of the key shortcomings dense retrievers still face. We first construct EntityQuestions, an evaluation benchmark of simple, entity-centric questions like \"Where was Arve Furset born?\", and show dense retrieval methods generalize very poorly. As shown in Table 1, a DPR model trained on either a single dataset Natural Questions (NQ) (Kwiatkowski et al., 2019) or a combination of common QA datasets drastically underperforms the sparse BM25 baseline (49.7% vs 71.2% on average), with the gap on some question patterns reaching 60% absolute! Based on these results, we perform a deep dive into why a single dense model performs so poorly on these simple questions. We decouple the two distinct aspects of these questions: the entities and the question pattern, and identify what about these questions gives dense models such a hard time. We discover the dense model is only able to successfully answer questions based on common entities, quickly degrading on rarer entities. We also observe that dense models can generalize to unseen entities only when the question pattern is explicitly observed during training.\n\nWe end with two investigations of practical solutions towards addressing this crucial problem. First, we consider data augmentation and analyze the trade-off between single-and multi-task finetuning. Second, we consider a single fixed passage index and fine-tune specialized question encoders, leading to memory-efficient transfer to new questions.\n\nWe find that data augmentation, while able to close gaps on a single domain, is unable to consistently improve performance on unseen domains. We also find that building a robust passage encoder is crucial in order to successfully adapt to",
            "score": 0.5054355749176516,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1990966796875
        },
        {
            "corpus_id": "220302524",
            "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval",
            "text": "The results in TREC Deep Learning Track benchmarks are presented in Table 1. ANCE empowered dense retrieval to significantly outperform all sparse retrieval baselines in all evaluation metrics. Without using any sparse bag-of-words in retrieval, ANCE leads to 20%+ relative NDCG gains over BM25 and significantly outperforms DeepCT, which uses BERT to optimize sparse retrieval [33]. \n\nAmong the learning mechanisms used in DR, the contemporary method that uses the combination of BM25 + Random Negatives [9,12,13,15] outperforms sparse retrieval in passage retrieval. However, the same as observed in various parallel research [9,13], their trained DR models are no better than tuned traditional retrieval (Best TREC Trad Retrieval) on long documents, where the term frequency signals are more robust. ANCE is the only one that elevates the same BERT-Siamese architecture to robustly exceed the sparse methods in document retrieval. It also convincingly surpasses the concurrent DR models in passage retrieval on OpenQA benchmarks as shown in Table 2. \n\nWhen reranking documents, ANCE-learned BERT-Siamese outperforms the interaction-based BERT Reranker (0.671 NDCG versus 0.646). This overthrows a previously-held belief that it is necessary to capture the interactions between the discrete query and document terms [34,35]. With ANCE, it is now feasible to learn a representation space that captures the finesse in search relevance. Solely using the first-stage retrieval, ANCE nearly matches the accuracy of the cascade retrieval-and-reranking pipeline (BERT Reranker) -with effective representation learning, dot product is all you need.",
            "score": 0.5054046097195317,
            "section_title": "Effectiveness of ANCE Dense Retrieval",
            "char_start_offset": 15658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1642
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1318,
                    "end": 1322,
                    "matchedPaperCorpusId": "5878197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1781005859375
        },
        {
            "corpus_id": "269605698",
            "title": "Beyond Relevance: Evaluate and Improve Retrievers on Perspective Awareness",
            "text": "We first compare with the traditional BM-25 retrieval method (Robertson & Zaragoza, 2009;Trotman et al., 2014), which is a popular key-word matching algorithm with TF-IDF token weights.\n\n2. Dense Passage Retrieval (DPR): DPR model (Karpukhin et al., 2020) was trained to retrieve the most relevant passage given a query with.In the experiment, we select a model trained with the natural question dataset as the representative.We treat all candidates (i.e., words/sentences/passages) as the passages and encode them with the pre-trained encoder.",
            "score": 0.5049320853141157,
            "section_title": "BM-25:",
            "char_start_offset": 35150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 187,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 544
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 89,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 89,
                    "end": 110,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 231,
                    "end": 255,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1375732421875
        },
        {
            "corpus_id": "271903375",
            "title": "FabricQA-Extractor: A Question Answering System to Extract Information from Documents using Natural Language Questions",
            "text": "We explain the online and offline components: 1) Chunker and Indexer (Offline): During an offline stage, the collection of documents is indexed into an information retrieval system 1 . The indexing strategy consists of splitting documents into passages of roughly the same size and often consists of several sentences. This size is compatible with state of the art RC models, which contributes to extracting good quality from those models. For example, BERT-based QA readers such as [14] do not accept passages longer than 512 subword tokens, and a single word is usually represented with more than one subword token, according to the wordpiece algorithm [52]. The released BERT-based reader implementation [53] uses at most 384 subword tokens and we split passages into 100 words approximately and show evidence for this decision in the evaluation section. \n\nThe chunker component of FabricQA reads documents as a stream of tokens. To chunk the stream into passages, it uses a sliding window that ensures there is some overlap of tokens across neighboring paragraphs. Although this redundancy increases the storage footprint in the information retrieval system, it also boosts the performance of the end to end system because it avoids possible context loss. \n\nFinally, during the offline stage, the Indexer component assigns an ID to each passage and stores it along with the document it belongs to in the information retrieval system. Note on information retrieval system. We assume the information retrieval system can return the K1 most relevant passages to an input query, along with the ranking score associated with each passage. Most information retrieval systems implement this functionality. \n\n2) Passage Ranker (Online): The Passage Ranker retrieves K1 passages from the information retrieval system and forwards K2 passages downstream to the Answer Ranker, where K2 < K1. K1 is usually in the hundreds, while K2 in the tens. The information retrieval system can return those passages fast because it only uses syntactic information to rank and retrieve passages. The Passage Ranker uses semantic information, as available through pre-trained language models, to narrow down the passages that get sent downstream to the more computationally expensive Answer Ranker. \n\nAfter extensive experimentation, we concluded that modern RC models subsume the role of a Passage Ranker.",
            "score": 0.5046329073508002,
            "section_title": "B. Query Lifecycle",
            "char_start_offset": 26234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2277
                },
                {
                    "start": 2280,
                    "end": 2385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10284423828125
        },
        {
            "corpus_id": "254877181",
            "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
            "text": "Dense retrieval models based on neural text representations have proven very effective Qu et al., 2021;Ram et al., 2022;Izacard et al., 2022a,b), improving upon strong traditional sparse models like BM25 (Robertson and Zaragoza, 2009). However, when applied off-theshelf (i.e., in out-of-domain settings) they often experience a severe drop in performance (Thakur et al., 2021;Sciavolino et al., 2021;Reddy et al., 2021). Moreover, the reasons for such failures are poorly understood, as the information captured in their representations remains under-investigated.  Figure 1: An example of our framework. We run the question \"Where was Michael Jack born?\" through the question encoder of DPR , and project the question representation e q to the vocabulary space using BERT's masked language modeling head . The result is a distribution over the vocabulary, Q. We apply the same procedure for passages as well. These projections enable reasoning about and improving retrieval representations.\n\nIn this work, we present a new approach for interpreting and reasoning about dense retrievers, through distributions induced by their query 2 and passage representations when projected to the vocabulary space, namely distributions over their vocabulary space (Figure 1). Such distributions enable a better understanding of the representational nature of dense models and their failures, which paves the way to simple solutions that improve their performance.  We begin by showing that dense retrieval representations can be projected to the vocabulary space, by feeding them through the masked language modeling (MLM) head of the pretrained model they were initialized from without any further training. This operation results in distributions over the vocabulary, which we refer to as query vocabulary projections and passage vocabulary projections.\n\nSurprisingly, we find these projections to be highly interpretable to humans ( Figure 2; Table 1). We analyze these projections and draw interesting connections between them and well-known concepts from sparse retrieval ( \u00a75). First, we highlight the high coverage of tokens shared by the query and the passage in the top-k of their projections. This obersvation suggests that the lexical overlap between query and passages plays an important role in the retrieval mechanism. Second, we show that vocabulary projections of passages they are likely to contain words that appear in",
            "score": 0.5044422675686575,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205810546875
        },
        {
            "corpus_id": "272330251",
            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
            "text": "We present target corrector networks for approximating the softmax function during the training of dual encoder models and retrieval augmented language models. The target corrector networks learn to update a stale buffer of target representations. We investigate the generalization properties of the corrector models theoretically. We furthermore show empirically how our correct model approach can be used to train models (both supervised retrievers and retrieval augmented language models) matching the accuracy of models that use 4x-80x the computational budget during training.",
            "score": 0.5031626544339753,
            "section_title": "Conclusion",
            "char_start_offset": 34416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 581
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33837890625
        },
        {
            "corpus_id": "254823295",
            "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
            "text": "Retrieval-augmented models There exists a large body of retrieval-augmented approaches. Some particularly well known models are REALM (Guu et al., 2020), RAG (Lewis et al., 2020), RETRO (Borgeaud et al., 2022) and Fusion-in-Decoder (Izacard and Grave, 2021). FiD in particular has achieved state-of-the-art performance on a wide variety of tasks (Izacard and Grave, 2021;Izacard et al., 2022;Yu et al., 2022b) and in this work we focus on improving the performanceefficiency trade-offs for FiD. RETRO is another closely related retrieval-augmented model, as it uses a small encoder for retrieved context and a larger primary decoder like FiDO does. Unlike RETRO, FiDO's efficiency improvements allow it to tractably attend to many retrieved passages with a much larger decoder. ing factor for computations on modern devices (Williams et al., 2009;Dao et al., 2022;Shazeer, 2019). Shazeer (2019) shows that autoregressive Transformers are particularly bandwidth bound during inference, and proposes multi-query attention as a partial solution. We find that this is exacerbated by the FiD setting, and adopt multi-query attention for FiDO to ameliorate the problem. Pope et al. ( 2022) also investigates multi-query attention, primarily in the context of efficient inference and parallelization for very large language models, whereas we focus on performance/cost trade-offs for the retrieval-augmented setting.",
            "score": 0.5030692880214233,
            "section_title": "Related Work",
            "char_start_offset": 12283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1409
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 151,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 186,
                    "end": 209,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 232,
                    "end": 257,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 346,
                    "end": 371,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 824,
                    "end": 847,
                    "matchedPaperCorpusId": "267923378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2362060546875
        },
        {
            "corpus_id": "276885340",
            "title": "Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence",
            "text": "Retrieval-based language models have demonstrated strong performance on a range of knowledge-intensive NLP tasks (Lewis et al., 2020;Asai et al., 2023;Gao et al., 2024). At the core of these models is a retriever that identifies relevant context to ground the generated output. Dense retrieval methods such as Contriever (Izacard et al., 2021)-where passages or documents are stored as learned embeddings-are especially appealing for their scalability across large knowledge bases and handling lexical gaps (Ni et al., 2022;Shao et al 1. Positive values indicate a retriever's preference for the more biased document in each bias scenario, while for answer importance, they reflect a preference for answer-containing documents. The results show that retrieval biases often outweigh the impact of answer presence. \n\n2024), compared to alternatives like BM25 (Robertson and Zaragoza, 2009) or ColBERT (Khattab and Zaharia, 2020). Despite their widespread use, relatively little is understood about how these dense models encode and organize information, leaving key questions about their robustness against adversarial attacks unanswered. \n\nExisting evaluations of retrieval models often focus on downstream task performance, as seen in benchmarks like BEIR (Thakur et al., 2021), without probing the underlying behavior of retrievers. Some studies have analyzed specific issues in information retrieval (IR) models, such as position bias (Coelho et al., 2024) or lexical overlap (Ram Table 1: Examples from our framework highlighting Evidence, Head Entity, and Tail Entity. In all cases, retrieval models favor Document 1 over Document 2, assigning higher retrieval scores accordingly. (Explained in \u00a73.3) in combination-for the first time. To enable finegrained control over document structure and factual positioning, we repurpose a document-level relation extraction dataset (Re-DocRED (Tan et al., 2022)). \n\nWe first investigate biases individually, identifying tendencies such as an over-prioritization of document beginnings, document brevity, repetition of matching entities, and literal matches at the expense of ignoring answer presence. Our statistical approach, illustrated in Figure 1, allows for comparative analysis across different biases.",
            "score": 0.5029334985550611,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2253
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 133,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 133,
                    "end": 151,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 507,
                    "end": 524,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 524,
                    "end": 534,
                    "matchedPaperCorpusId": "271269939"
                },
                {
                    "start": 857,
                    "end": 887,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 899,
                    "end": 926,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1256,
                    "end": 1277,
                    "matchedPaperCorpusId": "233296016"
                },
                {
                    "start": 1437,
                    "end": 1458,
                    "matchedPaperCorpusId": "268987613"
                },
                {
                    "start": 1888,
                    "end": 1906,
                    "matchedPaperCorpusId": "253107167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09368896484375
        },
        {
            "corpus_id": "243986045",
            "title": "Recent Advances in Automated Question Answering In Biomedical Domain",
            "text": "DrQA [43] TFIDF [160] LSTM [79] predicting the answer span from retrieved text DPR [101] BERT [51] based dual tower dense retriever BERT [51] based model predicting answer span from retrieved text. \n\nRAG [116] DPR retriever BART [114] based model generating answers using retrieved text. \n\nFiD [87] DPR retriever T5 [152] based model generating answers using retrieved text. \n\nGAR [130] BART [114] based model for query expansion; BM25 based retriever DPR Reader (BERT based model predicting answer span from retrieved text.) performance of the entire system significantly. A variety of methods ranging from sparse lexical models to dense deep learning based models have been used for retrieving relevant passages. DrQA [43] uses a TFIDF [153] based retrieval model to retrieve relevant Wikipedia passages. TFIDF and similar sparse lexical models like BM25 [160] have been used extensively for document retrieval in ODQA as well as other information retrieval tasks. First an inverted index of different words / word-tokens in the entire text corpus is created [129]. This produces a sparse high dimensional representation of the document. The query terms are used to match with the inverted index and a list of top scoring documents (scored by BM25 or TFIDF) is returned. These sparse retrieval models can also be modified to include higher order ngrams (bigrams or trigrams) along with the usual unigrams (individual word tokens), as they lead to better retrieval [7], albeit at the cost of reduced efficiency due to the increase in the number of features in the index. The sparse retrieval models only compare the exact matches, i.e. the lexical similarity among terms present in the documents and the query, and not their semantic relevance [101]. This is a major drawback which can be resolved using dense semantic encodings / embeddings produced by neural networks [101]. \n\nFigure 4: Dual Encoder or Two Tower architecture [42]. Query Encoder (also referred to as Query Tower) is used for encoding the question, Document Encoder (also referred to as Document Tower) is used to encode the documents.",
            "score": 0.5028871772538006,
            "section_title": "Method Retriver Reader",
            "char_start_offset": 33665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 316,
                    "end": 321,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15185546875
        },
        {
            "corpus_id": "266163052",
            "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
            "text": "Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.",
            "score": 0.5027249061622087,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3671875
        },
        {
            "corpus_id": "273901388",
            "title": "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?",
            "text": "Recently, dense retrieval (Karpukhin et al., 2020) has been widely used in first-stage retrieval that efficiently recalls candidate documents from the large corpus. One of the mainstream approaches to information retrieval is to build retrievers using dual-encoder architecture (Yih et al., 2011) to build. Gillick et al. (2019) employs the dual encoder architecture for separately encoding mentions and entities into high-dimensional vectors for entity retrieval. However, fine-grained information about entities in the modeling external database is often ignored through this coarse interaction metric calculation (Xu et al., 2023;Liu et al., 2023). Retrieval-Augmented Generation (RAG) is a technique that combines a retrieval system with a generator model to improve the accuracy and richness of generated content (Lewis et al., 2020). Although existing end-to-end task-oriented dialogue systems have followed this paradigm, they have explored less on how to balance the retrieval results with the user query.",
            "score": 0.5027218574118664,
            "section_title": "Dense Retrieval",
            "char_start_offset": 21044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1013
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 50,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "1965270"
                },
                {
                    "start": 307,
                    "end": 328,
                    "matchedPaperCorpusId": "202718954"
                },
                {
                    "start": 818,
                    "end": 838,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138671875
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https://github.com/microsoft/taser.",
            "score": 0.5025950807789776,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "251594591",
            "title": "ConTextual Masked Auto-Encoder for Dense Passage Retrieval",
            "text": "Dense passage retrieval aims to retrieve the relevant passages of a query from a large corpus based on dense representations (i.e., vectors) of the query and the passages. Recent studies have explored improving pre-trained language models to boost dense retrieval performance. This paper proposes CoT-MAE (ConTextual Masked Auto-Encoder), a simple yet effective generative pre-training method for dense passage retrieval. CoT-MAE employs an asymmetric encoder-decoder architecture that learns to compress the sentence semantics into a dense vector through self-supervised and context-supervised masked auto-encoding. Precisely, self-supervised masked auto-encoding learns to model the semantics of the tokens inside a text span, and context-supervised masked auto-encoding learns to model the semantical correlation between the text spans. We conduct experiments on large-scale passage retrieval benchmarks and show considerable improvements over strong baselines, demonstrating the high efficiency of CoT-MAE. Our code is available at https://github.com/caskcsg/ir/tree/main/cotmae.",
            "score": 0.5025409152232502,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "269457256",
            "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
            "text": "Word embeddings are a common approach in natural language processing.Similar to sparse vector representations, they use deep learning techniques to project words into a higher-dimensional vector space.Several articles in the RALM architecture utilize this technique, and we have selected representative ones to describe.Karpukhin et al. (2020) proposed the DPR retrieval model, which indexes all passages in a lowdimensional and continuous space.This allows the reader to efficiently retrieve the first k passages associated with the input problem at runtime.A dense encoder is used to map any text passage to a d-dimensional real-valued vector, creating an index for all M passages used for retrieval.Due to its excellent performance as a retriever in RALM architectures, DPR has been widely adopted by researchers such as Lewis et al. (2020), Izacard and Grave (2020b), and Karpukhin et al. (2020).Izacard and Grave (2020a) takes a similar tactic, unlike DPR, in that he uses the same encoding function for questions and paragraphs through shared parameters.In order to further minimise the intervention and reduce the cost of manual annotation, Izacard et al. (2021) proposed another retriever called Contriever, which was trained using unsupervised data.It is based on successive dense embeddings and has a dual-encoder architecture.Average pooling was applied on the output of the previous layer to obtain one vector representation for each query or document.The similarity score between the query and each document was obtained by computing the dot product between their corresponding embeddings.Researchers (Siriwardhana et al., 2023), (Sachan et al., 2021), and (Guu et al., 2020) have used it as a retriever in the RALM architecture due to its ability to utilize unsupervised data.",
            "score": 0.5024783623791571,
            "section_title": "Word Embedding",
            "char_start_offset": 13287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 201
                },
                {
                    "start": 201,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1790
                }
            ],
            "ref_mentions": [
                {
                    "start": 824,
                    "end": 843,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1614,
                    "end": 1641,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1670,
                    "end": 1688,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2115478515625
        },
        {
            "corpus_id": "257279794",
            "title": "Retrieval for Extremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-Ranker",
            "text": "It is noteworthy to mention that while our proposed methods leverage bi-encoders, specifically Sentence-BERT Reimers and Gurevych [56], to re-rank the first-stage retriever's ranked list, our approach fundamentally diverges from dense passage retrievers that employ bi-encoders as first stage retrievers in terms of both methodology and application [27]. Dense retrievers emphasize on the alignment of query and relevant document representations by optimizing a targeted loss function, such as the mean square error between the query and relevant document vectors while we do not have such training in our proposed. Moreover, optimizing dense retrievers on the query-by-document task with extremely lengthy queries or documents is not computationally possible due to BERT model's maximum word limit of 512 words which prevents from representing each query or document by a single pass to a bi-encoder, and as a result, prevents from being able to optimizing the representation of query and document as they do not have one union representation. There could be future work on studying how applying dense passage retrieval on query by document task which is out of the scope of this study.",
            "score": 0.5017632002762643,
            "section_title": "Sentence embedding",
            "char_start_offset": 14771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1187
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4658203125
        },
        {
            "corpus_id": "245219136",
            "title": "Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks",
            "text": "In this work, we use FiD (Izacard and Grave, 2021b), a state-of-the-art retrieval-augmented generation model, as our base generator model G. We include a high-level summary of the model for clarity, referring the reader to Izacard and Grave (2021b) for more details.\n\nEncoder. We first encode an input query and passages using a pre-trained T5 (Raffel et al., 2020) encoder. The input query x is prepended to each passage, and the encoder encodes all N passages independently. Formally, we transform passage p i into p i \u2208 R L\u00d7h , where L is the input text length and h is a hidden size.\n\nAnswer generator.P is a summary representation of the input, formed by concatenating the encoded passages. The answer generator takesP and outputs the final answer autoregressively. Specifically, it outputs the sequence probability for y as follows:\n\nP (y|x,P) = T j=1 p(y j |y <j , x,P).\n\nwhere y j denotes the jth token of the generated output y and T is the length of the final output. The generator is based on the T5 architecture and uses cross attentions to model the interactions between retrieved passages.",
            "score": 0.5015655504735776,
            "section_title": "Base Generator G",
            "char_start_offset": 6743,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40625
        },
        {
            "corpus_id": "262465393",
            "title": "Model-enhanced Vector Index",
            "text": "In this work, we introduce a novel Model-enhanced Vector Index (MEVI), which combines the advantages of both the sequence-to-sequence autoregressive model and the twin-tower dense representation model. It can be effectively applied in real-world applications due to its ability to achieve both high recall performance and fast retrieval speed on large-scale corpus. MEVI constructs an RQ structure to hierarchically cluster large-scale documents, enabling the sequence-to-sequence model to directly generate the relevant cluster identifiers given an input query; the retrieved candidate documents in the top-k clusters are further ensembled with embedding-based retrieval results for candidates re-ranking. We empirically show that MEVI achieves better model performance than baselines on the widely used large-scale retrieval datasets MSMARCO Passage and Natural Questions.",
            "score": 0.5015200678980458,
            "section_title": "Conclusion",
            "char_start_offset": 26601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 874
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3134765625
        },
        {
            "corpus_id": "250073156",
            "title": "Are We There Yet? A Decision Framework for Replacing Term Based Retrieval with Dense Retrieval Systems",
            "text": "In this section, we showcase our framework in a case study on dense retrieval models using MS MARCO and TREC Deep Learning Tracks data. Since we expect dense retrieval to have better search effectiveness but with higher cost, our primary criteria are NDCG@10 (requiring a statistically significant difference with a two-tailed paired t-test;  < 0.05; C-Effective \u2713) and an aggregated cost that combines query latency and indexing costs (Eq. ( 11)). \n\nMaking decisions about cost factors is not as general as effectiveness results. Different practitioners are likely constrained in different ranges, therefore we use our decision framework to showcase different decision makers, without making a general claim. Providing a single answer to the question Are we there yet (as a community)?, however convenient is simply not possible, due to the diverse nature of the retrieval settings. However, we provide the tools to answer specific use cases and settings for the question: Are we there yet (in our setting)? We carry out a Pareto analysis on our primary criteria, considering several alternative new systems for our specific use case. \n\nWe also consider several robustness criteria, which we count as secondary, where we are looking for failures rather than improvements (C-Robust \u2717). \n\nThe Dense Retrieval Model. In our experiments, we use the BERT DOT model as the dense retrieval system. It uses two independent BERT computations (each time pooling the CLS vector output) to obtain the query  1: and passage  1: representations. It then computes the retrieval score based on the dot product similarity of the two representations: \n\nThis architecture decouples the costly encoding from the search. We can store every passage in an (approximate) nearest neighbor index  for direct vector-based retrieval. The retrieval of the top  hits for a given query  is then formalized as: In this study, we use the Standalone and TAS-Balanced trained instances of BERT DOT , developed by Hofst\u00e4tter et al. [26]. The Standalone version is trained with binary relevance labels from MS MARCO [3]. The TAS-Balanced retriever is trained with pairwise and in-batch negative knowledge distillation using topic-aware sampling to compose batches. \n\nIt is currently a state-of-the-art training technique for dense retrieval [63].",
            "score": 0.5014927376006604,
            "section_title": "CASE STUDY",
            "char_start_offset": 25064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2228
                },
                {
                    "start": 2231,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 1997,
                    "end": 2001,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "1289517"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1431884765625
        },
        {
            "corpus_id": "238419331",
            "title": "Adversarial Retriever-Ranker for dense text retrieval",
            "text": "Dense text retrieval (Lee et al., 2019;Karpukhin et al., 2020) has achieved great successes in a wide variety of both research and industrial areas, such as search engines (Brickley et al., 2019;Shen et al., 2014), recommendation system (Hu et al., 2020), open-domain question answering (Guo et al., 2018;Liu et al., 2020), etc. A typical dense retrieval model adopts a dual-encoder (Huang et al., 2013) architecture to encode queries and documents into low-dimensional embedding vectors, with the relevance between query and document being measured by the similarity between embeddings. In real-world dense text retrieval applications, it pre-computes all the embedding vectors of documents in the corpus, and leverages the approximate nearest neighbor (ANN) (Johnson et al., 2019) technique for efficiency. To train a dense retriever, contrastive loss with negative samples is widely applied in the literature (Xiong et al., 2021;Karpukhin et al., 2020). During training, the model utilizes a negative sampling method to obtain negative documents for a given querydocument pair, and then minimizes the contrastive loss which relies on both the positive document and the sampled negative ones (Shen et al., 2014;Chen et al., 2017;Radford et al., 2021). \n\nRecent studies on contrastive learning (Xiong et al., 2021;Karpukhin et al., 2020) show that the iterative \"hard-negative\" sampling technique can significantly improve the performance compared with \"random-negative\" sampling approach, as it can pick more representative negative samples to learn a more discriminative retriever. In the work (Qu et al., 2021), it suggests leveraging crossencoder model to heuristically filter \"hard-negative\" samples to further improve performance and shows the importance of sampling technique in the contrastive learning. \n\nOn the other hand, the model architecture of dual-encoders enables the encoding of queries and documents independently which is essential for document indexing and fast retrieval.",
            "score": 0.5014095899145434,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 39,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 39,
                    "end": 62,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 237,
                    "end": 254,
                    "matchedPaperCorpusId": "220050047"
                },
                {
                    "start": 305,
                    "end": 322,
                    "matchedPaperCorpusId": "216915123"
                },
                {
                    "start": 383,
                    "end": 403,
                    "matchedPaperCorpusId": "8384258"
                },
                {
                    "start": 760,
                    "end": 782,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 932,
                    "end": 955,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1213,
                    "end": 1231,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 1315,
                    "end": 1338,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1597,
                    "end": 1614,
                    "matchedPaperCorpusId": "231815627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56396484375
        },
        {
            "corpus_id": "247450700",
            "title": "Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation",
            "text": "the large language model. However, since our interpolation and perturbation methods only manipulate the already   obtained representations of the documents from the encoder E D , we don't have to newly generate document texts and also to forward generated documents into the model, which greatly saves time and memory (see Table 3). We provide a detailed analysis and discussion of efficiency in Appendix B.  (Kwiatkowski et al., 2019); 2) TriviaQA (TQA) is a QA collection scraped from the Web (Joshi et al., 2017).\n\nRetrieval Models 1) BM25 is a sparse termbased retrieval model based on TF-IDF (Robertson et al., 1994). 2) Dense Passage Retriever (DPR) is a dense retrieval model with a dual-encoder of query-document pairs (Karpukhin et al., 2020). 3) DPR with Query Augmentation (DPR w/ QA) augments pairs with query generation for the document, adopting (Liang et al., 2020;Mao et al., 2021a). 4) DPR with Document Augmentation (DPR w/ DA) augments pairs by replacing words in the document (Ma, 2019). 5) DPR with Axiomatic Regularization (DPR w/ AR) regularizes the retrieval model to satisfy certain axioms (Rosset et al., 2019). 6) DAR is ours with interpolation and perturbation of document representations. Implementation Details For the dense retrieval model based on the DPR framework, we refer to the publicly available code from DPR (Karpukhin et al., 2020). We set the training epoch as 25 and batch size as 32 under academic budgets with a single GeForce RTX 3090 GPU having 24GB memory. We use in-batch negative sampling as our negative sampling strategy without hard negative samples. Also, we retrieve 100 passages per question. We use both interpolation and perturbation schemes for our augmentation methods. Specifically, for the interpolation method, we set \u03bb \u2208 [0, 1] in equation 3 to be sampled from the uniform distribution. Also, for the perturbation method, we set the dropping rate as 0.1, and the",
            "score": 0.5010538815544491,
            "section_title": "Method",
            "char_start_offset": 10233,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 515,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 597,
                    "end": 621,
                    "matchedPaperCorpusId": "41563977"
                },
                {
                    "start": 727,
                    "end": 751,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 880,
                    "end": 898,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 1348,
                    "end": 1372,
                    "matchedPaperCorpusId": "215737187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "247594093",
            "title": "Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval",
            "text": "On the other hand, recent bi-encoder dense retrieval models (Karpukhin et al., 2020;Santhanam et al., 2021;Hofst\u00e4tter et al., 2021;Formal et al., 2021;Gao and Callan, 2021) have been shown to be highly effective for in-domain retrieval while remaining efficient for large corpora in practice. Compared to traditional retrieval methods, these models use dual deep encoders, usually built on top of a Transformer encoder architecture (Vaswani et al., 2017;Devlin et al., 2019), to encode query and document passages separately and eventually output contextual embeddings. Similarity scores can be efficiently computed given these embeddings, which limits costly neural inference to indexing time. The effectiveness of these models can be attributed to the encoder's ability to capture contextual connections or even high-level semantics without the necessity for exact lexical matching. This very complementary benefit compared to more rigorous structure search methods motivates us to investigate whether dense retrieval models can improve MIR results when combined with existing structure search methods. We summarize the contributions of this work as follows: \n\n\u2022 We have performed a fair effectiveness comparison of a token-level and a passage-level dense retrieval baseline in the MIR domain. To our knowledge, this is the first time that a DPR model has been evaluated in this domain. \n\n\u2022 We have successfully combined dense retrievers with a structure search system and have been able to achieve new state-of-the-art effectiveness in recent MIR datasets. \n\n\u2022 A comprehensive list of dense retrievers and strong baselines for major MIR datasets are covered and compared. We believe our well-trained models and data pipeline 1 can serve as a stepping stone for future research in this domain, which suffers from a scarcity of resources. \n\n2 Background and Related Work",
            "score": 0.5009547565999428,
            "section_title": "Introduction",
            "char_start_offset": 2041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 131,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "240288895",
            "title": "Dense Hierarchical Retrieval for Open-Domain Question Answering",
            "text": "The goal of open-domain Question Answering (QA) is to answer a question without pre-specified source domain (Kwiatkowski et al., 2019). One of the most prevalent architectures in open-domain QA is the retriever-reader approach (Chen et al., 2017;. Given a question, the task Figure 1: An example of distracting passages in Natural Question (Kwiatkowski et al., 2019). The first DPR retrieved passage shares similar semantics with the gold passage. The document title of the second DPR retrieved passage matches most question tokens. Both of the retrieved passages tend to result in a wrong answer. of the retrieval stage is to identify a set of relevant contexts within a diversified large corpus (e.g., Wikipedia). The reader component then consumes the retrieved evidence as input and predicts an answer. In this paper, we focus on improving the efficiency and the effectiveness of the retrieval component, which in turn leads to improved overall answer generation for open-domain QA.\n\nPretrained transformer models, such as BERT , are widely used in recent studies on the retriever-reader framework (Asai et al., 2019;Guu et al., 2020). To serve as input to the retriever, documents are split into short passages, and in the Dense Passage Retrieval, DPR , a dual encoder framework is applied to encode questions and the split passages separately. State-of-the-art dense retrievers outperform sparse term-based retrievers, like BM25 (Robertson and Zaragoza, 2009), but they suffer from several weaknesses. First, due to the lack of effective pruning strategy, extracting relevant passages from a large corpus undergoes an efficiency issue especially in the inference time. Second, given a question, many passages may comprehend similar topics with subtle semantic difference. This fact requires the retriever and the reader to encode passages to their accurate semantic representations, which is an overwhelmed task. Moreover, passages contain only local and specific information, thus easily leading to distracting representations. As illustrated in Figure 1, distracting passages with similar semantics may lead to a wrong answer.\n\nTo alleviate these issues, we present a Dense Hierarchical Retriever (DHR) framework, which consists of a dense document-level retriever and a dense passage",
            "score": 0.5003564151000515,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 134,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 227,
                    "end": 246,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 340,
                    "end": 366,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 1102,
                    "end": 1121,
                    "matchedPaperCorpusId": "208267807"
                },
                {
                    "start": 1121,
                    "end": 1138,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415771484375
        },
        {
            "corpus_id": "271534029",
            "title": "FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis",
            "text": "Based on the results obtained from the conducted comparative experiments, the following are the main key findings and recommendations derived from this study: \n\nPerformance comparison. FG models consistently outperformed CG models in instancelevel retrieval tasks. CG models, however, demonstrated competitive performance in category-level retrieval, especially when retrieving large numbers of relevant samples. These findings challenge the conventional notion of FG models having universal superiority in retrieval performance over CG models; while FG models excel in specific instance-level retrieval, their performance advantage diminishes in broader, category-level tasks. This suggests that, when retrieval performance is the main concern, the choice between FG and CG models should be task-dependent rather than always defaulting to FG models. \n\nHybrid coarse-to-fine potential. An intuitive integration of CG and FG models could involve a two-step approach, where a CG model selects top-k candidates to reduce the computational load of a subsequent fine-grained reranking step. Our evaluation results showed that CG models trained specifically with instance-level loss functions could potentially serve as this initial screening step within a coarse-to-fine ITR pipeline. However, traditional CG models trained on category-level loss functions are not suitable for this purpose. Despite their effectiveness in broadly retrieving samples of the same category as the query, these models do not consistently rank the exact instance-level match high enough in the retrieval rank to serve as an effective initial screening step. \n\nAttention mechanisms. State-of-the-art benchmark recall results are achieved by models such as BLIP-2, which employ fusion-encoder reranking to refine search results. However, the computational load associated with such querytime attention mechanisms is difficult to justify for practical use. This inefficiency stems from attention mechanisms attending to every possible image-text pair. In scenarios where the increase of images and text samples scale at the same rate, the computational complexity of such attention mechanisms approximates O(n 2 ). Therefore, for large-scale retrieval applications where storage concerns can be addressed, dual-encoder VLP models which do not have query-time attention represent the most practical architecture type for instance-level retrieval. \n\nSearch scalability insights.",
            "score": 0.5001955890530905,
            "section_title": "Key findings and discussion",
            "char_start_offset": 39232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2416
                },
                {
                    "start": 2419,
                    "end": 2447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353515625
        },
        {
            "corpus_id": "278502406",
            "title": "Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition",
            "text": "Understanding knowledge processing and knowledge flows in dense retrieval models is essential for identifying potential methodological gaps and new directions, e.g., establishing new masked language model training stages after fine-tuning for more effective dense retrieval. Yet, these aspects are rarely investigated. \n\nIn this paper we considered previous work by Reichman and Heck who questioned the role of fine-tuning vs. that of pre-training within dense retrievers Reichman and Heck [2024]. In particular, we aimed to reproduce and validate two core claims emerging from their work: (1) that pre-trained language models already possess strong discriminative capacity for determining passage relevance, and (2) that dense retrieval training decentralizes knowledge by activating broader neuron pathways. To do so, we carried out extensive experimentation considering additional training recipes, representation methods, backbones and datasets. Key Findings. Our results lead to the following observations: \n\n\u2022 Discriminative Capacity: Linear probing experiments confirmed that pre-trained BERT models achieve 50-60% accuracy in distinguishing relevant passages, with DPR fine-tuning yielding comparable performance. This aligns with Reichman and Heck's argument that pre-trained knowledge, rather than fine-tuning, primarily governs retrieval effectiveness. Similar trends were observed in Contriever finetuning and mean-pooling. However, decoder-based dense retrievers (e.g., ReplLlama) using EOS token pooling exhibited an 18-22% improvement in deeper-layer accuracy, suggesting that architecture and embedding strategies play a significant role in how knowledge is altered during dense retrieval training. \n\n\u2022 Knowledge Decentralization: Integrated gradient analysis showed that DPR fine-tuning increases intermediate-layer activations by 32-41%, supporting the decentralization hypothesis proposed by Reichman and Heck. However, this effect reversed when using mean-pooling (Contriever) or an EOS-tokenbased decoder (Llama), where fine-tuning instead reduced neuron activation breadth. These results suggest that knowledge decentralization is dependent on the model's architecture and pooling strategy.",
            "score": 0.49969125813302906,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 28806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2212
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417236328125
        },
        {
            "corpus_id": "252568176",
            "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
            "text": "A critical capability for retrieval-augmented models is to be able to synthesize and utilize information from multiple distinct retrieved items (Zamani et al., 2022). To effectively implement this paradigm Izacard & Grave (2020) proposed the FiD model, which re-wires the computational graph between an of-the-shelf pre-trained Transformer Encoder and Decoder (Vaswani et al., 2017). Usually FiD is initialized with the pre-trained T5 model (Raffel et al., 2020). Given a query q, we retrieve a set of n candidate passages using a separate retrieval module. The retriever is independently trained, and can take any traditional, neural or hybrid architecture. We and Izacard & Grave (2020) use a single dense retriever, as they have been shown to outperform traditional retrieval methods (Hofst\u00e4tter et al., 2022). To encode the information, FiD concatenates the query q with each retrieved passage p and independently feeds (one per index i) the sequences through a Transformer encoder (T E ): e i = T E ([\"query: \"; q; \"context: \"; p i ]) \n\n(1) The resulting encoded representations -using one vector per token -are concatenated into a single long sequence, which is fed through the Transformer decoder (T D ), autoregressively during inference, to produce a single output sequence o: o = T D ([e 1 ; e 2 ; ...; e n ]) \n\n(2) \n\nFiD has two main limitations: (1) the text-only output does not provide any information about the exact passage(s) which were used to synthesize the output; and (2) the long input sequence leads to highly inefficient autoregressive decoding (as shown in Figure 1). While the expected output is relatively short (in the magnitude of dozens of tokens), the input to the decoder is large with O(n * (|q| + |p|)) tokens (in the magnitude of thousands of tokens).",
            "score": 0.4995123476360006,
            "section_title": "FID (FUSION IN DECODER) WITH EXPLANATIONS",
            "char_start_offset": 8891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 382,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 441,
                    "end": 462,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36474609375
        },
        {
            "corpus_id": "259370797",
            "title": "CORE: Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection",
            "text": "Retrieval-based Dialogues. In the past, retrievalbased dialogue systems focused on single-turn response selection using message-response pairs as inputs for matching models, as demonstrated in early studies such as (Wang et al., 2013;Ji et al., 2014;Wang et al., 2015). However, more recent attention has been given to multi-turn response selection using context-response matching. This includes methods such as dual-LSTM (Lowe et al., 2015), multi-view matching model (Zhou et al., 2016), deep attention matching network (DAM) (Zhou et al., 2018), and multi-hop selector network (MSN) (Yuan et al., 2019). With the success of pre-trained language models (Devlin et al., 2019;Liu et al., 2020)  Efficient Information Retrieval. Existing information retrieval models (Wang et al., 2013;Qiu et al., 2017;Nogueira and Cho, 2019;Nogueira et al., 2019) usually adopt a pipeline method where an efficient first-stage retriever retrieves a small set of candidates from the entire corpus, and then a powerful but slow second-stage ranker reranks them. However, most of the models rely on traditional lexical-based methods (such as BM25) to perform the first stage of retrieval and the ranking models of different stages are learned separately. Recently, as a promising approach, Dense Retrieval (DR) has been widely used for Ad-hoc retrieval (Zhan et al., 2020;Chang et al., 2020;Luan et al., 2021) and open-domain question answering (Lee et al., 2019;Karpukhin et al., 2020;Xiong et al., 2020) because it is as fast as traditional methods and can achieve impressive performance. \n\nIn retrieval-based dialogue, Humeau et al. (2020) presents the Poly-encoder, an architecture with an additional learned attention mechanism that represents more global features from which to perform self-attention, resulting in performance gains over Bi-encoders and large speed gains over PLM-based models.",
            "score": 0.4994084327901487,
            "section_title": "Related Works",
            "char_start_offset": 5005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 234,
                    "matchedPaperCorpusId": "12588798"
                },
                {
                    "start": 250,
                    "end": 268,
                    "matchedPaperCorpusId": "14029406"
                },
                {
                    "start": 422,
                    "end": 441,
                    "matchedPaperCorpusId": "8379583"
                },
                {
                    "start": 469,
                    "end": 488,
                    "matchedPaperCorpusId": "2867243"
                },
                {
                    "start": 528,
                    "end": 547,
                    "matchedPaperCorpusId": "51877568"
                },
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "202776649"
                },
                {
                    "start": 655,
                    "end": 676,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 766,
                    "end": 785,
                    "matchedPaperCorpusId": "12588798"
                },
                {
                    "start": 785,
                    "end": 802,
                    "matchedPaperCorpusId": "8338282"
                },
                {
                    "start": 1353,
                    "end": 1372,
                    "matchedPaperCorpusId": "211068995"
                },
                {
                    "start": 1372,
                    "end": 1390,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 1426,
                    "end": 1444,
                    "matchedPaperCorpusId": "26030198"
                },
                {
                    "start": 1444,
                    "end": 1467,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1467,
                    "end": 1486,
                    "matchedPaperCorpusId": "221970302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1998291015625
        },
        {
            "corpus_id": "259076101",
            "title": "I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval",
            "text": "The interactor outputs query-aware passage vectors for each passage, which can be pre-computed and cached before deploying the model for online inference. The final query-passage relevance scores can be computed with simple dot-product operation, which gives our model the same high efficiency and low storage cost as dual-encoders. The superior balance between effectiveness and efficiency makes our model more attractive in real-world applications. We summarize our main contributions as follows: \n\n\u2022 We propose a novel PLM-based retrieval model, namely I 3 retriever, which incorporates implicit interaction in dual-encoders. \u2022 We introduce two modules in I 3 retriever that are jointly trained with query and passage encoders in an end-to-end manner, i.e., query reconstructor and query-passage interactor. The query reconstructor is able to generate pseudo-queries for the querypassage interactor, which subsequently encodes query-aware information in the final passage vectors. \u2022 We conduct comprehensive evaluation on large scale datasets. \n\nThe results show that I 3 is able to achieve superior performance w.r.t both effectiveness and efficiency for passage retrieval. We also conduct a thorough study to clarify the effects of implicit interaction.",
            "score": 0.49935858834694447,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1258
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "272654178",
            "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
            "text": "Our retriever is based on the DPR (Karpukhin et al., 2020) architecture, where a dense passage encoder E P and a question encoder E Q is constructed to map candidate input c and query input q to latent representation vectors respectively. The similarity between the candidate representation and the query representation is defined as the dot-product of their vectors as shown in Equation 3: \n\nThe encoders E P and E Q of DPR are initialized with SpeechT5 encoder (Ao et al., 2022) or SONAR encoder (Duquenne et al., 2023). Given the large size of the SONAR encoder, for memory efficiency, only the top layer of the encoder is trained. This approach is not only for memory efficiency but also because the lower layers likely extract low-level acoustic features, which are less relevant for our retrieval task focused on word-level information. We further investigate the retrieval accuracy under different numbers of trainable parameters. As shown in Figure 2. We use the settings with the best retrieval accuracy for our ST task. which are:",
            "score": 0.4992363936445139,
            "section_title": "D Retriever Training and Inference Details D.1 Training Details",
            "char_start_offset": 31862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1040
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 58,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 463,
                    "end": 480,
                    "matchedPaperCorpusId": "238856828"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1845703125
        },
        {
            "corpus_id": "270380194",
            "title": "Scaling the Vocabulary of Non-autoregressive Models for Efficient Generative Retrieval",
            "text": "Generative Retrieval introduces a new approach to Information Retrieval by reframing it as a constrained generation task, leveraging recent advancements in Autoregressive (AR) language models. However, AR-based Generative Retrieval methods suffer from high inference latency and cost compared to traditional dense retrieval techniques, limiting their practical applicability. This paper investigates fully Non-autoregressive (NAR) language models as a more efficient alternative for generative retrieval. While standard NAR models alleviate latency and cost concerns, they exhibit a significant drop in retrieval performance (compared to AR models) due to their inability to capture dependencies between target tokens. To address this, we question the conventional choice of limiting the target token space to solely words or sub-words. We propose PIXAR, a novel approach that expands the target vocabulary of NAR models to include multi-word entities and common phrases (up to 5 million tokens), thereby reducing token dependencies. PIXAR employs inference optimization strategies to maintain low inference latency despite the significantly larger vocabulary. Our results demonstrate that PIXAR achieves a relative improvement of 31.0% in MRR@10 on MS MARCO and 23.2% in Hits@5 on Natural Questions compared to standard NAR models with similar latency and cost. Furthermore, online A/B experiments on a large commercial search engine show that PIXAR increases ad clicks by 5.08% and revenue by 4.02%.",
            "score": 0.499195446235124,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309326171875
        },
        {
            "corpus_id": "277634243",
            "title": "Can we repurpose multiple-choice question-answering models to rerank retrieved documents?",
            "text": "Retrieval-augmented generation (RAG) systems enhance generative outputs with contextually relevant information from external databases. Despite their success, selecting the most relevant information efficiently and accurately remains challenging. \n\nDense retrieval techniques, known for their ability to semantically represent text, offer a promising direction for RAG system enhancement. However, integrating large language models (LLMs) into dense retrieval, while effective, faces scalability and cost-related challenges. \n\nThis work explores the utility of multiple-choice question-answering (MCQA) in reranking within RAG systems. MCQA's potential for evaluating and selecting the most semantically relevant options aligns with the decision-making parallels of cross-encoder architectures. \n\nThe author introduces RoBERTA ReRanker for Retrieved Results or R*, a dual-purpose prototype model that can act as both an MCQA model and a cross-encoder. The author's contributions include proposing MCQA as an alternative to reranking passages and introducing R* for efficient and semantically aware retrieval mechanisms.",
            "score": 0.49897513688445283,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1119
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26806640625
        },
        {
            "corpus_id": "258486925",
            "title": "Hierarchical Representations in Dense Passage Retrieval for Question-Answering",
            "text": "Retrieval Question-answering tasks are usually tackled introducing retrieval components in order to efficiently select a subset of relevant documents (Voorhees et al., 1999). In the past, Q&A tasks would be generally attempted using sparse vector space models such as BM25 and TF-IDF (Chen et al., 2017;Yang et al., 2019;Nie et al., 2019;Wolfson et al., 2020;Min et al., 2019). In the past few years, these were replaced by transformerbased models mapping a model input to a dense vector representation. There are mainly two approaches of neural network-based retrievers based on single or multiple embedding vectors (Singh et al., 2021). Dual encoders belong to the single embedding approaches. Such retrievers use one encoder for the documents and another one for the query (Yih et al., 2011;Lee et al., 2019). Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) uses two BERT-style models to learn a similarity metric between document and query. In case of multivector retrievers instead, multiple embeddings are generated for each document, such as in (Khattab and Zaharia, 2020;Zhang et al., 2022;Luan et al., 2021). However this approach is computationally limiting in large-scale retrieval since it requires to retrieve in many search spaces (up to the document token length) leading to increased memory needs and search time. Instead we propose to perform retrieval in each layer of a BERT-based transformer network limiting the retrieval runs to 12. This approach borrows ideas from early work in information retrieval on multi-layer matching (Nie et al., 2018a,b), however retrieval is performed based on an aggregated score over all layers. Instead, our approach performs retrieval in each layer, while using a more modern transformer architecture, scales to large-scale retrieval and is evaluated on end-to-end QA.",
            "score": 0.4988143577420324,
            "section_title": "Related Work",
            "char_start_offset": 3909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 173,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 284,
                    "end": 303,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 303,
                    "end": 321,
                    "matchedPaperCorpusId": "59604492"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "202660724"
                },
                {
                    "start": 338,
                    "end": 359,
                    "matchedPaperCorpusId": "211003735"
                },
                {
                    "start": 617,
                    "end": 637,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 776,
                    "end": 794,
                    "matchedPaperCorpusId": "1965270"
                },
                {
                    "start": 794,
                    "end": 811,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 843,
                    "end": 867,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1086,
                    "end": 1105,
                    "matchedPaperCorpusId": "247476289"
                },
                {
                    "start": 1105,
                    "end": 1123,
                    "matchedPaperCorpusId": "218470027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454833984375
        },
        {
            "corpus_id": "271924082",
            "title": "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment",
            "text": "Pre-trained language models like BERT and T5 serve as crucial backbone encoders for dense retrieval. However, these models often exhibit limited generalization capabilities and face challenges in improving in-domain accuracy. Recent research has explored using large language models (LLMs) as retrievers, achieving state-of-the-art performance across various tasks. Despite these advancements, the specific benefits of LLMs over traditional retrievers and the impact of different LLM configurations\u2014such as parameter sizes, pre-training duration, and alignment processes\u2014on retrieval tasks remain unclear. In this work, we conduct a comprehensive empirical study on a wide range of retrieval tasks, including in-domain accuracy, data efficiency, zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. We evaluate over 15 different backbone LLMs and non-LLMs. Our findings reveal that larger models and extensive pre-training consistently enhance in-domain accuracy and data efficiency. Additionally, larger models demonstrate significant potential in zero-shot generalization, lengthy retrieval, instruction-based retrieval, and multi-task learning. These results underscore the advantages of LLMs as versatile and effective backbone encoders in dense retrieval, providing valuable insights for future research and development in this field.",
            "score": 0.4986671830194951,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466552734375
        },
        {
            "corpus_id": "268681628",
            "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
            "text": "Information Retrieval In recent years, deep learning-based retrieval models have achieved remarkable performance across various information retrieval tasks.The dense passage retriever (DPR) framework, which encodes documents and queries into dense representations, has emerged as a popular approach for dense retrieval (Karpukhin et al., 2020).With the advent of large language models (LLMs), numerous methods have leveraged these models for dense retrieval.GTR (Ni et al., 2022) utilizes LLM encoders for dense retrieval and demonstrates performance improvements with increased model size.Promptagator (Dai et al., 2023) and InPars (Bonifacio et al., 2022) propose the use of LLMs to generate synthetic query-document pairs, which are then employed for training dense retrievers.Our work is orthogonal to these methods, as we focus on utilizing LLMs for second-stage passage reranking.\n\nPassage Reranking Passage reranking typically serves as a second-stage component following large-scale retrieval.Several studies have proposed deep reranking models that encode query-document pairs to predict relevance scores (Nogueira and Cho, 2019).Nogueira et al.\n\n(2020) introduced a generation-based method for passage reranking by fine-tuning LLMs on MS-MARCO (Bajaj et al., 2016), a large-scale retrieval dataset with relevance annotations.Their model, MonoT5, generates the word true for relevant pairs and false for irrelevant pairs.Similarly, our method also adopts a generation-based approach.\n\nThe main difference is that our method does not require relevance annotations nor fine-tuning; instead, we leverage the instruction-following capabilities of LLMs to enable unsupervised estimation.TART (Asai et al., 2022) fine-tunes LLMs on extensive retrieval supervision from various tasks with instructions.Our method differs from TART in that we do not require any retrieval supervision and employ a generation-based approach in an unsupervised fashion.\n\nAnother research line is unsupervised passage reranking with LLMs, which eliminates the need for retrieval supervision.",
            "score": 0.49848556100139896,
            "section_title": "Related Work",
            "char_start_offset": 2432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1492
                },
                {
                    "start": 1494,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1951
                },
                {
                    "start": 1953,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 343,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 462,
                    "end": 479,
                    "matchedPaperCorpusId": "245144556"
                },
                {
                    "start": 603,
                    "end": 621,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 633,
                    "end": 657,
                    "matchedPaperCorpusId": "250340449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "258309799",
            "title": "A Static Pruning Study on Sparse Neural Retrievers",
            "text": "Pre-trained language models such as BERT [14] have been shown to significantly improve the effectiveness of Information Retrieval (IR) systems over traditional ranking models. Dense retrieval is one of the most promising applications of pre-trained language models in IR. In dense retrieval, a pre-trained language model computes dense representations of queries and documents, that must be stored and processed using approximate nearest neighbor algorithms [17], preferably on GPUs or TPUs [18]. However, the ranking of documents in dense retrieval incurs significant computational costs, even if this can be reduced through approaches such as ColBERT [19,35], or by re-ranking only a small set of candidates [29]. Nevertheless, query processing in dense retrieval is much more expensive than processing documents stored in an inverted index with simpler ranking functions and optimized processing algorithms [36]. \n\nSparse neural retrievers [15,22,27] aim to merge these two approaches, using a pre-trained language model to compute very compact document representations, up to a single value, to be stored in an inverted index. In doing so, these retrievers try to reach the effectiveness of the complex pre-trained language models, while retaining the efficiency of simple bag-of-words retrieval models and query processing strategies. Most sparse neural retrievers integrate two main learning goals: they learn which terms in a document should be indexed (document content learning), and how to represent these terms in a corresponding index (term impact learning). In document content learning, irrelevant terms are removed and relevant terms are added to the documents. During term impact learning, each term in each document is associated with an impact score that will be stored in a posting of the term's posting list. \n\nDespite the effectiveness and efficiency of sparse neural retrievers w.r.t. more complex neural ranking models, recent works have shown that, while effective retrieval is possible with learned sparse approaches, they are often still much slower than their traditional counterparts [27].",
            "score": 0.4981034254703106,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "237278083"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "57663616"
                },
                {
                    "start": 943,
                    "end": 947,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 950,
                    "end": 953,
                    "matchedPaperCorpusId": "233394068"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1695556640625
        },
        {
            "corpus_id": "251903309",
            "title": "LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval",
            "text": "Current passage retrieval systems are widely deployed as retrievethen-rank pipelines [18,56]. The first-stage retriever (i.e., dualencoder) [27,29,35,38,47,49] selects a small number of candidate passages (usually at most thousands) from the entire collection, and the second-stage ranker (i.e., cross-encoder [55]) scores these candidates again to provide a more accurate passages order. In this paper, we focus on enhancing the first-stage retriever. \n\nDense Retriever. Built upon Pre-trained Language Models [9,30], dense retriever [20,35] is to capture the semantic meaning of an entire sequence by encoding sequential text as a continuous representation into a low-dimensional space (e.g., 768). In this way, the dense retriever could handle vocabulary and semantic mismatch issues within the traditional term-based techniques like BM25 [40]. To train a better dense retriever, various techniques are proposed for providing hard negatives including reusing in-batch negatives [20,31,35], iteratively sampling [49], mining by a well-trained model or dynamic sampling [52,53], and denoising by cross-encoder [35]. To build retrieval-specific pre-trained language models, Lee et al. [23] proposed an unsupervised pre-training task, namely Inverse Cloze Task (ICT), Gao and Callan [12] decoupled model architecture during pre-training and further designed corpus-level contrastive learning [13] for better passage representations. \n\nLexicon-Aware Retriever. Another paradigm of work [11,14,26] takes advantage of strong PLMs to build lexicon-aware sparse retrievers by term-importance [7,26] and top coordinate terms [10,11]. These models have lexical properties and could be coupled with inverted indexing techniques.",
            "score": 0.49760254793808767,
            "section_title": "RELATED WORK",
            "char_start_offset": 4602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "219965935"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "235166906"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "218470027"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "233289894"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "238419331"
                },
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "235792467"
                },
                {
                    "start": 1488,
                    "end": 1491,
                    "matchedPaperCorpusId": "233241070"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "235792467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "266348832",
            "title": "Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models",
            "text": "We evaluate the performance of 18 models from four different model families: GPT-2 (Radford et al., 2019), Pythia (Biderman et al., 2023), Falcon (Almazrouei et al., 2023) and Llama 2 (Touvron et al., 2023). We study base language models for all families except Falcon where we include two instruction fine-tuned models. We choose the models to capture diverse scale, architecture, and training techniques. \n\nFigure 5 shows the accuracy of every model on all tasks in ORION. Unsurprisingly, larger models can solve a wider range of problems. Nonetheless, even GPT-2 small with 125M parameters, one of the smallest models, can solve the simplest version of the question-answering task with 100% accuracy. Evaluations using the token probability and logit difference are available in Appendix E. \n\nIn the following analyses, we only consider settings where the model can robustly solve the task. Thus, we focus on pairs of models and tasks that have greater than 70% accuracy. \n\n4 Macroscopic causal analysis on ORION: a universal emergent decomposition of retrieval tasks \n\nTo correctly solve retrieval tasks, an LM has to gather and combine at the last token position information coming from the request and the context. We focus our investigations on understanding how these two processing steps are organized in the intermediate layers of the last token position. \n\nIn this section, we choose to consider a coarse-grained division of the model, intervening on full layers instead of a finer-grained division, e.g. considering single-attention heads and MLP blocks. We find this level of analysis is sufficient to develop a high-level causal understanding of how language models solve retrieval tasks while providing a computationally tractable set of experiments to run at scale. We complement this general coarse-grained analysis in Section 5 with a finer-grained case study on Pythia-2.8b solving a question-answering task.",
            "score": 0.49738965622652076,
            "section_title": "Model performances",
            "char_start_offset": 17900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1927
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "258676297",
            "title": "Synergistic Interplay between Search and Large Language Models for Information Retrieval",
            "text": "Dense Retrieval Document retrieval has been an important component for several knowledgeintensive tasks (Voorhees et al., 1999;Karpukhin et al., 2020). Traditional techniques such as TF-IDF and BM25 depend on term matching and create sparse vectors (Robertson, 2009;Yang et al., 2017;Chen et al., 2017) to ensure efficient retrieval. After the emergence of pre-trained language models (Devlin et al., 2019;Liu et al., 2019), dense retrieval which encodes both queries and documents into low-dimension vectors and then calculates their relevance scores (Lee et al., 2019;Karpukhin et al., 2020), has recently undergone substantial research. Relevant studies include improving training approach (Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021), distillation (Lin et al., 2021;Hofst\u00e4tter et al., 2021) and task-specific pre-training (Izacard et al., 2022;Gao & Callan, 2021;Lu et al., 2021;Gao & Callan, 2022;Xiao et al., 2022) of dense retrieval models which significantly outperform sparse approaches. \n\nZero-shot Dense Retrieval Many prior works consider training dense retrieval models on highresource passage retrieval datasets like Natural Questions (NQ) (Kwiatkowski et al., 2019) (133k training examples) or MS-MARCO (Bajaj et al., 2016) (533k training examples) and then evaluating on queries from new tasks. These systems (Wang et al., 2022;Yu et al., 2022) are utilized in a transfer learning configuration (Thakur et al., 2021). However, on the one hand, it is time-consuming and expensive to collect such a vast training corpus. On the other hand, even MS-MARCO has limitations on commercial use and cannot be used in a wide range of real-world applications.",
            "score": 0.4971124070446021,
            "section_title": "RELATED WORK",
            "char_start_offset": 5760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1679
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 127,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 127,
                    "end": 150,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 249,
                    "end": 266,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "1340183"
                },
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 385,
                    "end": 406,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 552,
                    "end": 570,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 570,
                    "end": 593,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 693,
                    "end": 717,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 736,
                    "end": 752,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 767,
                    "end": 785,
                    "matchedPaperCorpusId": "235720578"
                },
                {
                    "start": 785,
                    "end": 809,
                    "matchedPaperCorpusId": "233231706"
                },
                {
                    "start": 841,
                    "end": 863,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 863,
                    "end": 882,
                    "matchedPaperCorpusId": "237581068"
                },
                {
                    "start": 882,
                    "end": 898,
                    "matchedPaperCorpusId": "243865399"
                },
                {
                    "start": 898,
                    "end": 917,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 1340,
                    "end": 1359,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1359,
                    "end": 1375,
                    "matchedPaperCorpusId": "253157773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1922607421875
        },
        {
            "corpus_id": "258762633",
            "title": "TOME: A Two-stage Approach for Model-based Retrieval",
            "text": "Text Retrieval. Text retrieval endeavors to find textual information related to a query from a large candidate corpus. Early studies on sparse retrieval focused on term matching by utilizing sparse representations and inverted indices, such as BM25 (Robertson et al., 2009). In recent years, with the resurgence of neural networks and the emergence of pre-trained language models (PLMs) (Devlin et al., 2019;Raffel et al., 2020), dense retrieval achieves better performance beyond traditional sparse retrieval on multiple tasks (Khattab and Zaharia, 2020;Karpukhin et al., 2020;Xiong et al., 2021;Qu et al., 2021). The dense retrieval and the technique of approximate nearest neighbor search have been widely adopted in various applications (Oguz et al., 2020;Ren et al., 2021a,b;Asai et al., 2021;Zhou et al., 2022a). Recently, Zhao et al. (2022) have made a very comprehensive survey about the recent progress of dense retrieval based on PLMs, and we refer the readers to this survey paper for more details.\n\nModel-based Retrieval. Both sparse retrieval and dense retrieval rely on explicit indices. Recently, researchers have proposed model-based retrieval (a.k.a., generative retrieval) models (Metzler et al., 2021;Tay et al., 2022). These methods consider model parameters as retrieval indices and directly generate the identifiers of related documents. Such an idea is initially proposed for entity retrieval (Cao et al., 2021), which autoregressively generates unique entity identifiers. Following this approach, researchers have introduced sequenceto-sequence encoder-decoder architecture for document retrieval (Zhou et al., 2022c;Bevilacqua et al., 2022;Zhuang et al., 2022;Wang et al., 2022;Lee et al., 2022;Chen et al., 2022;Zhou et al., 2022b). As discussed in the previous section, there still remain issues with model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. Our work tries to",
            "score": 0.49663053627894127,
            "section_title": "Related Works",
            "char_start_offset": 4771,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 273,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 387,
                    "end": 408,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 408,
                    "end": 428,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 528,
                    "end": 555,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 555,
                    "end": 578,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 578,
                    "end": 597,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 597,
                    "end": 613,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 780,
                    "end": 798,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 798,
                    "end": 817,
                    "matchedPaperCorpusId": "253080762"
                },
                {
                    "start": 1220,
                    "end": 1237,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 1416,
                    "end": 1434,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 1621,
                    "end": 1641,
                    "matchedPaperCorpusId": "247187834"
                },
                {
                    "start": 1641,
                    "end": 1665,
                    "matchedPaperCorpusId": "248366293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28662109375
        },
        {
            "corpus_id": "273963180",
            "title": "KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking with Large Language Models",
            "text": "The rapid development of large language models (LLMs) such as GPT and Llama has transformed diverse natural language processing (NLP) tasks, including information retrieval (IR), text generation, and comprehension. These models have demonstrated remarkable capabilities in capturing deep semantic relationships within text, leading to significant advancements in IR systems. Cross-encoder models, such as RankLLaMA [34], have been particularly effective at evaluating the relevance of queries and passages by encoding entire input sequences jointly. Despite these advancements, processing long documents with LLMs remains challenging, primarily due to the quadratic complexity of self-attention mechanisms, leading to high computational and memory demands. Furthermore, there remains room for improvement in the effectiveness of directly using LLMs. \n\nIn practical applications, such as legal document retrieval, enterprise search, and academic research, efficiently handling long documents is essential, as relevant information can be spread across lengthy text. Techniques like block selection, which divides long documents into smaller segments and focuses on the most pertinent portions, have emerged as practical solutions to reduce computational demands. The KeyB approach [30] has demonstrated that selectively processing key blocks can enhance both retrieval efficiency and accuracy. However, as LLM architectures continue to evolve, revisiting block selection strategies is crucial to fully exploit the potential of these strategies in long document IR. \n\nMoreover, the internal mechanisms of LLMs during the ranking process are still not fully understood, particularly how these models weigh and align relevant content between queries and documents. Understanding the Authors' Contact Information: Minghan Li, mhli@suda.edu.cn, Soochow University, Suzhou, China; Eric Gaussier, Eric.Gaussier@univgrenoble-alpes.fr, Universit\u00e9 Grenoble Alpes, Grenoble, France; Juntao Li, ljt@suda.edu.cn, Soochow University, Suzhou, China; Guodong Zhou, gdzhou@suda.edu.cn, Soochow University, Suzhou, China. arXiv:2411.06254v1",
            "score": 0.4965652686660983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1562
                },
                {
                    "start": 1565,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 419,
                    "matchedPaperCorpusId": "263908865"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "244346065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2095947265625
        },
        {
            "corpus_id": "260656514",
            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
            "text": "The dense passage retrieval approaches have been proposed to map both questions and documents to continuous vectors (i.e., embeddings), which has achieved better performance than sparse retrieval approaches (Chen et al., 2017;Dai and Callan, 2019). Existing approaches can be roughly divided into two categories: pre-training and finetuning. The first type of methods often explores pre-training objectives/architectures designed for retrieval. Lee et al. (2019) pre-trains the retriever with an unsupervised Inverse Cloze Task (ICT). Condenser (Gao and Callan, 2021) proposes a dense retrieval pre-training architecture which learns to condense information into the dense vector through LM pre-training. coCondenser (Gao and Callan, 2022) adds an unsupervised corpuslevel contrastive loss on top of the Condenser (Gao and Callan, 2021) to warm up passage embeddings. \n\nThe second type of methods often fine-tunes pre-trained language models on labeled data. Karpukhin et al. (2020) proposes a dense embedding model using only pairs of questions and passages, without additional pre-training. Xiong et al. (2020); Qu et al. (2021) identify that the negative samples during training may not be representative, thus mechanism of selecting hard training negatives is designed. Khattab and Zaharia (2020); Humeau et al. (2019) incorporate late interaction architectures into the learning process that independently encode the query and the document firstly. Tang et al. (2021) designs a method to mimic the queries on each of the documents by clustering to enhance the document representation. PAIR (Ren et al., 2021a) leverages passage-centric similarity relation into training object to discriminate between positive and negative passages. RocketQAv2 (Ren et al., 2021b) introduces dynamic listwise distillation to jointly train retriever and re-ranker.",
            "score": 0.4961302619968755,
            "section_title": "Dense Passage Retrieval",
            "char_start_offset": 4806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "162168864"
                },
                {
                    "start": 445,
                    "end": 462,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 717,
                    "end": 739,
                    "matchedPaperCorpusId": "236987190"
                },
                {
                    "start": 959,
                    "end": 982,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 1114,
                    "end": 1130,
                    "matchedPaperCorpusId": "231815627"
                },
                {
                    "start": 1274,
                    "end": 1300,
                    "matchedPaperCorpusId": "216553223"
                },
                {
                    "start": 1454,
                    "end": 1472,
                    "matchedPaperCorpusId": "234338073"
                },
                {
                    "start": 1595,
                    "end": 1614,
                    "matchedPaperCorpusId": "236477844"
                },
                {
                    "start": 1749,
                    "end": 1768,
                    "matchedPaperCorpusId": "238857121"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46923828125
        },
        {
            "corpus_id": "268032853",
            "title": "CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks",
            "text": "In this study, we conduct a comparative analysis involving our proposed Corpus-Brain++, traditional IR models, and generative IR models. \n\n\u2022 Traditional IR models. (i) BM25 is a typical sparse retrieval model, which utilizes term-based features to model the relevance between queries and documents. (ii) DPR is a representative dense retrieval model, which models the semantic relevance between queries and documents via a dual-encoder architecture. For the traditional IR models, our empirical results encompass both incremental and non-incremental scenarios thanks to the high reproducibility provided by Pyserini3 and Tevatron. 4  Generative IR models. We first consider several generative IR models in stationary scenarios, including (i) GENRE [12], which directly fine-tunes BART via multi-task training on the labeled KILT training datasets and supervised BLINK datasets [53]; and (ii) SEAL [3], which applies a BART-based autoregressive search engine to generate distinctive n-grams as docids. Given that the focus of both models is confined to the non-incremental scenario, we only compare them with CorpusBrain++ in terms of non-incremental retrieval performance. In the non-incremental scenario, CorpusBrain++ degenerates to CorpusBrain, and the experimental contrast between CorpusBrain and these two models has been previously conducted and reported in [8]. Furthermore, we explore some advanced generative IR models fitting in dynamic scenarios, including (i) DSI++, which continually fine-tunes DSI over new documents and allocates a unique integer as the docid for each new document; and (ii) CLEVER, which introduces a technique named incremental product quantization to assign a docid to each new document.",
            "score": 0.4961209052742661,
            "section_title": "Baselines.",
            "char_start_offset": 41305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1723
                }
            ],
            "ref_mentions": [
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "263877300"
                },
                {
                    "start": 897,
                    "end": 900,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1365,
                    "end": 1368,
                    "matchedPaperCorpusId": "251594672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368408203125
        },
        {
            "corpus_id": "252816088",
            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
            "text": "In this section, we provide necessary background about the bi-encoder architecture for dense passage retrieval which is widely used in ODQA  and is the primary baseline model in our experiments.\n\nAs illustrated in the left part of Figure 1, the bi-encoder architecture consists of a question encoder and a passage encoder, both of which are usually Transformer encoders (Vaswani et al., 2017). A Transformer encoder is built up with a stack of Transformer blocks. Each block consists of a multi-head self-attention (MHA) sub-layer and a feed-forward network (FFN) sub-layer, with residual connections (He et al., 2016) and layernormalization (Ba et al., 2016) applied to both sublayers. Given an input vector h \u2208 R d , the FFN sub-layer produces an output vector as following\n\nwhere W 1 \u2208 R m\u00d7d , W 2 \u2208 R d\u00d7m , b 1 \u2208 R m , and b 2 \u2208 R d are learnable parameters. For a sequence of N tokens, each Transformer block produces N corresponding vectors, together with a vector for the special prefix token [CLS] which can be used as the representation of the sequence. We refer readers to (Vaswani et al., 2017) for other details about Transformer. Typically the question encoder and passage encoder are initialized from a pretrained language model such as BERT , but they are parameterized separately, i.e., their parameters would differ after training.\n\nThe bi-encoder model independently encodes questions and passages into d-dimension vectors, using the final output vectors for [CLS] from the corresponding encoders, denoted as q \u2208 R d and p \u2208 R d , respectively. The relevance between a question and a passage can then be measured in the vector space using dot product, i.e., sim(q, p) = q T p.\n\n(2)\n\nDuring training, the model is optimized based on a contrastive learning objective,\n\nwhere p + is the relevant (positive) passage for the given question, and P is the set of irrelevant (negative) passages. During inference, all passages are pre-converted into vectors using the passage encoder. Then,",
            "score": 0.49598297441450456,
            "section_title": "Background",
            "char_start_offset": 4175,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 370,
                    "end": 392,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 601,
                    "end": 618,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1083,
                    "end": 1105,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60302734375
        },
        {
            "paperId": "f11b9b89a6bf298881586b889c259d3c85287f57",
            "corpusId": 259949811,
            "title": "Dense Passage Retrieval: Architectures and Augmentation Methods",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2023,
            "referenceCount": 3,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3539618.3591796?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3539618.3591796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2091044662",
                    "name": "T. Rajapakse"
                }
            ],
            "abstract": "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries. Considering out-of-domain queries, i.e., queries originating from domains other than the one the model was trained on, the loss in accuracy may be significant. A main factor for this is the mismatch in the information available to the context encoder and the query encoder during training. Common retireval training datasets contain an overwhelming majority of passages with one query from a passage. I hypothesize that this could lead the dual-encoder model, particularly the passage encoder, to overfit to a single potential query from a given passage to the detriment of out-of-domain performance. Based on this, I seek to answer the following research question: (RQ1.1) Does training a DPR model on data containing multiple queries per passage improve the generalizability of the model? To answer RQ1.1, I build generated datasets that have multiple queries for most passages, and compare dense passage retriever models trained on these datasets against models trained on (mostly) single query per passage datasets. I show that training on passages with multiple queries leads to models that generalize better to out-of-distribution and out-of-domain test datasets [2]. Language can be considered another domain in the context of a dense retrieval. Training a dense retrieval model is especially challenging in languages other than English due to the scarcity of training data. I propose a novel training technique, clustered training, aimed at improving the retrieval quality of dense retrievers, especially in out-of-distribution and zero-shot settings. I address the following research questions: (RQ2.1)Does clustered training improve the effectiveness of multilingual DPR models on in-distribution data? (RQ2.2) Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.2 Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.3) Does clustered training help multilingual DPR models to generalize to new languages (zero-shot)? I show that clustered training improves the out-of-distribution and zero-shot performance of a DPR model without a clear loss in in-distribution performance using the Mr. TyDi [3] dataset. Finally, I propose a modified dual-encoder architecture that can perform both retrieval and reranking with the same model in a single forward pass. While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder. I aim to answer the following research questions in this work: (RQ3.1), Can the same model be trained to effectively generate two representations intended for two uses? RQ3.2 Can the retrieval quality of the model be improved by simultaneously performing retrieval and reranking? (RQ3.3 What is the tradeoff between retrieval quality vs. latency and compute resource efficiency for the proposed method vs. a two stage retriever? I expect that my proposed architecture would improve the dual encoder retrieval quality without sacrificing throughput or needing more computational resources.",
            "corpus_id": "259949811",
            "text": "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries. Considering out-of-domain queries, i.e., queries originating from domains other than the one the model was trained on, the loss in accuracy may be significant. A main factor for this is the mismatch in the information available to the context encoder and the query encoder during training. Common retireval training datasets contain an overwhelming majority of passages with one query from a passage. I hypothesize that this could lead the dual-encoder model, particularly the passage encoder, to overfit to a single potential query from a given passage to the detriment of out-of-domain performance. Based on this, I seek to answer the following research question: (RQ1.1) Does training a DPR model on data containing multiple queries per passage improve the generalizability of the model? To answer RQ1.1, I build generated datasets that have multiple queries for most passages, and compare dense passage retriever models trained on these datasets against models trained on (mostly) single query per passage datasets. I show that training on passages with multiple queries leads to models that generalize better to out-of-distribution and out-of-domain test datasets [2]. Language can be considered another domain in the context of a dense retrieval. Training a dense retrieval model is especially challenging in languages other than English due to the scarcity of training data. I propose a novel training technique, clustered training, aimed at improving the retrieval quality of dense retrievers, especially in out-of-distribution and zero-shot settings. I address the following research questions: (RQ2.1)Does clustered training improve the effectiveness of multilingual DPR models on in-distribution data? (RQ2.2) Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.2 Does clustered training improve the effectiveness of multilingual DPR models on out-of-distribution data from languages that it is trained on? (RQ2.3) Does clustered training help multilingual DPR models to generalize to new languages (zero-shot)? I show that clustered training improves the out-of-distribution and zero-shot performance of a DPR model without a clear loss in in-distribution performance using the Mr. TyDi [3] dataset. Finally, I propose a modified dual-encoder architecture that can perform both retrieval and reranking with the same model in a single forward pass. While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder. I aim to answer the following research questions in this work: (RQ3.1), Can the same model be trained to effectively generate two representations intended for two uses? RQ3.2 Can the retrieval quality of the model be improved by simultaneously performing retrieval and reranking? (RQ3.3 What is the tradeoff between retrieval quality vs. latency and compute resource efficiency for the proposed method vs. a two stage retriever? I expect that my proposed architecture would improve the dual encoder retrieval quality without sacrificing throughput or needing more computational resources.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.837890625
        }
    ],
    "quotes": {
        "cost": 0.20933999999999997,
        "quotes": [
            {
                "idx": 0,
                "key": "[231815627 | Qu et al. | 2020 | Citations: 617]",
                "snippets": "For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever...Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task."
                },
                "metadata": [
                    {
                        "section_title": "Passage Retrieval for Open-domain QA",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 363,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "3618568"
                        ],
                        "quote": "For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever"
                    },
                    {
                        "section_title": "Passage Retrieval for Open-domain QA",
                        "pdf_hash": "",
                        "start": 861,
                        "end": 1103,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a)."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[243865399 | Lu et al. | 2021 | Citations: 74]",
                "snippets": "In the first-stage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[173990818 | Lee et al. | 2019 | Citations: 1018]": "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.",
                    "[220046458 | Wu et al. | 2020 | Citations: 498]": "News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body. We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The MIND dataset will be available at https://msnews.github.io."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 164,
                        "end": 620,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 230
                            },
                            {
                                "start": 231,
                                "end": 354
                            },
                            {
                                "start": 355,
                                "end": 686
                            }
                        ],
                        "ref_mentions": [
                            "220046458",
                            "173990818"
                        ],
                        "quote": "In the first-stage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248227479 | Lin et al. | 2022 | Citations: 49]",
                "snippets": "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Reranking Module",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 698,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 32
                            },
                            {
                                "start": 33,
                                "end": 167
                            },
                            {
                                "start": 168,
                                "end": 371
                            },
                            {
                                "start": 372,
                                "end": 537
                            },
                            {
                                "start": 538,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 698
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[248366563 | Ma et al. | 2022 | Citations: 53]",
                "snippets": "Dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[252816088 | Cheng et al. | 2022 | Citations: 6]",
                "snippets": "Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders...During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 139,
                        "end": 459,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders"
                    },
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 205,
                        "end": 481,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[253080873 | He et al. | 2022 | Citations: 11]",
                "snippets": "Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers...Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[201307832 | Wang et al. | 2019 | Citations: 244]": "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models."
                },
                "metadata": [
                    {
                        "section_title": "Experiment on Keyword Generation",
                        "pdf_hash": "",
                        "start": 287,
                        "end": 954,
                        "sentence_offsets": [
                            {
                                "start": 287,
                                "end": 491
                            },
                            {
                                "start": 492,
                                "end": 654
                            },
                            {
                                "start": 657,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 955
                            }
                        ],
                        "ref_mentions": [
                            "201307832"
                        ],
                        "quote": "Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers"
                    },
                    {
                        "section_title": "Experiment on Keyword Generation",
                        "pdf_hash": "",
                        "start": 1093,
                        "end": 1273,
                        "sentence_offsets": [
                            {
                                "start": 1093,
                                "end": 1272
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[253157959 | Long et al. | 2022 | Citations: 4]",
                "snippets": "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 334,
                        "end": 685,
                        "sentence_offsets": [
                            {
                                "start": 334,
                                "end": 461
                            },
                            {
                                "start": 462,
                                "end": 685
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[254564747 | Sun et al. | 2022 | Citations: 2]",
                "snippets": "Dual Encoder (DE) (Karpukhin et al., 2020) is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) (Khattab et al., 2020) can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) (Qu et al., 2020) has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231815627 | Qu et al. | 2020 | Citations: 617]": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                },
                "metadata": [
                    {
                        "section_title": "2",
                        "pdf_hash": "",
                        "start": 293,
                        "end": 1563,
                        "sentence_offsets": [
                            {
                                "start": 293,
                                "end": 494
                            },
                            {
                                "start": 495,
                                "end": 557
                            },
                            {
                                "start": 558,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 655
                            },
                            {
                                "start": 658,
                                "end": 794
                            },
                            {
                                "start": 795,
                                "end": 844
                            },
                            {
                                "start": 847,
                                "end": 909
                            },
                            {
                                "start": 912,
                                "end": 996
                            },
                            {
                                "start": 997,
                                "end": 1131
                            },
                            {
                                "start": 1134,
                                "end": 1282
                            },
                            {
                                "start": 1283,
                                "end": 1459
                            },
                            {
                                "start": 1460,
                                "end": 1518
                            },
                            {
                                "start": 1521,
                                "end": 1563
                            }
                        ],
                        "ref_mentions": [
                            "215737187",
                            "216553223",
                            "231815627"
                        ],
                        "quote": "Dual Encoder (DE) (Karpukhin et al., 2020) is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) (Khattab et al., 2020) can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) (Qu et al., 2020) has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[254685782 | Zhou et al. | 2022 | Citations: 16]",
                "snippets": "In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations...It consists of a query encoder E_q and a passage encoder E_p, mapping the query q and passage p into k-dimensional dense vectors h_q and h_p, respectively. Then, the semantic relevance score of q and p will be computed using dot product",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "PRELIMINARY",
                        "pdf_hash": "",
                        "start": 784,
                        "end": 986,
                        "sentence_offsets": [
                            {
                                "start": 784,
                                "end": 987
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations"
                    },
                    {
                        "section_title": "PRELIMINARY",
                        "pdf_hash": "",
                        "start": 282,
                        "end": 519,
                        "sentence_offsets": [
                            {
                                "start": 146,
                                "end": 357
                            },
                            {
                                "start": 358,
                                "end": 515
                            },
                            {
                                "start": 516,
                                "end": 599
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It consists of a query encoder E_q and a passage encoder E_p, mapping the query q and passage p into k-dimensional dense vectors h_q and h_p, respectively. Then, the semantic relevance score of q and p will be computed using dot product"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[254853896 | He et al. | 2022 | Citations: 3]",
                "snippets": "In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 980,
                        "end": 1674,
                        "sentence_offsets": [
                            {
                                "start": 980,
                                "end": 1077
                            },
                            {
                                "start": 1078,
                                "end": 1242
                            },
                            {
                                "start": 1243,
                                "end": 1410
                            },
                            {
                                "start": 1411,
                                "end": 1613
                            },
                            {
                                "start": 1614,
                                "end": 1674
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258331649 | Salemi et al. | 2023 | Citations: 14]",
                "snippets": "A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. (Qu et al., 2021) recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap...Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B (Hofst\u00e4tter et al., 2021), ColBERT (Khattab et al., 2020)(Santhanam et al., 2021), RocketQA (Qu et al., 2020)(Ren et al., 2021), and CLDRD (Zeng et al., 2022), use symmetric architectures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231815627 | Qu et al. | 2020 | Citations: 617]": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.",
                    "[233231706 | Hofstatter et al. | 2021 | Citations: 402]": "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",
                    "[234337940 | Qu et al. | 2021 | Citations: 43]": "In this work, we address multi-modal information needs that contain text questions and images by focusing on passage retrieval for outside-knowledge visual question answering. This task requires access to outside knowledge, which in our case we define to be a large unstructured passage collection. We first conduct sparse retrieval with BM25 and study expanding the question with object names and image captions. We verify that visual clues play an important role and captions tend to be more informative than object names in sparse retrieval. We then construct a dual-encoder dense retriever, with the query encoder being LXMERT, a multi-modal pre-trained transformer. We further show that dense retrieval significantly outperforms sparse retrieval that uses object expansion. Moreover, dense retrieval matches the performance of sparse retrieval that leverages human-generated captions.",
                    "[238857121 | Ren et al. | 2021 | Citations: 255]": "In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other\u2019s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.",
                    "[244799249 | Santhanam et al. | 2021 | Citations: 415]": "Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce Maize, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate Maize across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6\u201310x.",
                    "[248426770 | Zeng et al. | 2022 | Citations: 51]": "Recent work has shown that more effective dense retrieval models can be obtained by distilling ranking knowledge from an existing base re-ranking model. In this paper, we propose a generic curriculum learning based optimization framework called CL-DRD that controls the difficulty level of training data produced by the re-ranking (teacher) model. CL-DRD iteratively optimizes the dense retrieval (student) model by increasing the difficulty of the knowledge distillation data made available to it. In more detail, we initially provide the student model coarse-grained preference pairs between documents in the teacher's ranking, and progressively move towards finer-grained pairwise document ordering requirements. In our experiments, we apply a simple implementation of the CL-DRD framework to enhance two state-of-the-art dense retrieval models. Experiments on three public passage retrieval datasets demonstrate the effectiveness of our proposed framework."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 694,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 200
                            },
                            {
                                "start": 201,
                                "end": 493
                            },
                            {
                                "start": 494,
                                "end": 695
                            }
                        ],
                        "ref_mentions": [
                            "234337940"
                        ],
                        "quote": "A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. (Qu et al., 2021) recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap"
                    },
                    {
                        "section_title": "DEDR: DUAL ENCODING DENSE RETRIEVER FRAMEWORK",
                        "pdf_hash": "",
                        "start": 751,
                        "end": 1255,
                        "sentence_offsets": [
                            {
                                "start": 751,
                                "end": 924
                            },
                            {
                                "start": 925,
                                "end": 1105
                            },
                            {
                                "start": 1106,
                                "end": 1254
                            }
                        ],
                        "ref_mentions": [
                            "233231706",
                            "216553223",
                            "244799249",
                            "231815627",
                            "238857121",
                            "248426770"
                        ],
                        "quote": "Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B (Hofst\u00e4tter et al., 2021), ColBERT (Khattab et al., 2020)(Santhanam et al., 2021), RocketQA (Qu et al., 2020)(Ren et al., 2021), and CLDRD (Zeng et al., 2022), use symmetric architectures."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[258557604 | Zhuang et al. | 2023 | Citations: 8]",
                "snippets": "These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33]45].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 182,
                        "end": 564,
                        "sentence_offsets": [
                            {
                                "start": 182,
                                "end": 307
                            },
                            {
                                "start": 308,
                                "end": 394
                            },
                            {
                                "start": 395,
                                "end": 564
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33]45]."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[258714822 | Ziems et al. | 2023 | Citations: 42]",
                "snippets": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[258833383 | Li et al. | 2023 | Citations: 3]",
                "snippets": "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair...Currently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 534,
                        "end": 932,
                        "sentence_offsets": [
                            {
                                "start": 534,
                                "end": 935
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1769,
                        "end": 2199,
                        "sentence_offsets": [
                            {
                                "start": 1769,
                                "end": 1883
                            },
                            {
                                "start": 1884,
                                "end": 2198
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Currently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[258865354 | Soares et al. | 2023 | Citations: 1]",
                "snippets": "To overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14]26].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 176,
                        "end": 635,
                        "sentence_offsets": [
                            {
                                "start": 176,
                                "end": 383
                            },
                            {
                                "start": 384,
                                "end": 472
                            },
                            {
                                "start": 473,
                                "end": 635
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14]26]."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[259203703 | Wang et al. | 2023 | Citations: 2]",
                "snippets": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 200,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[259949811 | Rajapakse | 2023 | Citations: 7]",
                "snippets": "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries...While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    },
                    {
                        "quote": "While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[260656514 | Liu et al. | 2022 | Citations: 6]",
                "snippets": "Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1311,
                        "end": 2083,
                        "sentence_offsets": [
                            {
                                "start": 1311,
                                "end": 1428
                            },
                            {
                                "start": 1431,
                                "end": 1602
                            },
                            {
                                "start": 1603,
                                "end": 1763
                            },
                            {
                                "start": 1764,
                                "end": 1861
                            },
                            {
                                "start": 1862,
                                "end": 1953
                            },
                            {
                                "start": 1954,
                                "end": 2083
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[264935522 | Leonhardt et al. | 2023 | Citations: 7]",
                "snippets": "Dual-encoder models employ two language models to independently encode queries and documents as fixed-size vector representations. Usually, a similarity metric between query and document vector determines their relevance. As a result, dual-encoders are mostly used for dense retrieval, but also, less commonly, for re-ranking...Cross-attention models...perform query-document attention in order to compute the corresponding relevance score. These models are typically used as re-rankers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 155,
                        "end": 480,
                        "sentence_offsets": [
                            {
                                "start": 155,
                                "end": 285
                            },
                            {
                                "start": 286,
                                "end": 376
                            },
                            {
                                "start": 377,
                                "end": 481
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Dual-encoder models employ two language models to independently encode queries and documents as fixed-size vector representations. Usually, a similarity metric between query and document vector determines their relevance. As a result, dual-encoders are mostly used for dense retrieval, but also, less commonly, for re-ranking"
                    },
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 559,
                        "end": 582,
                        "sentence_offsets": [
                            {
                                "start": 484,
                                "end": 670
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Cross-attention models"
                    },
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 20,
                        "end": 155,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 107
                            },
                            {
                                "start": 108,
                                "end": 154
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "perform query-document attention in order to compute the corresponding relevance score. These models are typically used as re-rankers."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[265457188 | Jiang et al. | 2023 | Citations: 2]",
                "snippets": "The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2017) is employed to fetch top-k relevant passages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[926364 | Johnson et al. | 2017 | Citations: 3738]": "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."
                },
                "metadata": [
                    {
                        "section_title": "Dense Retrieval Model",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 639,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 286
                            },
                            {
                                "start": 289,
                                "end": 370
                            },
                            {
                                "start": 371,
                                "end": 473
                            },
                            {
                                "start": 474,
                                "end": 639
                            }
                        ],
                        "ref_mentions": [
                            "215737187",
                            "926364"
                        ],
                        "quote": "The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2017) is employed to fetch top-k relevant passages."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[265457282 | Jiang et al. | 2023 | Citations: 2]",
                "snippets": "In contrast to traditional IR methods, such as BM25 (Paszke et al., 2019), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings...The adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2017) can be performed to fetch the top-k passages.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[202786778 | Paszke et al. | 2019 | Citations: 42613]": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[926364 | Johnson et al. | 2017 | Citations: 3738]": "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."
                },
                "metadata": [
                    {
                        "section_title": "Dense Passage Retrieval",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 582,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 315
                            },
                            {
                                "start": 316,
                                "end": 512
                            },
                            {
                                "start": 513,
                                "end": 583
                            }
                        ],
                        "ref_mentions": [
                            "202786778",
                            "215737187"
                        ],
                        "quote": "In contrast to traditional IR methods, such as BM25 (Paszke et al., 2019), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings"
                    },
                    {
                        "section_title": "Dense Passage Retrieval",
                        "pdf_hash": "",
                        "start": 639,
                        "end": 926,
                        "sentence_offsets": [
                            {
                                "start": 639,
                                "end": 740
                            },
                            {
                                "start": 741,
                                "end": 796
                            },
                            {
                                "start": 797,
                                "end": 925
                            }
                        ],
                        "ref_mentions": [
                            "926364"
                        ],
                        "quote": "The adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2017) can be performed to fetch the top-k passages."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[266573365 | Li et al. | 2023 | Citations: 1]",
                "snippets": "The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 181,
                        "end": 1075,
                        "sentence_offsets": [
                            {
                                "start": 110,
                                "end": 419
                            },
                            {
                                "start": 420,
                                "end": 605
                            },
                            {
                                "start": 606,
                                "end": 739
                            },
                            {
                                "start": 740,
                                "end": 948
                            },
                            {
                                "start": 949,
                                "end": 1075
                            }
                        ],
                        "ref_mentions": [
                            "216553223"
                        ],
                        "quote": "The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[267751308 | Yang et al. | 2024 | Citations: 4]",
                "snippets": "The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dual-encoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model fine-grained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[173990818 | Lee et al. | 2019 | Citations: 1018]": "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 215,
                        "end": 884,
                        "sentence_offsets": [
                            {
                                "start": 187,
                                "end": 277
                            },
                            {
                                "start": 278,
                                "end": 295
                            },
                            {
                                "start": 296,
                                "end": 416
                            },
                            {
                                "start": 417,
                                "end": 556
                            },
                            {
                                "start": 557,
                                "end": 703
                            },
                            {
                                "start": 704,
                                "end": 963
                            }
                        ],
                        "ref_mentions": [
                            "173990818"
                        ],
                        "quote": "The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dual-encoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model fine-grained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[267938301 | Yoon et al. | 2024 | Citations: 16]",
                "snippets": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212725651 | Nogueira et al. | 2020 | Citations: 584]": "This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as \u201ctarget tokens\u201d, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model\u2019s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.",
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                },
                "metadata": [
                    {
                        "section_title": "Generative Models for Reranking",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 674,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 674
                            }
                        ],
                        "ref_mentions": [
                            "215737187",
                            "212725651"
                        ],
                        "quote": "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[268031876 | Wang et al. | 2024 | Citations: 1]",
                "snippets": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207178704 | Robertson et al. | 2009 | Citations: 3760]": "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.",
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                    "[59604492 | Yang et al. | 2019 | Citations: 495]": "We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 788,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 78
                            },
                            {
                                "start": 79,
                                "end": 212
                            },
                            {
                                "start": 213,
                                "end": 371
                            },
                            {
                                "start": 372,
                                "end": 593
                            },
                            {
                                "start": 596,
                                "end": 788
                            }
                        ],
                        "ref_mentions": [
                            "207178704",
                            "3618568",
                            "59604492"
                        ],
                        "quote": "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[268889861 | Abdi et al. | 2024 | Citations: 2]",
                "snippets": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering (Chen et al., 2017)(Kwiatkowski et al., 2019)(Voorhees, 2001), fact-checking [28], dialogue systems (Dinan et al., 2018), and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,(Shen et al., 2014)(Yih et al., 2011). Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15]36].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3618568 | Chen et al. | 2017 | Citations: 2019]": "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.",
                    "[53218829 | Dinan et al. | 2018 | Citations: 950]": "In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.",
                    "[86611921 | Kwiatkowski et al. | 2019 | Citations: 3361]": "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORKS 2.1 Retrieval-enhanced Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 653,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 239
                            },
                            {
                                "start": 240,
                                "end": 347
                            },
                            {
                                "start": 348,
                                "end": 462
                            },
                            {
                                "start": 463,
                                "end": 653
                            }
                        ],
                        "ref_mentions": [
                            "3618568",
                            "86611921",
                            "13403541",
                            "53218829",
                            "207210855",
                            "1965270"
                        ],
                        "quote": "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering (Chen et al., 2017)(Kwiatkowski et al., 2019)(Voorhees, 2001), fact-checking [28], dialogue systems (Dinan et al., 2018), and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,(Shen et al., 2014)(Yih et al., 2011). Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15]36]."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[272330251 | Monath et al. | 2024 | Citations: 0]",
                "snippets": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022)...P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                },
                "metadata": [
                    {
                        "section_title": "Latent Variables in Retrieval Augmented Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 250,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 251
                            }
                        ],
                        "ref_mentions": [
                            "211204736"
                        ],
                        "quote": "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022)"
                    },
                    {
                        "section_title": "Latent Variables in Retrieval Augmented Models",
                        "pdf_hash": "",
                        "start": 1270,
                        "end": 1422,
                        "sentence_offsets": [
                            {
                                "start": 1266,
                                "end": 1317
                            },
                            {
                                "start": 1318,
                                "end": 1421
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y)."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[272770506 | Sidiropoulos et al. | 2024 | Citations: 0]",
                "snippets": "On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers...At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multimodal Dense Retriever",
                        "pdf_hash": "",
                        "start": 1254,
                        "end": 1437,
                        "sentence_offsets": [
                            {
                                "start": 1254,
                                "end": 1438
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers"
                    },
                    {
                        "section_title": "Multimodal Dense Retriever",
                        "pdf_hash": "",
                        "start": 1089,
                        "end": 1254,
                        "sentence_offsets": [
                            {
                                "start": 1089,
                                "end": 1253
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273026177 | Huang et al. | 2024 | Citations: 1]",
                "snippets": "In order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Dense Retrieval",
                        "pdf_hash": "",
                        "start": 137,
                        "end": 636,
                        "sentence_offsets": [
                            {
                                "start": 137,
                                "end": 387
                            },
                            {
                                "start": 388,
                                "end": 492
                            },
                            {
                                "start": 493,
                                "end": 636
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[274822530 | Trung et al. | 2024 | Citations: 0]",
                "snippets": "The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023)(Fu et al., 2023)...Generative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2023), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246863488 | Tay et al. | 2022 | Citations: 286]": "In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.",
                    "[257767171 | Monath et al. | 2023 | Citations: 6]": "Dual encoder models are ubiquitous in modern classification and retrieval. Crucial for training such dual encoders is an accurate estimation of gradients from the partition function of the softmax over the large output space; this requires finding negative targets that contribute most significantly (\"hard negatives\"). Since dual encoder model parameters change during training, the use of traditional static nearest neighbor indexes can be sub-optimal. These static indexes (1) periodically require expensive re-building of the index, which in turn requires (2) expensive re-encoding of all targets using updated model parameters. This paper addresses both of these challenges. First, we introduce an algorithm that uses a tree structure to approximate the softmax with provable bounds and that dynamically maintains the tree. Second, we approximate the effect of a gradient update on target encodings with an efficient Nystrom low-rank approximation. In our empirical study on datasets with over twenty million targets, our approach cuts error by half in relation to oracle brute-force negative mining. Furthermore, our method surpasses prior state-of-the-art while using 150x less accelerator memory.",
                    "[257970928 | Fu et al. | 2023 | Citations: 4]": "Open-Domain Question Answering (Open-Domain QA) aims to answer any factoid questions from users. Recent progress in Open-Domain QA adopts the \u201cretriever-reader\u201d structure, which has proven effective. Retriever methods are mainly categorized as sparse retrievers and dense retrievers. In recent work, the dense retriever showed a stronger semantic interpretation than the sparse retriever. When training a dual-encoder dense retriever for document retrieval and reranking, there are two challenges: negative selection and a lack of training data. In this study, we make three major contributions to this topic: negative selection by query generation, data augmentation from negatives, and a passage evaluation method. We prove that the model performs better by focusing on false negatives and data augmentation in the Open-Domain QA passage rerank task. Our model outperforms other single dual-encoder rerankers over BERT-base and BM25 by 0.7 in MRR@10, achieving the highest Recall@50 and the max Recall@1000, which is restricted by the BM25 retrieval results.",
                    "[258048596 | Sun et al. | 2023 | Citations: 75]": "Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability."
                },
                "metadata": [
                    {
                        "section_title": "Dual-encoder approaches",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 667,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 232
                            },
                            {
                                "start": 233,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 668
                            }
                        ],
                        "ref_mentions": [
                            "257767171",
                            "257970928"
                        ],
                        "quote": "The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023)(Fu et al., 2023)"
                    },
                    {
                        "section_title": "Dual-encoder approaches",
                        "pdf_hash": "",
                        "start": 1136,
                        "end": 2023,
                        "sentence_offsets": [
                            {
                                "start": 1136,
                                "end": 1399
                            },
                            {
                                "start": 1400,
                                "end": 1651
                            },
                            {
                                "start": 1652,
                                "end": 2022
                            }
                        ],
                        "ref_mentions": [
                            "258048596",
                            "246863488"
                        ],
                        "quote": "Generative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2023), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[275931943 | Li et al. | 2025 | Citations: 0]",
                "snippets": "Karpukhin et al. (Karpukhin et al., 2020) show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder...In the late-interaction based approach, ColBERT (Khattab et al., 2020) is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215737187 | Karpukhin et al. | 2020 | Citations: 3794]": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "[216553223 | Khattab et al. | 2020 | Citations: 1377]": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                },
                "metadata": [
                    {
                        "section_title": "Representation-based Models for IR",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 481,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 414
                            },
                            {
                                "start": 415,
                                "end": 482
                            }
                        ],
                        "ref_mentions": [
                            "215737187"
                        ],
                        "quote": "Karpukhin et al. (Karpukhin et al., 2020) show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder"
                    },
                    {
                        "section_title": "Representation-based Models for IR",
                        "pdf_hash": "",
                        "start": 645,
                        "end": 1294,
                        "sentence_offsets": [
                            {
                                "start": 645,
                                "end": 792
                            },
                            {
                                "start": 793,
                                "end": 941
                            },
                            {
                                "start": 942,
                                "end": 1111
                            },
                            {
                                "start": 1112,
                                "end": 1293
                            }
                        ],
                        "ref_mentions": [
                            "216553223"
                        ],
                        "quote": "In the late-interaction based approach, ColBERT (Khattab et al., 2020) is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276928453 | Lu et al. | 2025 | Citations: 0]",
                "snippets": "Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query...Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format...Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrievers vs. Rankers",
                        "pdf_hash": "",
                        "start": 488,
                        "end": 922,
                        "sentence_offsets": [
                            {
                                "start": 488,
                                "end": 678
                            },
                            {
                                "start": 679,
                                "end": 802
                            },
                            {
                                "start": 803,
                                "end": 923
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query"
                    },
                    {
                        "section_title": "Retrievers vs. Rankers",
                        "pdf_hash": "",
                        "start": 1354,
                        "end": 1850,
                        "sentence_offsets": [
                            {
                                "start": 1354,
                                "end": 1687
                            },
                            {
                                "start": 1688,
                                "end": 1850
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format"
                    },
                    {
                        "section_title": "Retrievers vs. Rankers",
                        "pdf_hash": "",
                        "start": 1853,
                        "end": 2204,
                        "sentence_offsets": [
                            {
                                "start": 1853,
                                "end": 2078
                            },
                            {
                                "start": 2079,
                                "end": 2203
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.030513,
        "cot": "Let me analyze the user query, which asks about the key architectural differences between autoregressive language models and traditional dual-encoder models in dense passage retrieval, and how these differences affect retrieval accuracy and computational efficiency.\n\nI'll plan the dimensions that make sense to answer this query:\n\n1. \"Introduction/Background\" - This is necessary to provide context about what dense passage retrieval is and the general landscape of retrieval systems. This will help users understand the rest of the answer.\n\n2. \"Dual-Encoder Architecture\" - This dimension should cover the architectural details of dual-encoder models, which are extensively mentioned in the quotes. This should be synthesis format to provide a cohesive explanation of how these models work.\n\n3. \"Autoregressive Models for Retrieval\" - This dimension should cover the architectural details of autoregressive models for retrieval, how they're structured, and their fundamental approach. This should be synthesis format to provide a cohesive explanation.\n\n4. \"Architectural Differences\" - This dimension should directly compare the key architectural differences between the two approaches. This should be synthesis format to enable proper comparison.\n\n5. \"Retrieval Accuracy Comparison\" - This dimension should address how the architectural differences impact retrieval accuracy. This should be synthesis format to properly analyze the trade-offs.\n\n6. \"Computational Efficiency Comparison\" - This dimension should address how the architectural differences impact computational efficiency. This should be synthesis format to properly analyze the performance aspects.\n\nNow, I'll assign quotes to each dimension:",
        "plan": {
            "Introduction/Background (synthesis)": [
                0,
                24
            ],
            "Dual-Encoder Architecture (synthesis)": [
                0,
                1,
                4,
                5,
                6,
                8,
                9,
                10,
                11,
                13,
                15,
                16,
                17,
                18,
                19,
                20,
                21,
                22,
                27,
                28,
                29,
                30
            ],
            "Autoregressive Models for Retrieval (synthesis)": [
                3,
                12,
                14,
                23,
                24,
                25,
                26,
                29,
                30
            ],
            "Architectural Differences (synthesis)": [
                2,
                3,
                7,
                9,
                12,
                17,
                21,
                23,
                24,
                31
            ],
            "Retrieval Accuracy Comparison (synthesis)": [
                2,
                5,
                9,
                12,
                16,
                17,
                21,
                31
            ],
            "Computational Efficiency Comparison (synthesis)": [
                5,
                6,
                12,
                15,
                17,
                20,
                21,
                27,
                28,
                31
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Information retrieval systems have evolved from traditional sparse retrieval methods to more sophisticated approaches using dual-encoder architectures and, more recently, autoregressive language models. This evolution addresses limitations in representation capabilities and interaction between queries and documents. (4 sources)",
                "text": "\nInformation retrieval systems have undergone significant evolution in recent years, particularly in the context of open-domain question answering where identifying relevant passages is crucial for answer extraction. Traditional approaches relied on term-based passage retrievers such as TF-IDF and BM25, which match questions and passages based on lexical overlap <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"3618568\" paperTitle=\"(Chen et al., 2017)\" isShortName></Paper> <Paper corpusId=\"207178704\" paperTitle=\"(Robertson et al., 2009)\" isShortName></Paper>. These sparse retrieval methods, while foundational, suffer from limited representation capabilities as they primarily focus on exact term matches rather than semantic understanding <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>.\n\nTo address these limitations, researchers have developed dense passage retrieval approaches that represent both questions and documents as dense vectors (embeddings) using dual-encoder neural architectures <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>. Notable implementations such as ORQA and DPR employ dense context vectors for passage indexing, significantly enhancing retrieval performance compared to traditional methods <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. However, dual-encoder models have their own constraints, particularly because the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions between queries and documents <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nMore recently, research interest has surged in utilizing autoregressive language models for retrieval tasks, offering a new paradigm that can potentially overcome the interaction bottleneck inherent in dual-encoder models <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These models generate identifiers to simplify the retrieval process while enabling deeper interactions between queries and documents. This shift represents a fundamental architectural change in how retrieval systems are designed and implemented <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Qu et al., 2020)",
                        "snippets": [
                            "For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever",
                            "Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a)."
                        ],
                        "paper": {
                            "corpus_id": 231815627,
                            "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51281403",
                                    "name": "Yingqi Qu"
                                },
                                {
                                    "authorId": "2111070044",
                                    "name": "Yuchen Ding"
                                },
                                {
                                    "authorId": "46700619",
                                    "name": "Jing Liu"
                                },
                                {
                                    "authorId": "2146384872",
                                    "name": "Kai Liu"
                                },
                                {
                                    "authorId": "1708171825",
                                    "name": "Ruiyang Ren"
                                },
                                {
                                    "authorId": "2145734278",
                                    "name": "Xin Zhao"
                                },
                                {
                                    "authorId": "9532787",
                                    "name": "Daxiang Dong"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2020,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 617
                        },
                        "score": 0.64111328125
                    },
                    {
                        "id": "(Chen et al., 2017)",
                        "snippets": [
                            "This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task."
                        ],
                        "paper": {
                            "corpus_id": 3618568,
                            "title": "Reading Wikipedia to Answer Open-Domain Questions",
                            "authors": [
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "2064150446",
                                    "name": "Adam Fisch"
                                },
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                },
                                {
                                    "authorId": "1713934",
                                    "name": "Antoine Bordes"
                                }
                            ],
                            "year": 2017,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 2019
                        },
                        "score": 0
                    },
                    {
                        "id": "(Robertson et al., 2009)",
                        "snippets": [
                            "The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."
                        ],
                        "paper": {
                            "corpus_id": 207178704,
                            "title": "The Probabilistic Relevance Framework: BM25 and Beyond",
                            "authors": [
                                {
                                    "authorId": "144430625",
                                    "name": "S. Robertson"
                                },
                                {
                                    "authorId": "2833561",
                                    "name": "H. Zaragoza"
                                }
                            ],
                            "year": 2009,
                            "venue": "Foundations and Trends in Information Retrieval",
                            "n_citations": 3760
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                        ],
                        "paper": {
                            "corpus_id": 268031876,
                            "title": "Generative Retrieval with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2185022832",
                                    "name": "Ye Wang"
                                },
                                {
                                    "authorId": "2290204960",
                                    "name": "Xinrun Xu"
                                },
                                {
                                    "authorId": "2143721734",
                                    "name": "Rui Xie"
                                },
                                {
                                    "authorId": "2288018918",
                                    "name": "Wenxin Hu"
                                },
                                {
                                    "authorId": "2052980435",
                                    "name": "Wei Ye"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.70654296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Dual-Encoder Architecture",
                "tldr": "Dual-encoder architectures represent the dominant approach in dense passage retrieval, independently encoding queries and documents into vector representations that enable efficient similarity computation. This architecture allows for pre-computation of document embeddings, facilitating fast retrieval through approximate nearest neighbor search techniques at query time. (25 sources)",
                "text": "\nDual-encoder architectures form the backbone of modern dense retrieval systems, offering a balance between efficiency and effectiveness that has made them the standard approach for information retrieval tasks <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>. Unlike traditional term-based methods, these models encode both queries and documents as dense vectors in a shared semantic space <Paper corpusId=\"243865399\" paperTitle=\"(Lu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"173990818\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>.\n\nThe fundamental characteristic of dual-encoder models is their use of two separate encoders\u2014one for queries and one for documents\u2014that map inputs into a common representation space <Paper corpusId=\"254685782\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper>. The relevance between a query and document is typically calculated using lightweight similarity functions such as dot product or cosine similarity <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"258833383\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This architecture provides a crucial efficiency advantage: document representations can be pre-computed and indexed offline, with only the query needing to be encoded at inference time <Paper corpusId=\"252816088\" paperTitle=\"(Cheng et al., 2022)\" isShortName></Paper>.\n\nDuring inference, when a new query arrives, it is encoded by the query encoder, and relevant documents are retrieved through approximate nearest neighbor (ANN) search techniques <Paper corpusId=\"265457188\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"926364\" paperTitle=\"(Johnson et al., 2017)\" isShortName></Paper>. This approach dramatically reduces retrieval time compared to methods that require computing interactions between queries and all possible documents <Paper corpusId=\"265457282\" paperTitle=\"(Jiang et al._1, 2023)\" isShortName></Paper>.\n\nMost prominent implementations of dense retrievers, such as DPR <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>, RocketQA <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>, TAS-B <Paper corpusId=\"233231706\" paperTitle=\"(Hofstatter et al., 2021)\" isShortName></Paper>, and ColBERT <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>, utilize various forms of dual-encoder architectures <Paper corpusId=\"258331649\" paperTitle=\"(Salemi et al., 2023)\" isShortName></Paper>. These models generally start with pre-trained language models (PLMs) that are then fine-tuned for the retrieval task <Paper corpusId=\"258557604\" paperTitle=\"(Zhuang et al., 2023)\" isShortName></Paper>.\n\nHowever, dual-encoder architectures face a fundamental limitation: they encode queries and documents independently, without modeling the interactions between them <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. This independent encoding causes information loss and can lead to suboptimal retrieval performance compared to models that incorporate cross-attention mechanisms <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. In contrast, cross-encoder models that jointly process query-document pairs achieve higher accuracy but are computationally impractical for direct retrieval from large collections <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"201307832\" paperTitle=\"(Wang et al., 2019)\" isShortName></Paper>.\n\nTo address this trade-off, some researchers have developed hybrid approaches that maintain the efficiency of dual-encoders while incorporating limited forms of interaction. For example, ColBERT introduces a late interaction mechanism that retains fine-grained token-level interactions while still allowing for efficient pre-computation <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper> <Paper corpusId=\"275931943\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Others have explored multi-representation approaches <Paper corpusId=\"267751308\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> or asymmetric architectures with stronger query encoders <Paper corpusId=\"259949811\" paperTitle=\"(Rajapakse, 2023)\" isShortName></Paper>.\n\nDespite these limitations, dual-encoder architectures remain the preferred choice for large-scale retrieval systems due to their computational efficiency and scalability <Paper corpusId=\"259203703\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272770506\" paperTitle=\"(Sidiropoulos et al., 2024)\" isShortName></Paper>. Their ability to support fast retrieval from millions or billions of documents makes them indispensable for practical applications, even as research continues to improve their effectiveness <Paper corpusId=\"273026177\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Qu et al., 2020)",
                        "snippets": [
                            "For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever",
                            "Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a)."
                        ],
                        "paper": {
                            "corpus_id": 231815627,
                            "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51281403",
                                    "name": "Yingqi Qu"
                                },
                                {
                                    "authorId": "2111070044",
                                    "name": "Yuchen Ding"
                                },
                                {
                                    "authorId": "46700619",
                                    "name": "Jing Liu"
                                },
                                {
                                    "authorId": "2146384872",
                                    "name": "Kai Liu"
                                },
                                {
                                    "authorId": "1708171825",
                                    "name": "Ruiyang Ren"
                                },
                                {
                                    "authorId": "2145734278",
                                    "name": "Xin Zhao"
                                },
                                {
                                    "authorId": "9532787",
                                    "name": "Daxiang Dong"
                                },
                                {
                                    "authorId": "40354707",
                                    "name": "Hua Wu"
                                },
                                {
                                    "authorId": "144270731",
                                    "name": "Haifeng Wang"
                                }
                            ],
                            "year": 2020,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 617
                        },
                        "score": 0.64111328125
                    },
                    {
                        "id": "(Lu et al., 2021)",
                        "snippets": [
                            "In the first-stage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 243865399,
                            "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder",
                            "authors": [
                                {
                                    "authorId": "1830381674",
                                    "name": "Shuqi Lu"
                                },
                                {
                                    "authorId": "1391126980",
                                    "name": "Di He"
                                },
                                {
                                    "authorId": "2139787803",
                                    "name": "Chenyan Xiong"
                                },
                                {
                                    "authorId": "35286545",
                                    "name": "Guolin Ke"
                                },
                                {
                                    "authorId": "2060300532",
                                    "name": "Waleed Malik"
                                },
                                {
                                    "authorId": "1897235",
                                    "name": "Zhicheng Dou"
                                },
                                {
                                    "authorId": "144609235",
                                    "name": "Paul N. Bennett"
                                },
                                {
                                    "authorId": "2110264337",
                                    "name": "Tie-Yan Liu"
                                },
                                {
                                    "authorId": "2734525",
                                    "name": "Arnold Overwijk"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 74
                        },
                        "score": 0.6240234375
                    },
                    {
                        "id": "(Lee et al., 2019)",
                        "snippets": [
                            "Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match."
                        ],
                        "paper": {
                            "corpus_id": 173990818,
                            "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1018
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2022)",
                        "snippets": [
                            "In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations",
                            "It consists of a query encoder E_q and a passage encoder E_p, mapping the query q and passage p into k-dimensional dense vectors h_q and h_p, respectively. Then, the semantic relevance score of q and p will be computed using dot product"
                        ],
                        "paper": {
                            "corpus_id": 254685782,
                            "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers",
                            "authors": [
                                {
                                    "authorId": "1423651904",
                                    "name": "Kun Zhou"
                                },
                                {
                                    "authorId": "49544272",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "2542603",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "71790825",
                                    "name": "Daxin Jiang"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                },
                                {
                                    "authorId": "153693432",
                                    "name": "Ji-rong Wen"
                                }
                            ],
                            "year": 2022,
                            "venue": "ECML/PKDD",
                            "n_citations": 16
                        },
                        "score": 0.8369140625
                    },
                    {
                        "id": "(Long et al., 2022)",
                        "snippets": [
                            "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair."
                        ],
                        "paper": {
                            "corpus_id": 253157959,
                            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "2107949588",
                                    "name": "Yanzhao Zhang"
                                },
                                {
                                    "authorId": "2149131512",
                                    "name": "Guangwei Xu"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.71728515625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair",
                            "Currently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input."
                        ],
                        "paper": {
                            "corpus_id": 258833383,
                            "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "2109967721",
                                    "name": "Zehan Li"
                                },
                                {
                                    "authorId": "2107949588",
                                    "name": "Yanzhao Zhang"
                                },
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.6904296875
                    },
                    {
                        "id": "(Cheng et al., 2022)",
                        "snippets": [
                            "Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders",
                            "During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question."
                        ],
                        "paper": {
                            "corpus_id": 252816088,
                            "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "47413820",
                                    "name": "Hao Cheng"
                                },
                                {
                                    "authorId": "145204655",
                                    "name": "Hao Fang"
                                },
                                {
                                    "authorId": "46522098",
                                    "name": "Xiaodong Liu"
                                },
                                {
                                    "authorId": "48441311",
                                    "name": "Jianfeng Gao"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.8310546875
                    },
                    {
                        "id": "(Jiang et al., 2023)",
                        "snippets": [
                            "The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2017) is employed to fetch top-k relevant passages."
                        ],
                        "paper": {
                            "corpus_id": 265457188,
                            "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "51511290",
                                    "name": "Fan Jiang"
                                },
                                {
                                    "authorId": "2257020484",
                                    "name": "Qiongkai Xu"
                                },
                                {
                                    "authorId": "2268408815",
                                    "name": "Tom Drummond"
                                },
                                {
                                    "authorId": "2256997782",
                                    "name": "Trevor Cohn"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.734375
                    },
                    {
                        "id": "(Johnson et al., 2017)",
                        "snippets": [
                            "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."
                        ],
                        "paper": {
                            "corpus_id": 926364,
                            "title": "Billion-Scale Similarity Search with GPUs",
                            "authors": [
                                {
                                    "authorId": "2115354049",
                                    "name": "Jeff Johnson"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                },
                                {
                                    "authorId": "1681054",
                                    "name": "H. J\u00e9gou"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE Transactions on Big Data",
                            "n_citations": 3738
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jiang et al._1, 2023)",
                        "snippets": [
                            "In contrast to traditional IR methods, such as BM25 (Paszke et al., 2019), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings",
                            "The adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2017) can be performed to fetch the top-k passages."
                        ],
                        "paper": {
                            "corpus_id": 265457282,
                            "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "51511290",
                                    "name": "Fan Jiang"
                                },
                                {
                                    "authorId": "2268408815",
                                    "name": "Tom Drummond"
                                },
                                {
                                    "authorId": "2256997782",
                                    "name": "Trevor Cohn"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.7822265625
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hofstatter et al., 2021)",
                        "snippets": [
                            "A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further."
                        ],
                        "paper": {
                            "corpus_id": 233231706,
                            "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling",
                            "authors": [
                                {
                                    "authorId": "97393346",
                                    "name": "Sebastian Hofst\u00e4tter"
                                },
                                {
                                    "authorId": "122045993",
                                    "name": "Sheng-Chieh Lin"
                                },
                                {
                                    "authorId": "2109723027",
                                    "name": "Jheng-Hong Yang"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                },
                                {
                                    "authorId": "1699657",
                                    "name": "A. Hanbury"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 402
                        },
                        "score": 0
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    },
                    {
                        "id": "(Salemi et al., 2023)",
                        "snippets": [
                            "A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. (Qu et al., 2021) recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap",
                            "Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B (Hofst\u00e4tter et al., 2021), ColBERT (Khattab et al., 2020)(Santhanam et al., 2021), RocketQA (Qu et al., 2020)(Ren et al., 2021), and CLDRD (Zeng et al., 2022), use symmetric architectures."
                        ],
                        "paper": {
                            "corpus_id": 258331649,
                            "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering",
                            "authors": [
                                {
                                    "authorId": "2073044451",
                                    "name": "Alireza Salemi"
                                },
                                {
                                    "authorId": "2137765299",
                                    "name": "Juan Altmayer Pizzorno"
                                },
                                {
                                    "authorId": "2499986",
                                    "name": "Hamed Zamani"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 14
                        },
                        "score": 0.7646484375
                    },
                    {
                        "id": "(Zhuang et al., 2023)",
                        "snippets": [
                            "These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33]45]."
                        ],
                        "paper": {
                            "corpus_id": 258557604,
                            "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "1630489015",
                                    "name": "Shengyao Zhuang"
                                },
                                {
                                    "authorId": "24962156",
                                    "name": "Linjun Shou"
                                },
                                {
                                    "authorId": "1692855",
                                    "name": "G. Zuccon"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 8
                        },
                        "score": 0.6689453125
                    },
                    {
                        "id": "(He et al., 2022)",
                        "snippets": [
                            "In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."
                        ],
                        "paper": {
                            "corpus_id": 254853896,
                            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
                            "authors": [
                                {
                                    "authorId": "1754500",
                                    "name": "Xingwei He"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "15796861",
                                    "name": "Alex Jin"
                                },
                                {
                                    "authorId": "2119077859",
                                    "name": "Hang Zhang"
                                },
                                {
                                    "authorId": "3300216",
                                    "name": "Anlei Dong"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "145964453",
                                    "name": "S. Yiu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0.79345703125
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."
                        ],
                        "paper": {
                            "corpus_id": 260656514,
                            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "1656672968",
                                    "name": "Jiduan Liu"
                                },
                                {
                                    "authorId": "2108421184",
                                    "name": "Jiahao Liu"
                                },
                                {
                                    "authorId": "2152915671",
                                    "name": "Yang Yang"
                                },
                                {
                                    "authorId": "2109593338",
                                    "name": "Jingang Wang"
                                },
                                {
                                    "authorId": "2118256028",
                                    "name": "Wei Wu"
                                },
                                {
                                    "authorId": "144060462",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "144539156",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.84033203125
                    },
                    {
                        "id": "(He et al._1, 2022)",
                        "snippets": [
                            "Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers",
                            "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."
                        ],
                        "paper": {
                            "corpus_id": 253080873,
                            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "1754500",
                                    "name": "Xingwei He"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "15796861",
                                    "name": "Alex Jin"
                                },
                                {
                                    "authorId": "15629561",
                                    "name": "Weizhen Qi"
                                },
                                {
                                    "authorId": "2119077859",
                                    "name": "Hang Zhang"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "2109061043",
                                    "name": "Bartuer Zhou"
                                },
                                {
                                    "authorId": "2055922979",
                                    "name": "Biao Cheng"
                                },
                                {
                                    "authorId": "2060901199",
                                    "name": "Sm Yiu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.82861328125
                    },
                    {
                        "id": "(Wang et al., 2019)",
                        "snippets": [
                            "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models."
                        ],
                        "paper": {
                            "corpus_id": 201307832,
                            "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "40296541",
                                    "name": "Zhiguo Wang"
                                },
                                {
                                    "authorId": "145878390",
                                    "name": "Patrick Ng"
                                },
                                {
                                    "authorId": "47646605",
                                    "name": "Xiaofei Ma"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "144028698",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 244
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Karpukhin et al. (Karpukhin et al., 2020) show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder",
                            "In the late-interaction based approach, ColBERT (Khattab et al., 2020) is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs."
                        ],
                        "paper": {
                            "corpus_id": 275931943,
                            "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2291199148",
                                    "name": "Minghan Li"
                                },
                                {
                                    "authorId": "2288256282",
                                    "name": "\u00c9. Gaussier"
                                },
                                {
                                    "authorId": "2331371334",
                                    "name": "Guodong Zhou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6279296875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dual-encoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model fine-grained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance."
                        ],
                        "paper": {
                            "corpus_id": 267751308,
                            "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "2149234508",
                                    "name": "Zhen Yang"
                                },
                                {
                                    "authorId": "2284682533",
                                    "name": "Zhou Shao"
                                },
                                {
                                    "authorId": "2243402027",
                                    "name": "Yuxiao Dong"
                                },
                                {
                                    "authorId": "2238207092",
                                    "name": "Jie Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 4
                        },
                        "score": 0.67626953125
                    },
                    {
                        "id": "(Rajapakse, 2023)",
                        "snippets": [
                            "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries",
                            "While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder."
                        ],
                        "paper": {
                            "corpus_id": 259949811,
                            "title": "Dense Passage Retrieval: Architectures and Augmentation Methods",
                            "authors": [
                                {
                                    "authorId": "2091044662",
                                    "name": "T. Rajapakse"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 7
                        },
                        "score": 0.837890625
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings."
                        ],
                        "paper": {
                            "corpus_id": 259203703,
                            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
                            "authors": [
                                {
                                    "authorId": "2115829412",
                                    "name": "Yuxuan Wang"
                                },
                                {
                                    "authorId": "2220304036",
                                    "name": "Hong Lyu"
                                }
                            ],
                            "year": 2023,
                            "venue": "SUSTAINLP",
                            "n_citations": 2
                        },
                        "score": 0.640625
                    },
                    {
                        "id": "(Sidiropoulos et al., 2024)",
                        "snippets": [
                            "On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers",
                            "At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings."
                        ],
                        "paper": {
                            "corpus_id": 272770506,
                            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2267237867",
                                    "name": "Georgios Sidiropoulos"
                                },
                                {
                                    "authorId": "2314138767",
                                    "name": "Evangelos Kanoulas"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.66845703125
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "In order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval."
                        ],
                        "paper": {
                            "corpus_id": 273026177,
                            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "47396497",
                                    "name": "Chao-Wei Huang"
                                },
                                {
                                    "authorId": "2286884447",
                                    "name": "Yun-Nung Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.77197265625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Autoregressive Models for Retrieval",
                "tldr": "Autoregressive models offer a fundamentally different approach to retrieval by directly generating document identifiers rather than computing similarity between independently encoded query and document vectors. This approach enables deeper query-document interactions through cross-attention mechanisms, potentially overcoming the limited interaction bottleneck of dual-encoder architectures. (9 sources)",
                "text": "\nAutoregressive models represent a paradigm shift in information retrieval, moving away from the traditional dual-encoder architecture toward a generation-based approach. While dual-encoder models independently encode queries and documents to compute similarity scores, autoregressive search engines directly generate identifiers for relevant documents in the candidate pool <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. This fundamental difference allows autoregressive models to perform deep token-level cross-attention between queries and documents, resulting in more sophisticated interactions than the shallow vector comparisons used in dense retrievers <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nThe surge of interest in autoregressive retrieval models stems from their ability to address a critical limitation of dual-encoder architectures: the independent encoding of queries and documents that leads to limited interactions between them <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. By reformulating retrieval as a sequence generation problem, these models can leverage the powerful token-level interactions inherent in autoregressive language models <Paper corpusId=\"274822530\" paperTitle=\"(Trung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. The generative retrieval paradigm enables end-to-end modeling of document retrieval tasks, with models like GenRet learning to tokenize documents into short discrete representations (docids) via discrete auto-encoding approaches <Paper corpusId=\"274822530\" paperTitle=\"(Trung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258048596\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\nArchitecturally, these systems often consist of two major components: a retriever model (such as a dual-encoder) and a generative language model or reader model <Paper corpusId=\"272330251\" paperTitle=\"(Monath et al., 2024)\" isShortName></Paper>. The generative component typically operates in an autoregressive fashion, where the model takes both the encoder's representations and previously generated tokens as input when generating text <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. This architecture enables deeper semantic processing than is possible with the lightweight similarity functions used in dual-encoder models.\n\nIn reranking scenarios, formulating the task as sequence generation allows models to take advantage of the language model's autoregressive capabilities, such as conducting listwise sorting or generating rationales <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper>. This approach has shown advantages in zero-shot retrieval settings, demonstrating strong generalization abilities that can outperform traditional methods like BM25 without task-specific training <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nDespite these advantages, implementing autoregressive search at the scale of large language models (LLMs) presents significant computational challenges <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. To address efficiency concerns, some approaches have introduced non-autoregressive decoder architectures that are compatible with existing transformer-based language models, allowing for parallel scoring of vocabulary items in a single decode step <Paper corpusId=\"258865354\" paperTitle=\"(Soares et al., 2023)\" isShortName></Paper>. This makes document indexing with these models approximately as efficient as indexing with document encoders used in dual-encoder retrieval systems.\n\nThe historical progression from term-matching techniques like TF-IDF and BM25 to dense retrievers with dual-encoder architectures, and now to autoregressive generation models, represents an evolution toward more sophisticated semantic processing in information retrieval <Paper corpusId=\"268889861\" paperTitle=\"(Abdi et al., 2024)\" isShortName></Paper>. As research continues to advance, these autoregressive approaches offer promising directions for overcoming the limitations of previous retrieval paradigms while maintaining practical efficiency.",
                "citations": [
                    {
                        "id": "(Ziems et al., 2023)",
                        "snippets": [
                            "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 258714822,
                            "title": "Large Language Models are Built-in Autoregressive Search Engines",
                            "authors": [
                                {
                                    "authorId": "2264184691",
                                    "name": "Noah Ziems"
                                },
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                },
                                {
                                    "authorId": "72871419",
                                    "name": "Zhihan Zhang"
                                },
                                {
                                    "authorId": "2152153656",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0.81787109375
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                        ],
                        "paper": {
                            "corpus_id": 268031876,
                            "title": "Generative Retrieval with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2185022832",
                                    "name": "Ye Wang"
                                },
                                {
                                    "authorId": "2290204960",
                                    "name": "Xinrun Xu"
                                },
                                {
                                    "authorId": "2143721734",
                                    "name": "Rui Xie"
                                },
                                {
                                    "authorId": "2288018918",
                                    "name": "Wenxin Hu"
                                },
                                {
                                    "authorId": "2052980435",
                                    "name": "Wei Ye"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.70654296875
                    },
                    {
                        "id": "(Trung et al., 2024)",
                        "snippets": [
                            "The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023)(Fu et al., 2023)",
                            "Generative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2023), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search."
                        ],
                        "paper": {
                            "corpus_id": 274822530,
                            "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval",
                            "authors": [
                                {
                                    "authorId": "2335861500",
                                    "name": "Quang Hoang Trung"
                                },
                                {
                                    "authorId": "2335861321",
                                    "name": "Nguyen Van Hoang Phuc"
                                },
                                {
                                    "authorId": "2335859619",
                                    "name": "Le Trung Hoang"
                                },
                                {
                                    "authorId": "2281214899",
                                    "name": "Quang Huu Hieu"
                                },
                                {
                                    "authorId": "113438599",
                                    "name": "Vo Nguyen Le Duy"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.65087890625
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability."
                        ],
                        "paper": {
                            "corpus_id": 258048596,
                            "title": "Learning to Tokenize for Generative Retrieval",
                            "authors": [
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2117203270",
                                    "name": "Zheng Chen"
                                },
                                {
                                    "authorId": "2386396",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2387872",
                                    "name": "Haichao Zhu"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "2136400100",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "1696030",
                                    "name": "M. de Rijke"
                                },
                                {
                                    "authorId": "2780667",
                                    "name": "Z. Ren"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 75
                        },
                        "score": 0
                    },
                    {
                        "id": "(Monath et al., 2024)",
                        "snippets": [
                            "Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022)",
                            "P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y)."
                        ],
                        "paper": {
                            "corpus_id": 272330251,
                            "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks",
                            "authors": [
                                {
                                    "authorId": "2121348263",
                                    "name": "Nicholas Monath"
                                },
                                {
                                    "authorId": "2319130233",
                                    "name": "Will Sussman Grathwohl"
                                },
                                {
                                    "authorId": "51020741",
                                    "name": "Michael Boratko"
                                },
                                {
                                    "authorId": "2300098510",
                                    "name": "Rob Fergus"
                                },
                                {
                                    "authorId": "2286335051",
                                    "name": "Andrew McCallum"
                                },
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 0
                        },
                        "score": 0.6982421875
                    },
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "Dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."
                        ],
                        "paper": {
                            "corpus_id": 248366563,
                            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                            "authors": [
                                {
                                    "authorId": "121875983",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 53
                        },
                        "score": 0.7001953125
                    },
                    {
                        "id": "(Yoon et al., 2024)",
                        "snippets": [
                            "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities."
                        ],
                        "paper": {
                            "corpus_id": 267938301,
                            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
                            "authors": [
                                {
                                    "authorId": "2287336807",
                                    "name": "Soyoung Yoon"
                                },
                                {
                                    "authorId": "2287970016",
                                    "name": "Eunbi Choi"
                                },
                                {
                                    "authorId": "2287064006",
                                    "name": "Jiyeon Kim"
                                },
                                {
                                    "authorId": "2181032855",
                                    "name": "Yireun Kim"
                                },
                                {
                                    "authorId": "2286896884",
                                    "name": "Hyeongu Yun"
                                },
                                {
                                    "authorId": "2287694374",
                                    "name": "Seung-won Hwang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0.72216796875
                    },
                    {
                        "id": "(Soares et al., 2023)",
                        "snippets": [
                            "To overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14]26]."
                        ],
                        "paper": {
                            "corpus_id": 258865354,
                            "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
                            "authors": [
                                {
                                    "authorId": "7353832",
                                    "name": "Livio Baldini Soares"
                                },
                                {
                                    "authorId": "2396669",
                                    "name": "D. Gillick"
                                },
                                {
                                    "authorId": "30859623",
                                    "name": "Jeremy R. Cole"
                                },
                                {
                                    "authorId": "15652489",
                                    "name": "T. Kwiatkowski"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.64794921875
                    },
                    {
                        "id": "(Abdi et al., 2024)",
                        "snippets": [
                            "Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering (Chen et al., 2017)(Kwiatkowski et al., 2019)(Voorhees, 2001), fact-checking [28], dialogue systems (Dinan et al., 2018), and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,(Shen et al., 2014)(Yih et al., 2011). Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15]36]."
                        ],
                        "paper": {
                            "corpus_id": 268889861,
                            "title": "NL2KQL: From Natural Language to Kusto Query",
                            "authors": [
                                {
                                    "authorId": "2294879238",
                                    "name": "Amir H. Abdi"
                                },
                                {
                                    "authorId": "2295304070",
                                    "name": "Xinye Tang"
                                },
                                {
                                    "authorId": "50821288",
                                    "name": "Jeremias Eichelbaum"
                                },
                                {
                                    "authorId": "2294847106",
                                    "name": "Mahan Das"
                                },
                                {
                                    "authorId": "2294855455",
                                    "name": "Alex Klein"
                                },
                                {
                                    "authorId": "2294876696",
                                    "name": "Nihal Irmak Pakis"
                                },
                                {
                                    "authorId": "2294877213",
                                    "name": "William Blum"
                                },
                                {
                                    "authorId": "2294875530",
                                    "name": "Daniel L Mace"
                                },
                                {
                                    "authorId": "2294879377",
                                    "name": "Tanvi Raja"
                                },
                                {
                                    "authorId": "2294871555",
                                    "name": "Namrata Padmanabhan"
                                },
                                {
                                    "authorId": "2294870992",
                                    "name": "Ye Xing"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.73681640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Architectural Differences",
                "tldr": "Dual-encoder architectures and autoregressive models represent fundamentally different approaches to retrieval, with dual-encoders encoding queries and documents independently while autoregressive models generate document identifiers using cross-attention mechanisms. The key architectural distinction lies in how query-document interactions are processed, affecting the depth of semantic understanding and computational efficiency. (11 sources)",
                "text": "\nThe architectural differences between dual-encoder models and autoregressive language models for retrieval reflect fundamentally different approaches to the retrieval task. The most significant distinction is in how these architectures process and model interactions between queries and documents.\n\nDual-encoder architectures, as their name suggests, utilize two separate encoders that independently encode queries and documents into dense vector representations <Paper corpusId=\"254564747\" paperTitle=\"(Sun et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. After encoding, these models calculate relevance scores using lightweight similarity functions such as dot product or cosine similarity <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254564747\" paperTitle=\"(Sun et al., 2022)\" isShortName></Paper>. This independent encoding approach, while computationally efficient, creates a fundamental limitation: it only permits shallow interactions between queries and documents, leading to information loss that can compromise retrieval accuracy <Paper corpusId=\"248227479\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>.\n\nIn contrast, autoregressive models for retrieval adopt a generation-based paradigm that directly produces document identifiers rather than computing similarity scores between independently encoded vectors <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. Unlike dual-encoders, these models employ deep token-level cross-attention mechanisms that allow for much richer interactions between queries and documents <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. When generating text in an autoregressive fashion, the model considers both the encoder's representations and previously generated tokens as input, enabling more sophisticated semantic processing <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nCross-encoder architectures represent a middle ground that processes query-document pairs jointly, providing full interaction through attention mechanisms <Paper corpusId=\"254564747\" paperTitle=\"(Sun et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. However, these models face scalability issues as they cannot pre-compute document representations, making them impractical for direct retrieval from large collections <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. Autoregressive models adapt this cross-attention capability while reformulating retrieval as a sequence generation problem.\n\nThe computational architecture also differs significantly between these approaches. Dual-encoders allow for offline pre-computation of document representations, enabling efficient retrieval through approximate nearest neighbor search at query time <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. Autoregressive models, while potentially more expressive, face greater computational challenges due to their sequential processing nature <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper>. Some recent approaches have introduced non-autoregressive decoder architectures to address these efficiency concerns while maintaining the benefits of deeper query-document interactions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nAnother key architectural difference emerges in how these models handle complex queries. Reranking models, which include both cross-encoders and generative autoregressive models, demonstrate superior performance for simple queries by capturing fine-grained relevance signals. However, their performance declines more sharply than retrieval models as query complexity increases <Paper corpusId=\"276928453\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>. This suggests that the architectural differences between these approaches create distinct trade-offs that vary depending on query characteristics.\n\nThe computational cost implications of these architectural differences are substantial. While dual-encoder models like DPR significantly increase latency compared to traditional methods like BM25 (from 36ms to 293ms), cross-encoder models further increase computational demands, with models like ColBERT requiring approximately 13 times the latency of BM25 <Paper corpusId=\"266573365\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. Autoregressive models, especially those scaled to the size of large language models, face even greater computational challenges that can make practical implementation difficult <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Sun et al., 2022)",
                        "snippets": [
                            "Dual Encoder (DE) (Karpukhin et al., 2020) is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) (Khattab et al., 2020) can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) (Qu et al., 2020) has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation."
                        ],
                        "paper": {
                            "corpus_id": 254564747,
                            "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "2118181226",
                                    "name": "Hao-Lun Sun"
                                },
                                {
                                    "authorId": "49544272",
                                    "name": "Xiao Liu"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "3300216",
                                    "name": "Anlei Dong"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "2115404379",
                                    "name": "Jing Lu"
                                },
                                {
                                    "authorId": "2152822477",
                                    "name": "Yan Zhang"
                                },
                                {
                                    "authorId": "71790825",
                                    "name": "Daxin Jiang"
                                },
                                {
                                    "authorId": "7866194",
                                    "name": "Linjun Yang"
                                },
                                {
                                    "authorId": "32431940",
                                    "name": "Rangan Majumder"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Web Search and Data Mining",
                            "n_citations": 2
                        },
                        "score": 0.66748046875
                    },
                    {
                        "id": "(He et al., 2022)",
                        "snippets": [
                            "In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."
                        ],
                        "paper": {
                            "corpus_id": 254853896,
                            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
                            "authors": [
                                {
                                    "authorId": "1754500",
                                    "name": "Xingwei He"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "15796861",
                                    "name": "Alex Jin"
                                },
                                {
                                    "authorId": "2119077859",
                                    "name": "Hang Zhang"
                                },
                                {
                                    "authorId": "3300216",
                                    "name": "Anlei Dong"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "145964453",
                                    "name": "S. Yiu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0.79345703125
                    },
                    {
                        "id": "(Ma et al., 2022)",
                        "snippets": [
                            "Dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."
                        ],
                        "paper": {
                            "corpus_id": 248366563,
                            "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction",
                            "authors": [
                                {
                                    "authorId": "121875983",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "1777025",
                                    "name": "J. Guo"
                                },
                                {
                                    "authorId": "2109960367",
                                    "name": "Ruqing Zhang"
                                },
                                {
                                    "authorId": "7888704",
                                    "name": "Yixing Fan"
                                },
                                {
                                    "authorId": "1717004",
                                    "name": "Xueqi Cheng"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 53
                        },
                        "score": 0.7001953125
                    },
                    {
                        "id": "(Lin et al., 2022)",
                        "snippets": [
                            "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4)."
                        ],
                        "paper": {
                            "corpus_id": 248227479,
                            "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "2162783506",
                                    "name": "Kangmin Tan"
                                },
                                {
                                    "authorId": "2111522090",
                                    "name": "Chris Miller"
                                },
                                {
                                    "authorId": "2143694337",
                                    "name": "Beiwen Tian"
                                },
                                {
                                    "authorId": "1384550891",
                                    "name": "Xiang Ren"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 49
                        },
                        "score": 0.76220703125
                    },
                    {
                        "id": "(Ziems et al., 2023)",
                        "snippets": [
                            "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 258714822,
                            "title": "Large Language Models are Built-in Autoregressive Search Engines",
                            "authors": [
                                {
                                    "authorId": "2264184691",
                                    "name": "Noah Ziems"
                                },
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                },
                                {
                                    "authorId": "72871419",
                                    "name": "Zhihan Zhang"
                                },
                                {
                                    "authorId": "2152153656",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0.81787109375
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."
                        ],
                        "paper": {
                            "corpus_id": 268031876,
                            "title": "Generative Retrieval with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2185022832",
                                    "name": "Ye Wang"
                                },
                                {
                                    "authorId": "2290204960",
                                    "name": "Xinrun Xu"
                                },
                                {
                                    "authorId": "2143721734",
                                    "name": "Rui Xie"
                                },
                                {
                                    "authorId": "2288018918",
                                    "name": "Wenxin Hu"
                                },
                                {
                                    "authorId": "2052980435",
                                    "name": "Wei Ye"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.70654296875
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."
                        ],
                        "paper": {
                            "corpus_id": 260656514,
                            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "1656672968",
                                    "name": "Jiduan Liu"
                                },
                                {
                                    "authorId": "2108421184",
                                    "name": "Jiahao Liu"
                                },
                                {
                                    "authorId": "2152915671",
                                    "name": "Yang Yang"
                                },
                                {
                                    "authorId": "2109593338",
                                    "name": "Jingang Wang"
                                },
                                {
                                    "authorId": "2118256028",
                                    "name": "Wei Wu"
                                },
                                {
                                    "authorId": "144060462",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "144539156",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.84033203125
                    },
                    {
                        "id": "(Yoon et al., 2024)",
                        "snippets": [
                            "In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities."
                        ],
                        "paper": {
                            "corpus_id": 267938301,
                            "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
                            "authors": [
                                {
                                    "authorId": "2287336807",
                                    "name": "Soyoung Yoon"
                                },
                                {
                                    "authorId": "2287970016",
                                    "name": "Eunbi Choi"
                                },
                                {
                                    "authorId": "2287064006",
                                    "name": "Jiyeon Kim"
                                },
                                {
                                    "authorId": "2181032855",
                                    "name": "Yireun Kim"
                                },
                                {
                                    "authorId": "2286896884",
                                    "name": "Hyeongu Yun"
                                },
                                {
                                    "authorId": "2287694374",
                                    "name": "Seung-won Hwang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0.72216796875
                    },
                    {
                        "id": "(Lu et al., 2025)",
                        "snippets": [
                            "Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query",
                            "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format",
                            "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."
                        ],
                        "paper": {
                            "corpus_id": 276928453,
                            "title": "MultiConIR: Towards multi-condition Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "2349551985",
                                    "name": "Xuan Lu"
                                },
                                {
                                    "authorId": "2349802894",
                                    "name": "Sifan Liu"
                                },
                                {
                                    "authorId": "2349465013",
                                    "name": "Bochao Yin"
                                },
                                {
                                    "authorId": "2349749730",
                                    "name": "Yongqi Li"
                                },
                                {
                                    "authorId": "2325183464",
                                    "name": "Xinghao Chen"
                                },
                                {
                                    "authorId": "2349762405",
                                    "name": "Hui Su"
                                },
                                {
                                    "authorId": "2349879907",
                                    "name": "Yaohui Jin"
                                },
                                {
                                    "authorId": "2349955598",
                                    "name": "Wenjun Zeng"
                                },
                                {
                                    "authorId": "2287612923",
                                    "name": "Xiaoyu Shen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.673828125
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."
                        ],
                        "paper": {
                            "corpus_id": 266573365,
                            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
                            "authors": [
                                {
                                    "authorId": "2276748536",
                                    "name": "Haifeng Li"
                                },
                                {
                                    "authorId": "2157052",
                                    "name": "Mo Hai"
                                },
                                {
                                    "authorId": "2276610303",
                                    "name": "Dong Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.841796875
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Retrieval Accuracy Comparison",
                "tldr": "While dual-encoder models offer computational efficiency, they consistently underperform compared to cross-encoder and autoregressive models in retrieval accuracy due to their limited query-document interactions. Autoregressive models achieve superior accuracy through deep token-level cross-attention, though this advantage diminishes as query complexity increases. (9 sources)",
                "text": "\nWhen comparing the retrieval accuracy of dual-encoder architectures and autoregressive models, a clear performance hierarchy emerges. Dual-encoder models, despite their widespread adoption in real-world applications, consistently demonstrate suboptimal retrieval performance compared to both cross-encoder and autoregressive approaches <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>.\n\nThe primary factor limiting dual-encoder accuracy is the independent encoding of queries and documents, which prevents fine-grained interactions between them. This independent processing leads to information loss that distorts similarity calculations and ultimately compromises retrieval performance <Paper corpusId=\"266573365\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper>. The weakness of this architecture is particularly evident when handling nuanced semantic relationships that require deeper token-to-token attention across query-document pairs <Paper corpusId=\"248227479\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>.\n\nCross-encoder models, which take the concatenation of queries and documents as input, significantly outperform dual-encoders in retrieval accuracy by capturing deep correlations through full attention mechanisms <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. This improved performance comes from their ability to model token-level interactions that dual-encoders simply cannot capture <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"201307832\" paperTitle=\"(Wang et al., 2019)\" isShortName></Paper>.\n\nAutoregressive search engines push this advantage further by performing deep token-level cross-attention during the generation of document identifiers. This approach results in superior retrieval accuracy compared to the shallow interactions in dense retrievers <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. Notably, these models have demonstrated strong generalization abilities, outperforming traditional methods like BM25 even in zero-shot settings without task-specific training <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nHowever, the accuracy advantage of both cross-encoder and autoregressive models is context-dependent. While these reranking-oriented architectures excel at capturing fine-grained relevance signals for simple, single-condition queries, their performance declines more sharply than dual-encoder retrieval models as query complexity increases <Paper corpusId=\"276928453\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>. This suggests that the deeper interaction mechanisms that provide accuracy advantages in straightforward retrieval scenarios may become less effective when handling complex, multi-condition queries.\n\nDespite their accuracy limitations, dual-encoder models remain competitive in practical settings, particularly when combined with additional components. Two-stage retrieval pipelines that use dual-encoders for initial retrieval followed by cross-encoder reranking can achieve superior retrieval quality compared to single-stage approaches <Paper corpusId=\"259949811\" paperTitle=\"(Rajapakse, 2023)\" isShortName></Paper>. Some researchers have also proposed modifications to the dual-encoder architecture, such as using multiple representations to rerank passages without requiring a separate reranking model <Paper corpusId=\"259949811\" paperTitle=\"(Rajapakse, 2023)\" isShortName></Paper>.\n\nThe retrieval accuracy trade-offs between these architectural approaches highlight the continuing challenge in information retrieval: balancing the superior semantic understanding capabilities of cross-attention mechanisms with the practical efficiency requirements of real-world applications.",
                "citations": [
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."
                        ],
                        "paper": {
                            "corpus_id": 260656514,
                            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "1656672968",
                                    "name": "Jiduan Liu"
                                },
                                {
                                    "authorId": "2108421184",
                                    "name": "Jiahao Liu"
                                },
                                {
                                    "authorId": "2152915671",
                                    "name": "Yang Yang"
                                },
                                {
                                    "authorId": "2109593338",
                                    "name": "Jingang Wang"
                                },
                                {
                                    "authorId": "2118256028",
                                    "name": "Wei Wu"
                                },
                                {
                                    "authorId": "144060462",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "144539156",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.84033203125
                    },
                    {
                        "id": "(He et al., 2022)",
                        "snippets": [
                            "In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."
                        ],
                        "paper": {
                            "corpus_id": 254853896,
                            "title": "Curriculum Sampling for Dense Retrieval with Document Expansion",
                            "authors": [
                                {
                                    "authorId": "1754500",
                                    "name": "Xingwei He"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "15796861",
                                    "name": "Alex Jin"
                                },
                                {
                                    "authorId": "2119077859",
                                    "name": "Hang Zhang"
                                },
                                {
                                    "authorId": "3300216",
                                    "name": "Anlei Dong"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "145964453",
                                    "name": "S. Yiu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0.79345703125
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."
                        ],
                        "paper": {
                            "corpus_id": 266573365,
                            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
                            "authors": [
                                {
                                    "authorId": "2276748536",
                                    "name": "Haifeng Li"
                                },
                                {
                                    "authorId": "2157052",
                                    "name": "Mo Hai"
                                },
                                {
                                    "authorId": "2276610303",
                                    "name": "Dong Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.841796875
                    },
                    {
                        "id": "(He et al._1, 2022)",
                        "snippets": [
                            "Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers",
                            "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."
                        ],
                        "paper": {
                            "corpus_id": 253080873,
                            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "1754500",
                                    "name": "Xingwei He"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "15796861",
                                    "name": "Alex Jin"
                                },
                                {
                                    "authorId": "15629561",
                                    "name": "Weizhen Qi"
                                },
                                {
                                    "authorId": "2119077859",
                                    "name": "Hang Zhang"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "2109061043",
                                    "name": "Bartuer Zhou"
                                },
                                {
                                    "authorId": "2055922979",
                                    "name": "Biao Cheng"
                                },
                                {
                                    "authorId": "2060901199",
                                    "name": "Sm Yiu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.82861328125
                    },
                    {
                        "id": "(Lin et al., 2022)",
                        "snippets": [
                            "Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4)."
                        ],
                        "paper": {
                            "corpus_id": 248227479,
                            "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "2162783506",
                                    "name": "Kangmin Tan"
                                },
                                {
                                    "authorId": "2111522090",
                                    "name": "Chris Miller"
                                },
                                {
                                    "authorId": "2143694337",
                                    "name": "Beiwen Tian"
                                },
                                {
                                    "authorId": "1384550891",
                                    "name": "Xiang Ren"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 49
                        },
                        "score": 0.76220703125
                    },
                    {
                        "id": "(Wang et al., 2019)",
                        "snippets": [
                            "BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models."
                        ],
                        "paper": {
                            "corpus_id": 201307832,
                            "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "40296541",
                                    "name": "Zhiguo Wang"
                                },
                                {
                                    "authorId": "145878390",
                                    "name": "Patrick Ng"
                                },
                                {
                                    "authorId": "47646605",
                                    "name": "Xiaofei Ma"
                                },
                                {
                                    "authorId": "1701451",
                                    "name": "Ramesh Nallapati"
                                },
                                {
                                    "authorId": "144028698",
                                    "name": "Bing Xiang"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 244
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ziems et al., 2023)",
                        "snippets": [
                            "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 258714822,
                            "title": "Large Language Models are Built-in Autoregressive Search Engines",
                            "authors": [
                                {
                                    "authorId": "2264184691",
                                    "name": "Noah Ziems"
                                },
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                },
                                {
                                    "authorId": "72871419",
                                    "name": "Zhihan Zhang"
                                },
                                {
                                    "authorId": "2152153656",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0.81787109375
                    },
                    {
                        "id": "(Lu et al., 2025)",
                        "snippets": [
                            "Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query",
                            "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format",
                            "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."
                        ],
                        "paper": {
                            "corpus_id": 276928453,
                            "title": "MultiConIR: Towards multi-condition Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "2349551985",
                                    "name": "Xuan Lu"
                                },
                                {
                                    "authorId": "2349802894",
                                    "name": "Sifan Liu"
                                },
                                {
                                    "authorId": "2349465013",
                                    "name": "Bochao Yin"
                                },
                                {
                                    "authorId": "2349749730",
                                    "name": "Yongqi Li"
                                },
                                {
                                    "authorId": "2325183464",
                                    "name": "Xinghao Chen"
                                },
                                {
                                    "authorId": "2349762405",
                                    "name": "Hui Su"
                                },
                                {
                                    "authorId": "2349879907",
                                    "name": "Yaohui Jin"
                                },
                                {
                                    "authorId": "2349955598",
                                    "name": "Wenjun Zeng"
                                },
                                {
                                    "authorId": "2287612923",
                                    "name": "Xiaoyu Shen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.673828125
                    },
                    {
                        "id": "(Rajapakse, 2023)",
                        "snippets": [
                            "The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries",
                            "While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder."
                        ],
                        "paper": {
                            "corpus_id": 259949811,
                            "title": "Dense Passage Retrieval: Architectures and Augmentation Methods",
                            "authors": [
                                {
                                    "authorId": "2091044662",
                                    "name": "T. Rajapakse"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 7
                        },
                        "score": 0.837890625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Computational Efficiency Comparison",
                "tldr": "Dual-encoder architectures offer superior computational efficiency by allowing offline pre-computation of document embeddings and fast retrieval through approximate nearest neighbor search, while autoregressive models provide better retrieval accuracy at the cost of significantly higher computational demands. (13 sources)",
                "text": "\nThe computational efficiency differences between dual-encoder and autoregressive models represent one of the most significant trade-offs in information retrieval system design. Dual-encoder architectures have become the de facto standard for practical retrieval systems primarily due to their computational advantages <Paper corpusId=\"259203703\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272770506\" paperTitle=\"(Sidiropoulos et al., 2024)\" isShortName></Paper>.\n\nThe fundamental efficiency advantage of dual-encoder models stems from their ability to decouple the encoding process. Since queries and documents are encoded independently, document representations can be pre-computed and indexed offline <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This design allows systems to compute embeddings for millions of documents in advance and build efficient indexing structures, significantly reducing the computational burden during retrieval <Paper corpusId=\"265457282\" paperTitle=\"(Jiang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>.\n\nAt query time, dual-encoder systems only need to encode the new query and conduct an approximate nearest neighbor (ANN) search to locate the most relevant documents <Paper corpusId=\"273026177\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This approach enables efficient retrieval at scale, making it possible to search through millions of documents with reasonable latency <Paper corpusId=\"265457282\" paperTitle=\"(Jiang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"926364\" paperTitle=\"(Johnson et al., 2017)\" isShortName></Paper>. The computational efficiency of dual-encoders is particularly valuable in latency-sensitive applications, making them widely adopted in real-world deployment scenarios <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nIn contrast, both cross-encoder and autoregressive models face significant computational challenges. Cross-encoders, which jointly process query-document pairs using full attention mechanisms, require recomputing representations for each document whenever a new query arrives <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This makes them impractical for direct retrieval from large document collections, relegating them to reranking a smaller set of candidate documents retrieved by more efficient methods <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"272770506\" paperTitle=\"(Sidiropoulos et al., 2024)\" isShortName></Paper>.\n\nAutoregressive search engines face even greater computational demands. While these models offer superior retrieval accuracy through deep token-level cross-attention, scaling them to the size of large language models creates prohibitive computational overhead <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. The sequential generation process inherent in autoregressive models further limits their computational efficiency compared to the parallelizable nature of dual-encoder approaches.\n\nThe practical implications of these efficiency differences are substantial. Experimental results show that compared to traditional BM25 retrieval with a latency of 36 milliseconds, dual-encoder models like DPR increase latency to 293 milliseconds (an 8-fold increase), while cross-encoder models like ColBERT further increase latency to 458 milliseconds (nearly 13 times that of BM25) <Paper corpusId=\"266573365\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. Autoregressive models would likely show even higher latency, especially when scaled to the size of modern large language models.\n\nThese efficiency considerations have direct implications for system design choices. As query complexity increases, reranking models (including both cross-encoders and autoregressive approaches) show a sharper decline in performance compared to retrieval models <Paper corpusId=\"276928453\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>. This suggests that the computational efficiency advantages of dual-encoder architectures become even more valuable in scenarios involving complex, multi-condition queries.\n\nThe substantial efficiency gap between these architectural approaches explains why most practical information retrieval systems adopt a two-stage pipeline: using efficient dual-encoder models for initial retrieval from large collections, followed by more computationally intensive cross-encoder or autoregressive models to rerank a smaller candidate set <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper>. This hybrid approach attempts to balance the computational efficiency of dual-encoders with the superior accuracy of models that enable deeper query-document interactions.",
                "citations": [
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings."
                        ],
                        "paper": {
                            "corpus_id": 259203703,
                            "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency",
                            "authors": [
                                {
                                    "authorId": "2115829412",
                                    "name": "Yuxuan Wang"
                                },
                                {
                                    "authorId": "2220304036",
                                    "name": "Hong Lyu"
                                }
                            ],
                            "year": 2023,
                            "venue": "SUSTAINLP",
                            "n_citations": 2
                        },
                        "score": 0.640625
                    },
                    {
                        "id": "(Sidiropoulos et al., 2024)",
                        "snippets": [
                            "On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers",
                            "At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings."
                        ],
                        "paper": {
                            "corpus_id": 272770506,
                            "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2267237867",
                                    "name": "Georgios Sidiropoulos"
                                },
                                {
                                    "authorId": "2314138767",
                                    "name": "Evangelos Kanoulas"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.66845703125
                    },
                    {
                        "id": "(He et al._1, 2022)",
                        "snippets": [
                            "Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers",
                            "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."
                        ],
                        "paper": {
                            "corpus_id": 253080873,
                            "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "1754500",
                                    "name": "Xingwei He"
                                },
                                {
                                    "authorId": "2171182",
                                    "name": "Yeyun Gong"
                                },
                                {
                                    "authorId": "15796861",
                                    "name": "Alex Jin"
                                },
                                {
                                    "authorId": "15629561",
                                    "name": "Weizhen Qi"
                                },
                                {
                                    "authorId": "2119077859",
                                    "name": "Hang Zhang"
                                },
                                {
                                    "authorId": "2143968416",
                                    "name": "Jian Jiao"
                                },
                                {
                                    "authorId": "2109061043",
                                    "name": "Bartuer Zhou"
                                },
                                {
                                    "authorId": "2055922979",
                                    "name": "Biao Cheng"
                                },
                                {
                                    "authorId": "2060901199",
                                    "name": "Sm Yiu"
                                },
                                {
                                    "authorId": "46429989",
                                    "name": "Nan Duan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.82861328125
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."
                        ],
                        "paper": {
                            "corpus_id": 260656514,
                            "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "1656672968",
                                    "name": "Jiduan Liu"
                                },
                                {
                                    "authorId": "2108421184",
                                    "name": "Jiahao Liu"
                                },
                                {
                                    "authorId": "2152915671",
                                    "name": "Yang Yang"
                                },
                                {
                                    "authorId": "2109593338",
                                    "name": "Jingang Wang"
                                },
                                {
                                    "authorId": "2118256028",
                                    "name": "Wei Wu"
                                },
                                {
                                    "authorId": "144060462",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "144539156",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.84033203125
                    },
                    {
                        "id": "(Jiang et al._1, 2023)",
                        "snippets": [
                            "In contrast to traditional IR methods, such as BM25 (Paszke et al., 2019), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings",
                            "The adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2017) can be performed to fetch the top-k passages."
                        ],
                        "paper": {
                            "corpus_id": 265457282,
                            "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "51511290",
                                    "name": "Fan Jiang"
                                },
                                {
                                    "authorId": "2268408815",
                                    "name": "Tom Drummond"
                                },
                                {
                                    "authorId": "2256997782",
                                    "name": "Trevor Cohn"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.7822265625
                    },
                    {
                        "id": "(Karpukhin et al., 2020)",
                        "snippets": [
                            "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 215737187,
                            "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "48872685",
                                    "name": "Sewon Min"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "51183248",
                                    "name": "Ledell Yu Wu"
                                },
                                {
                                    "authorId": "2068070",
                                    "name": "Sergey Edunov"
                                },
                                {
                                    "authorId": "50536468",
                                    "name": "Danqi Chen"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3794
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "In order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval."
                        ],
                        "paper": {
                            "corpus_id": 273026177,
                            "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
                            "authors": [
                                {
                                    "authorId": "47396497",
                                    "name": "Chao-Wei Huang"
                                },
                                {
                                    "authorId": "2286884447",
                                    "name": "Yun-Nung Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.77197265625
                    },
                    {
                        "id": "(Johnson et al., 2017)",
                        "snippets": [
                            "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."
                        ],
                        "paper": {
                            "corpus_id": 926364,
                            "title": "Billion-Scale Similarity Search with GPUs",
                            "authors": [
                                {
                                    "authorId": "2115354049",
                                    "name": "Jeff Johnson"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                },
                                {
                                    "authorId": "1681054",
                                    "name": "H. J\u00e9gou"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE Transactions on Big Data",
                            "n_citations": 3738
                        },
                        "score": 0
                    },
                    {
                        "id": "(Long et al., 2022)",
                        "snippets": [
                            "To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair."
                        ],
                        "paper": {
                            "corpus_id": 253157959,
                            "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "8427191",
                                    "name": "Dingkun Long"
                                },
                                {
                                    "authorId": "2107949588",
                                    "name": "Yanzhao Zhang"
                                },
                                {
                                    "authorId": "2149131512",
                                    "name": "Guangwei Xu"
                                },
                                {
                                    "authorId": "35930962",
                                    "name": "Pengjun Xie"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.71728515625
                    },
                    {
                        "id": "(Ziems et al., 2023)",
                        "snippets": [
                            "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 258714822,
                            "title": "Large Language Models are Built-in Autoregressive Search Engines",
                            "authors": [
                                {
                                    "authorId": "2264184691",
                                    "name": "Noah Ziems"
                                },
                                {
                                    "authorId": "38767143",
                                    "name": "W. Yu"
                                },
                                {
                                    "authorId": "72871419",
                                    "name": "Zhihan Zhang"
                                },
                                {
                                    "authorId": "2152153656",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 42
                        },
                        "score": 0.81787109375
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."
                        ],
                        "paper": {
                            "corpus_id": 266573365,
                            "title": "A Multi-level Distillation based Dense Passage Retrieval Model",
                            "authors": [
                                {
                                    "authorId": "2276748536",
                                    "name": "Haifeng Li"
                                },
                                {
                                    "authorId": "2157052",
                                    "name": "Mo Hai"
                                },
                                {
                                    "authorId": "2276610303",
                                    "name": "Dong Tang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.841796875
                    },
                    {
                        "id": "(Khattab et al., 2020)",
                        "snippets": [
                            "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."
                        ],
                        "paper": {
                            "corpus_id": 216553223,
                            "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
                            "authors": [
                                {
                                    "authorId": "144112155",
                                    "name": "O. Khattab"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 1377
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2025)",
                        "snippets": [
                            "Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query",
                            "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format",
                            "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."
                        ],
                        "paper": {
                            "corpus_id": 276928453,
                            "title": "MultiConIR: Towards multi-condition Information Retrieval",
                            "authors": [
                                {
                                    "authorId": "2349551985",
                                    "name": "Xuan Lu"
                                },
                                {
                                    "authorId": "2349802894",
                                    "name": "Sifan Liu"
                                },
                                {
                                    "authorId": "2349465013",
                                    "name": "Bochao Yin"
                                },
                                {
                                    "authorId": "2349749730",
                                    "name": "Yongqi Li"
                                },
                                {
                                    "authorId": "2325183464",
                                    "name": "Xinghao Chen"
                                },
                                {
                                    "authorId": "2349762405",
                                    "name": "Hui Su"
                                },
                                {
                                    "authorId": "2349879907",
                                    "name": "Yaohui Jin"
                                },
                                {
                                    "authorId": "2349955598",
                                    "name": "Wenjun Zeng"
                                },
                                {
                                    "authorId": "2287612923",
                                    "name": "Xiaoyu Shen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.673828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.21741300000000002
    }
}