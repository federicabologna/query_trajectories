{
    "query": "What are the main methodologies used in membership inference attacks on machine learning models, and how do black-box and white-box approaches differ?",
    "user_id": "lib_user",
    "task_id": "49fdacd3-c6cf-4356-8f81-906bbe01c6bd",
    "timestamp": "2025-06-23T23:34:37.547585",
    "n_retrieval": 256,
    "n_retrieved": 266,
    "n_candidates": 44,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 1.00932,
    "decomposed_query": {
        "rewritten_query": "Main methodologies used in membership inference attacks on machine learning models, and differences between black-box and white-box approaches.",
        "keyword_query": "membership inference attacks machine learning models black-box white-box approaches differences",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009813,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.16763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267868555",
                    "name": "Depeng Chen"
                },
                {
                    "authorId": "2289471467",
                    "name": "Haozhe Chen"
                },
                {
                    "authorId": "2315301504",
                    "name": "Hulin Jin"
                },
                {
                    "authorId": "143643872",
                    "name": "Jie Cui"
                },
                {
                    "authorId": "2064917432",
                    "name": "Hong Zhong"
                }
            ],
            "abstract": "Membership inference attacks (MIAs) are critical tools for assessing privacy risks and ensuring compliance with regulations like the General Data Protection Regulation (GDPR). However, their potential for auditing unauthorized use of data remains under explored. To bridge this gap, we propose a novel clean-label backdoor-based approach for MIAs, designed specifically for robust and stealthy data auditing. Unlike conventional methods that rely on detectable poisoned samples with altered labels, our approach retains natural labels, enhancing stealthiness even at low poisoning rates. Our approach employs an optimal trigger generated by a shadow model that mimics the target model's behavior. This design minimizes the feature-space distance between triggered samples and the source class while preserving the original data labels. The result is a powerful and undetectable auditing mechanism that overcomes limitations of existing approaches, such as label inconsistencies and visual artifacts in poisoned samples. The proposed method enables robust data auditing through black-box access, achieving high attack success rates across diverse datasets and model architectures. Additionally, it addresses challenges related to trigger stealthiness and poisoning durability, establishing itself as a practical and effective solution for data auditing. Comprehensive experiments validate the efficacy and generalizability of our approach, outperforming several baseline methods in both stealth and attack success metrics.",
            "corpus_id": 274281228,
            "sentences": [
                {
                    "corpus_id": "274281228",
                    "title": "Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference",
                    "text": "As an emerging technique, membership inference attack aims to infer whether a specific sample (, ) belongs to the training data set   of the target model. According to the attacker's capabilities, MIAs can be roughly divided into two categories: white-box attacks and black-box attacks: \n\nBlack-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [10,27,28]. There are generally two strategies in black-box settings: model-based attacks and metric-based attacks. \n\n\u2022 Model-based Attacks: Shokri et al. [28] introduced the first MIA against machine learning models, where the attacker has black-box access to the target model. The attacker builds multiple shadow models to mimic the target model, constructs a dataset of membership labels, and trains a binary classifier to predict membership status. However, this method requires extensive resources to train multiple shadow models with the same architecture as the target model, necessitating access to a shadow dataset with a distribution similar to that of the target's training set. To mitigate these issues, Salem et al. [25] proposed using a single shadow model, allowing for effective attacks while relaxing some adversarial assumptions. \u2022 Metric-based Attacks: Song et al. [29] developed a metricbased attack where the attacker compares a calculated metric  (such as entropy) to a predefined threshold to infer membership. This method, however, is limited when the target model only provides predicted labels without prediction vectors. In contrast, our approach effectively utilizes predicted labels for membership inference. Bertran et al. [3] introduced a novel method that distinguishes between members and non-members using quantiles, eliminating the need for architectural knowledge and representing a true \"black box\" approach. Liu et al. [21] leverage the training process of the target model in their MIA, called TrajectoryMIA, utilizing knowledge distillation to extract membership information from loss records at various training epochs. These methods, while innovative, typically involve complex training processes and significant costs, particularly with knowledge distillation. \n\nWhite-box Membership Inference. In white-box settings, attackers gain access to model parameters  * and potentially intermediate training information, such as gradients \u2112  [16,23].",
                    "score": 0.8516684302985258,
                    "section_title": "RELATED WORK 6.1 Membership Inference",
                    "char_start_offset": 28546,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 286
                        },
                        {
                            "start": 289,
                            "end": 320
                        },
                        {
                            "start": 321,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 526
                        },
                        {
                            "start": 529,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2071
                        },
                        {
                            "start": 2072,
                            "end": 2214
                        },
                        {
                            "start": 2217,
                            "end": 2248
                        },
                        {
                            "start": 2249,
                            "end": 2397
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 411,
                            "end": 415,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 415,
                            "end": 418,
                            "matchedPaperCorpusId": "218514464"
                        },
                        {
                            "start": 418,
                            "end": 421,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 566,
                            "end": 570,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1140,
                            "end": 1144,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1295,
                            "end": 1299,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1664,
                            "end": 1667,
                            "matchedPaperCorpusId": "259375769"
                        },
                        {
                            "start": 1868,
                            "end": 1872,
                            "matchedPaperCorpusId": "251953448"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9931640625
                }
            ],
            "relevance_judgement": 0.9931640625,
            "relevance_judgment_input_expanded": "# Title: Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference\n# Venue: arXiv.org\n# Authors: Depeng Chen, Haozhe Chen, Hulin Jin, Jie Cui, Hong Zhong\n## Abstract\nMembership inference attacks (MIAs) are critical tools for assessing privacy risks and ensuring compliance with regulations like the General Data Protection Regulation (GDPR). However, their potential for auditing unauthorized use of data remains under explored. To bridge this gap, we propose a novel clean-label backdoor-based approach for MIAs, designed specifically for robust and stealthy data auditing. Unlike conventional methods that rely on detectable poisoned samples with altered labels, our approach retains natural labels, enhancing stealthiness even at low poisoning rates. Our approach employs an optimal trigger generated by a shadow model that mimics the target model's behavior. This design minimizes the feature-space distance between triggered samples and the source class while preserving the original data labels. The result is a powerful and undetectable auditing mechanism that overcomes limitations of existing approaches, such as label inconsistencies and visual artifacts in poisoned samples. The proposed method enables robust data auditing through black-box access, achieving high attack success rates across diverse datasets and model architectures. Additionally, it addresses challenges related to trigger stealthiness and poisoning durability, establishing itself as a practical and effective solution for data auditing. Comprehensive experiments validate the efficacy and generalizability of our approach, outperforming several baseline methods in both stealth and attack success metrics.\n## RELATED WORK 6.1 Membership Inference\nAs an emerging technique, membership inference attack aims to infer whether a specific sample (, ) belongs to the training data set   of the target model. According to the attacker's capabilities, MIAs can be roughly divided into two categories: white-box attacks and black-box attacks: \n\nBlack-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [10,27,28]. There are generally two strategies in black-box settings: model-based attacks and metric-based attacks. \n\n\u2022 Model-based Attacks: Shokri et al. [28] introduced the first MIA against machine learning models, where the attacker has black-box access to the target model. The attacker builds multiple shadow models to mimic the target model, constructs a dataset of membership labels, and trains a binary classifier to predict membership status. However, this method requires extensive resources to train multiple shadow models with the same architecture as the target model, necessitating access to a shadow dataset with a distribution similar to that of the target's training set. To mitigate these issues, Salem et al. [25] proposed using a single shadow model, allowing for effective attacks while relaxing some adversarial assumptions. \u2022 Metric-based Attacks: Song et al. [29] developed a metricbased attack where the attacker compares a calculated metric  (such as entropy) to a predefined threshold to infer membership. This method, however, is limited when the target model only provides predicted labels without prediction vectors. In contrast, our approach effectively utilizes predicted labels for membership inference. Bertran et al. [3] introduced a novel method that distinguishes between members and non-members using quantiles, eliminating the need for architectural knowledge and representing a true \"black box\" approach. Liu et al. [21] leverage the training process of the target model in their MIA, called TrajectoryMIA, utilizing knowledge distillation to extract membership information from loss records at various training epochs. These methods, while innovative, typically involve complex training processes and significant costs, particularly with knowledge distillation. \n\nWhite-box Membership Inference. In white-box settings, attackers gain access to model parameters  * and potentially intermediate training information, such as gradients \u2112  [16,23].",
            "reference_string": "[274281228 | Chen et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 61,
            "citation_count": 25,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2208.10445",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.10445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2146247989",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2110631311",
                    "name": "Weilin Xu"
                },
                {
                    "authorId": "35372584",
                    "name": "Cory Cornelius"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.",
            "corpus_id": 251719178,
            "sentences": [
                {
                    "corpus_id": "251719178",
                    "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
                    "text": "Membership Inference Attacks. In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models [17,19,26,35,42,44,50,55]. Shokri et al. [44] propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [42] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [44]. Later, Nasr et al. [35] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. [50] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang [30] and Choquette-Choo et al. [8] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nIn addition to the image classification models we concentrate on in this work, there are some other works demonstrating other types of ML models, e.g., language models [46,47], generative models [6,17], and graph-based models [19,36], are also vulnerable to membership inference attacks. In future work, we plan to extend our work to a broader range of ML application scenarios. Defenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different defense mechanisms.",
                    "score": 0.6815620404735555,
                    "section_title": "Related Work",
                    "char_start_offset": 53208,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 29
                        },
                        {
                            "start": 30,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1711
                        },
                        {
                            "start": 1714,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2092
                        },
                        {
                            "start": 2093,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 266,
                            "end": 270,
                            "matchedPaperCorpusId": "3052834"
                        },
                        {
                            "start": 273,
                            "end": 276,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 276,
                            "end": 279,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 279,
                            "end": 282,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 282,
                            "end": 285,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 285,
                            "end": 288,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 288,
                            "end": 291,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 307,
                            "end": 311,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 675,
                            "end": 679,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 804,
                            "end": 808,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 829,
                            "end": 833,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1000,
                            "end": 1004,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 1378,
                            "end": 1382,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1409,
                            "end": 1412,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1882,
                            "end": 1886,
                            "matchedPaperCorpusId": "214743021"
                        },
                        {
                            "start": 1886,
                            "end": 1889,
                            "matchedPaperCorpusId": "159041003"
                        },
                        {
                            "start": 1909,
                            "end": 1912,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 1912,
                            "end": 1915,
                            "matchedPaperCorpusId": "3052834"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                },
                {
                    "corpus_id": "251719178",
                    "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
                    "text": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.",
                    "score": 0.6182724782539426,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                }
            ],
            "relevance_judgement": 0.98388671875,
            "relevance_judgment_input_expanded": "# Title: Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models\n# Venue: arXiv.org\n# Authors: Xinlei He, Zheng Li, Weilin Xu, Cory Cornelius, Yang Zhang\n## Abstract\nMachine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.\n## Related Work\nMembership Inference Attacks. In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models [17,19,26,35,42,44,50,55]. Shokri et al. [44] propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [42] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [44]. Later, Nasr et al. [35] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. [50] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang [30] and Choquette-Choo et al. [8] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nIn addition to the image classification models we concentrate on in this work, there are some other works demonstrating other types of ML models, e.g., language models [46,47], generative models [6,17], and graph-based models [19,36], are also vulnerable to membership inference attacks. In future work, we plan to extend our work to a broader range of ML application scenarios. Defenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different defense mechanisms.",
            "reference_string": "[251719178 | He et al. | 2022 | Citations: 25]"
        },
        {
            "title": "Black-box based limited query membership inference attack",
            "venue": "IEEE Access",
            "year": 2022,
            "reference_count": 37,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09775801.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2022.3175824?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2022.3175824, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153635068",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2146383582",
                    "name": "Huaping Zhou"
                },
                {
                    "authorId": "2297143623",
                    "name": "Pengyan Wang"
                },
                {
                    "authorId": "2105622",
                    "name": "Gaoming Yang"
                }
            ],
            "abstract": "Conventional membership inference attacks usually require a large number of queries of the target model when training shadow models, and this task becomes extremely difficult when the number of queries is limited. Aiming at the problem of insufficient training data for shadow models due to the limited number of queries, we propose a membership inference attack method based on generative adversarial networks (GAN). First, we use generative adversarial networks to augment the samples obtained by a small number of queries to expand the training data of the model; Secondly, we use the improved CNN to obtain shadow models that have a higher degree of fitting on different target model structures; Finally, we evaluate the accuracy of the proposed algorithm on XgBoost, Logistic, and neural network models using public datasets MNIST and CIFAR10 in a black-box setting, and the results show that our model has an average attack accuracy of 62% and 83%, respectively. It can be seen that, compared with the existing research methods, our model can obtain better attack effects under the condition of significantly reducing the number of queries, which shows the feasibility of our proposed method in membership inference attacks.",
            "corpus_id": 248870291,
            "sentences": [
                {
                    "corpus_id": "248870291",
                    "title": "Black-box based limited query membership inference attack",
                    "text": "Since Shokri et al. [6] proposed an inference attack against MLaaS, the research on member inference has received extensive attention and has been successfully implemented in many fields. It uses the output information of the model to infer whether a sample exists in the training data set of the model, which seriously threatens the privacy and security of the machine learning model. Membership inference attacks are divided into white-box attacks [11][12][13][14] and blackbox attacks [5,6,15] according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability. In addition, membership inference attacks can also be divided into attacks against independent models [16][17][18][19][20][21] and attacks against Federated Learning [11,22]. \n\nAt present, the research on membership inference attack mainly focuses on the black-box attack of independent models. The attacker uses the model's predicted behavior difference between member data and non-member data to train a binary attack model that recognizes this difference. In order to obtain the training data of the attacking model in the blackbox mode, Shokri [6]  Shokri's method [6] is based on the assumption of extensive query and knowledge of the target model structure. They obtained synthetic data by querying the target model and used this data to train multiple shadow models with the same structure as the target model. In practical applications, a large number of accesses to the target model require high query costs, which largely limits the application scope of membership inference attacks. Therefore, how to construct a shadow model similar to the target model under a small number of queries is the key to solving the hypothesis of membership inference attack.",
                    "score": 0.8112096422922639,
                    "section_title": "A. MEMBERSHIP INFERENCE ATTACK",
                    "char_start_offset": 4381,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1088
                        },
                        {
                            "start": 1091,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1372
                        },
                        {
                            "start": 1373,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2079
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 23,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 454,
                            "end": 458,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 458,
                            "end": 462,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 462,
                            "end": 466,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 488,
                            "end": 491,
                            "matchedPaperCorpusId": "186206717"
                        },
                        {
                            "start": 491,
                            "end": 493,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 493,
                            "end": 496,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 1016,
                            "end": 1020,
                            "matchedPaperCorpusId": "233864706"
                        },
                        {
                            "start": 1020,
                            "end": 1024,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1024,
                            "end": 1028,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1032,
                            "end": 1036,
                            "matchedPaperCorpusId": "210888275"
                        },
                        {
                            "start": 1036,
                            "end": 1040,
                            "matchedPaperCorpusId": "86836429"
                        },
                        {
                            "start": 1462,
                            "end": 1465,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1483,
                            "end": 1486,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98193359375
                }
            ],
            "relevance_judgement": 0.98193359375,
            "relevance_judgment_input_expanded": "# Title: Black-box based limited query membership inference attack\n# Venue: IEEE Access\n# Authors: Yu Zhang, Huaping Zhou, Pengyan Wang, Gaoming Yang\n## Abstract\nConventional membership inference attacks usually require a large number of queries of the target model when training shadow models, and this task becomes extremely difficult when the number of queries is limited. Aiming at the problem of insufficient training data for shadow models due to the limited number of queries, we propose a membership inference attack method based on generative adversarial networks (GAN). First, we use generative adversarial networks to augment the samples obtained by a small number of queries to expand the training data of the model; Secondly, we use the improved CNN to obtain shadow models that have a higher degree of fitting on different target model structures; Finally, we evaluate the accuracy of the proposed algorithm on XgBoost, Logistic, and neural network models using public datasets MNIST and CIFAR10 in a black-box setting, and the results show that our model has an average attack accuracy of 62% and 83%, respectively. It can be seen that, compared with the existing research methods, our model can obtain better attack effects under the condition of significantly reducing the number of queries, which shows the feasibility of our proposed method in membership inference attacks.\n## A. MEMBERSHIP INFERENCE ATTACK\nSince Shokri et al. [6] proposed an inference attack against MLaaS, the research on member inference has received extensive attention and has been successfully implemented in many fields. It uses the output information of the model to infer whether a sample exists in the training data set of the model, which seriously threatens the privacy and security of the machine learning model. Membership inference attacks are divided into white-box attacks [11][12][13][14] and blackbox attacks [5,6,15] according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability. In addition, membership inference attacks can also be divided into attacks against independent models [16][17][18][19][20][21] and attacks against Federated Learning [11,22]. \n\nAt present, the research on membership inference attack mainly focuses on the black-box attack of independent models. The attacker uses the model's predicted behavior difference between member data and non-member data to train a binary attack model that recognizes this difference. In order to obtain the training data of the attacking model in the blackbox mode, Shokri [6]  Shokri's method [6] is based on the assumption of extensive query and knowledge of the target model structure. They obtained synthetic data by querying the target model and used this data to train multiple shadow models with the same structure as the target model. In practical applications, a large number of accesses to the target model require high query costs, which largely limits the application scope of membership inference attacks. Therefore, how to construct a shadow model similar to the target model under a small number of queries is the key to solving the hypothesis of membership inference attack.",
            "reference_string": "[248870291 | Zhang et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Improved Membership Inference Attacks Against Language Classification Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 18,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07219",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2593773",
                    "name": "Shlomit Shachor"
                },
                {
                    "authorId": "1994713",
                    "name": "N. Razinkov"
                },
                {
                    "authorId": "2652502",
                    "name": "Abigail Goldsteen"
                }
            ],
            "abstract": "Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.",
            "corpus_id": 263834611,
            "sentences": [
                {
                    "corpus_id": "263834611",
                    "title": "Improved Membership Inference Attacks Against Language Classification Models",
                    "text": "There are several types of privacy (inference) attacks against ML models, including membership inference, attribute inference, model inversion, database reconstruction, and most recently, training data extraction from generative models. The most commonly researched and employed attack is the membership inference attack, with dozens of papers published each year [5], and implementations being made available in open-source privacy assessment frameworks [7], [12]. \n\nMI attacks attempt to distinguish between members, which were part of a target model's training data, and non-members. MI attacks have been extensively studied in the context of classification models and in the black-box setting, where the model internals are unknown to the attacker. The first MI attacks were either threshold-based [17] or employed binary classifiers trained to distinguish between members and non-members based on model outputs [15]. For example, these outputs may include class probabilities or logits (for classification models), the model's loss, and possibly also activations from internal layers of the model (in white-box attacks) [11]. To generate labeled (member/non-member) data to train the attack classifier, without knowledge of the true member samples of the attacked model, shadow models are commonly used [15]. \n\nIn the past few years, investigations have begun into MI in the context of large language models (LLM), starting with embedding models and masked language models [16], [8], [10]. [14] looked at a similar setting as ours, focusing on NLP classification models. They proposed mostly threshold-based attacks, examining different features that can be used to distinguish between members and nonmembers. [6] focused specifically on language models that were fine-tuned for the medical domain, including classification tasks such as MedNLI, employing both black-box and white-box attacks. Their black-box attack applied thresholds to the training error of samples. More recently, Likelihood Ratio Attacks (LiRA) have been proposed [2], which compare target model scores to those obtained from a reference model trained on similar data. [9] tried to relieve the assumption that an adversary has access to samples closely resembling the original training data by utilizing synthetically generated neighbor texts.",
                    "score": 0.5979668777887075,
                    "section_title": "Related Work",
                    "char_start_offset": 4211,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 465
                        },
                        {
                            "start": 468,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1313
                        },
                        {
                            "start": 1316,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1714
                        },
                        {
                            "start": 1715,
                            "end": 1898
                        },
                        {
                            "start": 1899,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2320
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 364,
                            "end": 367,
                            "matchedPaperCorpusId": "232233426"
                        },
                        {
                            "start": 455,
                            "end": 458,
                            "matchedPaperCorpusId": "220647336"
                        },
                        {
                            "start": 802,
                            "end": 806,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 916,
                            "end": 920,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1125,
                            "end": 1129,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1308,
                            "end": 1312,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1478,
                            "end": 1482,
                            "matchedPaperCorpusId": "214743021"
                        },
                        {
                            "start": 1495,
                            "end": 1499,
                            "matchedPaperCorpusId": "245222525"
                        },
                        {
                            "start": 2041,
                            "end": 2044,
                            "matchedPaperCorpusId": "244920593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9814453125
                }
            ],
            "relevance_judgement": 0.9814453125,
            "relevance_judgment_input_expanded": "# Title: Improved Membership Inference Attacks Against Language Classification Models\n# Venue: arXiv.org\n# Authors: Shlomit Shachor, N. Razinkov, Abigail Goldsteen\n## Abstract\nArtificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.\n## Related Work\nThere are several types of privacy (inference) attacks against ML models, including membership inference, attribute inference, model inversion, database reconstruction, and most recently, training data extraction from generative models. The most commonly researched and employed attack is the membership inference attack, with dozens of papers published each year [5], and implementations being made available in open-source privacy assessment frameworks [7], [12]. \n\nMI attacks attempt to distinguish between members, which were part of a target model's training data, and non-members. MI attacks have been extensively studied in the context of classification models and in the black-box setting, where the model internals are unknown to the attacker. The first MI attacks were either threshold-based [17] or employed binary classifiers trained to distinguish between members and non-members based on model outputs [15]. For example, these outputs may include class probabilities or logits (for classification models), the model's loss, and possibly also activations from internal layers of the model (in white-box attacks) [11]. To generate labeled (member/non-member) data to train the attack classifier, without knowledge of the true member samples of the attacked model, shadow models are commonly used [15]. \n\nIn the past few years, investigations have begun into MI in the context of large language models (LLM), starting with embedding models and masked language models [16], [8], [10]. [14] looked at a similar setting as ours, focusing on NLP classification models. They proposed mostly threshold-based attacks, examining different features that can be used to distinguish between members and nonmembers. [6] focused specifically on language models that were fine-tuned for the medical domain, including classification tasks such as MedNLI, employing both black-box and white-box attacks. Their black-box attack applied thresholds to the training error of samples. More recently, Likelihood Ratio Attacks (LiRA) have been proposed [2], which compare target model scores to those obtained from a reference model trained on similar data. [9] tried to relieve the assumption that an adversary has access to samples closely resembling the original training data by utilizing synthetically generated neighbor texts.",
            "reference_string": "[263834611 | Shachor et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.07801",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.07801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47770210",
                    "name": "Derui Zhu"
                },
                {
                    "authorId": "153642281",
                    "name": "Dingfan Chen"
                },
                {
                    "authorId": "1718732",
                    "name": "Jens Grossklags"
                },
                {
                    "authorId": "1739548",
                    "name": "Mario Fritz"
                }
            ],
            "abstract": "In recent years, diffusion models have achieved tremendous success in the field of image generation, becoming the stateof-the-art technology for AI-based image processing applications. Despite the numerous benefits brought by recent advances in diffusion models, there are also concerns about their potential misuse, specifically in terms of privacy breaches and intellectual property infringement. In particular, some of their unique characteristics open up new attack surfaces when considering the real-world deployment of such models. With a thorough investigation of the attack vectors, we develop a systematic analysis of membership inference attacks on diffusion models and propose novel attack methods tailored to each attack scenario specifically relevant to diffusion models. Our approach exploits easily obtainable quantities and is highly effective, achieving near-perfect attack performance (>0.9 AUCROC) in realistic scenarios. Our extensive experiments demonstrate the effectiveness of our method, highlighting the importance of considering privacy and intellectual property risks when using diffusion models in image generation tasks.",
            "corpus_id": 256868849,
            "sentences": [
                {
                    "corpus_id": "256868849",
                    "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
                    "text": "Membership Inference Attack (MIA) was first introduced by Shokri et al. [35]. It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box [25], [28] as well as black-box [35], [31], [42], [33], [37] classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members [42]. Sablayrolles et al. [31] further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution. Our approach is built on top of these findings and is specifically tailored for diffusion models by carefully examining their training objectives and potential attack surface during development and deployment. \n\nRecent works have explore such attacks for popular generative models such as GANs [11], [4] and VAEs [13]. Specifically, Hayes et al. [11] observe that disclosing the discriminator in a GAN can result in leaked membership information in a whitebox setting and suggested using a shadow model for black-box attacks. Hilprecht et al. [13] proposed using the reconstruction error as a membership score for attacking white-box VAEs and counting generated samples within an \u03f5-ball of the query for a black-box membership score. Chen et al. [4] presented a taxonomy of MIAs against GANs and proposed an optimizationbased approach for attacks with only generator access and a distance-based approach for the black-box setting with only synthetic samples available. \n\nOur work presents the first systematic analysis of MIAs on diffusion models.",
                    "score": 0.7314897864135534,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 8398,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 77
                        },
                        {
                            "start": 78,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1313
                        },
                        {
                            "start": 1316,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1629
                        },
                        {
                            "start": 1630,
                            "end": 1837
                        },
                        {
                            "start": 1838,
                            "end": 2072
                        },
                        {
                            "start": 2075,
                            "end": 2151
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 72,
                            "end": 76,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 372,
                            "end": 376,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 404,
                            "end": 408,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 410,
                            "end": 414,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 416,
                            "end": 420,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 422,
                            "end": 426,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 428,
                            "end": 432,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 923,
                            "end": 927,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 949,
                            "end": 953,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 1398,
                            "end": 1402,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1404,
                            "end": 1407,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 1417,
                            "end": 1421,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1450,
                            "end": 1454,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1647,
                            "end": 1651,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1850,
                            "end": 1853,
                            "matchedPaperCorpusId": "221203089"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy\n# Venue: arXiv.org\n# Authors: Derui Zhu, Dingfan Chen, Jens Grossklags, Mario Fritz\n## Abstract\nIn recent years, diffusion models have achieved tremendous success in the field of image generation, becoming the stateof-the-art technology for AI-based image processing applications. Despite the numerous benefits brought by recent advances in diffusion models, there are also concerns about their potential misuse, specifically in terms of privacy breaches and intellectual property infringement. In particular, some of their unique characteristics open up new attack surfaces when considering the real-world deployment of such models. With a thorough investigation of the attack vectors, we develop a systematic analysis of membership inference attacks on diffusion models and propose novel attack methods tailored to each attack scenario specifically relevant to diffusion models. Our approach exploits easily obtainable quantities and is highly effective, achieving near-perfect attack performance (>0.9 AUCROC) in realistic scenarios. Our extensive experiments demonstrate the effectiveness of our method, highlighting the importance of considering privacy and intellectual property risks when using diffusion models in image generation tasks.\n## II. RELATED WORK\nMembership Inference Attack (MIA) was first introduced by Shokri et al. [35]. It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box [25], [28] as well as black-box [35], [31], [42], [33], [37] classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members [42]. Sablayrolles et al. [31] further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution. Our approach is built on top of these findings and is specifically tailored for diffusion models by carefully examining their training objectives and potential attack surface during development and deployment. \n\nRecent works have explore such attacks for popular generative models such as GANs [11], [4] and VAEs [13]. Specifically, Hayes et al. [11] observe that disclosing the discriminator in a GAN can result in leaked membership information in a whitebox setting and suggested using a shadow model for black-box attacks. Hilprecht et al. [13] proposed using the reconstruction error as a membership score for attacking white-box VAEs and counting generated samples within an \u03f5-ball of the query for a black-box membership score. Chen et al. [4] presented a taxonomy of MIAs against GANs and proposed an optimizationbased approach for attacks with only generator access and a distance-based approach for the black-box setting with only synthetic samples available. \n\nOur work presents the first systematic analysis of MIAs on diffusion models.",
            "reference_string": "[256868849 | Zhu et al. | 2023 | Citations: 14]"
        },
        {
            "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
            "venue": "European Conference on Computer Vision",
            "year": 2022,
            "reference_count": 39,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12535",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2110279247",
                    "name": "Hongbin Liu"
                },
                {
                    "authorId": "144516687",
                    "name": "N. Gong"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Semi-supervised learning (SSL) leverages both labeled and unlabeled data to train machine learning (ML) models. State-of-the-art SSL methods can achieve comparable performance to supervised learning by leveraging much fewer labeled data. However, most existing works focus on improving the performance of SSL. In this work, we take a different angle by studying the training data privacy of SSL. Specifically, we propose the first data augmentation-based membership inference attacks against ML models trained by SSL. Given a data sample and the black-box access to a model, the goal of membership inference attack is to determine whether the data sample belongs to the training dataset of the model. Our evaluation shows that the proposed attack can consistently outperform existing membership inference attacks and achieves the best performance against the model trained by SSL. Moreover, we uncover that the reason for membership leakage in SSL is different from the commonly believed one in supervised learning, i.e., overfitting (the gap between training and testing accuracy). We observe that the SSL model is well generalized to the testing data (with almost 0 overfitting) but ''memorizes'' the training data by giving a more confident prediction regardless of its correctness. We also explore early stopping as a countermeasure to prevent membership inference attacks against SSL. The results show that early stopping can mitigate the membership inference attack, but with the cost of model's utility degradation.",
            "corpus_id": 251066729,
            "sentences": [
                {
                    "corpus_id": "251066729",
                    "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
                    "text": "Membership inference attack [28,27,22,25,5,11,26,12,20,10,31,9,19,34,4] aims to determine whether a given data sample is used to train a target model. Multiple works studied the membership inference attacks against the supervised learning [28,27,23,19,6,9]. Shokri et al. [28] proposed the first black-box membership inference attack against machine learning models by leveraging multiple shadow models and attack models. The attack model takes a sample's posteriors generated from the target model as the input and predicts whether it is a member or not. Salem et al. [27] relaxed the assumption from Shokri et al. [28] and proposed novel model-independent and dataset-independent membership inference attacks. Nasr et al. [23] studied the white-box membership inference attacks in both centralized and federated learning settings. Li and Zhang [19] and Choo et al. [6] concentrated on a more restricted attack scenario (called label-only attack) where the target model only returns the predicted labels instead of posteriors when the adversary queries the target model with given samples. Roughly speaking, their proposed label-only attacks aim to infer a given sample's membership status via comparing a pre-defined threshold with the scale of adversarial perturbation that needs to be added to the given sample to change the target model's predicted label. However, these membership inference attacks are tailored to supervised learning and we show that semi-supervised learning is more vulnerable to our proposed data augmentation-based membership inference attack compared with existing membership inference attacks.",
                    "score": 0.7270841987883888,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 8022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1622
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 32,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 32,
                            "end": 35,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 35,
                            "end": 38,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 38,
                            "end": 41,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 41,
                            "end": 43,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 43,
                            "end": 46,
                            "matchedPaperCorpusId": "209439842"
                        },
                        {
                            "start": 46,
                            "end": 49,
                            "matchedPaperCorpusId": "211201635"
                        },
                        {
                            "start": 49,
                            "end": 52,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 52,
                            "end": 55,
                            "matchedPaperCorpusId": "237290083"
                        },
                        {
                            "start": 55,
                            "end": 58,
                            "matchedPaperCorpusId": "231846491"
                        },
                        {
                            "start": 58,
                            "end": 61,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 63,
                            "end": 66,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 239,
                            "end": 243,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 243,
                            "end": 246,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 246,
                            "end": 249,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 249,
                            "end": 252,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 252,
                            "end": 254,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 272,
                            "end": 276,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 569,
                            "end": 573,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 616,
                            "end": 620,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 724,
                            "end": 728,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 846,
                            "end": 850,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 867,
                            "end": 870,
                            "matchedPaperCorpusId": "220831381"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning\n# Venue: European Conference on Computer Vision\n# Authors: Xinlei He, Hongbin Liu, N. Gong, Yang Zhang\n## Abstract\nSemi-supervised learning (SSL) leverages both labeled and unlabeled data to train machine learning (ML) models. State-of-the-art SSL methods can achieve comparable performance to supervised learning by leveraging much fewer labeled data. However, most existing works focus on improving the performance of SSL. In this work, we take a different angle by studying the training data privacy of SSL. Specifically, we propose the first data augmentation-based membership inference attacks against ML models trained by SSL. Given a data sample and the black-box access to a model, the goal of membership inference attack is to determine whether the data sample belongs to the training dataset of the model. Our evaluation shows that the proposed attack can consistently outperform existing membership inference attacks and achieves the best performance against the model trained by SSL. Moreover, we uncover that the reason for membership leakage in SSL is different from the commonly believed one in supervised learning, i.e., overfitting (the gap between training and testing accuracy). We observe that the SSL model is well generalized to the testing data (with almost 0 overfitting) but ''memorizes'' the training data by giving a more confident prediction regardless of its correctness. We also explore early stopping as a countermeasure to prevent membership inference attacks against SSL. The results show that early stopping can mitigate the membership inference attack, but with the cost of model's utility degradation.\n## Membership Inference Attacks\nMembership inference attack [28,27,22,25,5,11,26,12,20,10,31,9,19,34,4] aims to determine whether a given data sample is used to train a target model. Multiple works studied the membership inference attacks against the supervised learning [28,27,23,19,6,9]. Shokri et al. [28] proposed the first black-box membership inference attack against machine learning models by leveraging multiple shadow models and attack models. The attack model takes a sample's posteriors generated from the target model as the input and predicts whether it is a member or not. Salem et al. [27] relaxed the assumption from Shokri et al. [28] and proposed novel model-independent and dataset-independent membership inference attacks. Nasr et al. [23] studied the white-box membership inference attacks in both centralized and federated learning settings. Li and Zhang [19] and Choo et al. [6] concentrated on a more restricted attack scenario (called label-only attack) where the target model only returns the predicted labels instead of posteriors when the adversary queries the target model with given samples. Roughly speaking, their proposed label-only attacks aim to infer a given sample's membership status via comparing a pre-defined threshold with the scale of adversarial perturbation that needs to be added to the given sample to change the target model's predicted label. However, these membership inference attacks are tailored to supervised learning and we show that semi-supervised learning is more vulnerable to our proposed data augmentation-based membership inference attack compared with existing membership inference attacks.",
            "reference_string": "[251066729 | He et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Scalable Membership Inference Attacks via Quantile Regression",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 50,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.03694",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03694, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "37335063",
                    "name": "Mart\u00edn Bertr\u00e1n"
                },
                {
                    "authorId": "46321498",
                    "name": "Shuai Tang"
                },
                {
                    "authorId": "81338045",
                    "name": "Michael Kearns"
                },
                {
                    "authorId": "144848816",
                    "name": "Jamie Morgenstern"
                },
                {
                    "authorId": "1682008",
                    "name": "Aaron Roth"
                },
                {
                    "authorId": "1768074",
                    "name": "Zhiwei Steven Wu"
                }
            ],
            "abstract": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.",
            "corpus_id": 259375769,
            "sentences": [
                {
                    "corpus_id": "259375769",
                    "title": "Scalable Membership Inference Attacks via Quantile Regression",
                    "text": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.",
                    "score": 0.6596820336507974,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Scalable Membership Inference Attacks via Quantile Regression\n# Venue: Neural Information Processing Systems\n# Authors: Mart\u00edn Bertr\u00e1n, Shuai Tang, Michael Kearns, Jamie Morgenstern, Aaron Roth, Zhiwei Steven Wu\n## Abstract\nMembership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.\n",
            "reference_string": "[259375769 | Bertran et al. | 2023 | Citations: 50]"
        },
        {
            "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 55,
            "citation_count": 98,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.05429, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "2054749404",
                    "name": "Rui Wen"
                },
                {
                    "authorId": "2127727861",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2117688523",
                    "name": "Yun Shen"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Many real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.",
            "corpus_id": 231861713,
            "sentences": [
                {
                    "corpus_id": "231861713",
                    "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
                    "text": "Membership Inference Attack. Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39,51]. Most of the existing attacks focus on deep learning models that are trained on sensitive data from the Euclidean space, such as images and texts. Shokri et al. [36] propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack. Salem et al. [35] further relax several key assumptions from [36], such as knowledge of the target model architecture, shadow dataset from the same distribution. Yeom et al. [51] discuss the relationship between overfitting and membership attacks. Nasr et al. [31] conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings. To mitigate the attacks, some defense mechanisms [22,30,35,36] have been proposed. Those strategies include using model stacking [35], dropout [35], adversarial training [30], jointly maximize privacy and prediction accuracy [22], etc.\n\nOther Exploratory Attacks Against ML Models. Besides membership inference, other exploratory attacks such as model inversion, attribute inference, and model stealing have been studied by many researchers. In model inversion attacks [16,17,52], an adversary aims to reconstruct input samples from a target ML model, i.e., model inversion enables the adversary to directly learn information about the training dataset. Fredrikson et al. [17] first propose a model inversion attack in the setting of drug dose classification. Later, Fredrikson et al. [16] further extend model inversion to general ML settings relying on back-propagation. More recently, Zhang et al. [52] develop a more advanced attack based on GANs to synthesize the training dataset.\n\nAttribute inference attacks [29,38] aim to infer some general properties of the training dataset. Meils et al",
                    "score": 0.7807697725380419,
                    "section_title": "Related Work",
                    "char_start_offset": 35696,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "170076423"
                        },
                        {
                            "start": 215,
                            "end": 218,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 218,
                            "end": 221,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 221,
                            "end": 224,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 224,
                            "end": 227,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "203593891"
                        },
                        {
                            "start": 230,
                            "end": 233,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 395,
                            "end": 399,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 786,
                            "end": 790,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 834,
                            "end": 838,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 947,
                            "end": 951,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1033,
                            "end": 1037,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1191,
                            "end": 1195,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1195,
                            "end": 1198,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1198,
                            "end": 1201,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1201,
                            "end": 1204,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1271,
                            "end": 1275,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1285,
                            "end": 1289,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1312,
                            "end": 1316,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1367,
                            "end": 1371,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1611,
                            "end": 1615,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 1615,
                            "end": 1618,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1618,
                            "end": 1621,
                            "matchedPaperCorpusId": "208139345"
                        },
                        {
                            "start": 1814,
                            "end": 1818,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1927,
                            "end": 1931,
                            "matchedPaperCorpusId": "207229839"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9794921875
                }
            ],
            "relevance_judgement": 0.9794921875,
            "relevance_judgment_input_expanded": "# Title: Node-Level Membership Inference Attacks Against Graph Neural Networks\n# Venue: arXiv.org\n# Authors: Xinlei He, Rui Wen, Yixin Wu, M. Backes, Yun Shen, Yang Zhang\n## Abstract\nMany real-world data comes in the form of graphs, such as social networks and protein structure. To fully utilize the information contained in graph data, a new family of machine learning (ML) models, namely graph neural networks (GNNs), has been introduced. Previous studies have shown that machine learning models are vulnerable to privacy attacks. However, most of the current efforts concentrate on ML models trained on data from the Euclidean space, like images and texts. On the other hand, privacy risks stemming from GNNs remain largely unstudied. In this paper, we fill the gap by performing the first comprehensive analysis of node-level membership inference attacks against GNNs. We systematically define the threat models and propose three node-level membership inference attacks based on an adversary's background knowledge. Our evaluation on three GNN structures and four benchmark datasets shows that GNNs are vulnerable to node-level membership inference even when the adversary has minimal background knowledge. Besides, we show that graph density and feature similarity have a major impact on the attack's success. We further investigate two defense mechanisms and the empirical results indicate that these defenses can reduce the attack performance but with moderate utility loss.\n## Related Work\nMembership Inference Attack. Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39,51]. Most of the existing attacks focus on deep learning models that are trained on sensitive data from the Euclidean space, such as images and texts. Shokri et al. [36] propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack. Salem et al. [35] further relax several key assumptions from [36], such as knowledge of the target model architecture, shadow dataset from the same distribution. Yeom et al. [51] discuss the relationship between overfitting and membership attacks. Nasr et al. [31] conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings. To mitigate the attacks, some defense mechanisms [22,30,35,36] have been proposed. Those strategies include using model stacking [35], dropout [35], adversarial training [30], jointly maximize privacy and prediction accuracy [22], etc.\n\nOther Exploratory Attacks Against ML Models. Besides membership inference, other exploratory attacks such as model inversion, attribute inference, and model stealing have been studied by many researchers. In model inversion attacks [16,17,52], an adversary aims to reconstruct input samples from a target ML model, i.e., model inversion enables the adversary to directly learn information about the training dataset. Fredrikson et al. [17] first propose a model inversion attack in the setting of drug dose classification. Later, Fredrikson et al. [16] further extend model inversion to general ML settings relying on back-propagation. More recently, Zhang et al. [52] develop a more advanced attack based on GANs to synthesize the training dataset.\n\nAttribute inference attacks [29,38] aim to infer some general properties of the training dataset. Meils et al",
            "reference_string": "[231861713 | He et al. | 2021 | Citations: 98]"
        },
        {
            "title": "Membership Leakage in Label-Only Exposures",
            "venue": "Conference on Computer and Communications Security",
            "year": 2020,
            "reference_count": 58,
            "citation_count": 246,
            "influential_citation_count": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2007.15528",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.15528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146247989",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
            "corpus_id": 237563320,
            "sentences": [
                {
                    "corpus_id": "237563320",
                    "title": "Membership Leakage in Label-Only Exposures",
                    "text": "Various research has shown that machine learning models are vulnerable to security and privacy attacks. In this section, we mainly survey the domains that are most relevant to us. \n\nMembership Inference. Membership inference attack has been successfully performed in various data domains, ranging form biomedical data [6,22,24] to mobility traces [43]. Shokri et al. [48] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. [46] later present another attack by gradually relaxing the assumptions made by Shokri et al. [48] achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks [25,31,35,49,57]. In the area of decision-based attacks, Yeom et al. [57] quantitatively analyzed the relationship between attack performance and loss for training and testing sets, and proposed the first decision-based attack, i.e., baseline attack aforementioned. We also acknowledge that a concurrent work [13] proposes an approach similar to our boundary attack. Specifically, the concurrent work assumes that an adversary has more knowledge of the target model, including training knowledge (model architecture, training algorithm, and training dataset size), and a shadow dataset from the same distribution as the target dataset to estimate the threshold. In our work, we relax all assumptions and propose a general threshold-choosing method. We further present a new perspective on the reasons for the success of membership inference. In addition, we introduce a novel transfer-attack. \n\nDefenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different types of generalization enhancement. For example, Shokri et al. [48] adopted L2 regularization with a polynomial in the model's loss function to penalize large parameters. Salem et al. [46] demonstrated two effective method of defending MI attacks, namely dropout and model stacking.",
                    "score": 0.714355853192282,
                    "section_title": "Related Works",
                    "char_start_offset": 38139,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 179
                        },
                        {
                            "start": 182,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1828
                        },
                        {
                            "start": 1829,
                            "end": 1879
                        },
                        {
                            "start": 1882,
                            "end": 1920
                        },
                        {
                            "start": 1921,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2180
                        },
                        {
                            "start": 2181,
                            "end": 2292
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 318,
                            "end": 321,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 321,
                            "end": 324,
                            "matchedPaperCorpusId": "52205573"
                        },
                        {
                            "start": 324,
                            "end": 327,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 347,
                            "end": 351,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 367,
                            "end": 371,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 738,
                            "end": 742,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 832,
                            "end": 836,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 987,
                            "end": 991,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 991,
                            "end": 994,
                            "matchedPaperCorpusId": "211532510"
                        },
                        {
                            "start": 997,
                            "end": 1000,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 1000,
                            "end": 1003,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1056,
                            "end": 1060,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 2073,
                            "end": 2077,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                },
                {
                    "corpus_id": "237563320",
                    "title": "Membership Leakage in Label-Only Exposures",
                    "text": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "score": 0.6211322742444431,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.978515625
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: Membership Leakage in Label-Only Exposures\n# Venue: Conference on Computer and Communications Security\n# Authors: Zheng Li, Yang Zhang\n## Abstract\nMachine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.\n## Related Works\nVarious research has shown that machine learning models are vulnerable to security and privacy attacks. In this section, we mainly survey the domains that are most relevant to us. \n\nMembership Inference. Membership inference attack has been successfully performed in various data domains, ranging form biomedical data [6,22,24] to mobility traces [43]. Shokri et al. [48] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. [46] later present another attack by gradually relaxing the assumptions made by Shokri et al. [48] achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks [25,31,35,49,57]. In the area of decision-based attacks, Yeom et al. [57] quantitatively analyzed the relationship between attack performance and loss for training and testing sets, and proposed the first decision-based attack, i.e., baseline attack aforementioned. We also acknowledge that a concurrent work [13] proposes an approach similar to our boundary attack. Specifically, the concurrent work assumes that an adversary has more knowledge of the target model, including training knowledge (model architecture, training algorithm, and training dataset size), and a shadow dataset from the same distribution as the target dataset to estimate the threshold. In our work, we relax all assumptions and propose a general threshold-choosing method. We further present a new perspective on the reasons for the success of membership inference. In addition, we introduce a novel transfer-attack. \n\nDefenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different types of generalization enhancement. For example, Shokri et al. [48] adopted L2 regularization with a polynomial in the model's loss function to penalize large parameters. Salem et al. [46] demonstrated two effective method of defending MI attacks, namely dropout and model stacking.",
            "reference_string": "[237563320 | Li et al. | 2020 | Citations: 246]"
        },
        {
            "title": "Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.14296, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2147200564",
                    "name": "Yao Wan"
                },
                {
                    "authorId": "2297784945",
                    "name": "Guanghua Wan"
                },
                {
                    "authorId": "2187852313",
                    "name": "Shijie Zhang"
                },
                {
                    "authorId": "2273385018",
                    "name": "Hongyu Zhang"
                },
                {
                    "authorId": "2290125814",
                    "name": "Pan Zhou"
                },
                {
                    "authorId": "2277587874",
                    "name": "Hai Jin"
                },
                {
                    "authorId": "2243348413",
                    "name": "Lichao Sun"
                }
            ],
            "abstract": "Recent years have witnessed significant progress in developing deep learning-based models for automated code completion. Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement. In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model? To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion. In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior. The acquired posteriors from these shadow models are subsequently employed to train a membership classifier. Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model. We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder). Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively. Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving ampler space for further improvement. Finally, we also try to explain the findings from the perspective of model memorization.",
            "corpus_id": 269293603,
            "sentences": [
                {
                    "corpus_id": "269293603",
                    "title": "Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach",
                    "text": "Membership inference attacks (MIAs) against machine learning models endeavor to discern whether a given data sample was utilized in the training of a target model or not. Membership inference methods can be broadly divided into two categories, i.e., shadow model training and metric-based techniques [17]. Shadow model training [42] aims to train a binary attack classifier by creating multiple shadow models to mimic the behavior of the target model. Metric-based techniques [40,44,57] proposed to infer the membership of a given data record by comparing the metric value (e.g., prediction loss [57]) calculated on the prediction vector of the given record to a preset threshold. Since both techniques are formulated from the attackers' standpoint, they assume the presence of abundant prior information, including knowledge of training data distribution and target model architectures. This assumption renders the direct application of these techniques unfeasible for data owners aiming to implement membership inference. Recently, Hu et al. [16] leveraged backdoor techniques to allow the data owner to achieve membership inference effectively without such prior information. Their approach enables individuals to carry out membership inference solely by utilizing black-box query access to the target model. One related work to ours is [55], which also investigates the membership leakage risks of code models through membership inference and has been released as a preprint. However, our work diverges from this study in three key aspects. First, in experiments, we extend the scope beyond small code models, delving into the feasibility of implementing membership inference on LLMs such as CodeGen and StarCoder to safeguard code intellectual property. Second, instead of relying on prediction probabilities, we utilize the distribution of ranks of ground truth in the model output as the feature for the membership classifier to learn. Third, we conduct a comprehensive analysis to scrutinize the factors contributing to the effectiveness of our membership inference method.",
                    "score": 0.589496949205247,
                    "section_title": "Membership Inference",
                    "char_start_offset": 48445,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1311
                        },
                        {
                            "start": 1312,
                            "end": 1479
                        },
                        {
                            "start": 1480,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1758
                        },
                        {
                            "start": 1759,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 2081
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 300,
                            "end": 304,
                            "matchedPaperCorpusId": "232233426"
                        },
                        {
                            "start": 328,
                            "end": 332,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 476,
                            "end": 480,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 480,
                            "end": 483,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 483,
                            "end": 486,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1044,
                            "end": 1048,
                            "matchedPaperCorpusId": "249605407"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach\n# Venue: arXiv.org\n# Authors: Yao Wan, Guanghua Wan, Shijie Zhang, Hongyu Zhang, Pan Zhou, Hai Jin, Lichao Sun\n## Abstract\nRecent years have witnessed significant progress in developing deep learning-based models for automated code completion. Although using source code in GitHub has been a common practice for training deep-learning-based models for code completion, it may induce some legal and ethical issues such as copyright infringement. In this paper, we investigate the legal and ethical issues of current neural code completion models by answering the following question: Is my code used to train your neural code completion model? To this end, we tailor a membership inference approach (termed CodeMI) that was originally crafted for classification tasks to a more challenging task of code completion. In particular, since the target code completion models perform as opaque black boxes, preventing access to their training data and parameters, we opt to train multiple shadow models to mimic their behavior. The acquired posteriors from these shadow models are subsequently employed to train a membership classifier. Subsequently, the membership classifier can be effectively employed to deduce the membership status of a given code sample based on the output of a target code completion model. We comprehensively evaluate the effectiveness of this adapted approach across a diverse array of neural code completion models, (i.e., LSTM-based, CodeGPT, CodeGen, and StarCoder). Experimental results reveal that the LSTM-based and CodeGPT models suffer the membership leakage issue, which can be easily detected by our proposed membership inference approach with an accuracy of 0.842, and 0.730, respectively. Interestingly, our experiments also show that the data membership of current large language models of code, e.g., CodeGen and StarCoder, is difficult to detect, leaving ampler space for further improvement. Finally, we also try to explain the findings from the perspective of model memorization.\n## Membership Inference\nMembership inference attacks (MIAs) against machine learning models endeavor to discern whether a given data sample was utilized in the training of a target model or not. Membership inference methods can be broadly divided into two categories, i.e., shadow model training and metric-based techniques [17]. Shadow model training [42] aims to train a binary attack classifier by creating multiple shadow models to mimic the behavior of the target model. Metric-based techniques [40,44,57] proposed to infer the membership of a given data record by comparing the metric value (e.g., prediction loss [57]) calculated on the prediction vector of the given record to a preset threshold. Since both techniques are formulated from the attackers' standpoint, they assume the presence of abundant prior information, including knowledge of training data distribution and target model architectures. This assumption renders the direct application of these techniques unfeasible for data owners aiming to implement membership inference. Recently, Hu et al. [16] leveraged backdoor techniques to allow the data owner to achieve membership inference effectively without such prior information. Their approach enables individuals to carry out membership inference solely by utilizing black-box query access to the target model. One related work to ours is [55], which also investigates the membership leakage risks of code models through membership inference and has been released as a preprint. However, our work diverges from this study in three key aspects. First, in experiments, we extend the scope beyond small code models, delving into the feasibility of implementing membership inference on LLMs such as CodeGen and StarCoder to safeguard code intellectual property. Second, instead of relying on prediction probabilities, we utilize the distribution of ranks of ground truth in the model output as the feature for the membership classifier to learn. Third, we conduct a comprehensive analysis to scrutinize the factors contributing to the effectiveness of our membership inference method.",
            "reference_string": "[269293603 | Wan et al. | 2024 | Citations: 3]"
        },
        {
            "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
            "venue": "Conference on Computer and Communications Security",
            "year": 2019,
            "reference_count": 76,
            "citation_count": 394,
            "influential_citation_count": 54,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3319535.3363201",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.10594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40661332",
                    "name": "Jinyuan Jia"
                },
                {
                    "authorId": "144664869",
                    "name": "Ahmed Salem"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "144516687",
                    "name": "N. Gong"
                }
            ],
            "abstract": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
            "corpus_id": 202734167,
            "sentences": [
                {
                    "corpus_id": "202734167",
                    "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
                    "text": "56] recently proposed new membership inference attacks for black-box target classifiers, which relax the assumptions of the attacks proposed by Shokri et al. from both model and data angles. For instance, they showed that the attacker can rank the entries in a confidence score vector before feeding it into an attack classifier, which improves the attack effectiveness. Moreover, they showed that it is sufficient for the attacker to train just one shadow classifier. These results indicate that membership inference threat is even larger than previously thought.\n\nMore recently, Nasr et al. [43] proposed membership inference attacks against white-box ML models. For a data sample, they calculate the corresponding gradients over the white-box target classifier's parameters and use these gradients as the data sample's feature for membership inference. Moreover, both Nasr et al. [43] and Melis et al. [36] proposed membership inference attacks against federated learning. While most of the previous works concentrated on classification models [33,34,42,43,56,58,69], Hayes et al. [21] studied membership inference against generative models, in particular generative adversarial networks (GANs) [18]. They designed attacks for both white-and black-box settings. Their results showed that generative models are also vulnerable to membership inference.\n\nDefense mechanisms against membership inference: Multiple defense mechanisms have been proposed to mitigate the threat of membership inference in the ML setting. We summarize them as the following.\n\nL 2 -Regularizer [58]. Overfitting, i.e., ML classifiers are more confident when facing data samples they are trained on (members) than others, is one major reason why membership inference is effective. Therefore, to defend against membership inference, people have explored to reduce overfitting using regularization. For instance, Shokri et al. [58] explored using conventional L 2 regularizer when training the target classifier.\n\nMin-Max Game [42]. Nasr et al. [42] proposed a min-max game-theoretic method to train a target classifier. Specifically, the method formulates a min-max optimization problem that aims to minimize the target classifier's prediction loss while maximizing the membership privacy. This formulation is equivalent to adding a new",
                    "score": 0.7881469309046502,
                    "section_title": "RELATED WORK 2.1 Membership Inference",
                    "char_start_offset": 9714,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 593,
                            "end": 597,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 883,
                            "end": 887,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 905,
                            "end": 909,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1054,
                            "end": 1057,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1057,
                            "end": 1060,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1060,
                            "end": 1063,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1063,
                            "end": 1066,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1066,
                            "end": 1069,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1084,
                            "end": 1088,
                            "matchedPaperCorpusId": "3052834"
                        },
                        {
                            "start": 1198,
                            "end": 1202,
                            "matchedPaperCorpusId": "10319744"
                        },
                        {
                            "start": 1571,
                            "end": 1575,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1901,
                            "end": 1905,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 2001,
                            "end": 2005,
                            "matchedPaperCorpusId": "49863840"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.978515625
                },
                {
                    "corpus_id": "202734167",
                    "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
                    "text": "Machine learning (ML) is transforming many aspects of our society. We consider a model provider deploys an ML classifier (called target classifier) as a black-box software or service, which returns a confidence score vector for a query data sample from a user. The confidence score vector is a probability distribution over the possible labels and the label of the query data sample is predicted as the one that has the largest confidence score. Multiple studies have shown that such black-box ML classifier is vulnerable to membership inference attacks [43,56,58,59]. Specifically, an attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts whether the data sample is a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to ML. In particular, in application scenarios where the training dataset is sensitive (e.g., biomedical records and location traces), successful membership inference leads to severe privacy violations. For instance, if an attacker knows her victim's data is used to train a medical diagnosis classifier, then the attacker can directly infer the victim's health status. Beyond privacy, membership inference also damages the model provider's intellectual property of the training dataset as collecting and labeling the training dataset may require lots of resources.\n\nTherefore, defending against membership inference attacks is an urgent research problem and multiple defenses [42,56,58] have been explored. A major reason why membership inference attacks succeed is that the target classifier is overfitted. As a result, the confidence score vectors predicted by the target classifier are distinguishable for members and non-members of the training dataset. Therefore, state-of-the-art defenses [42,56,58] essentially regularize the training process of the target classifier to reduce overfitting and the gaps of the confidence score vectors between members and non-members of the training dataset. For instance, L 2 regularization [58], min-max game based adversarial regularization [42], and dropout [56] have been explored to regularize the target classifier. Another line of defenses [1,6,12,24,30,60,66,70] leverage differential privacy",
                    "score": 0.6409913590027976,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 554,
                            "end": 558,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 558,
                            "end": 561,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 561,
                            "end": 564,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 564,
                            "end": 567,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 1577,
                            "end": 1581,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1581,
                            "end": 1584,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1584,
                            "end": 1587,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1896,
                            "end": 1900,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1900,
                            "end": 1903,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1903,
                            "end": 1906,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                }
            ],
            "relevance_judgement": 0.978515625,
            "relevance_judgment_input_expanded": "# Title: MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples\n# Venue: Conference on Computer and Communications Security\n# Authors: Jinyuan Jia, Ahmed Salem, M. Backes, Yang Zhang, N. Gong\n## Abstract\nIn a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.\n## INTRODUCTION\nMachine learning (ML) is transforming many aspects of our society. We consider a model provider deploys an ML classifier (called target classifier) as a black-box software or service, which returns a confidence score vector for a query data sample from a user. The confidence score vector is a probability distribution over the possible labels and the label of the query data sample is predicted as the one that has the largest confidence score. Multiple studies have shown that such black-box ML classifier is vulnerable to membership inference attacks [43,56,58,59]. Specifically, an attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts whether the data sample is a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to ML. In particular, in application scenarios where the training dataset is sensitive (e.g., biomedical records and location traces), successful membership inference leads to severe privacy violations. For instance, if an attacker knows her victim's data is used to train a medical diagnosis classifier, then the attacker can directly infer the victim's health status. Beyond privacy, membership inference also damages the model provider's intellectual property of the training dataset as collecting and labeling the training dataset may require lots of resources.\n\nTherefore, defending against membership inference attacks is an urgent research problem and multiple defenses [42,56,58] have been explored. A major reason why membership inference attacks succeed is that the target classifier is overfitted. As a result, the confidence score vectors predicted by the target classifier are distinguishable for members and non-members of the training dataset. Therefore, state-of-the-art defenses [42,56,58] essentially regularize the training process of the target classifier to reduce overfitting and the gaps of the confidence score vectors between members and non-members of the training dataset. For instance, L 2 regularization [58], min-max game based adversarial regularization [42], and dropout [56] have been explored to regularize the target classifier. Another line of defenses [1,6,12,24,30,60,66,70] leverage differential privacy\n\n## RELATED WORK 2.1 Membership Inference\n56] recently proposed new membership inference attacks for black-box target classifiers, which relax the assumptions of the attacks proposed by Shokri et al. from both model and data angles. For instance, they showed that the attacker can rank the entries in a confidence score vector before feeding it into an attack classifier, which improves the attack effectiveness. Moreover, they showed that it is sufficient for the attacker to train just one shadow classifier. These results indicate that membership inference threat is even larger than previously thought.\n\nMore recently, Nasr et al. [43] proposed membership inference attacks against white-box ML models. For a data sample, they calculate the corresponding gradients over the white-box target classifier's parameters and use these gradients as the data sample's feature for membership inference. Moreover, both Nasr et al. [43] and Melis et al. [36] proposed membership inference attacks against federated learning. While most of the previous works concentrated on classification models [33,34,42,43,56,58,69], Hayes et al. [21] studied membership inference against generative models, in particular generative adversarial networks (GANs) [18]. They designed attacks for both white-and black-box settings. Their results showed that generative models are also vulnerable to membership inference.\n\nDefense mechanisms against membership inference: Multiple defense mechanisms have been proposed to mitigate the threat of membership inference in the ML setting. We summarize them as the following.\n\nL 2 -Regularizer [58]. Overfitting, i.e., ML classifiers are more confident when facing data samples they are trained on (members) than others, is one major reason why membership inference is effective. Therefore, to defend against membership inference, people have explored to reduce overfitting using regularization. For instance, Shokri et al. [58] explored using conventional L 2 regularizer when training the target classifier.\n\nMin-Max Game [42]. Nasr et al. [42] proposed a min-max game-theoretic method to train a target classifier. Specifically, the method formulates a min-max optimization problem that aims to minimize the target classifier's prediction loss while maximizing the membership privacy. This formulation is equivalent to adding a new",
            "reference_string": "[202734167 | Jia et al. | 2019 | Citations: 394]"
        },
        {
            "title": "SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 68,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.06123",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.06123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151490917",
                    "name": "Jun Niu"
                },
                {
                    "authorId": "2170959378",
                    "name": "Xiaoyan Zhu"
                },
                {
                    "authorId": "2223133832",
                    "name": "Moxuan Zeng"
                },
                {
                    "authorId": "92852682",
                    "name": "Ge-ming Zhang"
                },
                {
                    "authorId": "2118377887",
                    "name": "Qingyang Zhao"
                },
                {
                    "authorId": "2111201249",
                    "name": "Chu-Chun Huang"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2223330895",
                    "name": "Suyu An"
                },
                {
                    "authorId": "2223107906",
                    "name": "Yangzhong Wang"
                },
                {
                    "authorId": "2223143616",
                    "name": "Xinghui Yue"
                },
                {
                    "authorId": "2109965982",
                    "name": "Zhipeng He"
                },
                {
                    "authorId": "2300410768",
                    "name": "Weihao Guo"
                },
                {
                    "authorId": "2223125266",
                    "name": "Kuo Shen"
                },
                {
                    "authorId": "145779142",
                    "name": "Peng Liu"
                },
                {
                    "authorId": "1885653",
                    "name": "Yulong Shen"
                },
                {
                    "authorId": "7325491",
                    "name": "Xiaohong Jiang"
                },
                {
                    "authorId": "2157314030",
                    "name": "Jianfeng Ma"
                },
                {
                    "authorId": "2145068832",
                    "name": "Yuqing Zhang"
                }
            ],
            "abstract": "Membership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model. However, it has been increasingly recognized that the\"comparing different MI attacks\"methodology used in the existing works has serious limitations. Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading. In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios. And we design the evaluation scenarios from four perspectives: the distance distribution of data samples in the target dataset, the distance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences by an MI attack. The evaluation metrics consist of ten typical evaluation metrics. We have identified three principles for the proposed\"comparing different MI attacks\"methodology, and we have designed and implemented the MIBench benchmark with 84 evaluation scenarios for each dataset. In total, we have used our benchmark to fairly and systematically compare 15 state-of-the-art MI attack algorithms across 588 evaluation scenarios, and these evaluation scenarios cover 7 widely used datasets and 7 representative types of models. All codes and evaluations of MIBench are publicly available at https://github.com/MIBench/MIBench.github.io/blob/main/README.md.",
            "corpus_id": 259837436,
            "sentences": [
                {
                    "corpus_id": "259837436",
                    "title": "SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark",
                    "text": "Membership Inference Attacks. Homer et al. [59] first proposed an MI attack on biological data. Shokri et al. [8] proposed the first black-box MI attack against ML. Huge literature followed these works to different scenarios (e.g., location data [60], language models [10], sentence embeddings [61], speech recognition models [62], federated learning [7], transfer learning [63], generative models [38], white box access [12], [21], [48]). Categories of Membership Inference Attacks. There are main three categories 1) Binary classifier-based MI attacks, which utilize the output predictions of shadow models to train a binary classifier to launch the MI attacks [8], [9]. 2) Evaluation metric-based MI attacks, which used the defined evaluation metrics to distinguish members and nonmembers [9], [13], [14], [37], [39], [51]. 3) Differential Comparisons-based MI attacks (BlindMI-Diff), which mainly utilized the differences between two datasets. Defenses against MI attacks. Multiple defenses [8], [9], [11], [64], [65], [66], [67], [68] have been proposed to mitigate MI attacks.",
                    "score": 0.7019243142192427,
                    "section_title": "Related Work",
                    "char_start_offset": 57635,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 29
                        },
                        {
                            "start": 30,
                            "end": 95
                        },
                        {
                            "start": 96,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 947
                        },
                        {
                            "start": 948,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1082
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 43,
                            "end": 47,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 110,
                            "end": 113,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 246,
                            "end": 250,
                            "matchedPaperCorpusId": "216553356"
                        },
                        {
                            "start": 268,
                            "end": 272,
                            "matchedPaperCorpusId": "229156229"
                        },
                        {
                            "start": 294,
                            "end": 298,
                            "matchedPaperCorpusId": "214743021"
                        },
                        {
                            "start": 326,
                            "end": 330,
                            "matchedPaperCorpusId": "236880971"
                        },
                        {
                            "start": 351,
                            "end": 354,
                            "matchedPaperCorpusId": "232372747"
                        },
                        {
                            "start": 398,
                            "end": 402,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 421,
                            "end": 425,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 427,
                            "end": 431,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 433,
                            "end": 437,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 663,
                            "end": 666,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 797,
                            "end": 801,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 803,
                            "end": 807,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 809,
                            "end": 813,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 815,
                            "end": 819,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 821,
                            "end": 825,
                            "matchedPaperCorpusId": "226266600"
                        },
                        {
                            "start": 995,
                            "end": 998,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1005,
                            "end": 1009,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1011,
                            "end": 1015,
                            "matchedPaperCorpusId": "207241585"
                        },
                        {
                            "start": 1017,
                            "end": 1021,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1029,
                            "end": 1033,
                            "matchedPaperCorpusId": "235349092"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark\n# Venue: arXiv.org\n# Authors: Jun Niu, Xiaoyan Zhu, Moxuan Zeng, Ge-ming Zhang, Qingyang Zhao, Chu-Chun Huang, Yang Zhang, Suyu An, Yangzhong Wang, Xinghui Yue, Zhipeng He, Weihao Guo, Kuo Shen, Peng Liu, Yulong Shen, Xiaohong Jiang, Jianfeng Ma, Yuqing Zhang\n## Abstract\nMembership inference (MI) attacks threaten user privacy through determining if a given data example has been used to train a target model. However, it has been increasingly recognized that the\"comparing different MI attacks\"methodology used in the existing works has serious limitations. Due to these limitations, we found (through the experiments in this work) that some comparison results reported in the literature are quite misleading. In this paper, we seek to develop a comprehensive benchmark for comparing different MI attacks, called MIBench, which consists not only the evaluation metrics, but also the evaluation scenarios. And we design the evaluation scenarios from four perspectives: the distance distribution of data samples in the target dataset, the distance between data samples of the target dataset, the differential distance between two datasets (i.e., the target dataset and a generated dataset with only nonmembers), and the ratio of the samples that are made no inferences by an MI attack. The evaluation metrics consist of ten typical evaluation metrics. We have identified three principles for the proposed\"comparing different MI attacks\"methodology, and we have designed and implemented the MIBench benchmark with 84 evaluation scenarios for each dataset. In total, we have used our benchmark to fairly and systematically compare 15 state-of-the-art MI attack algorithms across 588 evaluation scenarios, and these evaluation scenarios cover 7 widely used datasets and 7 representative types of models. All codes and evaluations of MIBench are publicly available at https://github.com/MIBench/MIBench.github.io/blob/main/README.md.\n## Related Work\nMembership Inference Attacks. Homer et al. [59] first proposed an MI attack on biological data. Shokri et al. [8] proposed the first black-box MI attack against ML. Huge literature followed these works to different scenarios (e.g., location data [60], language models [10], sentence embeddings [61], speech recognition models [62], federated learning [7], transfer learning [63], generative models [38], white box access [12], [21], [48]). Categories of Membership Inference Attacks. There are main three categories 1) Binary classifier-based MI attacks, which utilize the output predictions of shadow models to train a binary classifier to launch the MI attacks [8], [9]. 2) Evaluation metric-based MI attacks, which used the defined evaluation metrics to distinguish members and nonmembers [9], [13], [14], [37], [39], [51]. 3) Differential Comparisons-based MI attacks (BlindMI-Diff), which mainly utilized the differences between two datasets. Defenses against MI attacks. Multiple defenses [8], [9], [11], [64], [65], [66], [67], [68] have been proposed to mitigate MI attacks.",
            "reference_string": "[259837436 | Niu et al. | 2023 | Citations: 3]"
        },
        {
            "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
            "venue": "Conference on Computer and Communications Security",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690335",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276333336",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2312376529",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2312657759",
                    "name": "Siyuan Wu"
                },
                {
                    "authorId": "2312343419",
                    "name": "Chengrui Hu"
                },
                {
                    "authorId": "2162322403",
                    "name": "Yutong Ye"
                },
                {
                    "authorId": "2312376569",
                    "name": "Min Zhang"
                },
                {
                    "authorId": "2276275105",
                    "name": "Dengguo Feng"
                },
                {
                    "authorId": "2312358230",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Most existing membership inference attacks (MIAs) utilize metrics (e.g., loss) calculated on the model's final state, while recent advanced attacks leverage metrics computed at various stages, including both intermediate and final stages, throughout the model training. Nevertheless, these attacks often process multiple intermediate states of the metric independently, ignoring their time-dependent patterns. Consequently, they struggle to effectively distinguish between members and non-members who exhibit similar metric values, particularly resulting in a high false-positive rate. In this study, we delve deeper into the new membership signals in the black-box scenario. We identify a new, more integrated membership signal: the Pattern of Metric Sequence, derived from the various stages of model training. We contend that current signals provide only partial perspectives of this new signal: the new one encompasses both the model's multiple intermediate and final states, with a greater emphasis on temporal patterns among them. Building upon this signal, we introduce a novel attack method called Sequential-metric based Membership Inference Attack (SeqMIA). Specifically, we utilize knowledge distillation to obtain a set of distilled models representing various stages of the target model's training. We then assess multiple metrics on these distilled models in chronological order, creating distilled metric sequence. We finally integrate distilled multi-metric sequences as a sequential multiformat and employ an attention-based RNN attack model for inference. Empirical results show SeqMIA outperforms all baselines, especially can achieve an order of magnitude improvement in terms of TPR @ 0.1% FPR. Furthermore, we delve into the reasons why this signal contributes to SeqMIA's high attack performance, and assess various defense mechanisms against SeqMIA.",
            "corpus_id": 271328769,
            "sentences": [
                {
                    "corpus_id": "271328769",
                    "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
                    "text": "Nowadays, there exist a wide range of other security and privacy research in the machine learning domain [8, 17, 18, 21, 29, 31, 34, 35, 38, 39, 45, 48, 49, 57-59, 63, 67].In this work, we mainly focus on membership inference attacks.Membership inference attacks have been successfully performed in various settings about the adversary's knowledge, including white-box [27,42], black-box [9,24,46,50,52,68], and label-only [10,32] settings.They have been applied in many machine learning scenarios, such as federated learning [40,42,54] and multi-exit networks [30], etc.\n\nSpecifically, Shokri et al. [50] and Salem et al. [46] proposed a shadow training technique that employs shadow models to acquire the membership signals.Moreover, Song et al. [52] and Yeom et al. [65] proposed the metric-based attack that directly compares losses or other metric values of samples with a predefined threshold.In addition, some membership signals obtained in the white-box scenario are incorporated to improve the attack performance [12,42].Besides, label-only attacks [10,32,60] solely rely on the predicted labels to acquire the membership signals.Recently, researchers [6,33,44,56,64] focused on reducing the false positives of MIAs by using each sample's hardness threshold to calibrate the loss from the target model.Further, Bertran et al. [5] proposed a new attack via quantile regression, which can obtain performance close to that of LiRA [6] with less computation.Moreover, Liu et al. [33] presented Trajec-toryMIA, which utilizes the membership signals generated during the training of the target model.",
                    "score": 0.7038157242726962,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 39323,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 172,
                            "end": 234
                        },
                        {
                            "start": 234,
                            "end": 440
                        },
                        {
                            "start": 440,
                            "end": 571
                        },
                        {
                            "start": 573,
                            "end": 726
                        },
                        {
                            "start": 726,
                            "end": 899
                        },
                        {
                            "start": 899,
                            "end": 1030
                        },
                        {
                            "start": 1030,
                            "end": 1139
                        },
                        {
                            "start": 1139,
                            "end": 1311
                        },
                        {
                            "start": 1311,
                            "end": 1463
                        },
                        {
                            "start": 1463,
                            "end": 1603
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 369,
                            "end": 373,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 373,
                            "end": 376,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 388,
                            "end": 391,
                            "matchedPaperCorpusId": "218502126"
                        },
                        {
                            "start": 391,
                            "end": 394,
                            "matchedPaperCorpusId": "119302127"
                        },
                        {
                            "start": 394,
                            "end": 397,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 397,
                            "end": 400,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 400,
                            "end": 403,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 403,
                            "end": 406,
                            "matchedPaperCorpusId": "237532289"
                        },
                        {
                            "start": 427,
                            "end": 430,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 526,
                            "end": 530,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 530,
                            "end": 533,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 533,
                            "end": 536,
                            "matchedPaperCorpusId": "86836429"
                        },
                        {
                            "start": 561,
                            "end": 565,
                            "matchedPaperCorpusId": "251765197"
                        },
                        {
                            "start": 601,
                            "end": 605,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 623,
                            "end": 627,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 748,
                            "end": 752,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 769,
                            "end": 773,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1022,
                            "end": 1026,
                            "matchedPaperCorpusId": "247595200"
                        },
                        {
                            "start": 1026,
                            "end": 1029,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1062,
                            "end": 1065,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1161,
                            "end": 1164,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1164,
                            "end": 1167,
                            "matchedPaperCorpusId": "251953448"
                        },
                        {
                            "start": 1167,
                            "end": 1170,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 1170,
                            "end": 1173,
                            "matchedPaperCorpusId": "244130249"
                        },
                        {
                            "start": 1173,
                            "end": 1176,
                            "matchedPaperCorpusId": "244345608"
                        },
                        {
                            "start": 1335,
                            "end": 1338,
                            "matchedPaperCorpusId": "259375769"
                        },
                        {
                            "start": 1437,
                            "end": 1440,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1484,
                            "end": 1488,
                            "matchedPaperCorpusId": "251953448"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: SeqMIA: Sequential-Metric Based Membership Inference Attack\n# Venue: Conference on Computer and Communications Security\n# Authors: Hao Li, Zheng Li, Siyuan Wu, Chengrui Hu, Yutong Ye, Min Zhang, Dengguo Feng, Yang Zhang\n## Abstract\nMost existing membership inference attacks (MIAs) utilize metrics (e.g., loss) calculated on the model's final state, while recent advanced attacks leverage metrics computed at various stages, including both intermediate and final stages, throughout the model training. Nevertheless, these attacks often process multiple intermediate states of the metric independently, ignoring their time-dependent patterns. Consequently, they struggle to effectively distinguish between members and non-members who exhibit similar metric values, particularly resulting in a high false-positive rate. In this study, we delve deeper into the new membership signals in the black-box scenario. We identify a new, more integrated membership signal: the Pattern of Metric Sequence, derived from the various stages of model training. We contend that current signals provide only partial perspectives of this new signal: the new one encompasses both the model's multiple intermediate and final states, with a greater emphasis on temporal patterns among them. Building upon this signal, we introduce a novel attack method called Sequential-metric based Membership Inference Attack (SeqMIA). Specifically, we utilize knowledge distillation to obtain a set of distilled models representing various stages of the target model's training. We then assess multiple metrics on these distilled models in chronological order, creating distilled metric sequence. We finally integrate distilled multi-metric sequences as a sequential multiformat and employ an attention-based RNN attack model for inference. Empirical results show SeqMIA outperforms all baselines, especially can achieve an order of magnitude improvement in terms of TPR @ 0.1% FPR. Furthermore, we delve into the reasons why this signal contributes to SeqMIA's high attack performance, and assess various defense mechanisms against SeqMIA.\n## Membership Inference Attacks\nNowadays, there exist a wide range of other security and privacy research in the machine learning domain [8, 17, 18, 21, 29, 31, 34, 35, 38, 39, 45, 48, 49, 57-59, 63, 67].In this work, we mainly focus on membership inference attacks.Membership inference attacks have been successfully performed in various settings about the adversary's knowledge, including white-box [27,42], black-box [9,24,46,50,52,68], and label-only [10,32] settings.They have been applied in many machine learning scenarios, such as federated learning [40,42,54] and multi-exit networks [30], etc.\n\nSpecifically, Shokri et al. [50] and Salem et al. [46] proposed a shadow training technique that employs shadow models to acquire the membership signals.Moreover, Song et al. [52] and Yeom et al. [65] proposed the metric-based attack that directly compares losses or other metric values of samples with a predefined threshold.In addition, some membership signals obtained in the white-box scenario are incorporated to improve the attack performance [12,42].Besides, label-only attacks [10,32,60] solely rely on the predicted labels to acquire the membership signals.Recently, researchers [6,33,44,56,64] focused on reducing the false positives of MIAs by using each sample's hardness threshold to calibrate the loss from the target model.Further, Bertran et al. [5] proposed a new attack via quantile regression, which can obtain performance close to that of LiRA [6] with less computation.Moreover, Liu et al. [33] presented Trajec-toryMIA, which utilizes the membership signals generated during the training of the target model.",
            "reference_string": "[271328769 | Li et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "venue": "USENIX Security Symposium",
            "year": 2020,
            "reference_count": 60,
            "citation_count": 375,
            "influential_citation_count": 66,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.10595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144173853",
                    "name": "Liwei Song"
                },
                {
                    "authorId": "143615345",
                    "name": "Prateek Mittal"
                }
            ],
            "abstract": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
            "corpus_id": 214623088,
            "sentences": [
                {
                    "corpus_id": "214623088",
                    "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
                    "text": "A recent thread of research has shown that machine learning (ML) models memorize sensitive information of training data, indicating serious privacy risks [4,9,10,15,35,39,40]. In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not [39,47]. It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3,33]. For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. [39] conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models [7,12,14,45], to differentially private models [18,34], to decentralized settings where the models are trained across multiple users without sharing their data [28,30], and to white-box settings where the adversary also has the access to the target model's architecture and weights [30]. \n\nTo mitigate such privacy risks, several defenses against membership inference attacks have been proposed. Nasr et al. [29] propose to include membership inference attacks during the training process: they train the target model to simultaneously achieve correct predictions and low membership inference attack accuracy by adding the inference attack as an adversarial regularization term. Jia et al. [19] propose a defense method called MemGuard which does not require retraining the model: the model prediction outputs are obfuscated with noisy perturbations such that the adversary cannot distinguish between members and non-members based on the perturbed outputs. Both papers show that their defenses greatly mitigate membership inference privacy risks, resulting in attack performance that is close to random guessing.",
                    "score": 0.6269022402962346,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1350
                        },
                        {
                            "start": 1353,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2175
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 157,
                            "matchedPaperCorpusId": "170076423"
                        },
                        {
                            "start": 157,
                            "end": 159,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "52218951"
                        },
                        {
                            "start": 162,
                            "end": 165,
                            "matchedPaperCorpusId": "5051282"
                        },
                        {
                            "start": 165,
                            "end": 168,
                            "matchedPaperCorpusId": "91184074"
                        },
                        {
                            "start": 168,
                            "end": 171,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 171,
                            "end": 174,
                            "matchedPaperCorpusId": "2904063"
                        },
                        {
                            "start": 353,
                            "end": 357,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 357,
                            "end": 360,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 463,
                            "end": 466,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 616,
                            "end": 620,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1063,
                            "end": 1066,
                            "matchedPaperCorpusId": "202539472"
                        },
                        {
                            "start": 1066,
                            "end": 1069,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1069,
                            "end": 1072,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1072,
                            "end": 1075,
                            "matchedPaperCorpusId": "201126909"
                        },
                        {
                            "start": 1110,
                            "end": 1114,
                            "matchedPaperCorpusId": "173990571"
                        },
                        {
                            "start": 1114,
                            "end": 1117,
                            "matchedPaperCorpusId": "13699042"
                        },
                        {
                            "start": 1223,
                            "end": 1227,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1227,
                            "end": 1230,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1345,
                            "end": 1349,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1471,
                            "end": 1475,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1753,
                            "end": 1757,
                            "matchedPaperCorpusId": "202734167"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Systematic Evaluation of Privacy Risks of Machine Learning Models\n# Venue: USENIX Security Symposium\n# Authors: Liwei Song, Prateek Mittal\n## Abstract\nMachine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.\n## Introduction\nA recent thread of research has shown that machine learning (ML) models memorize sensitive information of training data, indicating serious privacy risks [4,9,10,15,35,39,40]. In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not [39,47]. It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3,33]. For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. [39] conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models [7,12,14,45], to differentially private models [18,34], to decentralized settings where the models are trained across multiple users without sharing their data [28,30], and to white-box settings where the adversary also has the access to the target model's architecture and weights [30]. \n\nTo mitigate such privacy risks, several defenses against membership inference attacks have been proposed. Nasr et al. [29] propose to include membership inference attacks during the training process: they train the target model to simultaneously achieve correct predictions and low membership inference attack accuracy by adding the inference attack as an adversarial regularization term. Jia et al. [19] propose a defense method called MemGuard which does not require retraining the model: the model prediction outputs are obfuscated with noisy perturbations such that the adversary cannot distinguish between members and non-members based on the perturbed outputs. Both papers show that their defenses greatly mitigate membership inference privacy risks, resulting in attack performance that is close to random guessing.",
            "reference_string": "[214623088 | Song et al. | 2020 | Citations: 375]"
        },
        {
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 67,
            "citation_count": 71,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.03915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "30348654",
                    "name": "Ziqi Yang"
                },
                {
                    "authorId": "1471460168",
                    "name": "Bin Shao"
                },
                {
                    "authorId": "1686498981",
                    "name": "Bohan Xuan"
                },
                {
                    "authorId": "144556100",
                    "name": "E. Chang"
                },
                {
                    "authorId": "2153304399",
                    "name": "Fan Zhang"
                }
            ],
            "abstract": "Neural networks are susceptible to data inference attacks such as the model inversion attack and the membership inference attack, where the attacker could infer the reconstruction and the membership of a data sample from the confidence scores predicted by the target classifier. In this paper, we propose a unified approach, namely purification framework, to defend data inference attacks. It purifies the confidence score vectors predicted by the target classifier by reducing their dispersion. The purifier can be further specialized in defending a particular attack via adversarial learning. We evaluate our approach on benchmark datasets and classifiers. We show that when the purifier is dedicated to one attack, it naturally defends the other one, which empirically demonstrates the connection between the two attacks. The purifier can effectively defend both attacks. For example, it can reduce the membership inference accuracy by up to 15% and increase the model inversion error by a factor of up to 4. Besides, it incurs less than 0.4% classification accuracy drop and less than 5.5% distortion to the confidence scores.",
            "corpus_id": 218571404,
            "sentences": [
                {
                    "corpus_id": "218571404",
                    "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
                    "text": "Membership inference and model inversion attacks are two types of data inference attacks that threaten the security and privacy of machine learning. They differ in their inference goals Membership Inference Attack. In the membership inference attack, the attacker is asked to determine whether a given data record is part of the training data of the target model [24,30,31,36,47,49]. \n\nConfidence-based Attack [47,49]. Shokri et al. [49] introduced membership inference against black-box models, where the attacker has access only to the prediction scores of the target model. To infer the membership, the attacker trains a binary classifier (also referred to as attack model) which takes as input the confidence scores of the target model on a given data sample and predicts the data sample to be a member or non-member of the training dataset of the target model. Prior to training the attack model, the attacker trains a set of shadow models on an auxiliary dataset drawn from the same data distribution as the target model's training data to replicate the target model. The attack model is then trained on the confidence scores predicted by the shadow models instead of the target model on the members and non-members of the shadow models' training data. Salem et al. [47] further showed that it is sufficient to train only one shadow model to replicate the target model for membership inference attack. Besides, they also showed that ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy. For example, their experimental results show that only the top one/three highest values in the confidence vector are sufficient to result in effective membership inference. These results indicate that the assumptions of membership inference attack could be largely relaxed which might lead to more practical threats. \n\nConfidence & Label-based Attack [35]. Nasr et al. [35] extended the attack model by also taking the label information as input. Their attack model is composed of three neural networks. The first two networks operate on the confidence score vector and the onehot encoded label respectively. They have the same size of input dimension, i.e., the number of classes of the target model. The third network operates on the concatenation of the output of the first two networks and predicts the membership.",
                    "score": 0.7317824717795074,
                    "section_title": "Data Inference Attacks",
                    "char_start_offset": 11289,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 383
                        },
                        {
                            "start": 386,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1744
                        },
                        {
                            "start": 1745,
                            "end": 1888
                        },
                        {
                            "start": 1891,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 2018
                        },
                        {
                            "start": 2019,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2180
                        },
                        {
                            "start": 2181,
                            "end": 2273
                        },
                        {
                            "start": 2274,
                            "end": 2390
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 367,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 373,
                            "end": 376,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 376,
                            "end": 379,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 379,
                            "end": 382,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 410,
                            "end": 414,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 414,
                            "end": 417,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 433,
                            "end": 437,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1272,
                            "end": 1276,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1923,
                            "end": 1927,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1941,
                            "end": 1945,
                            "matchedPaperCorpusId": "49863840"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9755859375
                }
            ],
            "relevance_judgement": 0.9755859375,
            "relevance_judgment_input_expanded": "# Title: Defending Model Inversion and Membership Inference Attacks via Prediction Purification\n# Venue: arXiv.org\n# Authors: Ziqi Yang, Bin Shao, Bohan Xuan, E. Chang, Fan Zhang\n## Abstract\nNeural networks are susceptible to data inference attacks such as the model inversion attack and the membership inference attack, where the attacker could infer the reconstruction and the membership of a data sample from the confidence scores predicted by the target classifier. In this paper, we propose a unified approach, namely purification framework, to defend data inference attacks. It purifies the confidence score vectors predicted by the target classifier by reducing their dispersion. The purifier can be further specialized in defending a particular attack via adversarial learning. We evaluate our approach on benchmark datasets and classifiers. We show that when the purifier is dedicated to one attack, it naturally defends the other one, which empirically demonstrates the connection between the two attacks. The purifier can effectively defend both attacks. For example, it can reduce the membership inference accuracy by up to 15% and increase the model inversion error by a factor of up to 4. Besides, it incurs less than 0.4% classification accuracy drop and less than 5.5% distortion to the confidence scores.\n## Data Inference Attacks\nMembership inference and model inversion attacks are two types of data inference attacks that threaten the security and privacy of machine learning. They differ in their inference goals Membership Inference Attack. In the membership inference attack, the attacker is asked to determine whether a given data record is part of the training data of the target model [24,30,31,36,47,49]. \n\nConfidence-based Attack [47,49]. Shokri et al. [49] introduced membership inference against black-box models, where the attacker has access only to the prediction scores of the target model. To infer the membership, the attacker trains a binary classifier (also referred to as attack model) which takes as input the confidence scores of the target model on a given data sample and predicts the data sample to be a member or non-member of the training dataset of the target model. Prior to training the attack model, the attacker trains a set of shadow models on an auxiliary dataset drawn from the same data distribution as the target model's training data to replicate the target model. The attack model is then trained on the confidence scores predicted by the shadow models instead of the target model on the members and non-members of the shadow models' training data. Salem et al. [47] further showed that it is sufficient to train only one shadow model to replicate the target model for membership inference attack. Besides, they also showed that ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy. For example, their experimental results show that only the top one/three highest values in the confidence vector are sufficient to result in effective membership inference. These results indicate that the assumptions of membership inference attack could be largely relaxed which might lead to more practical threats. \n\nConfidence & Label-based Attack [35]. Nasr et al. [35] extended the attack model by also taking the label information as input. Their attack model is composed of three neural networks. The first two networks operate on the confidence score vector and the onehot encoded label respectively. They have the same size of input dimension, i.e., the number of classes of the target model. The third network operates on the concatenation of the output of the first two networks and predicts the membership.",
            "reference_string": "[218571404 | Yang et al. | 2020 | Citations: 71]"
        },
        {
            "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2020,
            "reference_count": 45,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2011.14661",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.14661, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3222644",
                    "name": "Seira Hidano"
                },
                {
                    "authorId": "49224245",
                    "name": "Yusuke Kawamoto"
                },
                {
                    "authorId": "2116295",
                    "name": "Takao Murakami"
                }
            ],
            "abstract": "Transfer learning has been widely studied and gained increasing popularity to improve the accuracy of machine learning models by transferring some knowledge acquired in different training. However, no prior work has pointed out that transfer learning can strengthen privacy attacks on machine learning models. In this paper, we propose TransMIA (Transfer learning-based Membership Inference Attacks), which use transfer learning to perform membership inference attacks on the source model when the adversary is able to access the parameters of the transferred model. In particular, we propose a transfer shadow training technique, where an adversary employs the parameters of the transferred model to construct shadow models, to significantly improve the performance of membership inference when a limited amount of shadow training data is available to the adversary. We evaluate our attacks using two real datasets, and show that our attacks outperform the state-of-the-art that does not use our transfer shadow training technique. We also compare four combinations of the learning-based/entropy-based approach and the fine-tuning/freezing approach, all of which employ our transfer shadow training technique. Then we examine the performance of these four approaches based on the distributions of confidence values, and discuss possible countermeasures against our attacks.",
            "corpus_id": 227227868,
            "sentences": [
                {
                    "corpus_id": "227227868",
                    "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training",
                    "text": "We next review a privacy attack on machine learning models, called a membership inference attack (MIA) [14], where an adversary is not given a training dataset D train and attempts to reveal membership information on D train . Specifically, the adversary in this attack aims to determine whether a specific data point (x, y) is included in a training dataset D train used to build a classification model f . \n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f . To build an adversary A against f , we attempt to learn some statistical relevance between the distribution f (x) of confidence values and the membership (x, y) \u2208 D train . \n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train . An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f (x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network). \n\nFormally, an adversary in the black-box membership inference attack on a model f is a function A : X \u00d7 Y \u00d7 DY \u2192 {in, out} that given a data point (x, y) and its prediction vector f (x), outputs one of the labels in and out, where in (resp. out) represents that A predicts (x, y) \u2208 D train (resp. \n\n(x, y) \u2208 D train ). To evaluate the performance of the membership inference attack, we measure the accuracy, precision, and recall of the attack by using a test dataset D test \u2286 X \u00d7 Y disjoint from the training dataset D train . \n\nWe deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train .",
                    "score": 0.7538244452149193,
                    "section_title": "C. Membership Inference Attack",
                    "char_start_offset": 17223,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 407
                        },
                        {
                            "start": 410,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 717
                        },
                        {
                            "start": 720,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 1151
                        },
                        {
                            "start": 1154,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1449
                        },
                        {
                            "start": 1452,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1680
                        },
                        {
                            "start": 1683,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 1962
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 103,
                            "end": 107,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1789,
                            "end": 1793,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1825,
                            "end": 1829,
                            "matchedPaperCorpusId": "214623088"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: TransMIA: Membership Inference Attacks Using Transfer Shadow Training\n# Venue: IEEE International Joint Conference on Neural Network\n# Authors: Seira Hidano, Yusuke Kawamoto, Takao Murakami\n## Abstract\nTransfer learning has been widely studied and gained increasing popularity to improve the accuracy of machine learning models by transferring some knowledge acquired in different training. However, no prior work has pointed out that transfer learning can strengthen privacy attacks on machine learning models. In this paper, we propose TransMIA (Transfer learning-based Membership Inference Attacks), which use transfer learning to perform membership inference attacks on the source model when the adversary is able to access the parameters of the transferred model. In particular, we propose a transfer shadow training technique, where an adversary employs the parameters of the transferred model to construct shadow models, to significantly improve the performance of membership inference when a limited amount of shadow training data is available to the adversary. We evaluate our attacks using two real datasets, and show that our attacks outperform the state-of-the-art that does not use our transfer shadow training technique. We also compare four combinations of the learning-based/entropy-based approach and the fine-tuning/freezing approach, all of which employ our transfer shadow training technique. Then we examine the performance of these four approaches based on the distributions of confidence values, and discuss possible countermeasures against our attacks.\n## C. Membership Inference Attack\nWe next review a privacy attack on machine learning models, called a membership inference attack (MIA) [14], where an adversary is not given a training dataset D train and attempts to reveal membership information on D train . Specifically, the adversary in this attack aims to determine whether a specific data point (x, y) is included in a training dataset D train used to build a classification model f . \n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f . To build an adversary A against f , we attempt to learn some statistical relevance between the distribution f (x) of confidence values and the membership (x, y) \u2208 D train . \n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train . An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f (x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network). \n\nFormally, an adversary in the black-box membership inference attack on a model f is a function A : X \u00d7 Y \u00d7 DY \u2192 {in, out} that given a data point (x, y) and its prediction vector f (x), outputs one of the labels in and out, where in (resp. out) represents that A predicts (x, y) \u2208 D train (resp. \n\n(x, y) \u2208 D train ). To evaluate the performance of the membership inference attack, we measure the accuracy, precision, and recall of the attack by using a test dataset D test \u2286 X \u00d7 Y disjoint from the training dataset D train . \n\nWe deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train .",
            "reference_string": "[227227868 | Hidano et al. | 2020 | Citations: 13]"
        },
        {
            "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 33,
            "citation_count": 25,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.00395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1470468158",
                    "name": "Shadi Rahimian"
                },
                {
                    "authorId": "9517443",
                    "name": "Tribhuvanesh Orekondy"
                },
                {
                    "authorId": "1739548",
                    "name": "Mario Fritz"
                }
            ],
            "abstract": "Machine learning models have been shown to leak information violating the privacy of their training set. We focus on membership inference attacks on machine learning models which aim to determine whether a data point was used to train the victim model. Our work consists of two sides: We introduce sampling attack, a novel membership inference technique that unlike other standard membership adversaries is able to work under severe restriction of no access to scores of the victim model. We show that a victim model that only publishes the labels is still susceptible to sampling attacks and the adversary can recover up to 100% of its performance compared to when posterior vectors are provided. The other sides of our work includes experimental results on two recent membership inference attack models and the defenses against them. For defense, we choose differential privacy in the form of gradient perturbation during the training of the victim model as well as output perturbation at prediction time. We carry out our experiments on a wide range of datasets which allows us to better analyze the interaction between adversaries, defense mechanism and datasets. We find out that our proposed fast and easy-to-implement output perturbation technique offers good privacy protection for membership inference attacks at little impact on utility.",
            "corpus_id": 221397750,
            "sentences": [
                {
                    "corpus_id": "221397750",
                    "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
                    "text": "Machine learning models have been shown to leak information violating the privacy of their training set. We focus on membership inference attacks on machine learning models which aim to determine whether a data point was used to train the victim model. Our work consists of two sides: We introduce sampling attack, a novel membership inference technique that unlike other standard membership adversaries is able to work under severe restriction of no access to scores of the victim model. We show that a victim model that only publishes the labels is still susceptible to sampling attacks and the adversary can recover up to 100% of its performance compared to when posterior vectors are provided. The other sides of our work includes experimental results on two recent membership inference attack models and the defenses against them. For defense, we choose differential privacy in the form of gradient perturbation during the training of the victim model as well as output perturbation at prediction time. We carry out our experiments on a wide range of datasets which allows us to better analyze the interaction between adversaries, defense mechanism and datasets. We find out that our proposed fast and easy-to-implement output perturbation technique offers good privacy protection for membership inference attacks at little impact on utility.",
                    "score": 0.6966566536652486,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                },
                {
                    "corpus_id": "221397750",
                    "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
                    "text": "In the previous section we defined the membership inference attacks and described two generic models to carry out these attacks in practice. Now, we will explore methods to defend against them. \n\nTo defend against these attacks, we need to understand what factors make these attacks possible and how we can limit and paralyze the adversary. Most of the previous work on defenses against membership inference attacks can be summarized into two categories: \n\nGeneralization-based techniques (Shokri et al., 2017) was the first to define the membership inference attacks in a machine learning setting. They also identify the overfitting of the victim model as one of the main culprits for vulnerability to membership inference attacks. They hypothesize that the victim model memorizes its training set such that the posteriors show a statistical difference between the seen and hold-out data. A more comprehensive study about the correlation of overfitting to membership inference attacks can be found in (Yeom et al., 2018). \n\nThese findings prompt a line of defense that relies on enforcing generalization on the victim model. (Shokri et al., 2017) suggest using L2 regularization of the parameters and restricting the number of training epochs. (Salem et al., 2018) use dropout and ensemble learning to train the victim model to help it generalize better. In a slightly different approach, (Nasr et al., 2018) utilizes adversarial training of the victim model in the form of a min-max game to help the model generate indistinguishable predictions on its training set and an unseen dataset. \n\nNoising-based techniques Adding randomness to different parameters of the victim model at different stages is one of the most natural ways to confuse any adversary. In fact, the first defenses against membership inference attacks on the genome data (Wang et al., 2009) proposes adding carefully-crafted noise to the published dataset. \n\nJia et al (Jia et al., 2019) suggest adding noise to the output of the victim model. They generate specially-composed noise vectors for the victim model's posteriors such that they act as adversarial examples for the attacker.",
                    "score": 0.6633027621789911,
                    "section_title": "Defenses for Membership Inference Attacks",
                    "char_start_offset": 8510,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 193
                        },
                        {
                            "start": 196,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 454
                        },
                        {
                            "start": 457,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 1022
                        },
                        {
                            "start": 1025,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1589
                        },
                        {
                            "start": 1592,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1926
                        },
                        {
                            "start": 1929,
                            "end": 2013
                        },
                        {
                            "start": 2014,
                            "end": 2155
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 489,
                            "end": 510,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1002,
                            "end": 1021,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1126,
                            "end": 1147,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1245,
                            "end": 1265,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1390,
                            "end": 1409,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1841,
                            "end": 1860,
                            "matchedPaperCorpusId": "8091014"
                        },
                        {
                            "start": 1939,
                            "end": 1957,
                            "matchedPaperCorpusId": "202734167"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries\n# Venue: arXiv.org\n# Authors: Shadi Rahimian, Tribhuvanesh Orekondy, Mario Fritz\n## Abstract\nMachine learning models have been shown to leak information violating the privacy of their training set. We focus on membership inference attacks on machine learning models which aim to determine whether a data point was used to train the victim model. Our work consists of two sides: We introduce sampling attack, a novel membership inference technique that unlike other standard membership adversaries is able to work under severe restriction of no access to scores of the victim model. We show that a victim model that only publishes the labels is still susceptible to sampling attacks and the adversary can recover up to 100% of its performance compared to when posterior vectors are provided. The other sides of our work includes experimental results on two recent membership inference attack models and the defenses against them. For defense, we choose differential privacy in the form of gradient perturbation during the training of the victim model as well as output perturbation at prediction time. We carry out our experiments on a wide range of datasets which allows us to better analyze the interaction between adversaries, defense mechanism and datasets. We find out that our proposed fast and easy-to-implement output perturbation technique offers good privacy protection for membership inference attacks at little impact on utility.\n## Defenses for Membership Inference Attacks\nIn the previous section we defined the membership inference attacks and described two generic models to carry out these attacks in practice. Now, we will explore methods to defend against them. \n\nTo defend against these attacks, we need to understand what factors make these attacks possible and how we can limit and paralyze the adversary. Most of the previous work on defenses against membership inference attacks can be summarized into two categories: \n\nGeneralization-based techniques (Shokri et al., 2017) was the first to define the membership inference attacks in a machine learning setting. They also identify the overfitting of the victim model as one of the main culprits for vulnerability to membership inference attacks. They hypothesize that the victim model memorizes its training set such that the posteriors show a statistical difference between the seen and hold-out data. A more comprehensive study about the correlation of overfitting to membership inference attacks can be found in (Yeom et al., 2018). \n\nThese findings prompt a line of defense that relies on enforcing generalization on the victim model. (Shokri et al., 2017) suggest using L2 regularization of the parameters and restricting the number of training epochs. (Salem et al., 2018) use dropout and ensemble learning to train the victim model to help it generalize better. In a slightly different approach, (Nasr et al., 2018) utilizes adversarial training of the victim model in the form of a min-max game to help the model generate indistinguishable predictions on its training set and an unseen dataset. \n\nNoising-based techniques Adding randomness to different parameters of the victim model at different stages is one of the most natural ways to confuse any adversary. In fact, the first defenses against membership inference attacks on the genome data (Wang et al., 2009) proposes adding carefully-crafted noise to the published dataset. \n\nJia et al (Jia et al., 2019) suggest adding noise to the output of the victim model. They generate specially-composed noise vectors for the victim model's posteriors such that they act as adversarial examples for the attacker.",
            "reference_string": "[221397750 | Rahimian et al. | 2020 | Citations: 25]"
        },
        {
            "title": "Protect privacy of deep classification networks by exploiting their generative power",
            "venue": "Machine-mediated learning",
            "year": 2021,
            "reference_count": 40,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-021-05951-6.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10994-021-05951-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10994-021-05951-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "67313577",
                    "name": "Jiyu Chen"
                },
                {
                    "authorId": "2527106",
                    "name": "Yiwen Guo"
                },
                {
                    "authorId": "2098809297",
                    "name": "Qianjun Zheng"
                },
                {
                    "authorId": "2149051171",
                    "name": "Hao Chen"
                }
            ],
            "abstract": "Research showed that deep learning models are vulnerable to membership inference attacks, which aim to determine if an example is in the training set of the model. We propose a new framework to defend against this sort of attack. Our key insight is that if we retrain the original classifier with a new dataset that is independent of the original training set while their elements are sampled from the same distribution, the retrained classifier will leak no information that cannot be inferred from the distribution about the original training set. Our framework consists of three phases. First, we transferred the original classifier to a Joint Energy-based Model (JEM) to exploit the model\u2019s implicit generative power. Then, we sampled from the JEM to create a new dataset. Finally, we used the new dataset to retrain or fine-tune the original classifier. We empirically studied different transfer learning schemes for the JEM and fine-tuning/retraining strategies for the classifier against shadow-model attacks. Our evaluation shows that our framework can suppress the attacker\u2019s membership advantage to a negligible level while keeping the classifier\u2019s accuracy acceptable. We compared it with other state-of-the-art defenses considering adaptive attackers and showed our defense is effective even under the worst-case scenario. Besides, we also found that combining other defenses with our framework often achieves better robustness. Our code will be made available at https://github.com/ChenJiyu/meminf-defense.git.",
            "corpus_id": 234785882,
            "sentences": [
                {
                    "corpus_id": "234785882",
                    "title": "Protect privacy of deep classification networks by exploiting their generative power",
                    "text": "One main category of privacy attacks consists of inference attacks, which contains membership inference attacks and attribute inference attacks. Membership inference attacks aim to infer whether an example was in the target model's training dataset, e.g., inferring whether a patient's record was used in medical research. Shokri et al. (2017) designed a black-box membership inference attack against machine learning models. Subsequently, researchers introduced several variants of the attack, such as attacks on GANs (Hayes et al., 2019), VAEs (Hilprecht & H\u00e4rterich, 2019), model explanations (Shokri et al., 2019), and collaborative learning models (Nasr et al., 2019). We focus on mitigating membership inference attacks on DNN classifiers in this paper. \n\nA well-known membership inference attack is the shadow-model attack (Shokri et al., 2017). It requires the attacker to train several shadow models and attack models. To attack a victim model, first, the attacker collects or synthesizes data from the same domain as the victim classifier's training and test data, and divides the data into several private training and test sets. Then, the attacker uses each of the private training set to train one shadow model f i shadow to mimic the behavior of the victim classifier. Next, the attacker sends all the examples in each private training and test set to its corresponding shadow model to create a dataset D that contains the tuple (y, f i shadow (x), I(x)) for each example x , where y is the class label of x , f i shadow (x) is a vector containing the outputs of the i th shadow model, and I(x) indicates whether x is used for training the shadow models. Finally, the attacker partitions D based on the examples' class labels and uses them to train one attack model per class to distinguish the training data from the others.",
                    "score": 0.6411185513759401,
                    "section_title": "Membership inference attacks",
                    "char_start_offset": 6268,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 759
                        },
                        {
                            "start": 762,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1282
                        },
                        {
                            "start": 1283,
                            "end": 1668
                        },
                        {
                            "start": 1669,
                            "end": 1839
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 323,
                            "end": 343,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 519,
                            "end": 539,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 546,
                            "end": 575,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 653,
                            "end": 672,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 830,
                            "end": 851,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: Protect privacy of deep classification networks by exploiting their generative power\n# Venue: Machine-mediated learning\n# Authors: Jiyu Chen, Yiwen Guo, Qianjun Zheng, Hao Chen\n## Abstract\nResearch showed that deep learning models are vulnerable to membership inference attacks, which aim to determine if an example is in the training set of the model. We propose a new framework to defend against this sort of attack. Our key insight is that if we retrain the original classifier with a new dataset that is independent of the original training set while their elements are sampled from the same distribution, the retrained classifier will leak no information that cannot be inferred from the distribution about the original training set. Our framework consists of three phases. First, we transferred the original classifier to a Joint Energy-based Model (JEM) to exploit the model\u2019s implicit generative power. Then, we sampled from the JEM to create a new dataset. Finally, we used the new dataset to retrain or fine-tune the original classifier. We empirically studied different transfer learning schemes for the JEM and fine-tuning/retraining strategies for the classifier against shadow-model attacks. Our evaluation shows that our framework can suppress the attacker\u2019s membership advantage to a negligible level while keeping the classifier\u2019s accuracy acceptable. We compared it with other state-of-the-art defenses considering adaptive attackers and showed our defense is effective even under the worst-case scenario. Besides, we also found that combining other defenses with our framework often achieves better robustness. Our code will be made available at https://github.com/ChenJiyu/meminf-defense.git.\n## Membership inference attacks\nOne main category of privacy attacks consists of inference attacks, which contains membership inference attacks and attribute inference attacks. Membership inference attacks aim to infer whether an example was in the target model's training dataset, e.g., inferring whether a patient's record was used in medical research. Shokri et al. (2017) designed a black-box membership inference attack against machine learning models. Subsequently, researchers introduced several variants of the attack, such as attacks on GANs (Hayes et al., 2019), VAEs (Hilprecht & H\u00e4rterich, 2019), model explanations (Shokri et al., 2019), and collaborative learning models (Nasr et al., 2019). We focus on mitigating membership inference attacks on DNN classifiers in this paper. \n\nA well-known membership inference attack is the shadow-model attack (Shokri et al., 2017). It requires the attacker to train several shadow models and attack models. To attack a victim model, first, the attacker collects or synthesizes data from the same domain as the victim classifier's training and test data, and divides the data into several private training and test sets. Then, the attacker uses each of the private training set to train one shadow model f i shadow to mimic the behavior of the victim classifier. Next, the attacker sends all the examples in each private training and test set to its corresponding shadow model to create a dataset D that contains the tuple (y, f i shadow (x), I(x)) for each example x , where y is the class label of x , f i shadow (x) is a vector containing the outputs of the i th shadow model, and I(x) indicates whether x is used for training the shadow models. Finally, the attacker partitions D based on the examples' class labels and uses them to train one attack model per class to distinguish the training data from the others.",
            "reference_string": "[234785882 | Chen et al. | 2021 | Citations: 8]"
        },
        {
            "title": "Amplifying Membership Exposure via Data Poisoning",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 32,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.00463",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.00463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1711982",
                    "name": "Yufei Chen"
                },
                {
                    "authorId": "2088079836",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2117688523",
                    "name": "Yun Shen"
                },
                {
                    "authorId": "2116638601",
                    "name": "Cong Wang"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "As in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks. Such attacks typically lead to test-time accuracy degradation or controlled misprediction. In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples. To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class. We first propose a generic dirty-label attack for supervised classification algorithms. We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look\"natural\"to evade human moderation. We extensively evaluate our attacks on computer vision benchmarks. Our results show that the proposed attacks can substantially increase the membership inference precision with minimum overall test-time model performance degradation. To mitigate the potential negative impacts of our attacks, we also investigate feasible countermeasures.",
            "corpus_id": 253244424,
            "sentences": [
                {
                    "corpus_id": "253244424",
                    "title": "Amplifying Membership Exposure via Data Poisoning",
                    "text": "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time [24,33,38]. Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13,23]. Henceforth, we use membership inference attacks to demonstrate how to exploit data poisoning to amplify privacy leakage in this paper. \n\nBased on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [17,37,38]. This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased [38], where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased [33,39], where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker [22,27,31]. Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features. \n\nOur Setup. Our evaluation is carried out in the metric-based black-box membership inference setting. We adopt the following metric proposed by Song et al. [40] to measure the membership exposure:2",
                    "score": 0.759462692392547,
                    "section_title": "Membership Inference Attack",
                    "char_start_offset": 6204,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 876
                        },
                        {
                            "start": 879,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1070
                        },
                        {
                            "start": 1071,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1585
                        },
                        {
                            "start": 1588,
                            "end": 1621
                        },
                        {
                            "start": 1622,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2044
                        },
                        {
                            "start": 2047,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2147
                        },
                        {
                            "start": 2148,
                            "end": 2243
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 150,
                            "end": 154,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 154,
                            "end": 157,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 157,
                            "end": 160,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 634,
                            "end": 637,
                            "matchedPaperCorpusId": "17295025"
                        },
                        {
                            "start": 1003,
                            "end": 1007,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 1007,
                            "end": 1010,
                            "matchedPaperCorpusId": "218514464"
                        },
                        {
                            "start": 1010,
                            "end": 1013,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1148,
                            "end": 1152,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1407,
                            "end": 1411,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1411,
                            "end": 1414,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1754,
                            "end": 1758,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 1758,
                            "end": 1761,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1761,
                            "end": 1764,
                            "matchedPaperCorpusId": "174799799"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: Amplifying Membership Exposure via Data Poisoning\n# Venue: Neural Information Processing Systems\n# Authors: Yufei Chen, Chao Shen, Yun Shen, Cong Wang, Yang Zhang\n## Abstract\nAs in-the-wild data are increasingly involved in the training stage, machine learning applications become more susceptible to data poisoning attacks. Such attacks typically lead to test-time accuracy degradation or controlled misprediction. In this paper, we investigate the third type of exploitation of data poisoning - increasing the risks of privacy leakage of benign training samples. To this end, we demonstrate a set of data poisoning attacks to amplify the membership exposure of the targeted class. We first propose a generic dirty-label attack for supervised classification algorithms. We then propose an optimization-based clean-label attack in the transfer learning scenario, whereby the poisoning samples are correctly labeled and look\"natural\"to evade human moderation. We extensively evaluate our attacks on computer vision benchmarks. Our results show that the proposed attacks can substantially increase the membership inference precision with minimum overall test-time model performance degradation. To mitigate the potential negative impacts of our attacks, we also investigate feasible countermeasures.\n## Membership Inference Attack\nIn a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time [24,33,38]. Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13,23]. Henceforth, we use membership inference attacks to demonstrate how to exploit data poisoning to amplify privacy leakage in this paper. \n\nBased on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [17,37,38]. This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased [38], where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased [33,39], where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker [22,27,31]. Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features. \n\nOur Setup. Our evaluation is carried out in the metric-based black-box membership inference setting. We adopt the following metric proposed by Song et al. [40] to measure the membership exposure:2",
            "reference_string": "[253244424 | Chen et al. | 2022 | Citations: 32]"
        },
        {
            "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 24,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.01610",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.01610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1583483502",
                    "name": "Zitao Chen"
                },
                {
                    "authorId": "1715185",
                    "name": "K. Pattabiraman"
                }
            ],
            "abstract": "Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.",
            "corpus_id": 259342605,
            "sentences": [
                {
                    "corpus_id": "259342605",
                    "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
                    "text": "Membership inference attacks. Depending on the adversary capabilities, MIAs can be divided into black-box [37], [48], [17], [3], [38], [8], [47], [27] and white-box attacks [25], [18], [30]. The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies [19], [41], [29] (and in HAMP). Such attacks can be mounted by either shadow-training [37], [29], [48] or computing statistical metrics based on the partial knowledge of the private dataset [38], [8], [27]. Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with [8] or without [27] partial knowledge of the membership information. Carlini et al. [3] introduce the LiRA attack that can succeed in inferring membership when controlled at low false positive or false negative, through a well-calibrated Gaussian likelihood estimate. \n\nIn addition to supervised classification, MIAs have also been explored in other domains, including contrastive learning [28], generative models [7], [13], federated learning [30], graph neural networks [51], and recommender systems [49]. \n\nDefenses against membership inference attacks. These defenses can be divided into provable and practical defenses. The former can provide rigorous privacy guarantee, such as DP-SGD [2], PATE [32]. Nevertheless, these defenses often incur severe accuracy drop when used with acceptable provable bounds [35], [33]. Another line of practical defenses aim to achieve empirical privacy without severely degrading accuracy. Common regularization techniques such as dropout [39], weight decay [24] are shown to be able to reduce privacy leakage, but with limited effectiveness [37], [36].",
                    "score": 0.6095915645494564,
                    "section_title": "VI. RELATED WORK",
                    "char_start_offset": 59723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 29
                        },
                        {
                            "start": 30,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1215
                        },
                        {
                            "start": 1218,
                            "end": 1455
                        },
                        {
                            "start": 1458,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 2039
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 106,
                            "end": 110,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 112,
                            "end": 116,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 124,
                            "end": 127,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 129,
                            "end": 133,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 135,
                            "end": 138,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 146,
                            "end": 150,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 173,
                            "end": 177,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 179,
                            "end": 183,
                            "matchedPaperCorpusId": "218862728"
                        },
                        {
                            "start": 185,
                            "end": 189,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 485,
                            "end": 489,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 491,
                            "end": 495,
                            "matchedPaperCorpusId": "239016142"
                        },
                        {
                            "start": 497,
                            "end": 501,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 571,
                            "end": 575,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 577,
                            "end": 581,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 583,
                            "end": 587,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 675,
                            "end": 679,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 681,
                            "end": 684,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 686,
                            "end": 690,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 948,
                            "end": 951,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 963,
                            "end": 967,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1032,
                            "end": 1035,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1338,
                            "end": 1342,
                            "matchedPaperCorpusId": "237290083"
                        },
                        {
                            "start": 1362,
                            "end": 1365,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 1367,
                            "end": 1371,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1392,
                            "end": 1396,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1420,
                            "end": 1424,
                            "matchedPaperCorpusId": "238407736"
                        },
                        {
                            "start": 1450,
                            "end": 1454,
                            "matchedPaperCorpusId": "237532289"
                        },
                        {
                            "start": 1639,
                            "end": 1642,
                            "matchedPaperCorpusId": "207241585"
                        },
                        {
                            "start": 1759,
                            "end": 1763,
                            "matchedPaperCorpusId": "13699042"
                        },
                        {
                            "start": 1765,
                            "end": 1769,
                            "matchedPaperCorpusId": "220830768"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.974609375
                }
            ],
            "relevance_judgement": 0.974609375,
            "relevance_judgment_input_expanded": "# Title: Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction\n# Venue: Network and Distributed System Security Symposium\n# Authors: Zitao Chen, K. Pattabiraman\n## Abstract\nMachine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving high accuracy. To further reduce privacy risk, HAMP uniformly modifies all the prediction outputs to become low-confidence outputs while preserving the accuracy, which effectively obscures the differences between the prediction on members and non-members. We conduct extensive evaluation on five benchmark datasets, and show that HAMP provides consistently high accuracy and strong membership privacy. Our comparison with seven state-of-the-art defenses shows that HAMP achieves a superior privacy-utility trade off than those techniques.\n## VI. RELATED WORK\nMembership inference attacks. Depending on the adversary capabilities, MIAs can be divided into black-box [37], [48], [17], [3], [38], [8], [47], [27] and white-box attacks [25], [18], [30]. The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies [19], [41], [29] (and in HAMP). Such attacks can be mounted by either shadow-training [37], [29], [48] or computing statistical metrics based on the partial knowledge of the private dataset [38], [8], [27]. Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with [8] or without [27] partial knowledge of the membership information. Carlini et al. [3] introduce the LiRA attack that can succeed in inferring membership when controlled at low false positive or false negative, through a well-calibrated Gaussian likelihood estimate. \n\nIn addition to supervised classification, MIAs have also been explored in other domains, including contrastive learning [28], generative models [7], [13], federated learning [30], graph neural networks [51], and recommender systems [49]. \n\nDefenses against membership inference attacks. These defenses can be divided into provable and practical defenses. The former can provide rigorous privacy guarantee, such as DP-SGD [2], PATE [32]. Nevertheless, these defenses often incur severe accuracy drop when used with acceptable provable bounds [35], [33]. Another line of practical defenses aim to achieve empirical privacy without severely degrading accuracy. Common regularization techniques such as dropout [39], weight decay [24] are shown to be able to reduce privacy leakage, but with limited effectiveness [37], [36].",
            "reference_string": "[259342605 | Chen et al. | 2023 | Citations: 24]"
        },
        {
            "title": "PPA: Preference Profiling Attack Against Federated Learning",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2022,
            "reference_count": 78,
            "citation_count": 23,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14722/ndss.2023.23171",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.04856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1845880105",
                    "name": "Chunyi Zhou"
                },
                {
                    "authorId": "39922366",
                    "name": "Yansong Gao"
                },
                {
                    "authorId": "2068511826",
                    "name": "Anmin Fu"
                },
                {
                    "authorId": "2157740727",
                    "name": "Kai Chen"
                },
                {
                    "authorId": "151498397",
                    "name": "Zhiyang Dai"
                },
                {
                    "authorId": "2116763991",
                    "name": "Zhi Zhang"
                },
                {
                    "authorId": "2837434",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2155342827",
                    "name": "Yuqing Zhang"
                }
            ],
            "abstract": "Federated learning (FL) trains a global model across a number of decentralized users, each with a local dataset. Compared to traditional centralized learning, FL does not require direct access to local datasets and thus aims to mitigate data privacy concerns. However, data privacy leakage in FL still exists due to inference attacks, including membership inference, property inference, and data inversion. In this work, we propose a new type of privacy inference attack, coined Preference Profiling Attack (PPA), that accurately profiles the private preferences of a local user, e.g., most liked (disliked) items from the client's online shopping and most common expressions from the user's selfies. In general, PPA can profile top-k (i.e., k = 1, 2, 3 and k = 1 in particular) preferences contingent on the local client (user)'s characteristics. Our key insight is that the gradient variation of a local user's model has a distinguishable sensitivity to the sample proportion of a given class, especially the majority (minority) class. By observing a user model's gradient sensitivity to a class, PPA can profile the sample proportion of the class in the user's local dataset, and thus the user's preference of the class is exposed. The inherent statistical heterogeneity of FL further facilitates PPA. We have extensively evaluated the PPA's effectiveness using four datasets (MNIST, CIFAR10, RAF-DB and Products-10K). Our results show that PPA achieves 90% and 98% top-1 attack accuracy to the MNIST and CIFAR10, respectively. More importantly, in real-world commercial scenarios of shopping (i.e., Products-10K) and social network (i.e., RAF-DB), PPA gains a top-1 attack accuracy of 78% in the former case to infer the most ordered items (i.e., as a commercial competitor), and 88% in the latter case to infer a victim user's most often facial expressions, e.g., disgusted.",
            "corpus_id": 246706163,
            "sentences": [
                {
                    "corpus_id": "246706163",
                    "title": "PPA: Preference Profiling Attack Against Federated Learning",
                    "text": "The membership inference attack [41] proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. [40] optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. [34] designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. [8] provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. [24] utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. [38] focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks [20], [17], [18] attacked generative model under the white-box and blackbox settings.",
                    "score": 0.6855984155847902,
                    "section_title": "B. Membership Inference Attack",
                    "char_start_offset": 9055,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1332
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 32,
                            "end": 36,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 375,
                            "end": 379,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 624,
                            "end": 627,
                            "matchedPaperCorpusId": "221203089"
                        },
                        {
                            "start": 828,
                            "end": 832,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 1009,
                            "end": 1013,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 1247,
                            "end": 1251,
                            "matchedPaperCorpusId": "244077745"
                        },
                        {
                            "start": 1253,
                            "end": 1257,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1259,
                            "end": 1263,
                            "matchedPaperCorpusId": "199546273"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97412109375
                }
            ],
            "relevance_judgement": 0.97412109375,
            "relevance_judgment_input_expanded": "# Title: PPA: Preference Profiling Attack Against Federated Learning\n# Venue: Network and Distributed System Security Symposium\n# Authors: Chunyi Zhou, Yansong Gao, Anmin Fu, Kai Chen, Zhiyang Dai, Zhi Zhang, Minhui Xue, Yuqing Zhang\n## Abstract\nFederated learning (FL) trains a global model across a number of decentralized users, each with a local dataset. Compared to traditional centralized learning, FL does not require direct access to local datasets and thus aims to mitigate data privacy concerns. However, data privacy leakage in FL still exists due to inference attacks, including membership inference, property inference, and data inversion. In this work, we propose a new type of privacy inference attack, coined Preference Profiling Attack (PPA), that accurately profiles the private preferences of a local user, e.g., most liked (disliked) items from the client's online shopping and most common expressions from the user's selfies. In general, PPA can profile top-k (i.e., k = 1, 2, 3 and k = 1 in particular) preferences contingent on the local client (user)'s characteristics. Our key insight is that the gradient variation of a local user's model has a distinguishable sensitivity to the sample proportion of a given class, especially the majority (minority) class. By observing a user model's gradient sensitivity to a class, PPA can profile the sample proportion of the class in the user's local dataset, and thus the user's preference of the class is exposed. The inherent statistical heterogeneity of FL further facilitates PPA. We have extensively evaluated the PPA's effectiveness using four datasets (MNIST, CIFAR10, RAF-DB and Products-10K). Our results show that PPA achieves 90% and 98% top-1 attack accuracy to the MNIST and CIFAR10, respectively. More importantly, in real-world commercial scenarios of shopping (i.e., Products-10K) and social network (i.e., RAF-DB), PPA gains a top-1 attack accuracy of 78% in the former case to infer the most ordered items (i.e., as a commercial competitor), and 88% in the latter case to infer a victim user's most often facial expressions, e.g., disgusted.\n## B. Membership Inference Attack\nThe membership inference attack [41] proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. [40] optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. [34] designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. [8] provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. [24] utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. [38] focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks [20], [17], [18] attacked generative model under the white-box and blackbox settings.",
            "reference_string": "[246706163 | Zhou et al. | 2022 | Citations: 23]"
        },
        {
            "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 67,
            "citation_count": 22,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2203.09566",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.09566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2033372042",
                    "name": "Ganesh Del Grosso"
                },
                {
                    "authorId": "52195298",
                    "name": "Hamid Jalalzai"
                },
                {
                    "authorId": "134026950",
                    "name": "Georg Pichler"
                },
                {
                    "authorId": "1722055",
                    "name": "C. Palamidessi"
                },
                {
                    "authorId": "1743922",
                    "name": "P. Piantanida"
                }
            ],
            "abstract": "The use of personal data for training machine learning systems comes with a privacy threat and measuring the level of privacy of a model is one of the major challenges in machine learning today. Identifying training data based on a trained model is a standard way of measuring the privacy risks induced by the model. We develop a novel approach to address the problem of membership inference in pattern recognition models, relying on information provided by adversarial examples. The strategy we propose consists of measuring the magnitude of a perturbation necessary to build an adversarial example. Indeed, we argue that this quantity reflects the likelihood of belonging to the training data. Extensive numerical experiments on multivariate data and an array of state-of-the-art target models show that our method performs comparable or even outperforms state-of-the-art strategies, but without requiring any additional training samples.",
            "corpus_id": 247595200,
            "sentences": [
                {
                    "corpus_id": "247595200",
                    "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
                    "text": "To evaluate a membership inference strategy, two groups of samples are needed: samples from the training set and samples outside the training set of the target model. The pre-trained models considered in this work are trained on 50k samples from the CIFAR-100 dataset. The remaining 10k samples constitute the test set. We perform membership inference attacks using nine different strategies. The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker. \n\nIn our analysis we consider a balanced evaluation set and report the AUROC score and the maximum accuracy achieved by each strategy. In this setting, a subset of 10k samples from the training set is selected uniformly as intraining data and the entire test set (10k samples) is selected as out-of-training data. Since the choice of the subset of the training set influences our results, the experiments are repeated 20 times, choosing a different subset each time. All the quantities reported are averaged over these 20 runs of the experiment and the error reported is the empirical standard deviation. The results of this analysis are reported in Tab. 1. For each target model, the best performing attack strategies are highlighted in boldface. Note that the upper part of the table corresponds to strategies that do not require to train an attack model, nor require any additional samples, while the bottom part corresponds to strategies that require training an attack model. The best performing strategy when no additional samples are available is the adversarial distance strategy. Note that this strategy performs consistently across all target models and even surpasses the more resource hungry strategies for the case of AlexNet and ResNext.",
                    "score": 0.6424236491425327,
                    "section_title": "Comparison of MIA strategies",
                    "char_start_offset": 22071,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1147
                        },
                        {
                            "start": 1150,
                            "end": 1282
                        },
                        {
                            "start": 1283,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1805
                        },
                        {
                            "start": 1806,
                            "end": 1895
                        },
                        {
                            "start": 1896,
                            "end": 2128
                        },
                        {
                            "start": 2129,
                            "end": 2236
                        },
                        {
                            "start": 2237,
                            "end": 2399
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Leveraging Adversarial Examples to Quantify Membership Information Leakage\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Ganesh Del Grosso, Hamid Jalalzai, Georg Pichler, C. Palamidessi, P. Piantanida\n## Abstract\nThe use of personal data for training machine learning systems comes with a privacy threat and measuring the level of privacy of a model is one of the major challenges in machine learning today. Identifying training data based on a trained model is a standard way of measuring the privacy risks induced by the model. We develop a novel approach to address the problem of membership inference in pattern recognition models, relying on information provided by adversarial examples. The strategy we propose consists of measuring the magnitude of a perturbation necessary to build an adversarial example. Indeed, we argue that this quantity reflects the likelihood of belonging to the training data. Extensive numerical experiments on multivariate data and an array of state-of-the-art target models show that our method performs comparable or even outperforms state-of-the-art strategies, but without requiring any additional training samples.\n## Comparison of MIA strategies\nTo evaluate a membership inference strategy, two groups of samples are needed: samples from the training set and samples outside the training set of the target model. The pre-trained models considered in this work are trained on 50k samples from the CIFAR-100 dataset. The remaining 10k samples constitute the test set. We perform membership inference attacks using nine different strategies. The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker. \n\nIn our analysis we consider a balanced evaluation set and report the AUROC score and the maximum accuracy achieved by each strategy. In this setting, a subset of 10k samples from the training set is selected uniformly as intraining data and the entire test set (10k samples) is selected as out-of-training data. Since the choice of the subset of the training set influences our results, the experiments are repeated 20 times, choosing a different subset each time. All the quantities reported are averaged over these 20 runs of the experiment and the error reported is the empirical standard deviation. The results of this analysis are reported in Tab. 1. For each target model, the best performing attack strategies are highlighted in boldface. Note that the upper part of the table corresponds to strategies that do not require to train an attack model, nor require any additional samples, while the bottom part corresponds to strategies that require training an attack model. The best performing strategy when no additional samples are available is the adversarial distance strategy. Note that this strategy performs consistently across all target models and even surpasses the more resource hungry strategies for the case of AlexNet and ResNext.",
            "reference_string": "[247595200 | Grosso et al. | 2022 | Citations: 22]"
        },
        {
            "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
            "venue": "Knowledge Discovery and Data Mining",
            "year": 2022,
            "reference_count": 56,
            "citation_count": 14,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.12401",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.12401, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2173325427",
                    "name": "Na Huang"
                },
                {
                    "authorId": "143770118",
                    "name": "Fei Sun"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "2889630",
                    "name": "Hengliang Luo"
                },
                {
                    "authorId": "1696030",
                    "name": "M. de Rijke"
                },
                {
                    "authorId": "2780667",
                    "name": "Z. Ren"
                }
            ],
            "abstract": "Learned recommender systems may inadvertently leak information about their training data, leading to privacy violations. We investigate privacy threats faced by recommender systems through the lens of membership inference. In such attacks, an adversary aims to infer whether a user's data is used to train the target recommender. To achieve this, previous work has used a shadow recommender to derive training data for the attack model, and then predicts the membership by calculating difference vectors between users' historical interactions and recommended items. State-of-the-art methods face two challenging problems: (i) training data for the attack model is biased due to the gap between shadow and target recommenders, and (ii) hidden states in recommenders are not observational, resulting in inaccurate estimations of difference vectors. To address the above limitations, we propose a Debiasing Learning for Membership Inference Attacks against recommender systems (DL-MIA) framework that has four main components: (i) a difference vector generator, (ii) a disentangled encoder, (iii) a weight estimator, and (iv) an attack model. To mitigate the gap between recommenders, a variational auto-encoder (VAE) based disentangled encoder is devised to identify recommender invariant and specific features. To reduce the estimation bias, we design a weight estimator, assigning a truth-level score for each difference vector to indicate estimation accuracy. We evaluate DL-MIA against both general recommenders and sequential recommenders on three real-world datasets. Experimental results show that DL-MIA effectively alleviates training and estimation biases simultaneously, and \u00cdachieves state-of-the-art attack performance.",
            "corpus_id": 250048543,
            "sentences": [
                {
                    "corpus_id": "250048543",
                    "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
                    "text": "Recently, membership inference attacks (MIAs) have achieved promising performance in various domains, such as biomedical data [2,13,18] and mobility traces [37]. The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. [46] specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. [42] further relax several key assumptions from [46], including knowledge of the target model architecture and target dataset distribution. Yeom et al. [52] explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. [35] study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks [9,29] assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. [53] investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders. \n\nTo mitigate the attacks, some defense mechanisms, including model stacking [42], dropout [42], adversarial training [34], differential privacy [9,29], regularization [9,29], and jointly maximizing privacy and prediction accuracy [19], have been proposed. To protect membership privacy of recommender systems, Zhang et al. [53] design a defense mechanism, named Popularity Randomization, and randomly recommend popular items to non-member users.",
                    "score": 0.6307494461875687,
                    "section_title": "Membership inference attacks",
                    "char_start_offset": 5381,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1415
                        },
                        {
                            "start": 1418,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1862
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 126,
                            "end": 129,
                            "matchedPaperCorpusId": "1154437"
                        },
                        {
                            "start": 129,
                            "end": 132,
                            "matchedPaperCorpusId": "52205573"
                        },
                        {
                            "start": 132,
                            "end": 135,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 156,
                            "end": 160,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 295,
                            "end": 299,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 667,
                            "end": 671,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 715,
                            "end": 719,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 819,
                            "end": 823,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 949,
                            "end": 953,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1097,
                            "end": 1100,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1100,
                            "end": 1103,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1272,
                            "end": 1276,
                            "matchedPaperCorpusId": "237532289"
                        },
                        {
                            "start": 1493,
                            "end": 1497,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1507,
                            "end": 1511,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1534,
                            "end": 1538,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1561,
                            "end": 1564,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1564,
                            "end": 1567,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1584,
                            "end": 1587,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1587,
                            "end": 1590,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1647,
                            "end": 1651,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1740,
                            "end": 1744,
                            "matchedPaperCorpusId": "237532289"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Debiasing Learning for Membership Inference Attacks Against Recommender Systems\n# Venue: Knowledge Discovery and Data Mining\n# Authors: Zihan Wang, Na Huang, Fei Sun, Pengjie Ren, Zhumin Chen, Hengliang Luo, M. de Rijke, Z. Ren\n## Abstract\nLearned recommender systems may inadvertently leak information about their training data, leading to privacy violations. We investigate privacy threats faced by recommender systems through the lens of membership inference. In such attacks, an adversary aims to infer whether a user's data is used to train the target recommender. To achieve this, previous work has used a shadow recommender to derive training data for the attack model, and then predicts the membership by calculating difference vectors between users' historical interactions and recommended items. State-of-the-art methods face two challenging problems: (i) training data for the attack model is biased due to the gap between shadow and target recommenders, and (ii) hidden states in recommenders are not observational, resulting in inaccurate estimations of difference vectors. To address the above limitations, we propose a Debiasing Learning for Membership Inference Attacks against recommender systems (DL-MIA) framework that has four main components: (i) a difference vector generator, (ii) a disentangled encoder, (iii) a weight estimator, and (iv) an attack model. To mitigate the gap between recommenders, a variational auto-encoder (VAE) based disentangled encoder is devised to identify recommender invariant and specific features. To reduce the estimation bias, we design a weight estimator, assigning a truth-level score for each difference vector to indicate estimation accuracy. We evaluate DL-MIA against both general recommenders and sequential recommenders on three real-world datasets. Experimental results show that DL-MIA effectively alleviates training and estimation biases simultaneously, and \u00cdachieves state-of-the-art attack performance.\n## Membership inference attacks\nRecently, membership inference attacks (MIAs) have achieved promising performance in various domains, such as biomedical data [2,13,18] and mobility traces [37]. The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. [46] specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. [42] further relax several key assumptions from [46], including knowledge of the target model architecture and target dataset distribution. Yeom et al. [52] explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. [35] study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks [9,29] assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. [53] investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders. \n\nTo mitigate the attacks, some defense mechanisms, including model stacking [42], dropout [42], adversarial training [34], differential privacy [9,29], regularization [9,29], and jointly maximizing privacy and prediction accuracy [19], have been proposed. To protect membership privacy of recommender systems, Zhang et al. [53] design a defense mechanism, named Popularity Randomization, and randomly recommend popular items to non-member users.",
            "reference_string": "[250048543 | Wang et al. | 2022 | Citations: 14]"
        },
        {
            "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
            "venue": "USENIX Security Symposium",
            "year": 2019,
            "reference_count": 59,
            "citation_count": 257,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.01067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66697271",
                    "name": "A. Salem"
                },
                {
                    "authorId": "3407762",
                    "name": "Apratim Bhattacharyya"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "1739548",
                    "name": "Mario Fritz"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. \nIn this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model (CBM-GAN) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.",
            "corpus_id": 91184074,
            "sentences": [
                {
                    "corpus_id": "91184074",
                    "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
                    "text": "Membership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data [18,21] and location data [36,37]. Shokri et al. [40] propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28,31,32,38,42,53]. \n\nIn theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings. \n\nModel Inversion. Fredrikson et al. [12] propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g.\" reconstructing recognizable human faces [11,20]. As pointed out by other works [29,40], model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. \n\nModel Stealing. Another related line of work is model stealing. Tram\u00e8r et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's parameters.",
                    "score": 0.6797168317941028,
                    "section_title": "Related Works",
                    "char_start_offset": 46212,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 22,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 780
                        },
                        {
                            "start": 783,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1376
                        },
                        {
                            "start": 1379,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1475
                        },
                        {
                            "start": 1476,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1986
                        },
                        {
                            "start": 1989,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2052
                        },
                        {
                            "start": 2053,
                            "end": 2159
                        },
                        {
                            "start": 2160,
                            "end": 2237
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 186,
                            "end": 190,
                            "matchedPaperCorpusId": "52205573"
                        },
                        {
                            "start": 212,
                            "end": 216,
                            "matchedPaperCorpusId": "3570809"
                        },
                        {
                            "start": 235,
                            "end": 239,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 764,
                            "end": 767,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 767,
                            "end": 770,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 770,
                            "end": 773,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 776,
                            "end": 779,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1414,
                            "end": 1418,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1739,
                            "end": 1743,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 1743,
                            "end": 1746,
                            "matchedPaperCorpusId": "5051282"
                        },
                        {
                            "start": 1778,
                            "end": 1782,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1782,
                            "end": 1785,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 2067,
                            "end": 2071,
                            "matchedPaperCorpusId": "2984526"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97216796875
                }
            ],
            "relevance_judgement": 0.97216796875,
            "relevance_judgment_input_expanded": "# Title: Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning\n# Venue: USENIX Security Symposium\n# Authors: A. Salem, Apratim Bhattacharyya, M. Backes, Mario Fritz, Yang Zhang\n## Abstract\nMachine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. \nIn this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular, we propose a hybrid generative model (CBM-GAN) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.\n## Related Works\nMembership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data [18,21] and location data [36,37]. Shokri et al. [40] propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28,31,32,38,42,53]. \n\nIn theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings. \n\nModel Inversion. Fredrikson et al. [12] propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g.\" reconstructing recognizable human faces [11,20]. As pointed out by other works [29,40], model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. \n\nModel Stealing. Another related line of work is model stealing. Tram\u00e8r et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's parameters.",
            "reference_string": "[91184074 | Salem et al. | 2019 | Citations: 257]"
        },
        {
            "title": "When Relaxations Go Bad: \"Differentially-Private\" Machine Learning",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 84,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.08874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348109",
                    "name": "Bargav Jayaraman"
                },
                {
                    "authorId": "145685504",
                    "name": "David Evans"
                }
            ],
            "abstract": "Differential privacy is becoming a standard notion for performing privacy-preserving machine learning over sensitive data. It provides formal guarantees, in terms of the privacy budget, $\\epsilon$, on how much information about individual training records is leaked by the model. While the privacy budget is directly correlated to the privacy leakage, the calibration of the privacy budget is not well understood. As a result, many existing works on privacy-preserving machine learning select large values of $\\epsilon$ in order to get acceptable utility of the model, with little understanding of the concrete impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used which require privacy guarantees for each iteration, relaxed definitions of differential privacy are often used which further tradeoff privacy for better utility. In this paper, we evaluate the impacts of these choices on privacy in experiments with logistic regression and neural network models. We quantify the privacy leakage in terms of advantage of the adversary performing inference attacks and by analyzing the number of members at risk for exposure. Our main findings are that current mechanisms for differential privacy for machine learning rarely offer acceptable utility-privacy tradeoffs: settings that provide limited accuracy loss provide little effective privacy, and settings that provide strong privacy result in useless models. Open source code is available at this https URL.",
            "corpus_id": 67855651,
            "sentences": [
                {
                    "corpus_id": "67855651",
                    "title": "When Relaxations Go Bad: \"Differentially-Private\" Machine Learning",
                    "text": "The aim of a membership inference attack is to infer whether or not a given record is present in the training set. Membership inference attacks can uncover highly sensitive information from training data. An early membership inference attack showed that it is possible to identify individuals contributing DNA to studies that analyze a mixture of DNA from many individuals, using a statistical distance measure to determine if a known individual is in the mixture [28]. \n\nMembership inference attacks can either be completely black-box where an attacker only has query access to the target model [62], or can assume that the attacker has full white-box access to the target model, along with some auxillary information [76]. The first membership inference attack on machine learning was proposed by Shokri et al. [62]. They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62].",
                    "score": 0.6504977520006684,
                    "section_title": "Membership Inference",
                    "char_start_offset": 21108,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 469
                        },
                        {
                            "start": 472,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1437
                        },
                        {
                            "start": 1438,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 2058
                        },
                        {
                            "start": 2061,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2311
                        },
                        {
                            "start": 2312,
                            "end": 2419
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 464,
                            "end": 468,
                            "matchedPaperCorpusId": "916355"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 719,
                            "end": 723,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 813,
                            "end": 817,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9716796875
                }
            ],
            "relevance_judgement": 0.9716796875,
            "relevance_judgment_input_expanded": "# Title: When Relaxations Go Bad: \"Differentially-Private\" Machine Learning\n# Venue: arXiv.org\n# Authors: Bargav Jayaraman, David Evans\n## Abstract\nDifferential privacy is becoming a standard notion for performing privacy-preserving machine learning over sensitive data. It provides formal guarantees, in terms of the privacy budget, $\\epsilon$, on how much information about individual training records is leaked by the model. While the privacy budget is directly correlated to the privacy leakage, the calibration of the privacy budget is not well understood. As a result, many existing works on privacy-preserving machine learning select large values of $\\epsilon$ in order to get acceptable utility of the model, with little understanding of the concrete impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used which require privacy guarantees for each iteration, relaxed definitions of differential privacy are often used which further tradeoff privacy for better utility. In this paper, we evaluate the impacts of these choices on privacy in experiments with logistic regression and neural network models. We quantify the privacy leakage in terms of advantage of the adversary performing inference attacks and by analyzing the number of members at risk for exposure. Our main findings are that current mechanisms for differential privacy for machine learning rarely offer acceptable utility-privacy tradeoffs: settings that provide limited accuracy loss provide little effective privacy, and settings that provide strong privacy result in useless models. Open source code is available at this https URL.\n## Membership Inference\nThe aim of a membership inference attack is to infer whether or not a given record is present in the training set. Membership inference attacks can uncover highly sensitive information from training data. An early membership inference attack showed that it is possible to identify individuals contributing DNA to studies that analyze a mixture of DNA from many individuals, using a statistical distance measure to determine if a known individual is in the mixture [28]. \n\nMembership inference attacks can either be completely black-box where an attacker only has query access to the target model [62], or can assume that the attacker has full white-box access to the target model, along with some auxillary information [76]. The first membership inference attack on machine learning was proposed by Shokri et al. [62]. They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62].",
            "reference_string": "[67855651 | Jayaraman et al. | 2019 | Citations: 7]"
        },
        {
            "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
            "venue": "Conference on Computer and Communications Security",
            "year": 2022,
            "reference_count": 54,
            "citation_count": 111,
            "influential_citation_count": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2208.14933",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.14933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182511319",
                    "name": "Yiyong Liu"
                },
                {
                    "authorId": "2277275",
                    "name": "Zhengyu Zhao"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
            "corpus_id": 251953448,
            "sentences": [
                {
                    "corpus_id": "251953448",
                    "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
                    "text": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
                    "score": 0.5974900459914566,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9716796875
                }
            ],
            "relevance_judgement": 0.9716796875,
            "relevance_judgment_input_expanded": "# Title: Membership Inference Attacks by Exploiting Loss Trajectory\n# Venue: Conference on Computer and Communications Security\n# Authors: Yiyong Liu, Zhengyu Zhao, M. Backes, Yang Zhang\n## Abstract\nMachine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.\n",
            "reference_string": "[251953448 | Liu et al. | 2022 | Citations: 111]"
        },
        {
            "title": "FedDroidMeter: A Privacy Risk Evaluator for FL-Based Android Malware Classification Systems",
            "venue": "Entropy",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/e25071053",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10378591, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164682257",
                    "name": "Changnan Jiang"
                },
                {
                    "authorId": "2056988799",
                    "name": "Chunhe Xia"
                },
                {
                    "authorId": "2118483456",
                    "name": "Zhuodong Liu"
                },
                {
                    "authorId": "72259376",
                    "name": "Tianbo Wang"
                }
            ],
            "abstract": "In traditional centralized Android malware classifiers based on machine learning, the training sample uploaded by users contains sensitive personal information, such as app usage and device security status, which will undermine personal privacy if used directly by the server. Federated-learning-based Android malware classifiers have attracted much attention due to their privacy-preserving and multi-party joint modeling. However, research shows that indirect privacy inferences from curious central servers threaten this framework. We propose a privacy risk evaluation framework, FedDroidMeter, based on normalized mutual information in response to user privacy requirements to measure the privacy risk in FL-based malware classifiers. It captures the essential cause of the disclosure of sensitive information in classifiers, independent of the attack model and capability. We performed numerical assessments using the Androzoo dataset, the baseline FL-based classifiers, the privacy-inferred attack model, and the baseline methodology of privacy evaluation. The experimental results show that FedDroidMeter can measure the privacy risks of the classifiers more effectively. Meanwhile, by comparing different models, FL, and privacy parameter settings, we proved that FedDroidMeter could compare the privacy risk between different use cases equally. Finally, we preliminarily study the law of privacy risk in classifiers. The experimental results emphasize the importance of providing a systematic privacy risk evaluation framework for FL-based malware classifiers and provide experience and a theoretical basis for studying targeted defense methods.",
            "corpus_id": 259878257,
            "sentences": [
                {
                    "corpus_id": "259878257",
                    "title": "FedDroidMeter: A Privacy Risk Evaluator for FL-Based Android Malware Classification Systems",
                    "text": "There are four types of privacy inference attacks: attribute inference attack (AInf), model extraction attack (MExt), model inversion attack (MInv), and member inference attack (MInf).\n\nMember inference attacks are designed to determine whether candidate data belongs to the training dataset of the target model. Shokri et al. [8] proposed the first membership inference attack method for the black-box ML model. They trained multiple shadow models to simulate the target model and then use multiple attack models for reasoning. Salem et al. later relaxed several key assumptions [18], using multiple shadow models, knowledge of the structure of the target model, and having the same distribution of data sets as the auxiliary knowledge. Using new insights on how overfitting occurs in deep neural networks, Fredrikson [19] showed how the model's particular use of features provides evidence of membership for white-box attackers and demonstrates that such attacks are superior to previous black-box approaches. Shafran [20] used a new predictability error that can be calculated for each sample and does not require a training set. The membership error is used to achieve a higher precision of membership inference on many baseline models by subtracting the predictability error from the reconstruction error.\n\nThe attribute inference attack was first proposed by Ateniese [21]. An internal attacker trains a meta-classifier by collecting the intermediate results of the updated model to infer sensitive statistical attributes contained in the training dataset of other actors. Later, Melis [22] proposed the first sample-level attribute inference attack against federated learning systems. Song [23] revealed that the inference of risk attributes is caused by the overlearning characteristic inherent in ML models.\n\nModel inversion is mainly divided into data reconstruction and training class inference. Model inversion was first proposed by Fredrikson [24] in the task of drug dose classification. Fredrikson proposed a new inversion attack method in the article [25], aiming to reconstruct representative samples for each type of target model. They extend model inversion to general ML settings using the backpropagation of the target ML model parameters. Carlini [26] showed that model inversion can also be performed efficiently for natural language processing models. In the article [27], the author proposed an attack method to infer participants' private training data using the shared gradient",
                    "score": 0.6642629214051974,
                    "section_title": "Existing Inferential Attack Methods on Federated Learning",
                    "char_start_offset": 8248,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 327,
                            "end": 330,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 819,
                            "end": 823,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 1020,
                            "end": 1024,
                            "matchedPaperCorpusId": "231925091"
                        },
                        {
                            "start": 1374,
                            "end": 1378,
                            "matchedPaperCorpusId": "14757739"
                        },
                        {
                            "start": 1697,
                            "end": 1701,
                            "matchedPaperCorpusId": "167217888"
                        },
                        {
                            "start": 1956,
                            "end": 1960,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 2067,
                            "end": 2071,
                            "matchedPaperCorpusId": "207229839"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97119140625
                }
            ],
            "relevance_judgement": 0.97119140625,
            "relevance_judgment_input_expanded": "# Title: FedDroidMeter: A Privacy Risk Evaluator for FL-Based Android Malware Classification Systems\n# Venue: Entropy\n# Authors: Changnan Jiang, Chunhe Xia, Zhuodong Liu, Tianbo Wang\n## Abstract\nIn traditional centralized Android malware classifiers based on machine learning, the training sample uploaded by users contains sensitive personal information, such as app usage and device security status, which will undermine personal privacy if used directly by the server. Federated-learning-based Android malware classifiers have attracted much attention due to their privacy-preserving and multi-party joint modeling. However, research shows that indirect privacy inferences from curious central servers threaten this framework. We propose a privacy risk evaluation framework, FedDroidMeter, based on normalized mutual information in response to user privacy requirements to measure the privacy risk in FL-based malware classifiers. It captures the essential cause of the disclosure of sensitive information in classifiers, independent of the attack model and capability. We performed numerical assessments using the Androzoo dataset, the baseline FL-based classifiers, the privacy-inferred attack model, and the baseline methodology of privacy evaluation. The experimental results show that FedDroidMeter can measure the privacy risks of the classifiers more effectively. Meanwhile, by comparing different models, FL, and privacy parameter settings, we proved that FedDroidMeter could compare the privacy risk between different use cases equally. Finally, we preliminarily study the law of privacy risk in classifiers. The experimental results emphasize the importance of providing a systematic privacy risk evaluation framework for FL-based malware classifiers and provide experience and a theoretical basis for studying targeted defense methods.\n## Existing Inferential Attack Methods on Federated Learning\nThere are four types of privacy inference attacks: attribute inference attack (AInf), model extraction attack (MExt), model inversion attack (MInv), and member inference attack (MInf).\n\nMember inference attacks are designed to determine whether candidate data belongs to the training dataset of the target model. Shokri et al. [8] proposed the first membership inference attack method for the black-box ML model. They trained multiple shadow models to simulate the target model and then use multiple attack models for reasoning. Salem et al. later relaxed several key assumptions [18], using multiple shadow models, knowledge of the structure of the target model, and having the same distribution of data sets as the auxiliary knowledge. Using new insights on how overfitting occurs in deep neural networks, Fredrikson [19] showed how the model's particular use of features provides evidence of membership for white-box attackers and demonstrates that such attacks are superior to previous black-box approaches. Shafran [20] used a new predictability error that can be calculated for each sample and does not require a training set. The membership error is used to achieve a higher precision of membership inference on many baseline models by subtracting the predictability error from the reconstruction error.\n\nThe attribute inference attack was first proposed by Ateniese [21]. An internal attacker trains a meta-classifier by collecting the intermediate results of the updated model to infer sensitive statistical attributes contained in the training dataset of other actors. Later, Melis [22] proposed the first sample-level attribute inference attack against federated learning systems. Song [23] revealed that the inference of risk attributes is caused by the overlearning characteristic inherent in ML models.\n\nModel inversion is mainly divided into data reconstruction and training class inference. Model inversion was first proposed by Fredrikson [24] in the task of drug dose classification. Fredrikson proposed a new inversion attack method in the article [25], aiming to reconstruct representative samples for each type of target model. They extend model inversion to general ML settings using the backpropagation of the target ML model parameters. Carlini [26] showed that model inversion can also be performed efficiently for natural language processing models. In the article [27], the author proposed an attack method to infer participants' private training data using the shared gradient",
            "reference_string": "[259878257 | Jiang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
            "venue": "USENIX Security Symposium",
            "year": 2021,
            "reference_count": 59,
            "citation_count": 80,
            "influential_citation_count": 19,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.08324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2048002984",
                    "name": "Xinyu Tang"
                },
                {
                    "authorId": "2364685227",
                    "name": "Saeed Mahloujifar"
                },
                {
                    "authorId": "144173853",
                    "name": "Liwei Song"
                },
                {
                    "authorId": "148318826",
                    "name": "Virat Shejwalkar"
                },
                {
                    "authorId": "3490923",
                    "name": "Milad Nasr"
                },
                {
                    "authorId": "1972973",
                    "name": "Amir Houmansadr"
                },
                {
                    "authorId": "143615345",
                    "name": "Prateek Mittal"
                }
            ],
            "abstract": "Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models. These attacks aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. The goal of this work is to train ML models that have high membership privacy while largely preserving their utility; we therefore aim for an empirical membership privacy guarantee as opposed to the provable privacy guarantees provided by techniques like differential privacy, as such techniques are shown to deteriorate model utility. Specifically, we propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense is a novel ensemble architecture for training. This architecture, which we call Split-AI, splits the training data into random subsets, and trains a model on each subset of the data. We use an adaptive inference strategy at test time: our ensemble architecture aggregates the outputs of only those models that did not contain the input sample in their training data. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks. The Self-Distillation component (self-)distills the training dataset through our Split-AI ensemble, without using any external public datasets. Through extensive experiments on major benchmark datasets we show that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.",
            "corpus_id": 239016142,
            "sentences": [
                {
                    "corpus_id": "239016142",
                    "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
                    "text": "Membership inference attacks against machine learning. MIAs are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28,29]. Jayaraman et al. [20] analyze MIA in more realistic assumptions by relaxing proportion of training set size and testing set size in the MIA set up to be any positive value instead of 1. Hui et al. [18] study MIA in a practical scenario, assuming no true labels of target samples are known and utilizing differential comparison for MIAs. Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model [26,33], which can exploit model parameters to infer membership information. Membership inference defenses for machine learning. Membership inference defenses can be divided into two main categories. One category of defenses are specifically designed to mitigate such attacks. It has been shown that techniques to improve a model's generalization ability, including regularization [25] and dropout [48], can decrease the MIA success [41,43] limitedly. Several defenses [27,32] propose to add a specific constraint during training to mitigate the difference of model behavior on models and non-models. These optimization problems under multiple constraints in training are usually computationally hard to solve. Post-processing techniques on prediction vectors are also applied on membership inference defenses [21,53]. Note that these defenses which obfuscate prediction vectors can not defend against label-only attacks [8,28]. Moreover, Song et al. [46] re-evaluate two state-of-the-art defenses (adversarial regularization [32] and MemGuard [21]) and find that both of them underestimated metric-based attacks. Shejwalkar et al. [42] propose distillation of public data to protect membership privacy. However, public dataset is not usually available in many practical scenarios.",
                    "score": 0.6055000261460968,
                    "section_title": "Related Work",
                    "char_start_offset": 53098,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 54
                        },
                        {
                            "start": 55,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1858
                        },
                        {
                            "start": 1859,
                            "end": 2043
                        },
                        {
                            "start": 2044,
                            "end": 2133
                        },
                        {
                            "start": 2134,
                            "end": 2211
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 102,
                            "end": 106,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 106,
                            "end": 109,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 109,
                            "end": 112,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 473,
                            "end": 476,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 476,
                            "end": 479,
                            "matchedPaperCorpusId": "218862728"
                        },
                        {
                            "start": 479,
                            "end": 482,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 504,
                            "end": 508,
                            "matchedPaperCorpusId": "218862728"
                        },
                        {
                            "start": 684,
                            "end": 688,
                            "matchedPaperCorpusId": "230523638"
                        },
                        {
                            "start": 930,
                            "end": 934,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 934,
                            "end": 937,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 1311,
                            "end": 1315,
                            "matchedPaperCorpusId": "10137788"
                        },
                        {
                            "start": 1363,
                            "end": 1367,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 1367,
                            "end": 1370,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1399,
                            "end": 1403,
                            "matchedPaperCorpusId": "221297671"
                        },
                        {
                            "start": 1403,
                            "end": 1406,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1740,
                            "end": 1744,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 1851,
                            "end": 1854,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 1854,
                            "end": 1857,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1956,
                            "end": 1960,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 1974,
                            "end": 1978,
                            "matchedPaperCorpusId": "202734167"
                        },
                        {
                            "start": 2062,
                            "end": 2066,
                            "matchedPaperCorpusId": "235349092"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture\n# Venue: USENIX Security Symposium\n# Authors: Xinyu Tang, Saeed Mahloujifar, Liwei Song, Virat Shejwalkar, Milad Nasr, Amir Houmansadr, Prateek Mittal\n## Abstract\nMembership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models. These attacks aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. The goal of this work is to train ML models that have high membership privacy while largely preserving their utility; we therefore aim for an empirical membership privacy guarantee as opposed to the provable privacy guarantees provided by techniques like differential privacy, as such techniques are shown to deteriorate model utility. Specifically, we propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense is a novel ensemble architecture for training. This architecture, which we call Split-AI, splits the training data into random subsets, and trains a model on each subset of the data. We use an adaptive inference strategy at test time: our ensemble architecture aggregates the outputs of only those models that did not contain the input sample in their training data. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks. The Self-Distillation component (self-)distills the training dataset through our Split-AI ensemble, without using any external public datasets. Through extensive experiments on major benchmark datasets we show that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.\n## Related Work\nMembership inference attacks against machine learning. MIAs are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28,29]. Jayaraman et al. [20] analyze MIA in more realistic assumptions by relaxing proportion of training set size and testing set size in the MIA set up to be any positive value instead of 1. Hui et al. [18] study MIA in a practical scenario, assuming no true labels of target samples are known and utilizing differential comparison for MIAs. Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model [26,33], which can exploit model parameters to infer membership information. Membership inference defenses for machine learning. Membership inference defenses can be divided into two main categories. One category of defenses are specifically designed to mitigate such attacks. It has been shown that techniques to improve a model's generalization ability, including regularization [25] and dropout [48], can decrease the MIA success [41,43] limitedly. Several defenses [27,32] propose to add a specific constraint during training to mitigate the difference of model behavior on models and non-models. These optimization problems under multiple constraints in training are usually computationally hard to solve. Post-processing techniques on prediction vectors are also applied on membership inference defenses [21,53]. Note that these defenses which obfuscate prediction vectors can not defend against label-only attacks [8,28]. Moreover, Song et al. [46] re-evaluate two state-of-the-art defenses (adversarial regularization [32] and MemGuard [21]) and find that both of them underestimated metric-based attacks. Shejwalkar et al. [42] propose distillation of public data to protect membership privacy. However, public dataset is not usually available in many practical scenarios.",
            "reference_string": "[239016142 | Tang et al. | 2021 | Citations: 80]"
        },
        {
            "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
            "venue": "Proceedings on Privacy Enhancing Technologies",
            "year": 2019,
            "reference_count": 33,
            "citation_count": 191,
            "influential_citation_count": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://content.sciendo.com/downloadpdf/journals/popets/2019/4/article-p232.pdf",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.03006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "81786870",
                    "name": "Benjamin Hilprecht"
                },
                {
                    "authorId": "2736329",
                    "name": "Martin H\u00e4rterich"
                },
                {
                    "authorId": "13047311",
                    "name": "Daniel Bernau"
                }
            ],
            "abstract": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
            "corpus_id": 199546273,
            "sentences": [
                {
                    "corpus_id": "199546273",
                    "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
                    "text": "The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model.",
                    "score": 0.5884829424681519,
                    "section_title": "Background of Membership Inference",
                    "char_start_offset": 6352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 759
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.970703125
                }
            ],
            "relevance_judgement": 0.970703125,
            "relevance_judgment_input_expanded": "# Title: Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models\n# Venue: Proceedings on Privacy Enhancing Technologies\n# Authors: Benjamin Hilprecht, Martin H\u00e4rterich, Daniel Bernau\n## Abstract\nAbstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.\n## Background of Membership Inference\nThe goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model.",
            "reference_string": "[199546273 | Hilprecht et al. | 2019 | Citations: 191]"
        },
        {
            "title": "Evaluating Membership Inference Through Adversarial Robustness",
            "venue": "Computer/law journal",
            "year": 2022,
            "reference_count": 37,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.06986",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.06986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2156121564",
                    "name": "Zhaoxi Zhang"
                },
                {
                    "authorId": "2248789322",
                    "name": "Leo Yu Zhang"
                },
                {
                    "authorId": "2110300762",
                    "name": "Xufei Zheng"
                },
                {
                    "authorId": "2165378948",
                    "name": "Bilal Hussain Abbasi"
                },
                {
                    "authorId": "3115562",
                    "name": "Shengshan Hu"
                }
            ],
            "abstract": "The usage of deep learning is being escalated in many applications. Due to its outstanding performance, it is being used in a variety of security and privacy-sensitive areas in addition to conventional applications. One of the key aspects of deep learning efficacy is to have abundant data. This trait leads to the usage of data which can be highly sensitive and private, which in turn causes wariness with regard to deep learning in the general public. Membership inference attacks are considered lethal as they can be used to figure out whether a piece of data belongs to the training dataset or not. This can be problematic with regards to leakage of training data information and its characteristics. To highlight the significance of these types of attacks, we propose an enhanced methodology for membership inference attacks based on adversarial robustness, by adjusting the directions of adversarial perturbations through label smoothing under a white-box setting. We evaluate our proposed method on three datasets: Fashion-MNIST, CIFAR-10, and CIFAR-100. Our experimental results reveal that the performance of our method surpasses that of the existing adversarial robustness-based method when attacking normally trained models. Additionally, through comparing our technique with the state-of-the-art metric-based membership inference methods, our proposed method also shows better performance when attacking adversarially trained models. The code for reproducing the results of this work is available at \\url{https://github.com/plll4zzx/Evaluating-Membership-Inference-Through-Adversarial-Robustness}.",
            "corpus_id": 248810845,
            "sentences": [
                {
                    "corpus_id": "248810845",
                    "title": "Evaluating Membership Inference Through Adversarial Robustness",
                    "text": "As mentioned above, membership inference attack aims to figure out whether a piece of data belongs to the training dataset or not. There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings. \n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18,22].",
                    "score": 0.9283853679507632,
                    "section_title": "Privacy Leakage and Membership Inference",
                    "char_start_offset": 8957,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1651
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 481,
                            "end": 485,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 590,
                            "end": 594,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 594,
                            "end": 597,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 597,
                            "end": 600,
                            "matchedPaperCorpusId": "174799799"
                        },
                        {
                            "start": 600,
                            "end": 603,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 928,
                            "end": 932,
                            "matchedPaperCorpusId": "214623088"
                        },
                        {
                            "start": 1177,
                            "end": 1181,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1640,
                            "end": 1644,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1644,
                            "end": 1647,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1647,
                            "end": 1650,
                            "matchedPaperCorpusId": "214623088"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Evaluating Membership Inference Through Adversarial Robustness\n# Venue: Computer/law journal\n# Authors: Zhaoxi Zhang, Leo Yu Zhang, Xufei Zheng, Bilal Hussain Abbasi, Shengshan Hu\n## Abstract\nThe usage of deep learning is being escalated in many applications. Due to its outstanding performance, it is being used in a variety of security and privacy-sensitive areas in addition to conventional applications. One of the key aspects of deep learning efficacy is to have abundant data. This trait leads to the usage of data which can be highly sensitive and private, which in turn causes wariness with regard to deep learning in the general public. Membership inference attacks are considered lethal as they can be used to figure out whether a piece of data belongs to the training dataset or not. This can be problematic with regards to leakage of training data information and its characteristics. To highlight the significance of these types of attacks, we propose an enhanced methodology for membership inference attacks based on adversarial robustness, by adjusting the directions of adversarial perturbations through label smoothing under a white-box setting. We evaluate our proposed method on three datasets: Fashion-MNIST, CIFAR-10, and CIFAR-100. Our experimental results reveal that the performance of our method surpasses that of the existing adversarial robustness-based method when attacking normally trained models. Additionally, through comparing our technique with the state-of-the-art metric-based membership inference methods, our proposed method also shows better performance when attacking adversarially trained models. The code for reproducing the results of this work is available at \\url{https://github.com/plll4zzx/Evaluating-Membership-Inference-Through-Adversarial-Robustness}.\n## Privacy Leakage and Membership Inference\nAs mentioned above, membership inference attack aims to figure out whether a piece of data belongs to the training dataset or not. There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings. \n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18,22].",
            "reference_string": "[248810845 | Zhang et al. | 2022 | Citations: 16]"
        },
        {
            "title": "The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 49,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.00513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3469125",
                    "name": "Congzheng Song"
                },
                {
                    "authorId": "1723945",
                    "name": "Vitaly Shmatikov"
                }
            ],
            "abstract": "To help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we propose a new \\emph{model auditing} technique that enables users to check if their data was used to train a machine learning model. We focus on auditing deep-learning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of many popular online services. Furthermore, they are often trained on very sensitive personal data, such as users' messages, searches, chats, and comments. \nWe design and evaluate an effective black-box auditing method that can detect, with very few queries to a model, if a particular user's texts were used to train it (among thousands of other users). In contrast to prior work on membership inference against ML models, we do not assume that the model produces numeric confidence values. We empirically demonstrate that we can successfully audit models that are well-generalized and not overfitted to the training data. We also analyze how text-generation models memorize word sequences and explain why this memorization makes them amenable to auditing.",
            "corpus_id": 53172224,
            "sentences": [
                {
                    "corpus_id": "53172224",
                    "title": "The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model",
                    "text": "Membership inference. As discussed in Section II-C, release of aggregate statistics about private data is generically vulnerable to membership inference attacks [9]. Membership inference against ML models was studied in [13], [22], [29], [31]. \n\nShokri et al. [31] developed membership inference techniques against black-box models, exploiting the differences in the models' outputs on training and non-training inputs. Their attack performs best when target model is overfitted to the training data. Truex et al. [37] extend and generalize this work, including white-box and federated-learning setting. \n\nRahman et al. [29] extend the membership inference attack to differentially private ML models and show that the model may need to sacrifice its test accuracy to achieve membership privacy. Hayes et al. [13] studied membership inference attacks against generative models. Long et al. [22] showed that wellgeneralized models can leak membership information, but their attack requires the adversary to first identify a handful of vulnerable records in the training dataset. Yeom et al. [40] formalized membership inference and theoretically showed that ovefitting is sufficient but not necessary. \n\nWe believe that our auditing method is the first positive application of membership inference. Furthermore, it's the first example of user-level membership inference against ML models and the first that targets text-generation models, which are different and more complex than the models previously considered in the membership inference literature. Other innovations vs. prior methods were summarized in Sections I and II-C, such as the fact that our methods work with wellgeneralized methods that do not output numeric probabilities. \n\nMemorization in ML models. Zhang et al. [41] showed that deep learning models can achieve perfect accuracy even on randomly labeled training data. Song et al. [32] presented malicious training algorithms that intentionally encode the training data in the parameters of the model without affecting its accuracy on the main task. By contrast, we demonstrate that popular text-generation models unintentionally memorize training data in their hidden activation units and outputs.",
                    "score": 0.7461535961226152,
                    "section_title": "VII. RELATED WORK",
                    "char_start_offset": 46177,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 21
                        },
                        {
                            "start": 22,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 243
                        },
                        {
                            "start": 246,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 603
                        },
                        {
                            "start": 606,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1199
                        },
                        {
                            "start": 1202,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1737
                        },
                        {
                            "start": 1740,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 1886
                        },
                        {
                            "start": 1887,
                            "end": 2067
                        },
                        {
                            "start": 2068,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 161,
                            "end": 164,
                            "matchedPaperCorpusId": "217900441"
                        },
                        {
                            "start": 232,
                            "end": 236,
                            "matchedPaperCorpusId": "13699042"
                        },
                        {
                            "start": 238,
                            "end": 242,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 260,
                            "end": 264,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 620,
                            "end": 624,
                            "matchedPaperCorpusId": "13699042"
                        },
                        {
                            "start": 1089,
                            "end": 1093,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1899,
                            "end": 1903,
                            "matchedPaperCorpusId": "2904063"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model\n# Venue: arXiv.org\n# Authors: Congzheng Song, Vitaly Shmatikov\n## Abstract\nTo help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we propose a new \\emph{model auditing} technique that enables users to check if their data was used to train a machine learning model. We focus on auditing deep-learning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of many popular online services. Furthermore, they are often trained on very sensitive personal data, such as users' messages, searches, chats, and comments. \nWe design and evaluate an effective black-box auditing method that can detect, with very few queries to a model, if a particular user's texts were used to train it (among thousands of other users). In contrast to prior work on membership inference against ML models, we do not assume that the model produces numeric confidence values. We empirically demonstrate that we can successfully audit models that are well-generalized and not overfitted to the training data. We also analyze how text-generation models memorize word sequences and explain why this memorization makes them amenable to auditing.\n## VII. RELATED WORK\nMembership inference. As discussed in Section II-C, release of aggregate statistics about private data is generically vulnerable to membership inference attacks [9]. Membership inference against ML models was studied in [13], [22], [29], [31]. \n\nShokri et al. [31] developed membership inference techniques against black-box models, exploiting the differences in the models' outputs on training and non-training inputs. Their attack performs best when target model is overfitted to the training data. Truex et al. [37] extend and generalize this work, including white-box and federated-learning setting. \n\nRahman et al. [29] extend the membership inference attack to differentially private ML models and show that the model may need to sacrifice its test accuracy to achieve membership privacy. Hayes et al. [13] studied membership inference attacks against generative models. Long et al. [22] showed that wellgeneralized models can leak membership information, but their attack requires the adversary to first identify a handful of vulnerable records in the training dataset. Yeom et al. [40] formalized membership inference and theoretically showed that ovefitting is sufficient but not necessary. \n\nWe believe that our auditing method is the first positive application of membership inference. Furthermore, it's the first example of user-level membership inference against ML models and the first that targets text-generation models, which are different and more complex than the models previously considered in the membership inference literature. Other innovations vs. prior methods were summarized in Sections I and II-C, such as the fact that our methods work with wellgeneralized methods that do not output numeric probabilities. \n\nMemorization in ML models. Zhang et al. [41] showed that deep learning models can achieve perfect accuracy even on randomly labeled training data. Song et al. [32] presented malicious training algorithms that intentionally encode the training data in the parameters of the model without affecting its accuracy on the main task. By contrast, we demonstrate that popular text-generation models unintentionally memorize training data in their hidden activation units and outputs.",
            "reference_string": "[53172224 | Song et al. | 2018 | Citations: 18]"
        },
        {
            "title": "Privacy in Deep Learning: A Survey",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 170,
            "citation_count": 140,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.12254, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1659609741",
                    "name": "Fatemehsadat Mirshghallah"
                },
                {
                    "authorId": "2738953",
                    "name": "Mohammadkazem Taram"
                },
                {
                    "authorId": "2927870",
                    "name": "Praneeth Vepakomma"
                },
                {
                    "authorId": "2034349211",
                    "name": "Abhishek Singh"
                },
                {
                    "authorId": "145711633",
                    "name": "R. Raskar"
                },
                {
                    "authorId": "1696563",
                    "name": "H. Esmaeilzadeh"
                }
            ],
            "abstract": "The ever-growing advances of deep learning in many areas including vision, recommendation systems, natural language processing, etc., have led to the adoption of Deep Neural Networks (DNNs) in production systems. The availability of large datasets and high computational power are the main contributors to these advances. The datasets are usually crowdsourced and may contain sensitive information. This poses serious privacy concerns as this data can be misused or leaked through various vulnerabilities. Even if the cloud provider and the communication link is trusted, there are still threats of inference attacks where an attacker could speculate properties of the data used for training, or find the underlying model architecture and parameters. In this survey, we review the privacy concerns brought by deep learning, and the mitigating techniques introduced to tackle these issues. We also show that there is a gap in the literature regarding test-time inference privacy, and propose possible future research directions.",
            "corpus_id": 216553034,
            "sentences": [
                {
                    "corpus_id": "216553034",
                    "title": "Privacy in Deep Learning: A Survey",
                    "text": "Given a data instance and (black-box or white-box) access to a pre-trained target model, a membership inference attack speculates whether or not the given data instance has contributed to the training step of the target model. Shokri et al. [13] propose the first membership inference attack on machine learning where they consider an attacker who has black-box query access to the target model and can obtain confidence scores (probability vector) for the queried input. The attacker uses this confidence score to deduce the participation of given data in training. They first train shadow models on a labeled dataset that can be generated using three methods: model inversion attack (we will see next), statistics-based synthesis (through assumptions about the underlying distribution of training set), or noisy real data. Using these shadow models, the attacker trains an \"attack model\" that distinguishes the participation of a data instance in the training set of the shadow models. Lastly, for the main inference attack, the attacker makes queries to the target deployed model to receive confidence scores for each given input data instance and infers whether or not the input was part of the target training data. This attack is built on the assumption that if a record was used in the training of a model, it would yield a higher confidence score, than a record which was not seen before by the model. Some studies [46,47,15] attribute membership inference attacks to the generalization gap, the over-fitting of the model, and data memorization capabilities of neural networks. Deep neural networks have been shown to memorize the training data [48,49,50], rather than learning the latent properties of the data, which means they often tend to over-fit to the training data. Long et al. [36] propose an approach which more accurately tests the membership of a given instance. They train the shadow models with and without this given instance, and then at inference time the attacker tests to see if the instance was used for training the target model, similar to Shokri et al.'s approach.",
                    "score": 0.740789492694082,
                    "section_title": "MEMBERSHIP INFERENCE",
                    "char_start_offset": 6262,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1220
                        },
                        {
                            "start": 1221,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1585
                        },
                        {
                            "start": 1586,
                            "end": 1782
                        },
                        {
                            "start": 1783,
                            "end": 1883
                        },
                        {
                            "start": 1884,
                            "end": 2096
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 241,
                            "end": 245,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1427,
                            "end": 1430,
                            "matchedPaperCorpusId": "50778569"
                        },
                        {
                            "start": 1430,
                            "end": 1433,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1653,
                            "end": 1657,
                            "matchedPaperCorpusId": "11455421"
                        },
                        {
                            "start": 1657,
                            "end": 1660,
                            "matchedPaperCorpusId": "215745042"
                        },
                        {
                            "start": 1660,
                            "end": 1663,
                            "matchedPaperCorpusId": "232033672"
                        },
                        {
                            "start": 1795,
                            "end": 1799,
                            "matchedPaperCorpusId": "9517641"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: Privacy in Deep Learning: A Survey\n# Venue: arXiv.org\n# Authors: Fatemehsadat Mirshghallah, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, R. Raskar, H. Esmaeilzadeh\n## Abstract\nThe ever-growing advances of deep learning in many areas including vision, recommendation systems, natural language processing, etc., have led to the adoption of Deep Neural Networks (DNNs) in production systems. The availability of large datasets and high computational power are the main contributors to these advances. The datasets are usually crowdsourced and may contain sensitive information. This poses serious privacy concerns as this data can be misused or leaked through various vulnerabilities. Even if the cloud provider and the communication link is trusted, there are still threats of inference attacks where an attacker could speculate properties of the data used for training, or find the underlying model architecture and parameters. In this survey, we review the privacy concerns brought by deep learning, and the mitigating techniques introduced to tackle these issues. We also show that there is a gap in the literature regarding test-time inference privacy, and propose possible future research directions.\n## MEMBERSHIP INFERENCE\nGiven a data instance and (black-box or white-box) access to a pre-trained target model, a membership inference attack speculates whether or not the given data instance has contributed to the training step of the target model. Shokri et al. [13] propose the first membership inference attack on machine learning where they consider an attacker who has black-box query access to the target model and can obtain confidence scores (probability vector) for the queried input. The attacker uses this confidence score to deduce the participation of given data in training. They first train shadow models on a labeled dataset that can be generated using three methods: model inversion attack (we will see next), statistics-based synthesis (through assumptions about the underlying distribution of training set), or noisy real data. Using these shadow models, the attacker trains an \"attack model\" that distinguishes the participation of a data instance in the training set of the shadow models. Lastly, for the main inference attack, the attacker makes queries to the target deployed model to receive confidence scores for each given input data instance and infers whether or not the input was part of the target training data. This attack is built on the assumption that if a record was used in the training of a model, it would yield a higher confidence score, than a record which was not seen before by the model. Some studies [46,47,15] attribute membership inference attacks to the generalization gap, the over-fitting of the model, and data memorization capabilities of neural networks. Deep neural networks have been shown to memorize the training data [48,49,50], rather than learning the latent properties of the data, which means they often tend to over-fit to the training data. Long et al. [36] propose an approach which more accurately tests the membership of a given instance. They train the shadow models with and without this given instance, and then at inference time the attacker tests to see if the instance was used for training the target model, similar to Shokri et al.'s approach.",
            "reference_string": "[216553034 | Mirshghallah et al. | 2020 | Citations: 140]"
        },
        {
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "venue": "IEEE Symposium on Security and Privacy",
            "year": 2016,
            "reference_count": 41,
            "citation_count": 4160,
            "influential_citation_count": 619,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1610.05820",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.05820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2520493",
                    "name": "R. Shokri"
                },
                {
                    "authorId": "34828439",
                    "name": "M. Stronati"
                },
                {
                    "authorId": "3469125",
                    "name": "Congzheng Song"
                },
                {
                    "authorId": "1723945",
                    "name": "Vitaly Shmatikov"
                }
            ],
            "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
            "corpus_id": 10488675,
            "sentences": [
                {
                    "corpus_id": "10488675",
                    "title": "Membership Inference Attacks Against Machine Learning Models",
                    "text": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "score": 0.6653039083697723,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96923828125
                },
                {
                    "corpus_id": "10488675",
                    "title": "Membership Inference Attacks Against Machine Learning Models",
                    "text": "Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output. \n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class. \n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members. \n\nFormally, let f target () be the target model, and let D train target be its private training dataset which contains labeled data records (x {i} , y {i} ) target . A data record x {i} target is the input to the model, and y {i} target is the true label that can take values from a set of classes of size c target . The output of the target model is a probability vector of size c target . The elements of this vector are in [0, 1] and sum up to 1. \n\nLet f attack () be the attack model. Its input x attack is composed of a correctly labeled record and a prediction vector of size c target . Since the goal of the attack is decisional membership inference, the attack model is a binary classifier with two output classes, \"in\" and \"out.\" \n\nFigure 1 illustrates our end-to-end attack process.",
                    "score": 0.6556088800233637,
                    "section_title": "A. Overview of the attack",
                    "char_start_offset": 16269,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 535
                        },
                        {
                            "start": 538,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 785
                        },
                        {
                            "start": 788,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1325
                        },
                        {
                            "start": 1328,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1775
                        },
                        {
                            "start": 1778,
                            "end": 1814
                        },
                        {
                            "start": 1815,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2064
                        },
                        {
                            "start": 2067,
                            "end": 2118
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96533203125
                }
            ],
            "relevance_judgement": 0.96923828125,
            "relevance_judgment_input_expanded": "# Title: Membership Inference Attacks Against Machine Learning Models\n# Venue: IEEE Symposium on Security and Privacy\n# Authors: R. Shokri, M. Stronati, Congzheng Song, Vitaly Shmatikov\n## Abstract\nWe quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.\n## A. Overview of the attack\nOur membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output. \n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class. \n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members. \n\nFormally, let f target () be the target model, and let D train target be its private training dataset which contains labeled data records (x {i} , y {i} ) target . A data record x {i} target is the input to the model, and y {i} target is the true label that can take values from a set of classes of size c target . The output of the target model is a probability vector of size c target . The elements of this vector are in [0, 1] and sum up to 1. \n\nLet f attack () be the attack model. Its input x attack is composed of a correctly labeled record and a prediction vector of size c target . Since the goal of the attack is decisional membership inference, the attack model is a binary classifier with two output classes, \"in\" and \"out.\" \n\nFigure 1 illustrates our end-to-end attack process.",
            "reference_string": "[10488675 | Shokri et al. | 2016 | Citations: 4160]"
        },
        {
            "title": "GAMIN: An Adversarial Approach to Black-Box Model Inversion",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 54,
            "citation_count": 42,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.11835, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40907220",
                    "name": "U. A\u00efvodji"
                },
                {
                    "authorId": "1777382",
                    "name": "S. Gambs"
                },
                {
                    "authorId": "1388769197",
                    "name": "Timon Ther"
                }
            ],
            "abstract": "Recent works have demonstrated that machine learning models are vulnerable to model inversion attacks, which lead to the exposure of sensitive information contained in their training dataset. While some model inversion attacks have been developed in the past in the black-box attack setting, in which the adversary does not have direct access to the structure of the model, few of these have been conducted so far against complex models such as deep neural networks. In this paper, we introduce GAMIN (for Generative Adversarial Model INversion), a new black-box model inversion attack framework achieving significant results even against deep models such as convolutional neural networks at a reasonable computing cost. GAMIN is based on the continuous training of a surrogate model for the target model under attack and a generator whose objective is to generate inputs resembling those used to train the target model. The attack was validated against various neural networks used as image classifiers. In particular, when attacking models trained on the MNIST dataset, GAMIN is able to extract recognizable digits for up to 60% of labels produced by the target. Attacks against skin classification models trained on the pilot parliament dataset also demonstrated the capacity to extract recognizable features from the targets.",
            "corpus_id": 202888587,
            "sentences": [
                {
                    "corpus_id": "202888587",
                    "title": "GAMIN: An Adversarial Approach to Black-Box Model Inversion",
                    "text": "Membership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov [5]. Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov [37] in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets. In addition, Hayes, Melis, Danezis and de Cristofaro have demonstrated in the context of generative models [36] that generative adversarial networks [11] can be used to infer the presence of a particular individual in the training set. \n\nProperty inference attacks against machine learning models have been introduced by Ateniese and coauthors [34]. This type of attack involves training a metaclassifier to detect if the target model has a given property P . To conduct such an attack, the adversary trains a set of shadow models using a dataset and machine learning technique similar to that of the target model, but in addition, explicitly labeled as having the property P or not. Finally, the meta-classifier is trained to detect the presence of the property P . The authors have used this attack to learn that the training set of a speech recognition system have been produced by people speaking a particular dialect.",
                    "score": 0.6971556224771474,
                    "section_title": "Inference attacks against machine learning models",
                    "char_start_offset": 37362,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1684
                        },
                        {
                            "start": 1687,
                            "end": 1798
                        },
                        {
                            "start": 1799,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2132
                        },
                        {
                            "start": 2133,
                            "end": 2215
                        },
                        {
                            "start": 2216,
                            "end": 2371
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 112,
                            "end": 115,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1222,
                            "end": 1226,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1556,
                            "end": 1560,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1598,
                            "end": 1602,
                            "matchedPaperCorpusId": "10319744"
                        },
                        {
                            "start": 1793,
                            "end": 1797,
                            "matchedPaperCorpusId": "14757739"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: GAMIN: An Adversarial Approach to Black-Box Model Inversion\n# Venue: arXiv.org\n# Authors: U. A\u00efvodji, S. Gambs, Timon Ther\n## Abstract\nRecent works have demonstrated that machine learning models are vulnerable to model inversion attacks, which lead to the exposure of sensitive information contained in their training dataset. While some model inversion attacks have been developed in the past in the black-box attack setting, in which the adversary does not have direct access to the structure of the model, few of these have been conducted so far against complex models such as deep neural networks. In this paper, we introduce GAMIN (for Generative Adversarial Model INversion), a new black-box model inversion attack framework achieving significant results even against deep models such as convolutional neural networks at a reasonable computing cost. GAMIN is based on the continuous training of a surrogate model for the target model under attack and a generator whose objective is to generate inputs resembling those used to train the target model. The attack was validated against various neural networks used as image classifiers. In particular, when attacking models trained on the MNIST dataset, GAMIN is able to extract recognizable digits for up to 60% of labels produced by the target. Attacks against skin classification models trained on the pilot parliament dataset also demonstrated the capacity to extract recognizable features from the targets.\n## Inference attacks against machine learning models\nMembership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov [5]. Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov [37] in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets. In addition, Hayes, Melis, Danezis and de Cristofaro have demonstrated in the context of generative models [36] that generative adversarial networks [11] can be used to infer the presence of a particular individual in the training set. \n\nProperty inference attacks against machine learning models have been introduced by Ateniese and coauthors [34]. This type of attack involves training a metaclassifier to detect if the target model has a given property P . To conduct such an attack, the adversary trains a set of shadow models using a dataset and machine learning technique similar to that of the target model, but in addition, explicitly labeled as having the property P or not. Finally, the meta-classifier is trained to detect the presence of the property P . The authors have used this attack to learn that the training set of a speech recognition system have been produced by people speaking a particular dialect.",
            "reference_string": "[202888587 | Aivodji et al. | 2019 | Citations: 42]"
        },
        {
            "title": "Towards Demystifying Membership Inference Attacks",
            "venue": "arXiv.org",
            "year": 2018,
            "reference_count": 49,
            "citation_count": 112,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1807.09173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "25121568",
                    "name": "Stacey Truex"
                },
                {
                    "authorId": "46458150",
                    "name": "Ling Liu"
                },
                {
                    "authorId": "2327300",
                    "name": "M. E. Gursoy"
                },
                {
                    "authorId": "2112532900",
                    "name": "Lei Yu"
                },
                {
                    "authorId": "47747953",
                    "name": "Wenqi Wei"
                }
            ],
            "abstract": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
            "corpus_id": 50778569,
            "sentences": [
                {
                    "corpus_id": "50778569",
                    "title": "Towards Demystifying Membership Inference Attacks",
                    "text": "Most existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28,41]. However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning. Concretely, adversarial deep learning research to date has been centered on the generation of adversarial examples by injecting minimal amount of perturbation to a benign example such that the pre-trained classification model will misclassify with high probability. Thus, adversarial example-based attacks aim at altering the output of the model prediction without being visually noticed. On the contrary, a membership inference attack does not alter the prediction output at all, and it succeeds by simply making membership inference on the prediction output. \n\nScope and Contributions of the paper. In this paper, we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model. Our research results are novel from three perspectives. First, we describe a systematic approach to construct a membership inference attack model and the general formulation of each component of the attack model generation framework. We show that generating a membership inference attack model is a complex and multi-step strategic process. Second, to understand when and how membership inference attacks work and why certain models and datasets are more vulnerable, we take a holistic approach with extensive empirical evidence to study and characterize membership inference attacks across different target model types, different types of training datasets, and different combinations of model types for generating attack training dataset and attack models. Finally, we introduce and investigate a new membership threat, insider membership inference, which is launched by a member of a federated learning system against other participants in an collaborative learning environment. As federated learning systems become more popular with promises of increased accuracy and privacy, highlighting and understanding this risk is an important part of membership inference mitigation effort.",
                    "score": 0.5958440217570722,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 4475,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1057
                        },
                        {
                            "start": 1060,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1556
                        },
                        {
                            "start": 1557,
                            "end": 1663
                        },
                        {
                            "start": 1664,
                            "end": 2081
                        },
                        {
                            "start": 2082,
                            "end": 2304
                        },
                        {
                            "start": 2305,
                            "end": 2508
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 198,
                            "end": 201,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: Towards Demystifying Membership Inference Attacks\n# Venue: arXiv.org\n# Authors: Stacey Truex, Ling Liu, M. E. Gursoy, Lei Yu, Wenqi Wei\n## Abstract\nMembership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.\n## INTRODUCTION\nMost existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28,41]. However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning. Concretely, adversarial deep learning research to date has been centered on the generation of adversarial examples by injecting minimal amount of perturbation to a benign example such that the pre-trained classification model will misclassify with high probability. Thus, adversarial example-based attacks aim at altering the output of the model prediction without being visually noticed. On the contrary, a membership inference attack does not alter the prediction output at all, and it succeeds by simply making membership inference on the prediction output. \n\nScope and Contributions of the paper. In this paper, we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model. Our research results are novel from three perspectives. First, we describe a systematic approach to construct a membership inference attack model and the general formulation of each component of the attack model generation framework. We show that generating a membership inference attack model is a complex and multi-step strategic process. Second, to understand when and how membership inference attacks work and why certain models and datasets are more vulnerable, we take a holistic approach with extensive empirical evidence to study and characterize membership inference attacks across different target model types, different types of training datasets, and different combinations of model types for generating attack training dataset and attack models. Finally, we introduce and investigate a new membership threat, insider membership inference, which is launched by a member of a federated learning system against other participants in an collaborative learning environment. As federated learning systems become more popular with promises of increased accuracy and privacy, highlighting and understanding this risk is an important part of membership inference mitigation effort.",
            "reference_string": "[50778569 | Truex et al. | 2018 | Citations: 112]"
        },
        {
            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
            "venue": "Conference on Computer and Communications Security",
            "year": 2019,
            "reference_count": 73,
            "citation_count": 244,
            "influential_citation_count": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3319535.3354211",
                "status": "HYBRID",
                "license": "public-domain",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.10291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144173853",
                    "name": "Liwei Song"
                },
                {
                    "authorId": "2520493",
                    "name": "R. Shokri"
                },
                {
                    "authorId": "143615345",
                    "name": "Prateek Mittal"
                }
            ],
            "abstract": "The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.",
            "corpus_id": 165163934,
            "sentences": [
                {
                    "corpus_id": "165163934",
                    "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
                    "text": "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not [18,32,37,41,47,64]. The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. [47] design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. [64] suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.\n\nIn this paper, we use this confidence-thresholding membership inference approach in most cases. Note that when evaluating the privacy leakage with targeted adversarial examples in Section 3.3.1 and Section 5.2.5, the confidence-thresholding approach does not apply as there are multiple prediction vectors for each data point. Instead, we follow Shokri et al. [47] to train a neural network classifier for membership inference.",
                    "score": 0.6685826868609733,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 15158,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 149,
                            "end": 153,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 156,
                            "end": 159,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 159,
                            "end": 162,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 162,
                            "end": 165,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 165,
                            "end": 168,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 330,
                            "end": 334,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1192,
                            "end": 1196,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 1950,
                            "end": 1954,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: Privacy Risks of Securing Machine Learning Models against Adversarial Examples\n# Venue: Conference on Computer and Communications Security\n# Authors: Liwei Song, R. Shokri, Prateek Mittal\n## Abstract\nThe arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.\n## Membership Inference Attacks\nFor a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not [18,32,37,41,47,64]. The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. [47] design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. [64] suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.\n\nIn this paper, we use this confidence-thresholding membership inference approach in most cases. Note that when evaluating the privacy leakage with targeted adversarial examples in Section 3.3.1 and Section 5.2.5, the confidence-thresholding approach does not apply as there are multiple prediction vectors for each data point. Instead, we follow Shokri et al. [47] to train a neural network classifier for membership inference.",
            "reference_string": "[165163934 | Song et al. | 2019 | Citations: 244]"
        },
        {
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "venue": "USENIX Security Symposium",
            "year": 2019,
            "reference_count": 50,
            "citation_count": 272,
            "influential_citation_count": 21,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.11798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35802340",
                    "name": "Klas Leino"
                },
                {
                    "authorId": "2623167",
                    "name": "Matt Fredrikson"
                }
            ],
            "abstract": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
            "corpus_id": 195699554,
            "sentences": [
                {
                    "corpus_id": "195699554",
                    "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                    "text": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "score": 0.8025008798563562,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                },
                {
                    "corpus_id": "195699554",
                    "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                    "text": "Machine learning has enabled a wide range of applications in areas like computer vision, machine translation, health analytics, and advertising, among others. The fact that many compelling applications of this technology involve the collection and processing of sensitive personal data has given rise to concerns about privacy [2,4,7,10,11,25,32,37,45,46]. In particular, when machine learning algorithms are applied to private training data, the resulting models might unwittingly leak information about that data through either their behavior or the details of their structure and parameters. \n\nTwo particular attacks have emerged as concrete threats: model inversion and membership inference. In a model inversion attack, the adversary uses a machine learning model and incomplete information about a data point to infer the missing information for that point. For example, the adversary might be given partial information about an individual's medical record, and attempt to infer the individual's genotype by using a model trained on similar medical records [11]. \n\nTraining data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25]. \n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data.",
                    "score": 0.6358516350948444,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 594
                        },
                        {
                            "start": 597,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1068
                        },
                        {
                            "start": 1071,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1472
                        },
                        {
                            "start": 1475,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 2093
                        },
                        {
                            "start": 2094,
                            "end": 2276
                        },
                        {
                            "start": 2277,
                            "end": 2519
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 327,
                            "end": 330,
                            "matchedPaperCorpusId": "14757739"
                        },
                        {
                            "start": 330,
                            "end": 332,
                            "matchedPaperCorpusId": "14444919"
                        },
                        {
                            "start": 332,
                            "end": 334,
                            "matchedPaperCorpusId": "8921975"
                        },
                        {
                            "start": 334,
                            "end": 337,
                            "matchedPaperCorpusId": "207229839"
                        },
                        {
                            "start": 337,
                            "end": 340,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 340,
                            "end": 343,
                            "matchedPaperCorpusId": "17295025"
                        },
                        {
                            "start": 352,
                            "end": 355,
                            "matchedPaperCorpusId": "5921778"
                        },
                        {
                            "start": 1063,
                            "end": 1067,
                            "matchedPaperCorpusId": "2148083"
                        },
                        {
                            "start": 1467,
                            "end": 1471,
                            "matchedPaperCorpusId": "17295025"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference\n# Venue: USENIX Security Symposium\n# Authors: Klas Leino, Matt Fredrikson\n## Abstract\nMembership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.\n## Introduction\nMachine learning has enabled a wide range of applications in areas like computer vision, machine translation, health analytics, and advertising, among others. The fact that many compelling applications of this technology involve the collection and processing of sensitive personal data has given rise to concerns about privacy [2,4,7,10,11,25,32,37,45,46]. In particular, when machine learning algorithms are applied to private training data, the resulting models might unwittingly leak information about that data through either their behavior or the details of their structure and parameters. \n\nTwo particular attacks have emerged as concrete threats: model inversion and membership inference. In a model inversion attack, the adversary uses a machine learning model and incomplete information about a data point to infer the missing information for that point. For example, the adversary might be given partial information about an individual's medical record, and attempt to infer the individual's genotype by using a model trained on similar medical records [11]. \n\nTraining data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25]. \n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data.",
            "reference_string": "[195699554 | Leino et al. | 2019 | Citations: 272]"
        },
        {
            "title": "Auditing Membership Leakages of Multi-Exit Networks",
            "venue": "Conference on Computer and Communications Security",
            "year": 2022,
            "reference_count": 90,
            "citation_count": 34,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2208.11180",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.11180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146247989",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2182511319",
                    "name": "Yiyong Liu"
                },
                {
                    "authorId": "2116553732",
                    "name": "Xinlei He"
                },
                {
                    "authorId": "145648201",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "144588806",
                    "name": "M. Backes"
                },
                {
                    "authorId": "1698138",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Relying on the truth that not all inputs require the same level of computational cost to produce reliable predictions, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing predictions at intermediate layers of the model and thus saving computation time and energy. However, various current designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks. In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages, and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.",
            "corpus_id": 251765197,
            "sentences": [
                {
                    "corpus_id": "251765197",
                    "title": "Auditing Membership Leakages of Multi-Exit Networks",
                    "text": "Currently, membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29,49,61,71,76,80,91]. Shokri et al. [76] propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [71] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [76]. Later, Nasr et al. [61] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. [80] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang [56] and Choquette-Choo et al. [17] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nBesides, there exists a wide range of other attacks, defenses and applications in machine learning domain [10-12, 14-16, 20, 21, 24, 25, 28, 35, 39, 44, 50, 52, 54, 55, 63-65, 70, 74, 78, 83-86, 90, 93-95].",
                    "score": 0.6585773257681268,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 48970,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 776
                        },
                        {
                            "start": 777,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1110
                        },
                        {
                            "start": 1111,
                            "end": 1254
                        },
                        {
                            "start": 1257,
                            "end": 1463
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 123,
                            "matchedPaperCorpusId": "195699554"
                        },
                        {
                            "start": 123,
                            "end": 126,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 126,
                            "end": 129,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 129,
                            "end": 132,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 132,
                            "end": 135,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 135,
                            "end": 138,
                            "matchedPaperCorpusId": "2656445"
                        },
                        {
                            "start": 154,
                            "end": 158,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 465,
                            "end": 469,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 594,
                            "end": 598,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 619,
                            "end": 623,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 789,
                            "end": 793,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 920,
                            "end": 924,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 951,
                            "end": 955,
                            "matchedPaperCorpusId": "220831381"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: Auditing Membership Leakages of Multi-Exit Networks\n# Venue: Conference on Computer and Communications Security\n# Authors: Zheng Li, Yiyong Liu, Xinlei He, Ning Yu, M. Backes, Yang Zhang\n## Abstract\nRelying on the truth that not all inputs require the same level of computational cost to produce reliable predictions, multi-exit networks are gaining attention as a prominent approach for pushing the limits of efficient deployment. Multi-exit networks endow a backbone model with early exits, allowing predictions at intermediate layers of the model and thus saving computation time and energy. However, various current designs of multi-exit networks are only considered to achieve the best trade-off between resource usage efficiency and prediction accuracy, the privacy risks stemming from them have never been explored. This prompts the need for a comprehensive investigation of privacy risks in multi-exit networks. In this paper, we perform the first privacy analysis of multi-exit networks through the lens of membership leakages. In particular, we first leverage the existing attack methodologies to quantify the multi-exit networks' vulnerability to membership leakages. Our experimental results show that multi-exit networks are less vulnerable to membership leakages, and the exit (number and depth) attached to the backbone model is highly correlated with the attack performance. Furthermore, we propose a hybrid attack that exploits the exit information to improve the performance of existing attacks. We evaluate membership leakage threat caused by our hybrid attack under three different adversarial setups, ultimately arriving at a model-free and data-free adversary. These results clearly demonstrate that our hybrid attacks are very broadly applicable, thereby the corresponding risks are much more severe than shown by existing membership inference attacks. We further present a defense mechanism called TimeGuard specifically for multi-exit networks and show that TimeGuard mitigates the newly proposed attacks perfectly.\n## Membership Inference Attacks\nCurrently, membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29,49,61,71,76,80,91]. Shokri et al. [76] propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [71] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [76]. Later, Nasr et al. [61] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. [80] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang [56] and Choquette-Choo et al. [17] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nBesides, there exists a wide range of other attacks, defenses and applications in machine learning domain [10-12, 14-16, 20, 21, 24, 25, 28, 35, 39, 44, 50, 52, 54, 55, 63-65, 70, 74, 78, 83-86, 90, 93-95].",
            "reference_string": "[251765197 | Li et al. | 2022 | Citations: 34]"
        },
        {
            "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
            "venue": "Network and Distributed System Security Symposium",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2054749404",
                    "name": "Rui Wen"
                },
                {
                    "authorId": "2257034706",
                    "name": "Michael Backes"
                },
                {
                    "authorId": "2257291195",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.",
            "corpus_id": 272423578,
            "sentences": [
                {
                    "corpus_id": "272423578",
                    "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
                    "text": "Membership Inference Attacks (MIA) [20,31,50,55,65,74] have emerged as a significant threat to privacy in the context of machine learning models. These attacks aim to reveal the membership status of a target sample, i.e., whether the sam-ple was part of the training dataset or not, thereby directly breaching privacy. \n\nThe seminal work by Shokri et al. [55] introduced MIA against machine learning models, wherein multiple shadow models were trained to mimic the behavior of the target model. This attack originally required access to data from the same distribution as the training dataset. However, Salem et al. [50] relaxed this assumption by demonstrating the effectiveness of using only a single shadow model, substantially reducing the computational cost involved. \n\nSubsequent research [12,31] has explored more challenging settings for MIA. In these scenarios, the adversary only has access to hard-label predictions from the target model. Li and Zhang [31] proposed a method that approximates the distance between the target sample and its decision boundary using adversarial examples, enabling the attacker to make decisions based on this distance. \n\nRecent advancements in MIA have focused on enhancing attack performance. Carlini et al. [7] leveraged the discrepancy between models trained with and without the target sample to improve attack effectiveness. Liu et al. [34] demonstrated the utility of loss trajectory analysis in MIA. Furthermore, Tram\u00e8r et al. [61] highlighted the potential of data poisoning, showing that even with access to a small fraction of the training dataset, the attacker can significantly boost the performance of membership inference attacks.",
                    "score": 0.5936351305361321,
                    "section_title": "K.1 Membership Inference Attack",
                    "char_start_offset": 65364,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 318
                        },
                        {
                            "start": 321,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 772
                        },
                        {
                            "start": 775,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1160
                        },
                        {
                            "start": 1163,
                            "end": 1235
                        },
                        {
                            "start": 1236,
                            "end": 1371
                        },
                        {
                            "start": 1372,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1686
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 39,
                            "end": 42,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 42,
                            "end": 45,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 45,
                            "end": 48,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 48,
                            "end": 51,
                            "matchedPaperCorpusId": "272367776"
                        },
                        {
                            "start": 51,
                            "end": 54,
                            "matchedPaperCorpusId": "264820351"
                        },
                        {
                            "start": 355,
                            "end": 359,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 616,
                            "end": 620,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 795,
                            "end": 799,
                            "matchedPaperCorpusId": "220831381"
                        },
                        {
                            "start": 799,
                            "end": 802,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 963,
                            "end": 967,
                            "matchedPaperCorpusId": "237563320"
                        },
                        {
                            "start": 1251,
                            "end": 1254,
                            "matchedPaperCorpusId": "244920593"
                        },
                        {
                            "start": 1383,
                            "end": 1387,
                            "matchedPaperCorpusId": "251953448"
                        },
                        {
                            "start": 1476,
                            "end": 1480,
                            "matchedPaperCorpusId": "247922814"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?\n# Venue: Network and Distributed System Security Symposium\n# Authors: Rui Wen, Michael Backes, Yang Zhang\n## Abstract\nMachine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.\n## K.1 Membership Inference Attack\nMembership Inference Attacks (MIA) [20,31,50,55,65,74] have emerged as a significant threat to privacy in the context of machine learning models. These attacks aim to reveal the membership status of a target sample, i.e., whether the sam-ple was part of the training dataset or not, thereby directly breaching privacy. \n\nThe seminal work by Shokri et al. [55] introduced MIA against machine learning models, wherein multiple shadow models were trained to mimic the behavior of the target model. This attack originally required access to data from the same distribution as the training dataset. However, Salem et al. [50] relaxed this assumption by demonstrating the effectiveness of using only a single shadow model, substantially reducing the computational cost involved. \n\nSubsequent research [12,31] has explored more challenging settings for MIA. In these scenarios, the adversary only has access to hard-label predictions from the target model. Li and Zhang [31] proposed a method that approximates the distance between the target sample and its decision boundary using adversarial examples, enabling the attacker to make decisions based on this distance. \n\nRecent advancements in MIA have focused on enhancing attack performance. Carlini et al. [7] leveraged the discrepancy between models trained with and without the target sample to improve attack effectiveness. Liu et al. [34] demonstrated the utility of loss trajectory analysis in MIA. Furthermore, Tram\u00e8r et al. [61] highlighted the potential of data poisoning, showing that even with access to a small fraction of the training dataset, the attacker can significantly boost the performance of membership inference attacks.",
            "reference_string": "[272423578 | Wen et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders",
            "venue": "2023 IEEE Smart World Congress (SWC)",
            "year": 2023,
            "reference_count": 26,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2304.11336",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.11336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "27825205",
                    "name": "Benedikt Gro\u00df"
                },
                {
                    "authorId": "1721084",
                    "name": "G. Wunder"
                }
            ],
            "abstract": "Synthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person\u2019s privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals\u2019 data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thus severely compromise the utility of these private models. Even worse, the tight privacy budgets do not allow for many training epochs so that model quality cannot be properly controlled in practice. In this paper we explore an alternative approach for privately generating data that makes direct use of the inherent stochasticity in generative models, e.g., variational autoencoders. The main idea is to appropriately constrain the continuity modulus of the deep models instead of adding another noise mechanism on top. For this approach, we derive mathematically rigorous privacy guarantees and illustrate its effectiveness with practical experiments.",
            "corpus_id": 258298235,
            "sentences": [
                {
                    "corpus_id": "258298235",
                    "title": "Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders",
                    "text": "Membership inference [15], [16] is an attack on a machine learning model, where an adversary tries to infer whether a particular record was part of the training set of the attacked model . Depending on the type of ML model and the information about that model that is available to the adversary, there exist different variants of the membership inference attack (MIA). The types of MIAs that can be utilized against generative models depend on the knowledge of the attacker. In a white-box setting the attacker can access the parameters of the generative model itself and use this knowledge to craft an attack. Here we focus on a block-box setting, i.e. the model parameters of the target model are not available. \n\nIn [15] it is assumed that the attacker has access to public data that is drawn from the same distribution as the target model's training data, and the architecture and parameters of the target model are known. Several so-called shadow models are trained that mimic the behaviour of the target model. An attack model is then trained on the shadow models' outputs and used to classify the record under consideration as member or non-member of the target model's training data. Later works were able to relax these assumptions [16], [17].The most common attacks against generative models/synthetic data in a black box setting are as follows: \n\n1) Classifier trained on shadow models [15]: The MIA then proceeds with the following steps: a) Draw m datasets of size n from the public data. b) Train 2m shadow models, one for each data set with and without inserting the target record t into the set. c) Generate p synthetic data sets from each shadow model. This results in a labelled data set of size 2mp of pairs (x, y), where x is a synthetic data set and y = 0 if the target record was not in the set used to train the model that generated x, y = 1, if the target record was present. d) Train a (binary) attack classifier on the labelled set of data sets. e) Use the attack classifier to predict, whether the target record was present in the private data that was used to generate the synthetic data set. The drawback of this method is that it is computationally very expensive.",
                    "score": 0.6058928312781005,
                    "section_title": "C. Membership Inference Attacks",
                    "char_start_offset": 7640,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 713
                        },
                        {
                            "start": 716,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1355
                        },
                        {
                            "start": 1358,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2120
                        },
                        {
                            "start": 2121,
                            "end": 2194
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 21,
                            "end": 25,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 719,
                            "end": 723,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1247,
                            "end": 1251,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1397,
                            "end": 1401,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders\n# Venue: 2023 IEEE Smart World Congress (SWC)\n# Authors: Benedikt Gro\u00df, G. Wunder\n## Abstract\nSynthetic data has been hailed as the silver bullet for privacy preserving data analysis. If a record is not real, then how could it violate a person\u2019s privacy? In addition, deep-learning based generative models are employed successfully to approximate complex high-dimensional distributions from data and draw realistic samples from this learned distribution. It is often overlooked though that generative models are prone to memorising many details of individual training records and often generate synthetic data that too closely resembles the underlying sensitive training data, hence violating strong privacy regulations as, e.g., encountered in health care. Differential privacy is the well-known state-of-the-art framework for guaranteeing protection of sensitive individuals\u2019 data, allowing aggregate statistics and even machine learning models to be released publicly without compromising privacy. The training mechanisms however often add too much noise during the training process, and thus severely compromise the utility of these private models. Even worse, the tight privacy budgets do not allow for many training epochs so that model quality cannot be properly controlled in practice. In this paper we explore an alternative approach for privately generating data that makes direct use of the inherent stochasticity in generative models, e.g., variational autoencoders. The main idea is to appropriately constrain the continuity modulus of the deep models instead of adding another noise mechanism on top. For this approach, we derive mathematically rigorous privacy guarantees and illustrate its effectiveness with practical experiments.\n## C. Membership Inference Attacks\nMembership inference [15], [16] is an attack on a machine learning model, where an adversary tries to infer whether a particular record was part of the training set of the attacked model . Depending on the type of ML model and the information about that model that is available to the adversary, there exist different variants of the membership inference attack (MIA). The types of MIAs that can be utilized against generative models depend on the knowledge of the attacker. In a white-box setting the attacker can access the parameters of the generative model itself and use this knowledge to craft an attack. Here we focus on a block-box setting, i.e. the model parameters of the target model are not available. \n\nIn [15] it is assumed that the attacker has access to public data that is drawn from the same distribution as the target model's training data, and the architecture and parameters of the target model are known. Several so-called shadow models are trained that mimic the behaviour of the target model. An attack model is then trained on the shadow models' outputs and used to classify the record under consideration as member or non-member of the target model's training data. Later works were able to relax these assumptions [16], [17].The most common attacks against generative models/synthetic data in a black box setting are as follows: \n\n1) Classifier trained on shadow models [15]: The MIA then proceeds with the following steps: a) Draw m datasets of size n from the public data. b) Train 2m shadow models, one for each data set with and without inserting the target record t into the set. c) Generate p synthetic data sets from each shadow model. This results in a labelled data set of size 2mp of pairs (x, y), where x is a synthetic data set and y = 0 if the target record was not in the set used to train the model that generated x, y = 1, if the target record was present. d) Train a (binary) attack classifier on the labelled set of data sets. e) Use the attack classifier to predict, whether the target record was present in the private data that was used to generate the synthetic data set. The drawback of this method is that it is computationally very expensive.",
            "reference_string": "[258298235 | Gross et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Membership Inference Attacks on Lottery Ticket Networks",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 17,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.03506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1693718757",
                    "name": "Aadesh Bagmar"
                },
                {
                    "authorId": "51469126",
                    "name": "Shishira R. Maiya"
                },
                {
                    "authorId": "2122929492",
                    "name": "Shruti Bidwalka"
                },
                {
                    "authorId": "144520191",
                    "name": "A. Deshpande"
                }
            ],
            "abstract": "The vulnerability of the Lottery Ticket Hypothesis has not been studied from the purview of Membership Inference Attacks. Through this work, we are the first to empirically show that the lottery ticket networks are equally vulnerable to membership inference attacks. A Membership Inference Attack (MIA) is the process of determining whether a data sample belongs to a training set of a trained model or not. Membership Inference Attacks could leak critical information about the training data that can be used for targeted attacks. Recent deep learning models often have very large memory footprints and a high computational cost associated with training and drawing inferences. Lottery Ticket Hypothesis is used to prune the networks to find smaller sub-networks that at least match the performance of the original model in terms of test accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and ImageNet datasets to perform image classification tasks and observe that the attack accuracies are similar. We also see that the attack accuracy varies directly according to the number of classes in the dataset and the sparsity of the network. We demonstrate that these attacks are transferable across models with high accuracy.",
            "corpus_id": 236956533,
            "sentences": [
                {
                    "corpus_id": "236956533",
                    "title": "Membership Inference Attacks on Lottery Ticket Networks",
                    "text": "Membership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021). \n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too. \n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches. Defenses against such attacks have been studied as well and mostly focus around reducing overfitting and reducing the influence of certain data points. Nasr et al. (2018) suggest using adversarial regularization training to defend against this. Shokri et al. (2017) suggested defense techniques like restricting the prediction vector to top k classes; however, highly accurate attacks are still possible even when the model reveals minimal information (Li & Zhang, 2020).",
                    "score": 0.5999457847576347,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 2599,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 321
                        },
                        {
                            "start": 324,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1671
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 456,
                            "end": 476,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 870,
                            "end": 891,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 956,
                            "end": 978,
                            "matchedPaperCorpusId": "216652989"
                        },
                        {
                            "start": 1014,
                            "end": 1030,
                            "matchedPaperCorpusId": "209439842"
                        },
                        {
                            "start": 1445,
                            "end": 1465,
                            "matchedPaperCorpusId": "10488675"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Membership Inference Attacks on Lottery Ticket Networks\n# Venue: arXiv.org\n# Authors: Aadesh Bagmar, Shishira R. Maiya, Shruti Bidwalka, A. Deshpande\n## Abstract\nThe vulnerability of the Lottery Ticket Hypothesis has not been studied from the purview of Membership Inference Attacks. Through this work, we are the first to empirically show that the lottery ticket networks are equally vulnerable to membership inference attacks. A Membership Inference Attack (MIA) is the process of determining whether a data sample belongs to a training set of a trained model or not. Membership Inference Attacks could leak critical information about the training data that can be used for targeted attacks. Recent deep learning models often have very large memory footprints and a high computational cost associated with training and drawing inferences. Lottery Ticket Hypothesis is used to prune the networks to find smaller sub-networks that at least match the performance of the original model in terms of test accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and ImageNet datasets to perform image classification tasks and observe that the attack accuracies are similar. We also see that the attack accuracy varies directly according to the number of classes in the dataset and the sparsity of the network. We demonstrate that these attacks are transferable across models with high accuracy.\n## Membership Inference Attacks\nMembership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021). \n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too. \n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches. Defenses against such attacks have been studied as well and mostly focus around reducing overfitting and reducing the influence of certain data points. Nasr et al. (2018) suggest using adversarial regularization training to defend against this. Shokri et al. (2017) suggested defense techniques like restricting the prediction vector to top k classes; however, highly accurate attacks are still possible even when the model reveals minimal information (Li & Zhang, 2020).",
            "reference_string": "[236956533 | Bagmar et al. | 2021 | Citations: 5]"
        },
        {
            "title": "Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models",
            "venue": "IEEE Transactions on Information Forensics and Security",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIFS.2023.3324772?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIFS.2023.3324772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2019143562",
                    "name": "Gaoyang Liu"
                },
                {
                    "authorId": "2260601109",
                    "name": "Tianlong Xu"
                },
                {
                    "authorId": "2118403448",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2259065760",
                    "name": "Zixiong Wang"
                },
                {
                    "authorId": "40614774",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2213648984",
                    "name": "Ling Liu"
                }
            ],
            "abstract": "Machine Learning (ML) techniques have been applied to many real-world applications to perform a wide range of tasks. In practice, ML models are typically deployed as the black-box APIs to protect the model owner\u2019s benefits and/or defend against various privacy attacks. In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model\u2019s prediction behavior. By extracting the membership information of the given record from the gradient of the substituted local model using an intentionally modified autoencoder, Gradient-Leaks can thus breach the membership privacy of the target model\u2019s training data in an unsupervised manner, without any priori knowledge about the target model\u2019s internals or its training data. Extensive experiments on different types of ML models with real-world datasets have shown that Gradient-Leaks can achieve a better performance compared with state-of-the-art attacks.",
            "corpus_id": 264179931,
            "sentences": [],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models\n# Venue: IEEE Transactions on Information Forensics and Security\n# Authors: Gaoyang Liu, Tianlong Xu, Rui Zhang, Zixiong Wang, Chen Wang, Ling Liu\n## Abstract\nMachine Learning (ML) techniques have been applied to many real-world applications to perform a wide range of tasks. In practice, ML models are typically deployed as the black-box APIs to protect the model owner\u2019s benefits and/or defend against various privacy attacks. In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model\u2019s prediction behavior. By extracting the membership information of the given record from the gradient of the substituted local model using an intentionally modified autoencoder, Gradient-Leaks can thus breach the membership privacy of the target model\u2019s training data in an unsupervised manner, without any priori knowledge about the target model\u2019s internals or its training data. Extensive experiments on different types of ML models with real-world datasets have shown that Gradient-Leaks can achieve a better performance compared with state-of-the-art attacks.\n",
            "reference_string": "[264179931 | Liu et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Privacy Re\u2010Identification Attacks on Tabular GANs",
            "venue": "Security and Privacy",
            "year": 2024,
            "reference_count": 89,
            "citation_count": 4,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1002/spy2.469",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00696, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145327284",
                    "name": "Abdallah Alshantti"
                },
                {
                    "authorId": "2265981625",
                    "name": "Adil Rasheed"
                },
                {
                    "authorId": "2243101143",
                    "name": "Frank Westad"
                }
            ],
            "abstract": "Generative models are effective in producing realistic tabular synthetic data that resembles the properties and distribution of real datasets. While synthetic data has numerous applications across various domains, generative models are susceptible to overfitting, which can lead to the leakage of sensitive information from training data. Privacy attacks exacerbate this issue by attempting to identify original data records from synthetic data, especially when the attacker possesses some knowledge about the generative model. In this work, we investigate the privacy risks associated with using generative adversarial networks (GANs) to create tabular synthetic datasets. More specifically, we develop privacy reconstruction attacks designed to identify training samples by minimizing their proximity to synthetic records. Our experimental analysis considers various scenarios of reconstruction attacks, in which attackers have different levels of access to the generative models. Additionally, we propose multi\u2010objective optimization using evolutionary algorithms to perturb synthetic samples closer to original training data points. The experimental results show that reconstruction attacks can effectively identify training samples, with privacy threats significantly increasing when attackers have access to the generative model. Furthermore, our findings indicate that using evolutionary algorithms in reconstruction attacks further heightens the risk of identifying confidential samples. Comparing our attacks against state\u2010of\u2010the\u2010art privacy attacks on tabular GANs further reveals that our reconstructions attacks are considerably more effective in recovering real data records.",
            "corpus_id": 268819379,
            "sentences": [
                {
                    "corpus_id": "268819379",
                    "title": "Privacy Re\u2010Identification Attacks on Tabular GANs",
                    "text": "Membership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting.In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model.Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set.It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019).Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019).Federated learning can however introduce data leakages which add up to the privacy concerns.In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017;Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models.Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications.The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models.Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks.",
                    "score": 0.7058799465185921,
                    "section_title": "Membership Inference Attacks",
                    "char_start_offset": 10411,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 147,
                            "end": 318
                        },
                        {
                            "start": 318,
                            "end": 546
                        },
                        {
                            "start": 546,
                            "end": 729
                        },
                        {
                            "start": 729,
                            "end": 912
                        },
                        {
                            "start": 912,
                            "end": 1004
                        },
                        {
                            "start": 1004,
                            "end": 1337
                        },
                        {
                            "start": 1339,
                            "end": 1446
                        },
                        {
                            "start": 1446,
                            "end": 1715
                        },
                        {
                            "start": 1715,
                            "end": 1910
                        },
                        {
                            "start": 1910,
                            "end": 2111
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 58,
                            "end": 78,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 709,
                            "end": 728,
                            "matchedPaperCorpusId": "133091488"
                        },
                        {
                            "start": 891,
                            "end": 911,
                            "matchedPaperCorpusId": "53099247"
                        },
                        {
                            "start": 1097,
                            "end": 1118,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1446,
                            "end": 1465,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 1773,
                            "end": 1796,
                            "matchedPaperCorpusId": "199546273"
                        },
                        {
                            "start": 1921,
                            "end": 1940,
                            "matchedPaperCorpusId": "221203089"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                }
            ],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Privacy Re\u2010Identification Attacks on Tabular GANs\n# Venue: Security and Privacy\n# Authors: Abdallah Alshantti, Adil Rasheed, Frank Westad\n## Abstract\nGenerative models are effective in producing realistic tabular synthetic data that resembles the properties and distribution of real datasets. While synthetic data has numerous applications across various domains, generative models are susceptible to overfitting, which can lead to the leakage of sensitive information from training data. Privacy attacks exacerbate this issue by attempting to identify original data records from synthetic data, especially when the attacker possesses some knowledge about the generative model. In this work, we investigate the privacy risks associated with using generative adversarial networks (GANs) to create tabular synthetic datasets. More specifically, we develop privacy reconstruction attacks designed to identify training samples by minimizing their proximity to synthetic records. Our experimental analysis considers various scenarios of reconstruction attacks, in which attackers have different levels of access to the generative models. Additionally, we propose multi\u2010objective optimization using evolutionary algorithms to perturb synthetic samples closer to original training data points. The experimental results show that reconstruction attacks can effectively identify training samples, with privacy threats significantly increasing when attackers have access to the generative model. Furthermore, our findings indicate that using evolutionary algorithms in reconstruction attacks further heightens the risk of identifying confidential samples. Comparing our attacks against state\u2010of\u2010the\u2010art privacy attacks on tabular GANs further reveals that our reconstructions attacks are considerably more effective in recovering real data records.\n## Membership Inference Attacks\nMembership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting.In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model.Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set.It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019).Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019).Federated learning can however introduce data leakages which add up to the privacy concerns.In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017;Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models.Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications.The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models.Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks.",
            "reference_string": "[268819379 | Alshantti et al. | 2024 | Citations: 4]"
        },
        {
            "title": "The Audio Auditor: User-Level Membership Inference in Internet of Things Voice Services",
            "venue": "Proceedings on Privacy Enhancing Technologies",
            "year": 2019,
            "reference_count": 51,
            "citation_count": 36,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.sciendo.com/pdf/10.2478/popets-2021-0012",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.07082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "11168794",
                    "name": "Yuantian Miao"
                },
                {
                    "authorId": "2837434",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2145763295",
                    "name": "Chao Chen"
                },
                {
                    "authorId": "41052097",
                    "name": "Lei Pan"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "7352720",
                    "name": "Benjamin Zi Hao Zhao"
                },
                {
                    "authorId": "1971681",
                    "name": "Dali Kaafar"
                },
                {
                    "authorId": "98232756",
                    "name": "Yang Xiang"
                }
            ],
            "abstract": "Abstract With the rapid development of deep learning techniques, the popularity of voice services implemented on various Internet of Things (IoT) devices is ever increasing. In this paper, we examine user-level membership inference in the problem space of voice services, by designing an audio auditor to verify whether a specific user had unwillingly contributed audio used to train an automatic speech recognition (ASR) model under strict black-box access. With user representation of the input audio data and their corresponding translated text, our trained auditor is effective in user-level audit. We also observe that the auditor trained on specific data can be generalized well regardless of the ASR model architecture. We validate the auditor on ASR models trained with LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid ASR system and the end-to-end ASR system. Finally, we conduct a real-world trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding 80%. We hope the methodology developed in this paper and findings can inform privacy advocates to overhaul IoT privacy.",
            "corpus_id": 221808247,
            "sentences": [
                {
                    "corpus_id": "221808247",
                    "title": "The Audio Auditor: User-Level Membership Inference in Internet of Things Voice Services",
                    "text": "The membership inference attack is considered as a significant privacy threat for machine learning (ML) models [22]. The attack aims to determine whether a specific data sample is within the target model's training set or not. The attack is driven by the different behaviors of the target model when making predictions on samples within or out of its training set. \n\nVarious membership inference attack methods have been recently proposed. Shokri et al. [39] train shadow models to constitute the attack model against a target ML model with black-box access. The shadow models mimic the target model's prediction behavior. To improve accuracy, Liu et al. [15] and Hayes et al. [10] leverage Generative Adversarial Networks (GAN) to generate shadow models with increasingly similar outputs to the target model. Salem et al. [32] relax the attack assumptions mentioned in the work [39], demonstrating that shadow models are not necessary to launch the membership inference attack. Instead, a threshold of the predicted confidence score can be defined to substitute the attack model. Intuitively, a large confidence score indicates the sample as a member of the training set [41]. The attacks mentioned in the work above are all performed on the record level, while Song and Shmatikov [40] study a user-level membership inference attack against text generative models. Instead of using the prediction label along with the confidence score, Song and Shmatikov [40] utilize word's rank list infor-mation of several top-ranked predictions as key features to generate the shadow model. Apart from the blackbox access, Farokhi and Kaafar [8] model the recordlevel membership inference attack under the white-box access. \n\nUnlike image recognition systems or text generative systems, ASR systems present additional challenges [5]. With strict black-box access, attacks using confidence scores cannot be applied. With limited discriminative power, features can only be extracted from the predicted transcription and its input audio to launch membership inference attacks, i.e., audio auditing in our paper.",
                    "score": 0.6040993798265734,
                    "section_title": "Membership Inference Attack",
                    "char_start_offset": 9979,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 364
                        },
                        {
                            "start": 367,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1578
                        },
                        {
                            "start": 1579,
                            "end": 1711
                        },
                        {
                            "start": 1714,
                            "end": 1821
                        },
                        {
                            "start": 1822,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 2096
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 111,
                            "end": 115,
                            "matchedPaperCorpusId": "49863840"
                        },
                        {
                            "start": 454,
                            "end": 458,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 655,
                            "end": 659,
                            "matchedPaperCorpusId": "195477472"
                        },
                        {
                            "start": 677,
                            "end": 681,
                            "matchedPaperCorpusId": "52211986"
                        },
                        {
                            "start": 823,
                            "end": 827,
                            "matchedPaperCorpusId": "46933970"
                        },
                        {
                            "start": 879,
                            "end": 883,
                            "matchedPaperCorpusId": "10488675"
                        },
                        {
                            "start": 1172,
                            "end": 1176,
                            "matchedPaperCorpusId": "165163934"
                        },
                        {
                            "start": 1282,
                            "end": 1286,
                            "matchedPaperCorpusId": "159041003"
                        },
                        {
                            "start": 1456,
                            "end": 1460,
                            "matchedPaperCorpusId": "159041003"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96630859375
                }
            ],
            "relevance_judgement": 0.96630859375,
            "relevance_judgment_input_expanded": "# Title: The Audio Auditor: User-Level Membership Inference in Internet of Things Voice Services\n# Venue: Proceedings on Privacy Enhancing Technologies\n# Authors: Yuantian Miao, Minhui Xue, Chao Chen, Lei Pan, Jinchao Zhang, Benjamin Zi Hao Zhao, Dali Kaafar, Yang Xiang\n## Abstract\nAbstract With the rapid development of deep learning techniques, the popularity of voice services implemented on various Internet of Things (IoT) devices is ever increasing. In this paper, we examine user-level membership inference in the problem space of voice services, by designing an audio auditor to verify whether a specific user had unwillingly contributed audio used to train an automatic speech recognition (ASR) model under strict black-box access. With user representation of the input audio data and their corresponding translated text, our trained auditor is effective in user-level audit. We also observe that the auditor trained on specific data can be generalized well regardless of the ASR model architecture. We validate the auditor on ASR models trained with LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid ASR system and the end-to-end ASR system. Finally, we conduct a real-world trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding 80%. We hope the methodology developed in this paper and findings can inform privacy advocates to overhaul IoT privacy.\n## Membership Inference Attack\nThe membership inference attack is considered as a significant privacy threat for machine learning (ML) models [22]. The attack aims to determine whether a specific data sample is within the target model's training set or not. The attack is driven by the different behaviors of the target model when making predictions on samples within or out of its training set. \n\nVarious membership inference attack methods have been recently proposed. Shokri et al. [39] train shadow models to constitute the attack model against a target ML model with black-box access. The shadow models mimic the target model's prediction behavior. To improve accuracy, Liu et al. [15] and Hayes et al. [10] leverage Generative Adversarial Networks (GAN) to generate shadow models with increasingly similar outputs to the target model. Salem et al. [32] relax the attack assumptions mentioned in the work [39], demonstrating that shadow models are not necessary to launch the membership inference attack. Instead, a threshold of the predicted confidence score can be defined to substitute the attack model. Intuitively, a large confidence score indicates the sample as a member of the training set [41]. The attacks mentioned in the work above are all performed on the record level, while Song and Shmatikov [40] study a user-level membership inference attack against text generative models. Instead of using the prediction label along with the confidence score, Song and Shmatikov [40] utilize word's rank list infor-mation of several top-ranked predictions as key features to generate the shadow model. Apart from the blackbox access, Farokhi and Kaafar [8] model the recordlevel membership inference attack under the white-box access. \n\nUnlike image recognition systems or text generative systems, ASR systems present additional challenges [5]. With strict black-box access, attacks using confidence scores cannot be applied. With limited discriminative power, features can only be extracted from the predicted transcription and its input audio to launch membership inference attacks, i.e., audio auditing in our paper.",
            "reference_string": "[221808247 | Miao et al. | 2019 | Citations: 36]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "248810845",
            "title": "Evaluating Membership Inference Through Adversarial Robustness",
            "text": "As mentioned above, membership inference attack aims to figure out whether a piece of data belongs to the training dataset or not. There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings. \n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18,22].",
            "score": 0.9283853679507632,
            "section_title": "Privacy Leakage and Membership Inference",
            "char_start_offset": 8957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1651
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 594,
                    "end": 597,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1640,
                    "end": 1644,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1647,
                    "end": 1650,
                    "matchedPaperCorpusId": "214623088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "Nasr et al. [30] analyze membership inference attacks in the white-box setting, where the adversary has the full access to the target machine learning model and knows the model architecture and model parameters. They find that simply combining target model's final predictions and its intermediate computations to learn the attack classifier results in attack accuracy no better than that of the corresponding black-box attacks. Instead, by using the gradient of prediction loss with regard to model parameters \u2202 (F \u03b8 ,z) \u2202\u03b8 as additional features, the white-box membership inference attacks obtain higher attack accuracy than the black-box attacks. We show that the gap between white-box attack accuracy and black-box attack accuracy is much smaller than previous estimates in this paper.",
            "score": 0.9246027579916921,
            "section_title": "White-box membership inference attacks",
            "char_start_offset": 11218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 789
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "207847912",
            "title": "Theoretical Guarantees for Model Auditing with Finite Adversaries",
            "text": "Consider a membership inference attack where the goal of an attacker, the learner in the notation of Section 3.1, is to predict if the data of a given individual was used to train a target classifier g. To be more specific, we consider a membership inference attack composed by the following elements. \n\nPopulation data. Fix a population size N \u2208 N. We encode the population data in an N \u00d7 (p + 1) matrix \n\nwhere x i \u2208 R p and y i \u2208 {\u00b11} for each i \u2208 [N ]. The variables x i and y i are the feature vector and the label of the i-th individual, respectively. \n\nClassifier. In the literature, there are two main types of attacks: black-box and white-box. In black-box attacks, it is assumed that we are given access to a binary classifier g : R p \u2192 [\u22121, 1] only through the values of g for some inputs x 1 , . . . , x n \u2208 R p . In white-box attacks, it is assumed that we are given access to g itself. For example, when g is implemented by a neural network, a white-box attack requires knowing the weights of the neural network, while a black-box attack only requires being able to evaluate g for some inputs. In both cases, the classifier g is trained using some of the individual records in D. We encode which records were used to train g via a vector (s 1 , . . . , s N ) \u2208 {\u00b11} N . Namely, we set s i = 1 if and only if (x i , y i ) was used to train g. \n\nAttacker. Given the data (x i , y i ) of the i-th individual and the confidence value g(x i ), the attacker's goal is to learn the membership variable s i . In other words, the attacker wants to learn whether the data of the i-th individual was used to train g or not. The attacker achieves this goal by designing a function h : R \u2192 R which ideally satisfies that \n\nFollowing our standing assumptions, we assume that the attacker is capable to implement functions in H k for some k \u2208 N and can minimize L Sn over H k where \n\nfor some i 1 , . . .",
            "score": 0.851977252052735,
            "section_title": "Unintended Memorization",
            "char_start_offset": 13948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1721
                },
                {
                    "start": 1724,
                    "end": 1880
                },
                {
                    "start": 1883,
                    "end": 1903
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "274281228",
            "title": "Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference",
            "text": "As an emerging technique, membership inference attack aims to infer whether a specific sample (, ) belongs to the training data set   of the target model. According to the attacker's capabilities, MIAs can be roughly divided into two categories: white-box attacks and black-box attacks: \n\nBlack-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [10,27,28]. There are generally two strategies in black-box settings: model-based attacks and metric-based attacks. \n\n\u2022 Model-based Attacks: Shokri et al. [28] introduced the first MIA against machine learning models, where the attacker has black-box access to the target model. The attacker builds multiple shadow models to mimic the target model, constructs a dataset of membership labels, and trains a binary classifier to predict membership status. However, this method requires extensive resources to train multiple shadow models with the same architecture as the target model, necessitating access to a shadow dataset with a distribution similar to that of the target's training set. To mitigate these issues, Salem et al. [25] proposed using a single shadow model, allowing for effective attacks while relaxing some adversarial assumptions. \u2022 Metric-based Attacks: Song et al. [29] developed a metricbased attack where the attacker compares a calculated metric  (such as entropy) to a predefined threshold to infer membership. This method, however, is limited when the target model only provides predicted labels without prediction vectors. In contrast, our approach effectively utilizes predicted labels for membership inference. Bertran et al. [3] introduced a novel method that distinguishes between members and non-members using quantiles, eliminating the need for architectural knowledge and representing a true \"black box\" approach. Liu et al. [21] leverage the training process of the target model in their MIA, called TrajectoryMIA, utilizing knowledge distillation to extract membership information from loss records at various training epochs. These methods, while innovative, typically involve complex training processes and significant costs, particularly with knowledge distillation. \n\nWhite-box Membership Inference. In white-box settings, attackers gain access to model parameters  * and potentially intermediate training information, such as gradients \u2112  [16,23].",
            "score": 0.8516684302985258,
            "section_title": "RELATED WORK 6.1 Membership Inference",
            "char_start_offset": 28546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 286
                },
                {
                    "start": 289,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2214
                },
                {
                    "start": 2217,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 415,
                    "end": 418,
                    "matchedPaperCorpusId": "218514464"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1664,
                    "end": 1667,
                    "matchedPaperCorpusId": "259375769"
                },
                {
                    "start": 1868,
                    "end": 1872,
                    "matchedPaperCorpusId": "251953448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "227227868",
            "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training",
            "text": "Membership inference attacks. [14], a number of studies have been made on this attack [17]- [23]. \n\nFor example, Jayaraman et al. [23] assumed a realistic scenario where the candidate pool from which the adversary samples records is imbalanced, and proposed a membership inference attack based on the direction of change in loss of a query record when it is perturbed with a small amount of noise. They showed that their attack outperforms previous attacks in imbalanced prior settings. However, their attack still considers only the black-box access to the model. Nasr et al. [17] evaluated white-box membership inference attacks against deep learning algorithms. They first showed that a straightforward extension of the black-box attack to the white-box setting, which uses the outputs of individual layers (rather than the outputs of the last layers), does not provide better accuracy than the black-box attack. Then they proposed a white-box attack using the gradient of the prediction loss with regard to all parameters, and showed that it outperforms the black-box attack. Their proposed attack cannot be used in our setting where the adversary does not have a whitebox access to h t , because the adversary cannot calculate the gradient by the back-propagation algorithm in this case. \n\nRecently, Song et al. [24] proposed a membership inference attack based on modified prediction entropy (MPE). They showed that the MPE-based attack outperforms the DNN-based attack in their experiments. On the other hand, the MPE-based attack results in low accuracy in our experiments, because the distributions of confidence values are similar between training and test datasets and a single threshold does not separate the two distributions. \n\nDefences against membership inference attacks. Defences against membership inference attacks have also been widely studied; e.g., regularization-based defenses [14], [25], [26], DP (Differential Privacy)-based defenses [10], [27]- [29], adding noise to (or transforming) a confidence score vector [30], [31], and DAMIA [32].",
            "score": 0.8461468558489912,
            "section_title": "A. Related Work",
            "char_start_offset": 8454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "214623088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95703125
        },
        {
            "corpus_id": "53172224",
            "title": "The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model",
            "text": "Membership inference attacks involve an adversary who observes the output of some computations over a hidden dataset D-for example, calculations of aggregate statistics or training of a machine learning model on D-and aims to determine whether a specific data record is a member of D. \n\nSuccessful membership inference attacks against aggregate statistics have been demonstrated in the context of genomic studies [3], [16], location time-series [28], and noisy statistics in general [9]. \n\nMembership inference attacks against ML models are surveyed in more detail in Section VII. They can be performed in a white-box or black-box setting. In the black-box setting, the adversary queries the model with a specific record and attempts to infer from the model's outputs (e.g., probabilities assigned to different classes by a classification model) whether the record was among those used to train the model or not. For example, Shokri et al. [31] demonstrated a method for learning the statistical difference between the outputs of a classification model on members and non-members. The key technique-which inspires our approach in this paper-is to train a membership discriminator using the probability vector output by the model as the feature. \n\nAuditing text-generation models involves an application of membership inference, but there are several critical differences that require the development of new membership inference techniques specifically adapted to the auditing scenarios and the idiosyncrasies of memorization in text-generation models. We discussed these differences in Section I and briefly summarize them here. In realistic auditing scenarios, (1) the auditor's goal is user-level membership inference, as opposed to recordlevel membership inference which was the focus of prior work. \n\n(2) Deployed models that a user may wish to audit can be well-generalized and not overfitted to the training data in the conventional sense. Furthermore, (3) deployed models output only ranked lists of words and not the underlying numeric probabilities, removing the main feature that was used by previous membership inference attacks. Also, (4) the models' outputs may be limited to only a few dozen or hundreds of words, as opposed to the entire vocabulary, and the auditor may be limited to a small number of queries.",
            "score": 0.8453888345793311,
            "section_title": "C. Membership inference",
            "char_start_offset": 11758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 416,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "220525609",
            "title": "A Survey of Privacy Attacks in Machine Learning",
            "text": "Membership inference tries to determine whether an input sample x was used as part of the training set D. This is the most popular category of attacks and was first introduced by Shokri et al. [101]. The attack only assumes knowledge of the model's output prediction vector (black-box) and was carried out against supervised machine learning models. White-box attacks in this category are also a threat, especially in a collaborative setting, where an adversary can mount both passive and active attacks. If there is access to the model parameters and gradients, then this allows for more effective white-box membership inference attacks in terms of accuracy [80]. \n\nApart from supervised models, generative models such as GANs and VAEs are also susceptible to membership inference attacks [15,35,39]. The goal of the attack, in this case, is to retrieve information about the training data using varying degrees of knowledge of the data generating components. \n\nFinally, these types of attacks can be viewed from a different perspective, that of the data owner. In such a scenario, the owner of the data may have the ability to audit black-box models to see if the data have been used without authorization [41,103].",
            "score": 0.8433957948005694,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 22068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1217
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 198,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1208,
                    "end": 1212,
                    "matchedPaperCorpusId": "119302127"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "159041003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "265498519",
            "title": "Survey on AI Ethics: A Socio-technical Perspective",
            "text": "Different attacks are possible depending on the model architecture and attackers' capabilities. In terms of security, attackers may aim to gain access to the model, steal its information, or disrupt its normal functioning. For example, an attacker may target a spam detector to make it unable to classify spam correctly by poisoning training data [Barreno et al. 2010]. \n\nThe attacks can be categorized into three main categories: black-box, partial white-box, and white-box attacks, according to the attacker's knowledge of the machine learning model [Pitropakis et al. 2019]. In black-box attacks, the attacker has no information about the training dataset or model's architecture, and the only access is through querying the model; in white-box attacks, the attacker has full access to the model and all information about it, including the training dataset, model parameters, model architecture, prediction vectors, etc. Partial white-box(gray-box) attacks stand between these extremes, meaning the attacker has some information about the model's architecture and training data distribution. The most common security and privacy attacks against machine learning models are described in this section. \n\nMembership Inference. The membership inference is an attack in which an attacker attempts to determine if a particular data sample , is part of a model 's training dataset [Baluta et al. 2022;Hu et al. 2022;Rigaki and Garcia 2020]. This attack is often carried out using black-box techniques to query the model. Different querying techniques are used to optimize the attack to gain more information about the membership of individual records in the training dataset. One of the first Membership inference attacks, implemented by [Shokri et al. 2017] which could achieve high accuracy in their inference, was performed on Google and Amazon's APIs that provide machine learning as a service (MLaaS). [Quan et al. 2022] showed that having additional knowledge about the model or training dataset distribution can improve the attack's success rate. \n\nModel Inversion. In a model inversion attack, the adversary tries to get information from the target model to reconstruct some representation of its input dataset. The first category aims to generate an actual data reconstruction [He et al. 2019].",
            "score": 0.8166147347863858,
            "section_title": "Privacy and security attacks and defenses overview",
            "char_start_offset": 3579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2215
                },
                {
                    "start": 2216,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 576,
                    "matchedPaperCorpusId": "208102198"
                },
                {
                    "start": 1377,
                    "end": 1397,
                    "matchedPaperCorpusId": "252367825"
                },
                {
                    "start": 1397,
                    "end": 1412,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1412,
                    "end": 1435,
                    "matchedPaperCorpusId": "220525609"
                },
                {
                    "start": 1734,
                    "end": 1753,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "248870291",
            "title": "Black-box based limited query membership inference attack",
            "text": "Since Shokri et al. [6] proposed an inference attack against MLaaS, the research on member inference has received extensive attention and has been successfully implemented in many fields. It uses the output information of the model to infer whether a sample exists in the training data set of the model, which seriously threatens the privacy and security of the machine learning model. Membership inference attacks are divided into white-box attacks [11][12][13][14] and blackbox attacks [5,6,15] according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability. In addition, membership inference attacks can also be divided into attacks against independent models [16][17][18][19][20][21] and attacks against Federated Learning [11,22]. \n\nAt present, the research on membership inference attack mainly focuses on the black-box attack of independent models. The attacker uses the model's predicted behavior difference between member data and non-member data to train a binary attack model that recognizes this difference. In order to obtain the training data of the attacking model in the blackbox mode, Shokri [6]  Shokri's method [6] is based on the assumption of extensive query and knowledge of the target model structure. They obtained synthetic data by querying the target model and used this data to train multiple shadow models with the same structure as the target model. In practical applications, a large number of accesses to the target model require high query costs, which largely limits the application scope of membership inference attacks. Therefore, how to construct a shadow model similar to the target model under a small number of queries is the key to solving the hypothesis of membership inference attack.",
            "score": 0.8112096422922639,
            "section_title": "A. MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 4381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 488,
                    "end": 491,
                    "matchedPaperCorpusId": "186206717"
                },
                {
                    "start": 491,
                    "end": 493,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "233864706"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1032,
                    "end": 1036,
                    "matchedPaperCorpusId": "210888275"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1462,
                    "end": 1465,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
            "score": 0.8025008798563562,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "Attacker's goal: Like many studies on membership inference attacks, we consider an attacker's goal is to determine whether an input sample x was used as part of the training set D \u2032 . But in our solution, we focus on protecting the raw dataset D rather than the synthetic dataset D \u2032 . Thus, we replace D \u2032 with D. In short, in this paper, we focus on the privacy of the source data D. Attacker's capability: Because there are three important entities in our solution, we need to classify attackers (shown in TABLE 1) based on the knowledge of framework. Firstly, we assume that they only have the ability to access the application model after data generated. With the knowledge of model structure and parameters, we can separate them into black-box attack and white-box attack. These two types of attacks are almost the same as the traditional membership inference attack mode: black-box attack and white-box attack except the protected data. In the other situation, the attacker can get the synthetic data. Inspired by the Chen [5], according to the knowledge of the extractor and reconstructor, the attack mode can be divided into a black-box generator attack, partial black-box generator attack, and white-box generator attack. These three types of attacks belong to the attack category facing data release and the generative model release. Note that, in this paper, we just consider the following black-box attack and white-box attack. Taxonomy of membership inference attacks discussed in this paper: \n\n1. black-box attack: This is the least knowledgeable setting to attackers where the attacker is only able to send query data samples to the target application model and obtain their output predicted by the target model. Besides, they have no information about the extractor and reconstructor in this framework.",
            "score": 0.7980413081013362,
            "section_title": "Threat Model",
            "char_start_offset": 14469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1819
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "174799799",
            "title": "White-box vs Black-box: Bayes Optimal Strategies for Membership Inference",
            "text": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
            "score": 0.795903116700962,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "256627812",
            "title": "Membership Inference Attacks against Diffusion Models",
            "text": "A membership inference attack is a kind of attack whereby an adversary infers whether a particular example was contained in the training dataset of a model [21]- [23]. A model vulnerable to the attack potentially contains threats to privacy leakage, and hence recent works discuss membership inference attacks for various machine learning models [15], [16], [18]. There are two settings [14], i.e., the white-box setting where an adversary has access to model parameters, and the blackbox setting where he/she utilizes only outputs of the model.\n\nA typical approach for membership inference attacks is to leverage the large divergence between the loss distribution over members and non-members [24]. The divergence can be embedded by an adversary. For instance, privacy leakage, including membership inference attacks, can be more effective by training a model with poisoning samples [25]- [28].\n\nMembership inference attacks can be prevented by differential privacy [29] where gradients are perturbed [30], [31]. Since differential privacy often deteriorates inference accuracy, several works evaluated differential privacy on membership inference attacks in a quantitative fashion [32], [33].",
            "score": 0.7896888243679339,
            "section_title": "A. Membership Inference",
            "char_start_offset": 4305,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "251135031"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "251765197"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "201598829"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "247922814"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "231718884"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "2565493"
                },
                {
                    "start": 1002,
                    "end": 1006,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1008,
                    "end": 1012,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1183,
                    "end": 1187,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1189,
                    "end": 1193,
                    "matchedPaperCorpusId": "219686822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "202734167",
            "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
            "text": "56] recently proposed new membership inference attacks for black-box target classifiers, which relax the assumptions of the attacks proposed by Shokri et al. from both model and data angles. For instance, they showed that the attacker can rank the entries in a confidence score vector before feeding it into an attack classifier, which improves the attack effectiveness. Moreover, they showed that it is sufficient for the attacker to train just one shadow classifier. These results indicate that membership inference threat is even larger than previously thought.\n\nMore recently, Nasr et al. [43] proposed membership inference attacks against white-box ML models. For a data sample, they calculate the corresponding gradients over the white-box target classifier's parameters and use these gradients as the data sample's feature for membership inference. Moreover, both Nasr et al. [43] and Melis et al. [36] proposed membership inference attacks against federated learning. While most of the previous works concentrated on classification models [33,34,42,43,56,58,69], Hayes et al. [21] studied membership inference against generative models, in particular generative adversarial networks (GANs) [18]. They designed attacks for both white-and black-box settings. Their results showed that generative models are also vulnerable to membership inference.\n\nDefense mechanisms against membership inference: Multiple defense mechanisms have been proposed to mitigate the threat of membership inference in the ML setting. We summarize them as the following.\n\nL 2 -Regularizer [58]. Overfitting, i.e., ML classifiers are more confident when facing data samples they are trained on (members) than others, is one major reason why membership inference is effective. Therefore, to defend against membership inference, people have explored to reduce overfitting using regularization. For instance, Shokri et al. [58] explored using conventional L 2 regularizer when training the target classifier.\n\nMin-Max Game [42]. Nasr et al. [42] proposed a min-max game-theoretic method to train a target classifier. Specifically, the method formulates a min-max optimization problem that aims to minimize the target classifier's prediction loss while maximizing the membership privacy. This formulation is equivalent to adding a new",
            "score": 0.7881469309046502,
            "section_title": "RELATED WORK 2.1 Membership Inference",
            "char_start_offset": 9714,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 883,
                    "end": 887,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 905,
                    "end": 909,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1054,
                    "end": 1057,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1057,
                    "end": 1060,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1063,
                    "end": 1066,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1066,
                    "end": 1069,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "3052834"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1571,
                    "end": 1575,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1901,
                    "end": 1905,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2001,
                    "end": 2005,
                    "matchedPaperCorpusId": "49863840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "249940711",
            "title": "Data Privacy and Trustworthy Machine Learning",
            "text": "the black-box setting, the attacker can only interact with the model's predictions. Shokri et al. 12 designed membership inference attacks, which are algorithms that simulate the game and measure the information leakage of models about their training data. Moreover, reports by the National Institute of Standards and Technology 15 and the U.K.'s Information Commissioner's Office 10 specifically mention membership inference as a confidentiality violation and potential threat to the training data in artificial intelligence.\n\nIn the black-box setting, membership inference attacks attempt to exploit the signals contained in a model's predictions. The major examples of such a signal are prediction error and uncertainty. Predictions tend to be more accurate and certain for members of the training set compared to unseen points. Powerful membership inference attacks have been demonstrated for many different scenarios in machine learning. 9, 16 We can use membership inference attacks to quantitatively measure the privacy risks of machine learning algorithms in many diverse scenarios. Throughout this article, we use membership inference attacks as a tool to measure information leakage of private data. This way, we can study privacy implications of the other aspects of trustworthy machine learning.\n\nGiven a set of points X R n 3 with membership : m X {0,1}\n\n\" to a training set, we define the average privacy risk of a trained model as (see Ye et al. 16 for a more comprehensive discussion on how to measure privacy risk in machine learning.)\n\nis the signal the attacker can observe for each point after a model is trained. The signal can be the model's prediction, model's loss, an explanation, or",
            "score": 0.7829465805328973,
            "section_title": "Threats to Data Privacy",
            "char_start_offset": 3892,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 100,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "49863840",
            "title": "Machine Learning with Membership Privacy using Adversarial Regularization",
            "text": "The objective of membership inference attacks, also referred to as tracing attacks, is to determine whether or not a target data record is in a dataset, assuming that the attacker can observe a function over the dataset (e.g., aggregate statistics, model). \n\nThe membership inference attacks have mostly been studied for analyzing data privacy with respect to simple statistical linear functions [5,17,18,23,42,43]. The attacker compares the released statistics from the dataset, and the same statistics computed on random samples from the population, to see which one is closer to the target data record. Alternatively, the adversary can compare the target data record and samples from the population, to see which one is closer to the released statistics. In either case, if the target is closer to the released statistics, then there is a high chance that it was a member of the dataset. The problem could be formulated as a hypothesis test, and the adversary can make use of likelihood ratio test to run the inference attack. \n\nIn the case of machine learning models, the membership inference attack is not as simple, especially in the black-box setting. The adversary needs to distinguish training set members from non-members from observing the model's predictions, which are indirect nonlinear computations on the training data. The existing inference algorithm suggests training another machine learning model, as the inference model, to find the statistical differences between predictions on members and predictions on nonmembers [45]. In this section, we formally present this attack and the optimization problem to model the adversary's objective. Table 2 summarizes the notations and the optimization problem. Figure 1 illustrates the relation between different components of a membership inference attack against machine learning models in the black-box setting. \n\nLet h be the inference model h : \n\nFor any data point (x, ) and the model's prediction vector f (x), it outputs the probability of (x, ) being a member of D (the training set of f ). Let Pr D (X, Y) and Pr \\D (X, Y) be the conditional probabilities of (X, Y) for samples in D and outside D, respectively. In an ideal setting (of knowing these conditional probability distributions), the gain function for the membership inference attack can be computed as the following.",
            "score": 0.7825457881560901,
            "section_title": "MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 11770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 399,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 408,
                    "end": 411,
                    "matchedPaperCorpusId": "125100846"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "231861713",
            "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
            "text": "Membership Inference Attack. Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API [5,9,19,28,30,31,35,36,39,51]. Most of the existing attacks focus on deep learning models that are trained on sensitive data from the Euclidean space, such as images and texts. Shokri et al. [36] propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack. Salem et al. [35] further relax several key assumptions from [36], such as knowledge of the target model architecture, shadow dataset from the same distribution. Yeom et al. [51] discuss the relationship between overfitting and membership attacks. Nasr et al. [31] conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings. To mitigate the attacks, some defense mechanisms [22,30,35,36] have been proposed. Those strategies include using model stacking [35], dropout [35], adversarial training [30], jointly maximize privacy and prediction accuracy [22], etc.\n\nOther Exploratory Attacks Against ML Models. Besides membership inference, other exploratory attacks such as model inversion, attribute inference, and model stealing have been studied by many researchers. In model inversion attacks [16,17,52], an adversary aims to reconstruct input samples from a target ML model, i.e., model inversion enables the adversary to directly learn information about the training dataset. Fredrikson et al. [17] first propose a model inversion attack in the setting of drug dose classification. Later, Fredrikson et al. [16] further extend model inversion to general ML settings relying on back-propagation. More recently, Zhang et al. [52] develop a more advanced attack based on GANs to synthesize the training dataset.\n\nAttribute inference attacks [29,38] aim to infer some general properties of the training dataset. Meils et al",
            "score": 0.7807697725380419,
            "section_title": "Related Work",
            "char_start_offset": 35696,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "203593891"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1191,
                    "end": 1195,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1195,
                    "end": 1198,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1198,
                    "end": 1201,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1285,
                    "end": 1289,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1618,
                    "end": 1621,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 1814,
                    "end": 1818,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1927,
                    "end": 1931,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9794921875
        },
        {
            "corpus_id": "265654401",
            "title": "Preserving data privacy in machine learning systems",
            "text": "In a membership inference (Shokri et al., 2017;Bernau et al., 2019;Jia et al., 2019a;Li et al., 2020) attack (see Fig. 2) the adversary tries to identify whether a given data point (  ,   ) or a sample of data points were part of the training data set   used to train a given model \u210e  . Revealing that a certain record was used to train a specific machine learning model is a strong indication of private information leakage about the individual data points in the training set. e.g., knowing that a medical record was used to train a machine learning model deployed for diabetes detection can reveal that the person concerned has diabetes. \n\nThese attacks exist in the black-box and the white-box modes. In the black box mode, the attacker has only a query access to the model without any inner information about it, hence only the query results are used to infer the membership of data points within the original training set. In the white-box setting, the attacker has either access to the inner details (description) of the model or can download it locally.",
            "score": 0.7785481776420782,
            "section_title": "Membership inference attacks",
            "char_start_offset": 29317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1061
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 47,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "174799799",
            "title": "White-box vs Black-box: Bayes Optimal Strategies for Membership Inference",
            "text": "We refer to the white-box setting as the case where the attacker knows all the network parameters. On a side note, the setup commonly adopted in differential privacy (Dwork et al., 2006) corresponds to the white-box setting, where the attacker additionally knows all the training samples except the one to be tested. \n\nThe black-box setting is when these parameters are unknown. For classification models, the attacker has only access to the output for a given input, in one of the following forms: \n\n(i) the classifier decision; (ii) the loss of the correct label; (iii) the full response for all classes. \n\nPrior works on membership inference commonly assume (i) or (iii). Our paper focuses on the black-box case (ii), in which we know the loss incurred by the correct label. The state of the art in this setting are the shadow models proposed by Shokri et al. (2017). \n\nIn our work, we use a probabilistic framework to derive a formal analysis of the optimal attack. This framework encompasses both Bayesian learning, and noisy training, where the noise is injected (Welling & Teh, 2011) or comes from the stochasticity of SGD. Under mild assumptions on the distribution of the parameters, we derive the optimal membership inference strategy. This strategy only depends on the classifier through evaluation of the loss, thereby showing that black-box attacks will perform as well as white-box attacks in this optimal asymptotic setting. This result may explain why, to the best of our knowledge, the literature does not report white-box attacks outperforming the stateof-the-art black-box-(ii) attacks. \n\nThe aforementioned optimal strategy is not tractable, therefore we introduce approximations to derive an explicit method for membership inference. \n\nAs a byproduct of this derivation, we show that state-of-theart approaches (Shokri et al., 2017;Yeom et al., 2018) are coarser approximations of the optimal strategy. One of the approximation drastically simplifies the membership inference procedure by simply relying on the loss and a calibration term. We employ this strategy to the more complex case of neural networks, and show that it outperforms all approaches we are aware of.",
            "score": 0.7719806176341121,
            "section_title": "body",
            "char_start_offset": 1951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 186,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 849,
                    "end": 869,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1832,
                    "end": 1853,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8642578125
        },
        {
            "corpus_id": "220831381",
            "title": "Label-Only Membership Inference Attacks",
            "text": "Membership inference attacks (Shokri et al., 2016) are a form of privacy leakage that identify if a given data sample was in a machine learning model's training dataset. Given a sample x and access to a trained model h, the adversary uses a classifier or decision rule f h to compute a membership prediction f (x; h) \u2208 {0, 1}, with the goal that f (x; h) = 1 whenever x is a training point. The main challenge in mounting a membership inference attack is creating the classifier f , under various assumptions about the adversary's knowledge of h and its training data distribution.\n\nPrior work assumes that an adversary has only black-box access to the trained model h, via a query interface that on input x returns part or all of the confidence vector h(x).",
            "score": 0.7714652783846322,
            "section_title": "A.2. Membership Inference",
            "char_start_offset": 37003,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "221507916",
            "title": "A Comprehensive Analysis of Information Leakage in Deep Transfer Learning",
            "text": "Membership Inference. The study [32] developed the first membership attack against machine learning models with only black-box access using a shallow training method. This method assumes that we have some prior knowledge of the data for training the attack classifier is from the same distribution as the original training data. Later study of [19] followed the idea of shallow training and explored two more targeted membership attacks, i.e., frequency-based and distance-based. The study [30] further relaxed key attack assumptions of [32] and demonstrated more applicable attacks. Aside from the black-box setting, these studies [22,26] examined the membership attacks against federated/collaborative learning under the white-box setting, where an adversary can access the model and potentially is able to observe/eavesdrop the intermediate computations at hidden layers. They share a similar idea that leverages the gradients or model snapshots to produce the labeled examples for training a binary membership classifier. This work [9] presented the first membership attacks on both black-box and white-box for generative models, in particular generative adversarial networks (GANs). Property Inference. Property attack aims to infer the properties hold for the whole or certain subsets of the training data. Prior works [2,6,7] studied property inference attacks that characterize the entire training dataset. A property attack was developed [7] based on the concept of permutation invariance for fully connected neural networks, with the assumption that adversary has white-box knowledge. Concurrently, the study [22] developed attacks under the collaborative learning setting, where they focus on inferring properties for single batches of training inputs.",
            "score": 0.7712874807212151,
            "section_title": "RELATED WORK",
            "char_start_offset": 38217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 636,
                    "end": 639,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1036,
                    "end": 1039,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1325,
                    "end": 1328,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1328,
                    "end": 1330,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1330,
                    "end": 1332,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1447,
                    "end": 1450,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "From Table 5, we can see that compared to the blackbox metric-based attacks, the improvement of white-box membership inference attacks is limited. The attack accuracy of white-box membership inference adversary is only 0.5% and 0.6% higher than the attack accuracy achieved by our black-box benchmark attacks, on the Texas100 and the CI-FAR100 classifiers. The white-box attack on the Purchase100 classifier still has 5.8% increase in attack accuracy compared",
            "score": 0.7705539476755965,
            "section_title": "Re-evaluating MemGuard [19]",
            "char_start_offset": 33731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 459
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33935546875
        },
        {
            "corpus_id": "274281228",
            "title": "Hide in Plain Sight: Clean-Label Backdoor for Auditing Membership Inference",
            "text": "In white-box settings, attackers gain access to model parameters  * and potentially intermediate training information, such as gradients \u2112  [16,23]. This access can significantly enhance the effectiveness of membership inference attacks (MIAs), particularly in collaborative learning environments. For instance, Nasr et al. [22] introduced a white-box attack that targets privacy vulnerabilities inherent in the stochastic gradient descent algorithm used for training deep neural networks. Although many white-box attacks rely on model-based strategies, our focus here is on a black-box approach. It is important to note that leveraging the additional information available in white-box scenarios can be challenging in practical applications, making blackbox attacks more relevant in many real-world contexts.",
            "score": 0.7649470790523327,
            "section_title": "RELATED WORK 6.1 Membership Inference",
            "char_start_offset": 30795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 809
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "195346528",
            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
            "text": "In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used to train a given model. The adversary is given a data point z = (x, y), access to a model A S , the size of the model's training set |S| = n, and the distribution D that the training set was drawn from. With this information the adversary must decide whether z \u2208 S. For the purposes of this discussion, we do not distinguish whether A's access to A S is \"black-box\", i.e., consisting only of input/output queries, or \"white-box\", i.e., involving the internal structure of the model itself. However, unless otherwise noted all of the attacks presented in this section assume black-box access. \n\nExperiment 1 below formalizes membership inference attacks. The experiment first samples a fresh dataset from D, and then flips a coin b to decide whether to draw the adversary's challenge point z from the training set or the original distribution. A is then given the challenge, along with the additional information described above, and must guess the value of b. \n\nExperiment 1 (Membership experiment Exp M (A, A, n, D)). Let A be an adversary, A be a learning algorithm, n be a positive integer, and D be a distribution over data points (x, y). The membership experiment proceeds as follows: \n\n1. Sample S \u223c D n , and let A S = A(S). \n\n2. Choose b \u2190 {0, 1} uniformly at random.",
            "score": 0.7638781523151689,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 12901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "253244424",
            "title": "Amplifying Membership Exposure via Data Poisoning",
            "text": "In a membership inference attack, an attacker aims to infer whether a specific sample (x, y) belongs to the training dataset D train at the test time [24,33,38]. Unintended membership exposure causes catastrophic privacy loss for individuals. For example, in the real world, a data sample x can be a clinical record or an individual. Membership inference attacks enable the attackers to infer whether this clinical record or individual has been used to train a model associated with a certain disease. As such, these attacks are widely adopted as basic metrics to quantify privacy exposure in statistical data analysis algorithms [13,23]. Henceforth, we use membership inference attacks to demonstrate how to exploit data poisoning to amplify privacy leakage in this paper. \n\nBased on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs [17,37,38]. This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased [38], where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased [33,39], where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker [22,27,31]. Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features. \n\nOur Setup. Our evaluation is carried out in the metric-based black-box membership inference setting. We adopt the following metric proposed by Song et al. [40] to measure the membership exposure:2",
            "score": 0.759462692392547,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 6204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 634,
                    "end": 637,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 1003,
                    "end": 1007,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 1007,
                    "end": 1010,
                    "matchedPaperCorpusId": "218514464"
                },
                {
                    "start": 1010,
                    "end": 1013,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1148,
                    "end": 1152,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "174799799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "201126909",
            "title": "Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection",
            "text": "Membership attacks towards deep learning algorithms. Recently, membership attacks have arisen as a common threat model against machine learning algorithms and attained increasing attraction from the research community [32,31,5,13]. A pioneer work [31] investigated the risk of membership attacks on different machine learning models. Specifically, they developed a shadow training technique to obtain an attack model in the black-box setting (i.e., without knowing the machine learning model structure and parameters). Carlini et al. [5] proposed a metric to measure the vulnerability of deep learning models. In the context of GANs, Hayes et al. [13] studied membership attacks against GANs in both black-box and white-box settings. In this paper, we use the membership attack to assess the information leakage of the dataset used for training GAN models.",
            "score": 0.7566357890856883,
            "section_title": "Related Work",
            "char_start_offset": 5997,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 856
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 247,
                    "end": 251,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "227227868",
            "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training",
            "text": "We next review a privacy attack on machine learning models, called a membership inference attack (MIA) [14], where an adversary is not given a training dataset D train and attempts to reveal membership information on D train . Specifically, the adversary in this attack aims to determine whether a specific data point (x, y) is included in a training dataset D train used to build a classification model f . \n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f . To build an adversary A against f , we attempt to learn some statistical relevance between the distribution f (x) of confidence values and the membership (x, y) \u2208 D train . \n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train . An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f (x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network). \n\nFormally, an adversary in the black-box membership inference attack on a model f is a function A : X \u00d7 Y \u00d7 DY \u2192 {in, out} that given a data point (x, y) and its prediction vector f (x), outputs one of the labels in and out, where in (resp. out) represents that A predicts (x, y) \u2208 D train (resp. \n\n(x, y) \u2208 D train ). To evaluate the performance of the membership inference attack, we measure the accuracy, precision, and recall of the attack by using a test dataset D test \u2286 X \u00d7 Y disjoint from the training dataset D train . \n\nWe deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train .",
            "score": 0.7538244452149193,
            "section_title": "C. Membership Inference Attack",
            "char_start_offset": 17223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 107,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1789,
                    "end": 1793,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "214623088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "227745260",
            "title": "Privacy-Preserving Synthetic Smart Meters Data",
            "text": "Both attacks compute a score for every input and select the top scoring input as the most likely candidate. \n\nBlack-box Membership Inference Attacks: All blackbox attacks we have seen in the literature, including those proposed in [11], [16], [18], [19], are based on training a surrogate model that can be used to perform a white-box version of the attack. In general, this yields degraded results in comparison to the white-box attack. We propose to exploit the similarity between artificial and natural data to perform a black-box attack. Our black-box attack is inspired by our evaluation method using statistical properties and uses the earth mover's distance between the empirical distributions of artificial and natural curves in terms of their indicators. \n\nDefense Mechanisms: We study the interplay between privacy and generalization, and how the choice of training hyper-parameters can protect against membership inference attacks. Additionally, we propose Gradient Norm Regularization, a defense mechanism designed to protect against membership attacks that exploit the gradient of the loss function.",
            "score": 0.7486488487799543,
            "section_title": "B. Our contribution",
            "char_start_offset": 7348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1112
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "53172224",
            "title": "The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model",
            "text": "Membership inference. As discussed in Section II-C, release of aggregate statistics about private data is generically vulnerable to membership inference attacks [9]. Membership inference against ML models was studied in [13], [22], [29], [31]. \n\nShokri et al. [31] developed membership inference techniques against black-box models, exploiting the differences in the models' outputs on training and non-training inputs. Their attack performs best when target model is overfitted to the training data. Truex et al. [37] extend and generalize this work, including white-box and federated-learning setting. \n\nRahman et al. [29] extend the membership inference attack to differentially private ML models and show that the model may need to sacrifice its test accuracy to achieve membership privacy. Hayes et al. [13] studied membership inference attacks against generative models. Long et al. [22] showed that wellgeneralized models can leak membership information, but their attack requires the adversary to first identify a handful of vulnerable records in the training dataset. Yeom et al. [40] formalized membership inference and theoretically showed that ovefitting is sufficient but not necessary. \n\nWe believe that our auditing method is the first positive application of membership inference. Furthermore, it's the first example of user-level membership inference against ML models and the first that targets text-generation models, which are different and more complex than the models previously considered in the membership inference literature. Other innovations vs. prior methods were summarized in Sections I and II-C, such as the fact that our methods work with wellgeneralized methods that do not output numeric probabilities. \n\nMemorization in ML models. Zhang et al. [41] showed that deep learning models can achieve perfect accuracy even on randomly labeled training data. Song et al. [32] presented malicious training algorithms that intentionally encode the training data in the parameters of the model without affecting its accuracy on the main task. By contrast, we demonstrate that popular text-generation models unintentionally memorize training data in their hidden activation units and outputs.",
            "score": 0.7461535961226152,
            "section_title": "VII. RELATED WORK",
            "char_start_offset": 46177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "2904063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "270560767",
            "title": "Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives",
            "text": "For example, Hitaj et al. [60] outline a novel class of active attacks that involve launching a model inversion attack on some GANs to reconstruct facial images of the users through white-box mechanisms.Here, the adversary uses deception to extract accurate yet sensitive information about the victim.\n\nModel extract attacks.In a model extraction attacks, the adversary has no prior knowledge of the target model's parameters or training data and so attempts to obtain the model's parameters by extracting the target model.As a result, the adversary actually increases the success rate of subsequent attacks on the model's training data.Ateniese and colleagues [6] were the first to address the notion of extracting unexpected but useful information from a trained model.They designed a meta-classifier and trained it to hack other classifiers so as to infer sensitive information about the training set.In [121] Membership Inference Attacks.In a membership inference attack, the goal is to determine whether a sample record has been used to train a model, i.e., to infer whether that record is a member of the training set.The consequences of such attacks can be very serious for the individuals involved.For instance, if a data record is known to have been used in a model trained to classify types of cancer, a membership inference attack could potentially leak information about the health of that individual [112].Several recent studies have demonstrated that machine learning models, no matter the learning schema, are vulnerable to membership inference attacks [61,93,112,125], even in black-box settings [125].For example, an adversary might discern whether a data record forms part of the model's training set through an API.In this category of attack, Nasr [93] devised a white-box membership inference strike that can retrieve private data from the model but fails to obtain data from a model but cannot obtain data from other models with the same distribution.\n\nProperty Inference Attacks.Property inference involves identifying properties that hold for specific subsets of the training data but are not universally applicable to all class members.In these attacks, adversaries access trained models to extract global statistics about the training data [22].",
            "score": 0.7432702491553016,
            "section_title": "Privacy attacks",
            "char_start_offset": 60905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 301
                },
                {
                    "start": 303,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1618
                },
                {
                    "start": 1618,
                    "end": 1734
                },
                {
                    "start": 1734,
                    "end": 1972
                },
                {
                    "start": 1974,
                    "end": 2001
                },
                {
                    "start": 2001,
                    "end": 2160
                },
                {
                    "start": 2160,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 907,
                    "end": 912,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1413,
                    "end": 1418,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1572,
                    "end": 1575,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1575,
                    "end": 1579,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1612,
                    "end": 1617,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1767,
                    "end": 1771,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
            "score": 0.7418410372574205,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "216553034",
            "title": "Privacy in Deep Learning: A Survey",
            "text": "Given a data instance and (black-box or white-box) access to a pre-trained target model, a membership inference attack speculates whether or not the given data instance has contributed to the training step of the target model. Shokri et al. [13] propose the first membership inference attack on machine learning where they consider an attacker who has black-box query access to the target model and can obtain confidence scores (probability vector) for the queried input. The attacker uses this confidence score to deduce the participation of given data in training. They first train shadow models on a labeled dataset that can be generated using three methods: model inversion attack (we will see next), statistics-based synthesis (through assumptions about the underlying distribution of training set), or noisy real data. Using these shadow models, the attacker trains an \"attack model\" that distinguishes the participation of a data instance in the training set of the shadow models. Lastly, for the main inference attack, the attacker makes queries to the target deployed model to receive confidence scores for each given input data instance and infers whether or not the input was part of the target training data. This attack is built on the assumption that if a record was used in the training of a model, it would yield a higher confidence score, than a record which was not seen before by the model. Some studies [46,47,15] attribute membership inference attacks to the generalization gap, the over-fitting of the model, and data memorization capabilities of neural networks. Deep neural networks have been shown to memorize the training data [48,49,50], rather than learning the latent properties of the data, which means they often tend to over-fit to the training data. Long et al. [36] propose an approach which more accurately tests the membership of a given instance. They train the shadow models with and without this given instance, and then at inference time the attacker tests to see if the instance was used for training the target model, similar to Shokri et al.'s approach.",
            "score": 0.740789492694082,
            "section_title": "MEMBERSHIP INFERENCE",
            "char_start_offset": 6262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 245,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1427,
                    "end": 1430,
                    "matchedPaperCorpusId": "50778569"
                },
                {
                    "start": 1430,
                    "end": 1433,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "11455421"
                },
                {
                    "start": 1657,
                    "end": 1660,
                    "matchedPaperCorpusId": "215745042"
                },
                {
                    "start": 1660,
                    "end": 1663,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 1795,
                    "end": 1799,
                    "matchedPaperCorpusId": "9517641"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "In this section, we first briefly introduce machine learning basics and notations. Next, we present existing membership inference attacks, including black-box attacks and white-box attacks. Finally, we discuss two state-of-the-art defense methods: adversarial regularization [29] and MemGuard [19].",
            "score": 0.7391408608707811,
            "section_title": "Background and Related Work",
            "char_start_offset": 7825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 298
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85888671875
        },
        {
            "corpus_id": "210942951",
            "title": "Modelling and Quantifying Membership Information Leakage in Machine Learning",
            "text": "Membership inference attacks, a class of adversarial inference algorithms designed to distinguish data used for training a machine learning model, have recently gained much attention [1,5,6,7]. These attacks have been deployed on various machine learning models; see, e.g., [1,8,9,10,11,12]. The success of the attacks is often attributed to that a machine learning model behaves differently on the training dataset and the test dataset, e.g., it shows higher confidence on the training dataset due to an array of reasons, such as over-fitting. \n\nMany defence mechanisms have been proposed against membership inference attacks. A game-theoretic approach is proposed in [13], where a regularization term using the accuracy of membership inference attacks is incorporated when training machine learning models. Others have introduced indistinguishability for membership inference attacks as an estimate of the discrimination of the model on training and test datasets [14]. Alternatively, it has been suggested that we can counter membership inference attacks by reducing over-fitting [15]. Membership inference attacks are shown to work better on certain subgroups of the population, e.g., underrepresented minorities, resulting in disparate vulnerability [14]. Furthermore, success of membership inference attack may not predict success of attribute inference attacks with only access to partial view of data records [16]. Another approach is to use differentially-private machine learning at the cost of significantly reducing the utility [17,18]. However, none of these capture the possibly many factors contributing to the success of membership inference attacks. \n\nThis motivates taking a deeper look at the factors behind the success of membership inference attacks using information-theoretic membership information leakage metrics. This is the topic of this paper. \n\nFinally, we would like to point out recent results exploring differential privacy and mutual information, e.g., see [19,20]. Although these results provide important insights into information-theoretic guarantees of differential privacy, they are far from the context of this paper and do not consider membership inference attacks.",
            "score": 0.7377303963615625,
            "section_title": "Related Work",
            "char_start_offset": 3591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 186,
                    "end": 188,
                    "matchedPaperCorpusId": "50778569"
                },
                {
                    "start": 188,
                    "end": 190,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 190,
                    "end": 192,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 277,
                    "end": 279,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 279,
                    "end": 281,
                    "matchedPaperCorpusId": "202539472"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 284,
                    "end": 287,
                    "matchedPaperCorpusId": "195477472"
                },
                {
                    "start": 287,
                    "end": 290,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "201660180"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1990,
                    "end": 1994,
                    "matchedPaperCorpusId": "9204999"
                },
                {
                    "start": 1994,
                    "end": 1997,
                    "matchedPaperCorpusId": "1484107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "The first paper about the membership inference attack was published by Shokri et al. [31] under the assumption of the black-box model. The shadow model was used to train a machine learning model similar to the target model, and the shadow model was used to learn differences of confidence vector about members and non-members. Later, Nasr et al. [27] first proposed the membership inference attack under the white-box model and federated learning model. They classified between members and non-members by distinguishing model data such as the gradient changes of each layer or the output of each layer. Song et al. [33] mainly discussed the practical threat of machine learning in the article. The defensive measures against adversarial samples were to generate a robust machine learning model. But, this robust machine learning model was more sensitive to membership inference attacks. The article [32] set different thresholds, which was used to determine whether the prediction confidence belongs to the member, for a different class label to improve the accuracy of the attack. At the same time, the redefined prediction entropy was used to strengthen the membership inference attack. The paper [40] designed new membership inference algorithms against machine learning models and achieved significantly higher inference accuracy when the augmented data was also used in training but the augmented mechanism was known to the adversary. However, the malicious attacker could only utilize the original image or randomly chosen transformations which yielded a significantly lower inference success rate. There are also some membership inference attacks where the adversary's advantages are limited. Hui et al. [17] proposed an membership inference attack when the adversary couldn't collect enough sample with output probabilities and labels as either members or non-members and Choquette et al. [8] introduced label-only membership attacks with the adversary only got access to models' predicted labels. In addition, there was an new membership inference attack [42] to recommander systems where the adversary could only observe the ordered recommended items.",
            "score": 0.7369501890529271,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 39513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1897,
                    "end": 1900,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "54444175",
            "title": "Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks",
            "text": "The adversary's observations of the deep learning algorithm are what constitute the inputs for the inference attack. Black-box. In this setting, the adversary's observation is limited to the output of the model on arbitrary inputs. For any data point x, the attacker can only obtain f (x; W). The parameters of the model W and the intermediate steps of the computation are not accessible to the attacker. This is the setting of machine learning as a service platforms. Membership inference attacks against black-box models are already designed, which exploit the statistical differences between a model's predictions on its training set versus unseen data [6]. White-box. In this setting, the attacker obtains the model f (x; W) including its parameters which are needed for prediction. Thus, for any input x, in addition to its output, the attacker can compute all the intermediate computations of the model. That is, the adversary can compute any function over W and x given the model. The most straightforward functions are the outputs of the hidden layers, h i (x) on the input x. As a simple extension, the attacker can extend blackbox membership inference attacks (which are limited to the model's output) to the outputs of all activation functions of the model. However, this does not necessarily contain all the useful information for membership inference. Notably, the model output and activation functions could generalize if the model is well regularized. Thus, there might not be much",
            "score": 0.7351100477477823,
            "section_title": "A. Attack Observations: Black-box vs. White-box Inference",
            "char_start_offset": 9002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1496
                }
            ],
            "ref_mentions": [
                {
                    "start": 656,
                    "end": 659,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.876953125
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "In this section, we first define membership inference attack in the machine learning setting. Then, we introduce the datasets used for our evaluation.",
            "score": 0.732502562794159,
            "section_title": "II. PRELIMINARIES",
            "char_start_offset": 10548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "218571404",
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "text": "Membership inference and model inversion attacks are two types of data inference attacks that threaten the security and privacy of machine learning. They differ in their inference goals Membership Inference Attack. In the membership inference attack, the attacker is asked to determine whether a given data record is part of the training data of the target model [24,30,31,36,47,49]. \n\nConfidence-based Attack [47,49]. Shokri et al. [49] introduced membership inference against black-box models, where the attacker has access only to the prediction scores of the target model. To infer the membership, the attacker trains a binary classifier (also referred to as attack model) which takes as input the confidence scores of the target model on a given data sample and predicts the data sample to be a member or non-member of the training dataset of the target model. Prior to training the attack model, the attacker trains a set of shadow models on an auxiliary dataset drawn from the same data distribution as the target model's training data to replicate the target model. The attack model is then trained on the confidence scores predicted by the shadow models instead of the target model on the members and non-members of the shadow models' training data. Salem et al. [47] further showed that it is sufficient to train only one shadow model to replicate the target model for membership inference attack. Besides, they also showed that ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy. For example, their experimental results show that only the top one/three highest values in the confidence vector are sufficient to result in effective membership inference. These results indicate that the assumptions of membership inference attack could be largely relaxed which might lead to more practical threats. \n\nConfidence & Label-based Attack [35]. Nasr et al. [35] extended the attack model by also taking the label information as input. Their attack model is composed of three neural networks. The first two networks operate on the confidence score vector and the onehot encoded label respectively. They have the same size of input dimension, i.e., the number of classes of the target model. The third network operates on the concatenation of the output of the first two networks and predicts the membership.",
            "score": 0.7317824717795074,
            "section_title": "Data Inference Attacks",
            "char_start_offset": 11289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2390
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 376,
                    "end": 379,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 379,
                    "end": 382,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1923,
                    "end": 1927,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1941,
                    "end": 1945,
                    "matchedPaperCorpusId": "49863840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9755859375
        },
        {
            "corpus_id": "256868849",
            "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
            "text": "Membership Inference Attack (MIA) was first introduced by Shokri et al. [35]. It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box [25], [28] as well as black-box [35], [31], [42], [33], [37] classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members [42]. Sablayrolles et al. [31] further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution. Our approach is built on top of these findings and is specifically tailored for diffusion models by carefully examining their training objectives and potential attack surface during development and deployment. \n\nRecent works have explore such attacks for popular generative models such as GANs [11], [4] and VAEs [13]. Specifically, Hayes et al. [11] observe that disclosing the discriminator in a GAN can result in leaked membership information in a whitebox setting and suggested using a shadow model for black-box attacks. Hilprecht et al. [13] proposed using the reconstruction error as a membership score for attacking white-box VAEs and counting generated samples within an \u03f5-ball of the query for a black-box membership score. Chen et al. [4] presented a taxonomy of MIAs against GANs and proposed an optimizationbased approach for attacks with only generator access and a distance-based approach for the black-box setting with only synthetic samples available. \n\nOur work presents the first systematic analysis of MIAs on diffusion models.",
            "score": 0.7314897864135534,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8398,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 923,
                    "end": 927,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1404,
                    "end": 1407,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1647,
                    "end": 1651,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1850,
                    "end": 1853,
                    "matchedPaperCorpusId": "221203089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "We formulate the membership inference attack as a binary classification task where the attacker aims to classify whether a sample has been used to train the victim machine learning model. Formally, we define \n\nwhere the attack model A output 1 if the attacker infers that the sample x is included in the training set, and 0 otherwise. \u03b8 denotes the target model parameters while M represents the general model publishing mechanism, i.e., type of access available to the attacker. For example, M is an identity function for the white-box access case and can be the inference function for the black-box case.",
            "score": 0.7292039312706675,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 12317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 606
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "252118524",
            "title": "Black-Box Audits for Group Distribution Shifts",
            "text": "In the literature on ML security, we find several attacks that allow to recover information about a model's training set. Ateniese et al. first proposed property inference attacks against (Ateniese et al. 2015) and, follow-up work, has adapted them to neural networks to infer the demographic attribute of members of the training set (Ganju et al. 2018). \n\nHowever, these works assume knowledge of the weights of the neural networks they target. (Ganju et al. 2018). As we argue in Section 2, it is unrealistic to assume white-box access in an audit without the model holder's collaboration. Therefore, these techniques are not directly applicable to auditing. \n\nAnother attack that allows to learn information about the training set is membership inference. Membership inference allows an adversary with only query access to a model to learn whether a given data point was in the model's training set. Yeom et al. (Yeom et al. 2018, 2020) formally define membership inference and study the role that overfitting of the target model plays on the attack's success. Our formal definition of the GDS audit is based on their definition; we modify it to convey our goal of learning information about the entire training distribution rather than a specific point. \n\nA recent study by Maini et al. (Maini, Yaghini, and Papernot 2020) presents black-box methods to verify dataset ownership. Dataset inference is related to membership and attribute inference in that it checks the model for knowledge about specific data points. By contrast, our audit checks for differences between training data distributions. \n\nMore recently, Suri and Evans proposed a formalization of DS inference that is similar to Definition 2.1 (Suri and Evans 2021). Our definition differs from theirs in that we let the challenger choose between only two distributions D and D while they consider a set of distribution transformations on D. Because we restrict the space of distributions, the advantage of the adversary we consider is an upper bound of the advantage of the adversary in their definition. \n\nSuri and Evans also propose black-box distribution inference attacks and compare them to Ganju et al.'s white-box property inference attacks.",
            "score": 0.727454327506404,
            "section_title": "Related Work",
            "char_start_offset": 27283,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 2071
                },
                {
                    "start": 2074,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 210,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 334,
                    "end": 353,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 446,
                    "end": 464,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 903,
                    "end": 932,
                    "matchedPaperCorpusId": "208127526"
                },
                {
                    "start": 1278,
                    "end": 1326,
                    "matchedPaperCorpusId": "231609191"
                },
                {
                    "start": 1710,
                    "end": 1731,
                    "matchedPaperCorpusId": "237491948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "251066729",
            "title": "Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning",
            "text": "Membership inference attack [28,27,22,25,5,11,26,12,20,10,31,9,19,34,4] aims to determine whether a given data sample is used to train a target model. Multiple works studied the membership inference attacks against the supervised learning [28,27,23,19,6,9]. Shokri et al. [28] proposed the first black-box membership inference attack against machine learning models by leveraging multiple shadow models and attack models. The attack model takes a sample's posteriors generated from the target model as the input and predicts whether it is a member or not. Salem et al. [27] relaxed the assumption from Shokri et al. [28] and proposed novel model-independent and dataset-independent membership inference attacks. Nasr et al. [23] studied the white-box membership inference attacks in both centralized and federated learning settings. Li and Zhang [19] and Choo et al. [6] concentrated on a more restricted attack scenario (called label-only attack) where the target model only returns the predicted labels instead of posteriors when the adversary queries the target model with given samples. Roughly speaking, their proposed label-only attacks aim to infer a given sample's membership status via comparing a pre-defined threshold with the scale of adversarial perturbation that needs to be added to the given sample to change the target model's predicted label. However, these membership inference attacks are tailored to supervised learning and we show that semi-supervised learning is more vulnerable to our proposed data augmentation-based membership inference attack compared with existing membership inference attacks.",
            "score": 0.7270841987883888,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 8022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 38,
                    "end": 41,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 41,
                    "end": 43,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 43,
                    "end": 46,
                    "matchedPaperCorpusId": "209439842"
                },
                {
                    "start": 46,
                    "end": 49,
                    "matchedPaperCorpusId": "211201635"
                },
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "237290083"
                },
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "231846491"
                },
                {
                    "start": 58,
                    "end": 61,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 243,
                    "end": 246,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 252,
                    "end": 254,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 846,
                    "end": 850,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "257038160",
            "title": "Digital Privacy Under Attack: Challenges and Enablers",
            "text": "On the whole, the model extraction attack may violate the training-set privacy, destroy model monetization and promote model evasion. At the same time, when the model is successfully extracted, it can be further exploited together with whitebox inversion attacks [29] or membership inference attacks [122], which poses a greater privacy threat. 5.2. Membership Inference Attack. When the machine learning models are associated with sensitive domains, such as financial services [61] or medical research [4], not only the models themselves, but the membership information in the training sets may motivate the privacy risks of individuals. So another branch of research related to membership inference attacks focuses on the field of machine learning, unlike what discussed in Section 4.3, aiming at determining whether a targeted individual-related record is part of a specific sensitive training dataset. \n\nAs we know, in the process of constructing machine learning models, numerous sensitive data, including individuals' transactions and preferences, medical health records, locations or face images, are used as training data. There is a certain possibility that black-box models or white-box models may unintentionally disclose secrets of private training data by either the specific predictive behavior or the details of their structures and parameters. In the background of machine learning, the black-box setting is referred to as the condition where the adversary can only obtain the model's output results under given inputs, while the white-box setting represents the condition where the adversary can obtain almost all the secret parameters and internal structure of the model, and both are common in practical application scenarios. Figure 9 shows the visual difference between above two settings. \n\nMany researchers in the privacy field believe that the privacy-preserving machine learning models have the risks of disclosing the training set membership in both settings. Shokri et al. [122] firstly verified the possibility of successfully implementing member inference attacks on the ML models under ML-as-a-service (MLaaS), known as a popular black-box API. They innovatively proposed a novel shadow training technique, which can help train a special attacking model so as to conduct membership inference according to the targeted model's output without any auxiliary information, namely, quantifying the membership information disclosure \n\nCommitted by: (None) through the model prediction outputs.",
            "score": 0.7248478072292163,
            "section_title": "Extraction Attack.",
            "char_start_offset": 61702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2455
                },
                {
                    "start": 2458,
                    "end": 2516
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 300,
                    "end": 305,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 478,
                    "end": 482,
                    "matchedPaperCorpusId": "3142952"
                },
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 2000,
                    "end": 2005,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "There is extensive prior literature on privacy attacks on statistical summaries. Homer et al. [17] proposed what is considered the first membership inference attack on genomic data in 2008. Following the work by Homer et al., a number of studies [9,14,35,38,44] have looked into membership attacks on statistics commonly published in genome-wide association studies. In a similar vein, Komarova et al. [21] looked into partial disclosure scenarios, where an adversary is given fixed statistical estimates from combined public and private sources and attempts to infer the sensitive feature of an individual referenced in those sources. \n\nMore recently, membership inference attacks have been applied to machine learning models. Ateniese et al. [2] demonstrated that given access to the parameters of support vector machines (SVMs) or Hidden Markov Models (HMMs), an adversary can extract information about the training data. \n\nAs deep learning has become more ubiquitous, membership inference attacks have been particularly directed at deep neural networks. A number of different recent works [26,27,32,34,37,47] have taken different approaches to membership inference against deep networks in a standard supervised learning setting. Additionally, Hayes et al. [15] have studied membership inference against generative adversarial networks (GANs); and others [16,29,32] have studied membership inference in the context of collaborative, or federated, learning. \n\nBlack-box attacks. We study membership inference as it applies to deep networks in classic supervised learning problems. Most of the prior work in this area [26,27,34,37,47] has used the black-box threat model. Yeom et al. [47] showed that generalization error necessarily leads to membership vulnerability; a natural consequence of this is that a simple \"naive\" attack, which predicts a point is a member if and only if it was classified correctly, can be found to be quite effective on models that overfit to a large degree. Other approaches have leveraged not only the predictions of the model, but the confidence outputs. A particularly canonical approach, along these lines, is the attack introduced by Shokri et al. [37].",
            "score": 0.7247912964747611,
            "section_title": "Related Work",
            "char_start_offset": 57321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "28024953"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "3473659"
                },
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "26111466"
                },
                {
                    "start": 258,
                    "end": 261,
                    "matchedPaperCorpusId": "8091014"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "35556485"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1103,
                    "end": 1106,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1627,
                    "end": 1630,
                    "matchedPaperCorpusId": "46933970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "273507251",
            "title": "Publishing neural networks in drug discovery might compromise training data privacy",
            "text": "There are various levels of information the adversary might have access to regarding the model [12]. In our study, we focus on the so-called black-box scenario, where the adversary is provided with the output logits of the trained model, rather than the model's weights, which would correspond to a white-box scenario. This black-box scenario is similar to making machine learning models available as web services. \n\nAlgorithm 1 Membership Inference Attack. This algorithm formalizes the membership inference attack game we use to evaluate the privacy of our neural networks. The attack assumes knowledge about the underlying data distribution (chemical space) \u03a0 from which the training dataset is sampled. Given an adversary A, a training algorithm T , and the data distribution \u03a0, the process involves sampling points from the data distribution, training a model on these samples, and then using the adversary to infer whether a specific data point (chemical structure) was part of the training set or not. The algorithm tests the adversary's ability to distinguish between data points sampled from the training set and those not included, thereby evaluating potential information leakage from the model. Building on the growing body of research on membership inference attacks, Hu et al. conducted an extensive survey, highlighting that they have been studied in the domains of image data, text data, tabular data, as well as node classification in graph data [13]. Among the different implementations of attacks, likelihood ratio attacks (LiRA) and robust membership inference attacks (RMIA) have been shown to be the most effective in identifying training data samples, setting state-of-the-art performance benchmarks for the most commonly used benchmark datasets [11,14]. Despite the growing interest in membership inference attacks, their application to molecular property prediction in drug discovery remains largely unexplored. To the best of our knowledge, Pejo et al. conducted the only study about membership inference attacks in the context of molecular property prediction, but they focused on federated learning scenarios using attacks tailored to this approach [15]. The broader implications and potential risks of membership inference attacks in molecular property prediction, particularly in traditional centralised machine learning models, still require investigation. \n\nIn this study, we provide the first comprehensive analysis of membership inference attacks against neural networks trained to predict molecular properties.",
            "score": 0.7241799531069425,
            "section_title": "Main",
            "char_start_offset": 2327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2387
                },
                {
                    "start": 2390,
                    "end": 2545
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 99,
                    "matchedPaperCorpusId": "254926499"
                },
                {
                    "start": 1463,
                    "end": 1467,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1773,
                    "end": 1776,
                    "matchedPaperCorpusId": "268032391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "278501612",
            "title": "Securing Genomic Data Against Inference Attacks in Federated Learning Environments",
            "text": "Membership inference attacks were initially introduced by Shokri, demonstrating that adversaries can exploit overfitted models to infer whether specific data samples were part of the training set [17]. Since then, several variants have emerged, including black-box and white-box attacks, each with varying degrees of attacker access. Salem proposed shadow models to simulate the target model's behavior under different conditions, increasing the attack's generalizability [24]. More recently, Nasr explored gradient-based MIAs in white-box settings, showing how model updates can be reverse-engineered to reveal sensitive sample membership [16]. \n\nLabel inference attacks, though less explored, have been discussed in the context of collaborative learning. These attacks leverage model outputs or internal states to infer sensitive labels, especially in cases where class distributions are skewed or correlated with demographic information [18] [26].",
            "score": 0.7229519833837371,
            "section_title": "A. Inference Attacks in Machine Learning",
            "char_start_offset": 7701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 950
                }
            ],
            "ref_mentions": [
                {
                    "start": 640,
                    "end": 644,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "208247985",
            "title": "Effects of Differential Privacy and Data Skewness on Membership Inference Vulnerability",
            "text": "The most limited access environment in which an attacker may conduct the membership inference attack is the black-box access environment wherein the attacker may only query the target model F t through some MLaaS API and receive only the corresponding prediction vectors.\n\nConsider an attacker with such black-box access to F t .  Many different datasets and model types have demonstrated vulnerability to membership inference attacks in black-box settings. Table I reports 5 accuracy results for black-box attackers targeting decision tree models for problems ranging from binary classification (Adult) to 100-class classification (Purchases-100). We note that all experiments evaluated the attack model against an equal number of instances in the target training dataset D as those not in D. The baseline membership inference accuracy is therefore 50%. We refer readers to [8] for more details on these datasets and experimental set up.\n\nThese results demonstrate both the viability of membership inference attacks as well as the variation in vulnerability between datasets. This accentuates the need for practitioners to evaluate their system's specific vulnerability.\n\nRecently, researchers showed similar membership inference vulnerability in settings where attackers have white-box access to the target model, including the output from the intermediate layers of a pre-trained neural network model or the gradients for the target instance [9]. Interestingly, this study showed that the intermediate layer outputs, in most cases, do not lead to significant improvements in attack accuracy. This result further supports the understanding that the attackers can gain sufficient knowledge from only the black-box access to the pre-trained models which is common in MLaaS platforms.",
            "score": 0.7225559177481394,
            "section_title": "2) Membership Inference Definition:",
            "char_start_offset": 7682,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 878,
                    "matchedPaperCorpusId": "86836429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "270870233",
            "title": "Unveiling the Unseen: Exploring Whitebox Membership Inference through the Lens of Explainability",
            "text": "The increased utilization of sensitive data in the training of machine learning models has led researchers to extensively explore inference attacks targeting the sensitive training samples [1,7,8,26].One of the most prominent attacks is the Membership Inference Attack (MIA) where the adversary, during the inference phase, aims to infer whether a given sample was utilized in training [24].To conduct these attacks the adversary often relies on an auxiliary dataset, also referred to as a shadow distribution, which is similar to the training distribution and can potentially share some samples with the training distribution [16].Based on the threat model and attack features, the attack methodologies can broadly be classified into black-box and white-box attacks.In a black-box setting the adversary only has an API-level access to the ML model and relies on the prediction vectors to conduct the attack [10,17,24,30].White-box settings assume a stronger adversary who also has access to the model parameters, activations, and gradients [14,20,22].Even though in the white-box setting the adversary has complete access to the target model, researchers have only seen a marginal improvement over black-box attacks [16,20].\n\nWhile the prior work has put forward several hypotheses for the success of MIA, it fails at explaining the behaviors we notice in the white-box setting.This makes explainable ML solutions an ideal candidate as they have also been previously used to assess the security and privacy of ML models [11,14,29].However, the explainability of white-box attacks is still underexplored.\n\nIn this work, we explore the limitations of existing state-of-the-art white-box attacks and develop a neuron selection framework, leading to a stronger whitebox attack.We also utilize explainable ML techniques to develop a framework to identify and evaluate the most important attack features.",
            "score": 0.7207473424968908,
            "section_title": "Related Work",
            "char_start_offset": 5589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1225
                },
                {
                    "start": 1227,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1532
                },
                {
                    "start": 1532,
                    "end": 1604
                },
                {
                    "start": 1606,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 1899
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 194,
                    "end": 196,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 915,
                    "end": 918,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1045,
                    "end": 1048,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 1221,
                    "end": 1224,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 1521,
                    "end": 1525,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1528,
                    "end": 1531,
                    "matchedPaperCorpusId": "244345608"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "270878039",
            "title": "A Method to Facilitate Membership Inference Attacks in Deep Learning Models",
            "text": "Membership inference attacks.Shokri et al. [72] demonstrated the first MIAs against ML models.Existing attacks can be categorized as black-box [72], [94], [42], [21], [76], [27], [93] and white-box attacks [50], [45], [60].Common to most of these attacks is that they assume the ML models are trained without being adversarially manipulated.",
            "score": 0.7197496064458024,
            "section_title": "B. Related Work",
            "char_start_offset": 8337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 341
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 161,
                    "end": 165,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "216553034",
            "title": "Privacy in Deep Learning: A Survey",
            "text": "As shown in figure 1, we categorize indirect attacks into 5 main groups of membership inference, model inversion, hyperparameter inference, parameter inference, and property inference attacks. Table 1 shows a summary of different attacks and their properties. The \"Access to Model\" column determines whether the attack needs white-box or black-box access to model to successfully mount. White-box access assumes access to the full target model, whereas black-box assumes only query access to the model, without knowledge on the architecture or parameters of the target model. The last column shows whether the attacker needs access to the output confidence values of the model (the probabilities, logits), or whether only the predicted labels suffice.",
            "score": 0.719439838535826,
            "section_title": "Indirect (Inferred) Information Exposure",
            "char_start_offset": 5486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 751
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "272910597",
            "title": "Investigating Privacy Attacks in the Gray-Box Setting to Enhance Collaborative Learning Schemes",
            "text": "Membership inference attacks aim to determine whether or not a given data point was part of the training set. These attacks exploit the intrinsic difference in the model's behavior when performing prediction over known training data versus unseen data. Membership inference attacks reveal how much a model retains from its training data, helping to gauge the potential effectiveness of other privacy attacks such as data reconstruction but can also pose significant privacy risks on their own. Since the introduction of the first membership inference attack by Shokri et al. [66] in 2017, numerous studies have investigated the underlying causes of membership leakage in ML models. The primary contributing factor to membership leakage appears to be model overfitting or poor generalization [66], [74]. Several factors can exacerbate this issue, including a limited number of training samples [66], [31], high model complexity leading to overparametrization [55], and high feature dimensionality [66]. \n\nGray-Box Setting: In the gray-box setting, the efficacy of membership inference attacks heavily depends on access to the last layers of the model. While the initial layers of a neural network tend to extract simple features from the input, enabling them to generalize well, the later layers specialize in detecting higher-level abstract features in the input, making them prone to overfitting and memorizing the specific training examples. For instance, in a CNN model trained for image classification, you can expect the first layers to learn more about edges and abstract shapes of the input image, while  the last layers more about intricate texture and artifacts within those shapes [75]. Moreover, as the neural network progresses to the later layers, the parameter capacity increases, causing the target model to store information about the exact training samples [55]. Therefore, if the last layers of the model are accessible, membership inference attacks tend to be stronger due to the higher degree of membership information leakage. \n\nExperimental Assessment: To experimentally evaluate membership inference in the gray-box setting, we chose the white-box attack by Nasr et al. [55] due to its componentwise approach. Like other lines of work, their attack treats membership inference as a binary classification task, and trains a machine learning model to accomplish this task.",
            "score": 0.7184693778917841,
            "section_title": "A. Membership Inference",
            "char_start_offset": 20242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2232
                },
                {
                    "start": 2233,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 996,
                    "end": 1000,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1691,
                    "end": 1695,
                    "matchedPaperCorpusId": "3960646"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 2193,
                    "end": 2197,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "221725546",
            "title": "Privacy-Preserving Deep Learning on Machine Learning as a Service\u2014a Comprehensive Survey",
            "text": "Generally, membership inference means deciding whether given data were used for generating some aggregation of the data (or not). In the context of deep learning, a model itself (including the model parameters) can be regarded as the 'aggregation' of the training data. Therefore, membership inference attacks on DL models indicate attacks to decide whether given data belong to the training dataset (or not). Shokri et al. [96] provided one of the first suggestions of membership inference attacks. \n\nMembership inference attacks are the attacks for the models violating the first security goal of PPDL. Stronger versions of membership inference attacks include extraction of some properties of sensitive training data or even recovery of the training data, which can be reduced to normal membership inference attacks. Usually, membership inference attacks harness overfitting during training, producing a difference in accuracy between the training data and the other data. Some defensive mechanisms dedicated to membership inference have been proposed including dropout [114] and adversarial regularization [115]. \n\nIn cryptography-based PPDL models, the security against the membership inference attack can be reduced to the security of the underlying cryptosystems. In such models, the adversarial server cannot obtain model parameters in plaintext. Only if the model is public can the adversary have black-box access of the model, just like any outsider attacker. For HW-based models, the adversarial server owns white-box models, allowing the use of white-box membership inference attacks. \n\nFor DP-based models, the trade-off between the model accuracy and the performance of membership inference attacks according to the selection of the privacy parameter has been studied [116]. Appropriate choices of the privacy parameter result in moderate utility with low membership inference accuracy. However, further experiments are required for the extensibility of their analysis toward other types of tasks outside image classification.",
            "score": 0.7150735435365982,
            "section_title": "B. MEMBERSHIP INFERENCE ATTACK",
            "char_start_offset": 65858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1116
                },
                {
                    "start": 1119,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1110,
                    "end": 1115,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1782,
                    "end": 1787,
                    "matchedPaperCorpusId": "13699042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "266362863",
            "title": "Traces of Memorisation in Large Language Models for Code",
            "text": "Membership inference attacks are a type of attack that aims to determine whether a specific data point was included in the training data of a machine learning model. The goal of these attacks is to infer whether a given data point was used to train the model or not, without having access to the training data itself. \n\nThe first membership inference attack against machine learning models was proposed by Shokri et al. to target classification models deployed by Machine Learning as a Service (MLaaS) providers [45]. Since then the field has expanded and attacks have been proposed that target generative models [24] and LLMs [25]. Recently, membership inference attacks have been proposed against transformerbased image diffusion models such as Stable Diffusion [18]. \n\nWe refer to the security game defined by Carlini et al. [9] to define a membership inference attack in Definition 1. In this game, the adversary wins if they have a non-negligible advantage > 1 2 + . In simpler terms, the adversary needs to be able to distinguish between data that was included and which was not included in the training data for a given model, while only being allowed query access to the model and data distribution. \n\nMembership inference attacks are primitive for measuring the leakage of a machine learning model and are often a starting point for more extensive attacks [9,26,38]. While membership inference is a weaker privacy violation than memorisation, the National Institute of Standards and Technology (NIST) still considers membership inference to be a violation of the confidentiality of training data [26]. \n\nDefinition 1 (Membership inference security game [9]). The game proceeds between a challenger C, an adversary A, a data distribution D and a model  : \n\n(1) The challenger samples a training dataset  \u2190 D and trains a model   \u2190 T () on the dataset . \n\n(2) The challenger flips a bit , and if  = 0, samples a fresh challenge point from the distribution (, ) \u2190 D (such that (, ) / \u2208 ). Otherwise, the challenger selects a point from the training set (, ) \u2190 . \n\n(3) The challenger sends (, ) to the adversary. (4) The adversary gets query access to the distribution D, and to the model   , and outputs a bit b (5) Output 1 if b = , and 0 otherwise.",
            "score": 0.7144414019366159,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 6911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2067
                },
                {
                    "start": 2070,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "119302127"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1365,
                    "end": 1368,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1368,
                    "end": 1371,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1371,
                    "end": 1374,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 1605,
                    "end": 1609,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Various research has shown that machine learning models are vulnerable to security and privacy attacks. In this section, we mainly survey the domains that are most relevant to us. \n\nMembership Inference. Membership inference attack has been successfully performed in various data domains, ranging form biomedical data [6,22,24] to mobility traces [43]. Shokri et al. [48] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. [46] later present another attack by gradually relaxing the assumptions made by Shokri et al. [48] achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks [25,31,35,49,57]. In the area of decision-based attacks, Yeom et al. [57] quantitatively analyzed the relationship between attack performance and loss for training and testing sets, and proposed the first decision-based attack, i.e., baseline attack aforementioned. We also acknowledge that a concurrent work [13] proposes an approach similar to our boundary attack. Specifically, the concurrent work assumes that an adversary has more knowledge of the target model, including training knowledge (model architecture, training algorithm, and training dataset size), and a shadow dataset from the same distribution as the target dataset to estimate the threshold. In our work, we relax all assumptions and propose a general threshold-choosing method. We further present a new perspective on the reasons for the success of membership inference. In addition, we introduce a novel transfer-attack. \n\nDefenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different types of generalization enhancement. For example, Shokri et al. [48] adopted L2 regularization with a polynomial in the model's loss function to penalize large parameters. Salem et al. [46] demonstrated two effective method of defending MI attacks, namely dropout and model stacking.",
            "score": 0.714355853192282,
            "section_title": "Related Works",
            "char_start_offset": 38139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 324,
                    "end": 327,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 832,
                    "end": 836,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "211532510"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 2073,
                    "end": 2077,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "221507916",
            "title": "A Comprehensive Analysis of Information Leakage in Deep Transfer Learning",
            "text": "The basic idea of the inference attack is to exploit the leakages when a model is being trained or released to reveal some unintended information from the training data. In this section, we briefly present two types of inference attacks, i.e., membership and property attacks, for machine learning models. Membership Inference Attack. Membership inference is a typical attack that aims to determine whether a sample is used as part of the training dataset. Membership inference may reveal sensitive information that leads to privacy breach. For example, if we can infer a patient's existence in the training dataset for a medical study of a certain disease, we can probably claim that this patient has such a disease. Recent works have demonstrated the membership attack attempts for machine learning models under the black-box setting [19,30,32], white-box setting [22,26] or both [9]. Shadow training is a widely adopted technique for membership inference, where multiple shadow models are trained to mimic the behavior of the target model [19,30,32]. This technique assumes the attacker to have some prior knowledge about the population from the targeted model training dataset was drawn. Recent works by [22,26] explicitly exploit the vulnerabilities in gradient updates to perform attacks with white-box access. Property Inference Attack. Another common type of attack is property inference that aims to reveal certain unintended or sensitive properties (e.g., the fraction of the data belongs to a certain minority group) of the participating training datasets that the model producer does not intend to share when the model is released. A property is usually uncorrelated or loosely correlated with the main training task. Pioneer works of [2,6,7] conducted the property attacks that characterize the entire training dataset. While, [22] aimed to infer properties for a subset of the training inputs, i.e., in terms of single batches which they termed as single-batch properties. In this regard, membership attack can be viewed as a special case of property attack when scope for property attack is on a sample basis. \n\nThe above-mentioned two types of attacks are closely related. Most of existing works perform those attacks against general machine learning models, while a few focus on the federated learning and collaborative learning scenarios [22,26].",
            "score": 0.7135679119092422,
            "section_title": "Inference Attacks for DNN Models",
            "char_start_offset": 7033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2124
                },
                {
                    "start": 2127,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 840,
                    "end": 843,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1208,
                    "end": 1212,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1212,
                    "end": 1215,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1747,
                    "end": 1750,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1750,
                    "end": 1752,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1752,
                    "end": 1754,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "266149496",
            "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
            "text": "Membership inference (MI) is a common privacy attack that attempts to predict whether a given example was used to train a machine learning model (e.g., [12,26,20,13,15,18,3]). Our work focuses on performing MI attacks on diffusion models. \n\nProblem statement. Given a training dataset Z drawn from an underlying distribution P , a diffusion model \u03b8 is trained on Z. The goal of a membership inference attack is to infer whether a target example z * was included in the training set Z or not. \n\nAdversary's side information. Similar to almost all prior work on membership inference [3,7,1,20], we assume the adversary has access to some public data drawn from P . In the standard terminology of MI, there are two types of access to the algorithm's output. In a black-box attack, the adversary only has access to the generated synthetic data. In a white-box attack, the adversary has access to the generative model G. \n\nIn this work, we focus on white-box attacks, and specifically we only need access to the parameters of the trained diffusion model, without information regarding the training algorithm.",
            "score": 0.7122247490281323,
            "section_title": "Membership inference attacks",
            "char_start_offset": 5762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1103
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "219686822"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "231583084"
                },
                {
                    "start": 171,
                    "end": 173,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 584,
                    "end": 586,
                    "matchedPaperCorpusId": "256503774"
                },
                {
                    "start": 588,
                    "end": 591,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "259137851",
            "title": "Gaussian Membership Inference Privacy",
            "text": "There is a long line of prior work developing [5,7,15,23,29,30,31] or analyzing [35,36,37] privacy attacks on machine learning models. A common class of attacks called membership inference attacks focus on determining if a given instance is present in the training data of a particular model [5,8,9,29,30,31,32,40,41]. Most of these attacks typically exploit the differences in the distribution of model confidence on the true label (or the loss) between the instances that are in the training set and those that are not [5,30,31,40]. For example, Shokri et al. [31] proposed a loss-based membership inference attack which determines if an instance is in the training set by testing if the loss of the model for that instance is less than a specific threshold. Other membership inference attacks are also predominantly loss-based attacks where the calibration of the threshold varies from one proposed attack to the other [5,30,40]. Some Info Loss [41] CFD [29] Loss LRT [5] CFD LRT [29] Gradient LRT Query access to 2: Summarizing the assumptions underlying the different MI attacks. The recourse based attacks do not require access to the true labels nor do they need to know the correct loss functions, but they additionally require access to a recourse generating API R. To the best our knowledge, our gradient attack is the only one for which analytical results exist. \n\nworks leverage different information that goes beyond the loss functions to do membership inference attacks. For instance, Shokri et al. [32] and Pawelczyk et al. [29] leverage model explanations to orchestrate membership inference attacks. \n\nComparison to existing attacks. In Table 2, we summarize the assumptions underlying different membership inference attacks. Note that our attack does not require the training of multiple shadow models on data from the data distribution D N . Instead, we derive the distributions of the LRT test statistic under the null and alternative hypotheses in closed form (see Appendix E), which drops the requirement of training (appropriately parameterized) shadow models to approximate these two distributions.",
            "score": 0.7092148780425316,
            "section_title": "Extended comparison to privacy attacks.",
            "char_start_offset": 29444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "249712435"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 60,
                    "end": 63,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "246485733"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "258170547"
                },
                {
                    "start": 295,
                    "end": 297,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 297,
                    "end": 299,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 308,
                    "end": 311,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 314,
                    "end": 317,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 527,
                    "end": 530,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 925,
                    "end": 928,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 983,
                    "end": 987,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 1513,
                    "end": 1517,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "253446930"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "277349983",
            "title": "Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead",
            "text": "Assume that there exists a machine learning model that has been trained for some task. We consider the case of a specific data record that has plausibly been included in training the model, i.e. the record contains information that can be consumed by the model in some form. The purpose of a membership inference attack is to determine if a specific data record was used in the training data of the target model. There are two broad classes of MIA, white-box attack scenarios in which the attacker has information about the model's structure and parameters, and black-box scenarios, considerably more tricky, in which one has access only to the outputs of the model, and the ability to provide specific inputs for the inference process of a model treated only as a black-box. \n\nThe feasibility of black-box attacks was somewhat surprisingly demonstrated by Shokri et al [37] in an approach involving the training of so-called shadow models. The shadow models do not need to be trained on the precise dataset used to train the target model to be attacked, but should have access to data from the same distribution. The attacker need know only the structure of the input data records, and to have access to the prediction vector for, say, a multi-class decision problem. The attack model is simply taught to observe the output of a collection of networks, known as the shadow models, which are trained with specific data items drawn from a similar population and record structure to that of the target model. The attack model -with access to the included and omitted training data items -learns to detect the differences in the shadow models' responses, during shadow inference, to data items that might have been included or excluded from the training data for each shadow model. \n\nAs a general rule, attack models perform better when there is more information available about the nature of the training of the target model, the distribution of data used to train it, and the architecture of the target network. Several studies have subsequently extended the shadow attack methodology, improved scalability [38], or demonstrated application to other types of inference problem such as regression [39]. \n\nMembership inference attacks might be successful due to the fact that models often overfit to the training data, leading to a response that indicates higher confidence at inference time for input data that have been seen during training.",
            "score": 0.7089680045188315,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 16836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2200
                },
                {
                    "start": 2203,
                    "end": 2440
                }
            ],
            "ref_mentions": [
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2106,
                    "end": 2110,
                    "matchedPaperCorpusId": "259375769"
                },
                {
                    "start": 2195,
                    "end": 2199,
                    "matchedPaperCorpusId": "233864706"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "268819379",
            "title": "Privacy Re\u2010Identification Attacks on Tabular GANs",
            "text": "Membership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting.In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model.Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set.It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019).Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019).Federated learning can however introduce data leakages which add up to the privacy concerns.In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017;Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models.Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications.The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models.Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks.",
            "score": 0.7058799465185921,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 10411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1337
                },
                {
                    "start": 1339,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1910
                },
                {
                    "start": 1910,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 78,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 709,
                    "end": 728,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 891,
                    "end": 911,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1097,
                    "end": 1118,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1446,
                    "end": 1465,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1773,
                    "end": 1796,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1921,
                    "end": 1940,
                    "matchedPaperCorpusId": "221203089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "271328769",
            "title": "SeqMIA: Sequential-Metric Based Membership Inference Attack",
            "text": "Nowadays, there exist a wide range of other security and privacy research in the machine learning domain [8, 17, 18, 21, 29, 31, 34, 35, 38, 39, 45, 48, 49, 57-59, 63, 67].In this work, we mainly focus on membership inference attacks.Membership inference attacks have been successfully performed in various settings about the adversary's knowledge, including white-box [27,42], black-box [9,24,46,50,52,68], and label-only [10,32] settings.They have been applied in many machine learning scenarios, such as federated learning [40,42,54] and multi-exit networks [30], etc.\n\nSpecifically, Shokri et al. [50] and Salem et al. [46] proposed a shadow training technique that employs shadow models to acquire the membership signals.Moreover, Song et al. [52] and Yeom et al. [65] proposed the metric-based attack that directly compares losses or other metric values of samples with a predefined threshold.In addition, some membership signals obtained in the white-box scenario are incorporated to improve the attack performance [12,42].Besides, label-only attacks [10,32,60] solely rely on the predicted labels to acquire the membership signals.Recently, researchers [6,33,44,56,64] focused on reducing the false positives of MIAs by using each sample's hardness threshold to calibrate the loss from the target model.Further, Bertran et al. [5] proposed a new attack via quantile regression, which can obtain performance close to that of LiRA [6] with less computation.Moreover, Liu et al. [33] presented Trajec-toryMIA, which utilizes the membership signals generated during the training of the target model.",
            "score": 0.7038157242726962,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 39323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 571
                },
                {
                    "start": 573,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1030
                },
                {
                    "start": 1030,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1311
                },
                {
                    "start": 1311,
                    "end": 1463
                },
                {
                    "start": 1463,
                    "end": 1603
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 373,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 373,
                    "end": 376,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 388,
                    "end": 391,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "119302127"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "251765197"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "247595200"
                },
                {
                    "start": 1026,
                    "end": 1029,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1161,
                    "end": 1164,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1164,
                    "end": 1167,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 1167,
                    "end": 1170,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1170,
                    "end": 1173,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 1173,
                    "end": 1176,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 1335,
                    "end": 1338,
                    "matchedPaperCorpusId": "259375769"
                },
                {
                    "start": 1437,
                    "end": 1440,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "251953448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "259837436",
            "title": "SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark",
            "text": "Membership Inference Attacks. Homer et al. [59] first proposed an MI attack on biological data. Shokri et al. [8] proposed the first black-box MI attack against ML. Huge literature followed these works to different scenarios (e.g., location data [60], language models [10], sentence embeddings [61], speech recognition models [62], federated learning [7], transfer learning [63], generative models [38], white box access [12], [21], [48]). Categories of Membership Inference Attacks. There are main three categories 1) Binary classifier-based MI attacks, which utilize the output predictions of shadow models to train a binary classifier to launch the MI attacks [8], [9]. 2) Evaluation metric-based MI attacks, which used the defined evaluation metrics to distinguish members and nonmembers [9], [13], [14], [37], [39], [51]. 3) Differential Comparisons-based MI attacks (BlindMI-Diff), which mainly utilized the differences between two datasets. Defenses against MI attacks. Multiple defenses [8], [9], [11], [64], [65], [66], [67], [68] have been proposed to mitigate MI attacks.",
            "score": 0.7019243142192427,
            "section_title": "Related Work",
            "char_start_offset": 57635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1082
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "216553356"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "236880971"
                },
                {
                    "start": 351,
                    "end": 354,
                    "matchedPaperCorpusId": "232372747"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 815,
                    "end": 819,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "226266600"
                },
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1005,
                    "end": 1009,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "235349092"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "218571404",
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "text": "Machine learning has been widely adopted in a variety of applications, transforming many aspects of our daily life such as handling users' sensitive data. Machine learning itself is also been provided as a service, e.g., machine-learning-as-a-service, by many platforms. Users access these models through prediction APIs which return a prediction score vector. Such vector is a probability distribution over the possible classes and each score indicates the confidence in predicting the corresponding class. The class with the largest confidence is predicted as the label of the input data. In this paper, we are interested in data inference attacks, notably membership inference and model inversion that exploit such prediction scores to threaten the privacy and security of machine learning. \n\nA series of studies has indicated that the prediction scores of black-box machine learning models could be exploited to perform data inference attacks to get useful information about the data on which the machine learning model operates [14,20,24,35,36,47,49,60]. For examples, membership inference attack [24,35,36,47,49] and model inversion attack [14,20,60] are two of the most important and exemplary ones. In a membership inference attack, the adversary is asked to determine whether a given data sample is in the target model's training data or not according to its confidence scores predicted by the target model. Specifically, the adversary trains a binary classifier which takes the prediction scores as input and predicts whether the data sample is a member or non-member of the target model. In a model inversion attack, the adversary aims at inferring information about the data sample from the prediction scores such as the sensitive attributes [15,19,58] or the reconstruction of the sample [14,20,60]. Recently, Yang et al. [60] proposed an effective black-box model inversion attack where the attacker leverages auxiliary knowledge to construct an inversion model which can reconstruct the original input sample from the prediction scores with high accuracy. \n\nThe main reason of why such data inference attacks work is that the prediction scores contain not only confidence of classifying the query data but also unwanted redundant information, i.e., membership information and inversion information, which could be exploited to infer useful information about the query data.",
            "score": 0.6990138834975043,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2070
                },
                {
                    "start": 2073,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1040,
                    "end": 1043,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1043,
                    "end": 1046,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 1102,
                    "end": 1106,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1106,
                    "end": 1109,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1109,
                    "end": 1112,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1112,
                    "end": 1115,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1146,
                    "end": 1150,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1150,
                    "end": 1153,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1153,
                    "end": 1156,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "52916090"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 1801,
                    "end": 1805,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1805,
                    "end": 1808,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1808,
                    "end": 1811,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 1835,
                    "end": 1839,
                    "matchedPaperCorpusId": "207941008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "253265030",
            "title": "GRAIMATTER Green Paper: Recommendations for disclosure control of trained Machine Learning (ML) models from Trusted Research Environments (TREs)",
            "text": "of attacks.\n\nThe situation in which the attacker can only query the model is known as a black box attack. Sometimes an attacker might have more access and be able to examine the model itself. This is known as a white box attack. The kind of information available in white-and black-box attacks is depicted in Figure 10 above.\n\nMembership Inference Attacks (MIA) are when an adversary or attacker is trying to determine whether a set of input values (they have access to or have generated) are part of the original training dataset of the target model (referred to as target train). Our objective in assessing a model was to measure the highest potential MIA attack accuracy and, establish criteria in which models with identifiable data were safe.\n\nOur simulated attack experiments proceeded as follows. After pre-processing, the target dataset was split into 3 equal parts: train, shadow and test. The split was repeated 5 times, varying the rows included in each part. For each classifier of interest, a set of values of hyperparameters to be explored was defined. Five target models were created for each combination of classifier hyperparameters, one for each data split. All of our experiments were on health record data.\n\nSeveral attack scenarios were defined to determine the risk of personal data leak from ML models: Worst Case, Salem 1, Salem-synth and, Salem 2. The Salem 1, Salem-synth and Salem 2 scenarios are based on adversary attacks described on Salem [37]. Table 2 contains a summary of their main characteristics.",
            "score": 0.6974729586932408,
            "section_title": "Legal and ethical:",
            "char_start_offset": 156013,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "202888587",
            "title": "GAMIN: An Adversarial Approach to Black-Box Model Inversion",
            "text": "Membership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov [5]. Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov [37] in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets. In addition, Hayes, Melis, Danezis and de Cristofaro have demonstrated in the context of generative models [36] that generative adversarial networks [11] can be used to infer the presence of a particular individual in the training set. \n\nProperty inference attacks against machine learning models have been introduced by Ateniese and coauthors [34]. This type of attack involves training a metaclassifier to detect if the target model has a given property P . To conduct such an attack, the adversary trains a set of shadow models using a dataset and machine learning technique similar to that of the target model, but in addition, explicitly labeled as having the property P or not. Finally, the meta-classifier is trained to detect the presence of the property P . The authors have used this attack to learn that the training set of a speech recognition system have been produced by people speaking a particular dialect.",
            "score": 0.6971556224771474,
            "section_title": "Inference attacks against machine learning models",
            "char_start_offset": 37362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2215
                },
                {
                    "start": 2216,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1598,
                    "end": 1602,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1793,
                    "end": 1797,
                    "matchedPaperCorpusId": "14757739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "221397750",
            "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
            "text": "Machine learning models have been shown to leak information violating the privacy of their training set. We focus on membership inference attacks on machine learning models which aim to determine whether a data point was used to train the victim model. Our work consists of two sides: We introduce sampling attack, a novel membership inference technique that unlike other standard membership adversaries is able to work under severe restriction of no access to scores of the victim model. We show that a victim model that only publishes the labels is still susceptible to sampling attacks and the adversary can recover up to 100% of its performance compared to when posterior vectors are provided. The other sides of our work includes experimental results on two recent membership inference attack models and the defenses against them. For defense, we choose differential privacy in the form of gradient perturbation during the training of the victim model as well as output perturbation at prediction time. We carry out our experiments on a wide range of datasets which allows us to better analyze the interaction between adversaries, defense mechanism and datasets. We find out that our proposed fast and easy-to-implement output perturbation technique offers good privacy protection for membership inference attacks at little impact on utility.",
            "score": 0.6966566536652486,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "258236265",
            "title": "Selective and Collaborative Influence Function for Efficient Recommendation Unlearning",
            "text": "Membership inference is a well-acknowledged method used to analyze information leakage from a trained model (Yu et al., 2021). Specifically, given a trained model (target) and a data point (query), membership inference determines whether this point was in the model's training dataset. Membership inference attack against machine learning models was pioneered by Shokri et al. (2017). The main idea is regarding the membership inference problem as a binary classification task, and using machine learning classifiers to attack the target machine learning model. To improve the performance of machine learning classifiers, i.e., attacker, Shokri et al. (2017) uses shadow models, which simulate the behavior of target model, to generate sufficient training data for the attacker. The following work has investigated various settings of shadow model training and presented several defence techniques (Salem et al., 2018;Wu et al., 2020;Yu et al., 2021). However, current research has primarily concentrated on exploiting shallow classification models, leaving deep learning and regression models such as collaborative filtering largely unexplored.",
            "score": 0.6949455323911763,
            "section_title": "Membership Inference",
            "char_start_offset": 11153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1145
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 638,
                    "end": 658,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 918,
                    "end": 934,
                    "matchedPaperCorpusId": "203836240"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "244920593",
            "title": "Membership Inference Attacks From First Principles",
            "text": "Previous works [46,62] suggested that is possible to achieve better membership inference if the adversary has white-box access to the target model. In particular, previous works showed that using the norm of the model's gradient at a target point could increase the balanced accuracy of membership inference attacks. Figure 15 highlights the comparison between a white-box and a black-box adversary. The results show that using gradient norms will improve the overall AUC both for our online attack, as well as when using a global threshold as in the LOSS attack. However, at lower false-positive rates we do not observe any improvement of using gradient norms compared to just using model confidences.",
            "score": 0.6941086549939548,
            "section_title": "B. White-box Attacks",
            "char_start_offset": 61150,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 19,
                    "end": 22,
                    "matchedPaperCorpusId": "165163934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "236570958",
            "title": "Defense against Membership Inference Attack Applying Domain Adaptation with Addictive Noise",
            "text": "Shokri et al. [10] propose the first membership inference against deep learning, which explores the vulnerability of deep learning models. \n\nTo simplify the attack by decreasing the number of shadow models, as well as the datasets required to train these models, Salem et al. [11] propose a new method to initiate membership inference. Considered that the classical membership inference attack requires numerous models and datasets, despite the simplification, the new attack is still of effectiveness. Owing to the effectiveness and simplicity, this method is adopted as the membership inference attack in our experiments. \n\nYeom et al. [19] and Salem et al. [11]  , where j p is the confidence score. To infer the membership, the attack compare according to the rule as follows, where k p is the confidence score of the target class. \n\nHowever, compared with the shadow-model-based attack, these attacks require more information [11] [19]. \n\nApart from attacks under the black-box access, there are other attacks proposed under the white-box access. In this access model, an attacker has the ability to access more information; therefore, the attacker has more options to initiate the attack rather than only using the outputs from the models. Nasr et al. [11] shown that, in this scenario, an attacker can leverage other useful information, such as the activation values, gradients to perform the attack. Especially, in federated learning [21] scenario, the attacker can also use the information of parameter updates as the role of parameter aggregator. In spite of the effectiveness, these attacks are less practical compared with those in a black-box manner, since in the real world, deep learning models usually provide service under an MLaaS (i.e. Machine-Learning-as-a-Service) scenario, where less information of the model, except that the output is sent to the users. Some membership inference attacks aiming at the real-world application are also proposed. Pyrgelis et al. [16] propose an attacker to infer the membership of a given user in aggregate location data and further acquire the user's location. \n\nLiu et al. [15] propose another membership inference attack to acquire the membership of patients, whose data are used to train a disease prediction model.",
            "score": 0.692935568462874,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 3108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2116
                },
                {
                    "start": 2119,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "234482894",
            "title": "Privacy Inference Attacks and Defenses in Cloud-based Deep Neural Network: A Survey",
            "text": "That is to say, the attacker can only observe the model predictions. However, extending the known black-box membership inference attack mechanism directly to the white-box setting (by analyzing the output of the activation function) does not work well [89], because the generalization capability of the activation functions is much faster than that in the output layer. Moreover, the black-box attack may be not effective for DNNs with good performance on generalization. Therefore, The current work [89] leveraged the privacy vulnerability of the SGD algorithm used to train deep neural networks, and designed a novel white-box membership inference algorithm. The ultimate goal pursued by the privacy attack community is to gradually relax attack assumptions and make the designed attacks applicable for far more general attack settings. \n\nWe argue that one of the main reasons for a member inference attack is that the attacker can accurately infer whether a piece of data is in the training set of the target model. Overfitting means that the prediction ability of the model for the data from the training set is better than that for the test data, i.e. the non-training set. Therefore, overfitting models are more vulnerable to membership inference attacks. There are two explanations for model overfitting. The first one is that the target model is trained inadequately or excessively, which makes the trained model has a poor performance on the test data. The second reason may be that the training set is unrepresentative. If there is a significant difference between the distribution of the training set and the test set, the target model learned from the training set will show its diverse performance when an encounter with the test samples.",
            "score": 0.6929131080769048,
            "section_title": "D. Membership Inference Attack",
            "char_start_offset": 58800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "52211986",
            "title": "LOGAN: Membership Inference Attacks Against Generative Models",
            "text": "Over the past few years, a few privacy attacks on machine learning have been proposed. For instance, attacks targeting distributed recommender systems [10] have focused on inferring which inputs cause output changes by looking at temporal patterns of the model. Specific to membership inference are attacks against supervised models by Shokri et al. [57]. Their approach exploits differences in the model's response to inputs that were or were not seen during training. For each class of the targeted blackbox model, they train a shadow model, with the same machine learning technique. Whereas, our approach targets generative models and relies on GANs to provide a general framework for measuring the information leakage. As mentioned earlier, membership inference on generative models is much more challenging than on discriminative models: in the former, the attacker cannot exploit confidence values on inputs belonging to the same classes, thus it is more difficult to detect overfitting and mount the attack. As a matter of fact, detecting overfitting in generative models is regarded as one of the most important research problems in machine learning [68]. Overall, our work presents black-box attacks that do not rely on any prediction vectors from the target model, as generic generative models output synthetic samples. \n\nAdditional membership inference attacks focus on genomic research studies [5,24], whereby an attacker aims to infer the presence of a particular individual's data within an aggregate genomic dataset, or aggregate locations [50]. \n\nThen, in model inversion attacks [19], an adversary extracts training data from outputted model predictions. Fredrikson et al. [18] show how an attacker can rely on outputs from a model to infer sensitive features used as inputs to the model itself: given the model and some demographic information about a patient whose records are used for training, an attacker predicts sensitive attributes of the patient. However, the attack does not generalize on inputs not seen at training time, thus, the attacker relies on statistical inference about the total population [40]. The record extracted by the attacker is not an actual training record, but an average representation of the inputs that are classified in a particular class. Long et al. [37] and Yeom et al. [70] investigate connections between membership inference and model inversion attacks against machine learning classifiers.",
            "score": 0.6900607184842428,
            "section_title": "Attacks",
            "char_start_offset": 7008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2291
                },
                {
                    "start": 2292,
                    "end": 2448
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "13583585"
                },
                {
                    "start": 1406,
                    "end": 1409,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 1409,
                    "end": 1412,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 1555,
                    "end": 1559,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 1596,
                    "end": 1600,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "231933642",
            "title": "Machine Learning\u2013based Cyber Attacks Targeting on Controlled Information",
            "text": "The counts were rolled over and infused with differential privacy noise to preserve the training set [29]. In addition, the balance between training set minimization and model performances (accuracy and scalability) should also be considered. [34] no YES no YES YES no White-box inversion attack [34] no YES no YES no YES GAN attack [49] no YES YES no no YES Membership inference attack [116] no YES no YES YES no Membership inference attack [110] no YES no YES YES no Property inference attack [38] no YES no YES no YES Property inference attack [91] no YES YES no no YES Table 8. Attack's prior knowledge under black-box access and white-box access. The black-box access allows the users to query the model and obtain prediction outputs which include the predicted label and confidence value. The white-box access allows the users to access any information of its model which includes predicted label, predicted confidence, parameters, and hyperparameters. 7, the other two ways focus on attacks at training/inference phase and with black-/white-box access [102]. Model extraction attacks [101,125] and hyperparameter stealing attacks [98,131] leak the model's internal information happened at inference phase. Attackers steal model's training data mostly at inference phase, except the GAN attack [49] and the property inference attack [91] which happen at training phase of collaborative learning. When attacking during training phase, attackers with white-box access to the model can exploit its internal information. As shown in Table 8, the white-box access allows attackers to have more prior knowledge than black-box, which results in high performance of the stealing attack [34]. \n\nOn the other hand, black-box attacks can be more applicable in the real world. Except [110], most of the attackers in this category under black-box access know the learning algorithm of the target model [34,71,98,101,125,131]. \n\nCountermeasures: Concerning the ML pipeline, the protection methods will be applied in data preprocessing phase, training phase, and inference phase respectively. Differential privacy noise used in the first phase can build a privacypreserving training set [70].",
            "score": 0.6900448983666772,
            "section_title": "Stealing",
            "char_start_offset": 66007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1918
                },
                {
                    "start": 1921,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 387,
                    "end": 392,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 442,
                    "end": 447,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1059,
                    "end": 1064,
                    "matchedPaperCorpusId": "44237208"
                },
                {
                    "start": 1091,
                    "end": 1096,
                    "matchedPaperCorpusId": "1090603"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1137,
                    "end": 1141,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1339,
                    "end": 1343,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1778,
                    "end": 1783,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1895,
                    "end": 1899,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1899,
                    "end": 1902,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 1902,
                    "end": 1905,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 1905,
                    "end": 1909,
                    "matchedPaperCorpusId": "1090603"
                },
                {
                    "start": 1909,
                    "end": 1913,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "3423242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "231802143",
            "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
            "text": "We now review relevant related work on inference attacks and defenses, as well as software dedicated to evaluating them. \n\nMembership Inference Attacks. Shokri et al. [53] propose the first membership inference attack against black-box ML models: they train multiple shadow models to simulate the target model and use multiple attack models to conduct the inference. Salem et al. [51] later relax several key assumptions from [53]; namely, using multiple shadow models, the knowledge of the target model structure, and having a dataset from the same distribution as the target model's. Yeom et al. [63] assume that the adversary knows the target model's training dataset's distribution and size, and they collude with the training algorithm. Both [51] and [63] are close in performance to Shokri et al.'s attacks [53]. In this paper, we implement the attack proposed by Salem et al. [51], i.e., one shadow model, one attack model, and a shadow dataset. More recently, researchers have studied membership inference in other settings, including natural language processing [5,56], generative models [6,16,20], recommender systems [64], and federated learning [35,38]. Also, Song and Mittal have performed a systematic evaluation on membership inference [58]. Previous work [51,53] also shows that overfitting is the major factor causing membership inference. To the best of our knowledge, however, no one has investigated other factors studied in our paper, such as the influence of dataset complexity or the relationship among different inference attacks. \n\nAttribute Inference. Prior research [2,14] has studied macro-level attribute inference attacks against ML models, whereby the adversary aims to infer some general properties of the training dataset. Melis et al. [35] propose the first sample-level attribute inference attack against federated machine learning systems. Song and Shmatikov [57] reveal that the risks of attribute inference are caused by the intrinsic overlearning characteristics of machine learning models.",
            "score": 0.6892376999887164,
            "section_title": "Related Work",
            "char_start_offset": 47543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 883,
                    "end": 887,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1071,
                    "end": 1074,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1074,
                    "end": 1077,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1097,
                    "end": 1100,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1161,
                    "end": 1164,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1251,
                    "end": 1255,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1275,
                    "end": 1278,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1593,
                    "end": 1596,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1596,
                    "end": 1599,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1895,
                    "end": 1899,
                    "matchedPaperCorpusId": "167217888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93603515625
        },
        {
            "corpus_id": "246706163",
            "title": "PPA: Preference Profiling Attack Against Federated Learning",
            "text": "The membership inference attack [41] proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. [40] optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. [34] designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. [8] provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. [24] utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. [38] focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks [20], [17], [18] attacked generative model under the white-box and blackbox settings.",
            "score": 0.6855984155847902,
            "section_title": "B. Membership Inference Attack",
            "char_start_offset": 9055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1332
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 828,
                    "end": 832,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "244077745"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1259,
                    "end": 1263,
                    "matchedPaperCorpusId": "199546273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "We first re-evaluate the effectiveness of two membership inference defenses [19,29], and then re-evaluate the white-box membership inference attacks proposed by Nasr et al. [30]. Following prior work [39,42], we sample the input (x, y) from either the target model's training set or test set with an equal 50% probability. Thus, the random guessing strategy results in a 50% membership inference attack accuracy.",
            "score": 0.6855375655953191,
            "section_title": "Experiment results",
            "char_start_offset": 24088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 412
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "165163934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "232110565",
            "title": "On the privacy-utility trade-off in differentially private hierarchical text classification",
            "text": "Membership Inference attacks strive for identifying the presence or absence of individual records in the training data of a machine learning model. Throughout this paper we refer to the trained machine learning model as target model and the data owner's training data as D train target . We solely consider ANNs as target models in this paper. ANNs are structured in layers of neurons that are connected by weights. We denote the weights between a layer l and its preceding layer l \u2212 1 as w (l) . The output of the l-th layer is denoted as o (l) . The ANN's final output is the output of the last layer. This paper builds upon the white-box MI attack against ANNs proposed by Nasr et al. [36]. Essentially, the white-box MI attack assumes an honest-but-curious adversary with access to the target model weight matrix w (l) . The white-box MI adversary leverages this knowledge to calculate attack features for each record (x, y) in the form of layer outputs o (l) (x; W ), losses L(o(x; w), y), and gradients \u2202L \u2202w (l) . With the aforementioned data the white-box MI adversary trains a binary classifier, the attack model. [36] assumed the portion of known records at 50% and we follow this assumption to allow comparison. The performance metrics of an MI attack model are typically evaluated on a balanced dataset including members (target model training data) and an equal number of non-members (target model test data). An illustration of the data preparation for the white-box MI attack and its evaluation is shown in Figure 1.",
            "score": 0.685051939585273,
            "section_title": "Membership Inference",
            "char_start_offset": 7236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1531
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1123,
                    "end": 1127,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "In this section, we formalize membership inference attacks against machine learning models as follows: Given an instance x and blackbox access to a classification model F t trained on a dataset D, can an adversary infer with high confidence that the instance x was contained in D at the train time of F t ? This definition states that membership inference focuses on the question of the membership of x in D and not about the contents of x. This divergence separates membership inference from existing areas of privacy research, such as differential privacy [4], [43], [14] or secure multiparty computation [44], [10], [9]. Also notable is that membership inference attacks are at the local level: an adversary wishes to know if a particular x is in D and not D in its entirety. \n\nFigure 1 illustrates the workflow of membership inference attack development. Given a training dataset D and a classification model F t trained on D, the machine learning service provider may provide a classification service through a prediction API. This API offers users black box access to the model F t . Users may send prediction queries with their own data to the service and receive classification predictions. An adversary uses such a service to collect information about the private dataset D on which the prediction model F t was privately trained. By leveraging any public or background knowledge of the training dataset D or the target model F t , an adversary builds a membership inference attack model F a to deploy for launching membership inference attacks in real time. \n\nTo gain an in-depth understanding of the general formulation of the membership inference attack model, we first characterize the types of adversarial knowledge and datasets required to train the attack model as well as the attack cost, the attack value, and their evaluation metrics. We then present a systematic formulation of general attacks in Section 3.",
            "score": 0.6832511076459556,
            "section_title": "MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 7016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1927
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "15331398"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "589685"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "2887752"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "6942138"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "18290778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "231718884",
            "title": "Property Inference from Poisoning",
            "text": "The information leakage of machine learning models could be studied in both white-box and black-box setting. In this paper, we consider the black box model, where the attacker is only allowed to make a limited number of queries to the trained model. We show that these attacks can be very successful. \"Black box\" attacks is sometimes used to refer to attacks which also have access to model's confidence values on each query [41]. We emphasize here that we use the stricter notion of black box and our attacker will use only the model predictions. This type of attack is studied independently in [9] where they study \"labelonly\" membership inference attacks.",
            "score": 0.6825296020198722,
            "section_title": "Black box or white box model access",
            "char_start_offset": 20965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 658
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "209444528",
            "title": "Privacy Attacks on Network Embeddings",
            "text": "Here, given a traditional machine learning model (e.g., a classifier on tabular data), an adversary tries to infer values of sensitive attributes in the training data. Generally, attack scenarios can be divided into two variations [22]: White-box attacks (e.g., [1]) where the adversary has access to information about the model architecture and/or parameters, and black-box attacks where the adversary does not have access to properties of the model. Black-box attacks are more common and relevant in terms of data privacy. In most scenarios, the adversary can use the model, i.e., apply inputs to the model and observe corresponding outputs (e.g., machine learning as a service offers). Many approaches evaluate different inputs to the model on how likely they are part of the training data. If the input space is small enough, all possible inputs can be tested, for larger input spaces (domain-specific) gradient descent based approaches can be used [8]. \n\nAttacks to detect if an input for a machine learning model was used during training are often referred to as membership inference attacks [25,26,28]. The basic idea is to extract differences in the confidences of the outputs for data used during training and data that was not used. Advanced techniques, therefore, train shadow models, which imitate the attacked model, on different generated training datasets. An attack classifier is trained on data from these shadow models and is applied to data of the attacked machine learning model to achieve the attack's goal. We see the privacy attack on network embeddings proposed in this work as the transfer of this general idea to the setting of network embeddings.",
            "score": 0.6819978357037377,
            "section_title": "RELATED WORK",
            "char_start_offset": 5580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1673
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "53215926"
                },
                {
                    "start": 953,
                    "end": 956,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1102,
                    "end": 1105,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "257038160",
            "title": "Digital Privacy Under Attack: Challenges and Enablers",
            "text": "Committed by: (None) through the model prediction outputs. However, owing to the assumption that the target model to be attacked and the generated shadow model are consistent in both structure and training data, the scope of their membership inference attack was largely reduced. Liu et al. [81] presented more effective membership inference attacks called SocInf, building the mimic model based on neural networks and generative adversarial networks to disclose the prediction differences. Through repeated queries, the adversary can attain the highest possible membership inference attack performance on the targeted models by reconstructing the posterior vectors from the prediction labels. \n\nConsidering the problem of inadequate data for training the attack model, Bai et al. [6] developed GANMIA to generate synthetic data. But the above works all discussed membership inference attacks against ML models conducted under the black-box settings, in which adversaries can only obtain model predictions. Such attacks may be inefficient against deep-learning models with large sets of parameters. \n\nRahman et al. [114] conducted membership inference against an advanced differentially private deep model under the white-box settings for the first time. Nasr et al. [102] designed a novel white-box inference attack exploiting the stochastic gradient descent (SGD) the algorithm's privacy vulnerability. They also discussed and evaluated the attack in all major scenarios with different criteria, such as prior knowledge or model training architecture. Sablayrolles et al. [119] proposed an optimal membership inference strategy beyond with mild assumptions on the parameters' distribution, which depended only on the loss function rather than the parameters. They also demonstrated that the state-of-the-art membership inference methods can closely substitute the optimal strategy [122,160]. Besides, Azadmanesh et al. [3] applied generative adversarial network (GAN) models and launch a white box membership inference attack for improving performance.",
            "score": 0.6818380343134041,
            "section_title": "Extraction Attack.",
            "char_start_offset": 64160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "195477472"
                },
                {
                    "start": 781,
                    "end": 784,
                    "matchedPaperCorpusId": "236939414"
                },
                {
                    "start": 1267,
                    "end": 1272,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1574,
                    "end": 1579,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1883,
                    "end": 1888,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1888,
                    "end": 1892,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1921,
                    "end": 1924,
                    "matchedPaperCorpusId": "247192597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "Membership Inference Attacks. In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models [17,19,26,35,42,44,50,55]. Shokri et al. [44] propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [42] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [44]. Later, Nasr et al. [35] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. [50] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang [30] and Choquette-Choo et al. [8] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nIn addition to the image classification models we concentrate on in this work, there are some other works demonstrating other types of ML models, e.g., language models [46,47], generative models [6,17], and graph-based models [19,36], are also vulnerable to membership inference attacks. In future work, we plan to extend our work to a broader range of ML application scenarios. Defenses Against Membership Inference. Researchers have proposed to improve privacy against membership inference via different defense mechanisms.",
            "score": 0.6815620404735555,
            "section_title": "Related Work",
            "char_start_offset": 53208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "3052834"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1000,
                    "end": 1004,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1378,
                    "end": 1382,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1409,
                    "end": 1412,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 1886,
                    "end": 1889,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1909,
                    "end": 1912,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1912,
                    "end": 1915,
                    "matchedPaperCorpusId": "3052834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "226282157",
            "title": "On the Privacy Risks of Algorithmic Fairness",
            "text": "In this subsection, we formalize the privacy risk of individuals and subgroups. Recall that, the privacy is preserved if the output distributions are indistinguishable when a participant in or out of the input dataset. In other words, the privacy of a participant is preserved if upon observing the output, an adversary could not be able to tell whether the record of the participant is in training dataset or not. \n\nBased on this, it is natural to measure the privacy risk of an individual as the success of the adversary whose goal is to infer whether or not the individual's data record was used for training a model. Such attacks are called membership inference attacks which are used as a tool to measure information leakage of different machine learning algorithms, including deep learning algorithms [19], adversarial robust learning algorithms [18], learning algorithms for explanations models [16], learning algorithms for embedding models [15] and reinforcement learning algorithms [32]. In our paper, by leveraging the membership inference attacks, we analyze the information leakage of fair machine learning algorithms and the gap of the leakage between standard learning algorithms and fair learning algorithms. \n\nRoughly speaking, in membership inference attacks, an adversary attempts to infer whether a specific data record (challenge data point) was included in the training dataset of a learned model or not. Suppose a training dataset S of size n and each data point in S is sampled i.i.d. from the distribution D, a model A S is learned by applying a learning algorithm A on S. The learning algorithm A can be a fair learning algorithm or a standard learning algorithm. Given knowledge about the trained model, which is denoted as K(A S ), and the learning algorithm A, the adversary A needs to infer whether a challenge point z is a member of S or not. It is important to emphasize that the adversary can only access the learned model via function K. In black-box setting, the K is generally the loss function of the learned model on test data points [20]. In white-box setting, the K outputs the learned model [21]. In this paper, we are interested in the privacy risk of individuals through the output of the model. Accordingly, we measure the privacy risk in the black-box setting where the adversary only observes the loss.",
            "score": 0.6809423673680804,
            "section_title": "Definition of Privacy Risk",
            "char_start_offset": 19048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 852,
                    "end": 856,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "260540686"
                },
                {
                    "start": 2072,
                    "end": 2076,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 2132,
                    "end": 2136,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "276903745",
            "title": "Do Fairness Interventions Come at the Cost of Privacy: Evaluations for Binary Classifiers",
            "text": "Table 1 summarizes the main notations adopted throughout the paper. In the table , x and y presents model inputs and labels, respectively. Predicted labels are represented as y. Our models consist of feature extractors h(x), and classification heads g(\u2022). In our privacy analysis, membership inference results for a sample x are denoted as M (x), while attribute inference results are represented as A(x). The attack models for membership and attribute inference are denoted as f m a (\u2022) and f a a (\u2022), respectively. We consider different types of attacks: score-based (MIA s ) and reference-based (MIA l ) membership inference attacks, as well as black-box (AIA b ) and white-box (AIA w ) attribute inference attacks. We adopt metrics including accuracy of target classifiers (Acc t ) and attack classifiers (Acc a ), as well as true positive rate (TPR) and false positive rate (FPR).",
            "score": 0.680122702690476,
            "section_title": "Notations",
            "char_start_offset": 21737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 885
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "91184074",
            "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
            "text": "Membership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data [18,21] and location data [36,37]. Shokri et al. [40] propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28,31,32,38,42,53]. \n\nIn theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings. \n\nModel Inversion. Fredrikson et al. [12] propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g.\" reconstructing recognizable human faces [11,20]. As pointed out by other works [29,40], model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. \n\nModel Stealing. Another related line of work is model stealing. Tram\u00e8r et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's parameters.",
            "score": 0.6797168317941028,
            "section_title": "Related Works",
            "char_start_offset": 46212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 764,
                    "end": 767,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 776,
                    "end": 779,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1739,
                    "end": 1743,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1743,
                    "end": 1746,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1778,
                    "end": 1782,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1782,
                    "end": 1785,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2067,
                    "end": 2071,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "53099247",
            "title": "Exploiting Unintended Feature Leakage in Collaborative Learning",
            "text": "Bonawitz et al. [5] use secure multi-party aggregation techniques, tailored for federated learning, to let participants encrypt their updates so that the central parameter server only recovers the sum of the updates. In Section 7.2, we showed that inference attacks can be successful even if the adversary only observes aggregated updates. \n\nMembership inference. Prior work demonstrated the feasibility of membership inference from aggregate statistics, e.g., in the context of genomic studies [3,27], location timeseries [50], or noisy statistics in general [14]. \n\nMembership inference against black-box ML models has also been studied extensively in recent work. Shokri et al. [53] demonstrate membership inference against black-box supervised models, exploiting the differences in the models' outputs on training and non-training inputs. Hayes et al. [24] focus on generative models in machine-learning-as-a-service applications and train GANs [22] to detect overfitting and recognize training inputs. Long et al. [34] and Yeom et al. [66] study the relationship between overfitting and information leakage. \n\nTruex et al. [58] extend [53] to a more general setting and show how membership inference attacks are data-driven and largely transferable. They also show that an adversary who participates in collaborative learning, with access to individual model updates from all honest participants, can boost the performance of membership inference vs. a centralized model. Nasr et al. [39] design a privacy mechanism to adversarially train centralized machine learning models with provable protections against membership inference. \n\nOther attacks on machine learning models. Several techniques infer class features and/or construct class representatives if the adversary has black-box [16,17] or white-box [2] access to a classifier model. As discussed in detail in Section 3, these techniques infer features that characterize an entire class and not specifically the training data, except in the cases of pathological overfitting where the training sample constitutes the entire membership of the class. Hitaj et al. [25] show that a participant in collaborative deep learning can use GANs to construct class representatives.",
            "score": 0.6734298332842973,
            "section_title": "Related Work",
            "char_start_offset": 54994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "3833774"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1040,
                    "end": 1044,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1489,
                    "end": 1493,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1794,
                    "end": 1797,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1811,
                    "end": 1814,
                    "matchedPaperCorpusId": "14757739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "270379839",
            "title": "Rethinking the impact of noisy labels in graph classification: A utility and privacy perspective",
            "text": "Membership inference attacks in machine learning (ML) models occur when an attacker attempts to ascertain if a specific data sample was part of the training dataset for a given ML model (Shokri et al., 2017;Liu et al., 2022a).More specifically, for a given one candidate data sample, a trained machine learning model F and the adversary's external knowledge \u2126, the membership inference attack a can be defined as the following function:\n\nA : X, F , \u2126 \u2192 {0, 1}.\n\n(1)\n\nwhere 0 indicates that X is not a member of the F training set, and 1 indicates that it is a member.A symbolizes the attack mechanism.The unintended revelation of data sample membership can lead to significant privacy breaches.For instance, if X denotes an individual's health records or private data, such attacks could reveal whether this information contributed to training a disease-specific model, thus posing a privacy risk.Membership inference attacks are critical for assessing privacy exposure in statistical data analysis techniques.These attacks are categorized into black-box or white-box types depending on the attacker's capabilities.Black-box attacks limit the attacker to model outputs through strategies like shadow models or metric-based methods (e.g., cross-entropy, confidence scores).\n\nIn contrast, white-box attacks provide access to more in-depth information, including the model's optimal parameters and intermediate embeddings.In this paper, to measure the impact of noise labels on data privacy of graph classification, we set two scenarios: graph-level member inference attack and subgraphlevel member inference attack.The graph-level member inference attack is consistent with the method of member inference attack in conventional deep learning (Liu et al., 2022a), that is, to judge whether the queried graph data is in the training graph data set.Here, we adopt the black-box scenario proposed by Song et al. (2019) to measure the degree of privacy disclosure based on cross-entropy and output accuracy.As for the inference attack scenario of subgraph members, we measure the disclosure of graph data privacy according to (Zhang et al., 2022) in the white box scenario.We use graph embedding information to query whether the subgraph is in the original training graph dataset.",
            "score": 0.6696599751403808,
            "section_title": "Membership inference attack",
            "char_start_offset": 15685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 436
                },
                {
                    "start": 438,
                    "end": 460
                },
                {
                    "start": 462,
                    "end": 465
                },
                {
                    "start": 467,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1272
                },
                {
                    "start": 1274,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1613
                },
                {
                    "start": 1613,
                    "end": 1844
                },
                {
                    "start": 1844,
                    "end": 2000
                },
                {
                    "start": 2000,
                    "end": 2166
                },
                {
                    "start": 2166,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 207,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 207,
                    "end": 225,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 1740,
                    "end": 1759,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 1894,
                    "end": 1912,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 2119,
                    "end": 2139,
                    "matchedPaperCorpusId": "238407736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "236154858",
            "title": "Generative Models for Security: Attacks, Defenses, and Opportunities",
            "text": "A prominent type of exploratory attack on machine learning is membership inference attacks. In a membership inference attack, the attacker aims to determine whether a specific individual's data was part of the target model's training data [209]. These attacks exploit the fact that machine learning models such as classifiers often behave differently when asked to make a prediction for a data point from their training dataset compared to for a data point not part of the training dataset (but from the same distribution as the training dataset). This behavior is related to overfitting, although recent work suggests that models may be vulnerable even in the absence of overfitting [136,146,148]. \n\nMembership inference attacks on generative models seek to exploit the same effect. In particular, some membership inference attacks have been developed against GANs [94] and generative models [98]. Additionally, VAEs are vulnerable to reconstruction attacks [98].",
            "score": 0.6691708470911811,
            "section_title": "Exploratory Attacks",
            "char_start_offset": 44248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 964
                }
            ],
            "ref_mentions": [
                {
                    "start": 239,
                    "end": 244,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 684,
                    "end": 689,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "226266600"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "199546273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "259803133",
            "title": "Can the use of privacy enhancing technologies enable federated learning for health data applications in a Swedish regulatory context?",
            "text": "1) Membership inference attack: (MI) A membership inference attack aims to determine whether a specific data point has been used during the training of a machine learning model. This method can potentially expose sensitive information about individuals, i.e. whether a person with certain characteristics and a particular medical condition has been included in the model's training data. The attacker can either have black-box access [10], where they only have query access to the model, or white-box access [11], where they have full access to the model's parameters and architecture. Shokri et al. [10] proposed one of the first attacks, which considers an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. Among the multitude of attack procedures that were proposed later on, we mention [11] that is computationally simpler, but requires that the attacker can calculate the training loss of a candidate data point threshold and compare it with a threshold (the average training loss). A naive baseline procedure was proposed by [12], which predicts a sample as a member if it is correctly labeled by the target model and predicts it as a non-member if misclassified. In a recent experimental comparison [13], the naive model demonstrates similar performance as the more involved MI attack procedures. The two approaches both have a high false positive rate. Indeed, MI attack accuracy is reported to be highly correlated to the model's overfitting or generalization gap [19,20,22], and furthermore troubled by high false positive [13]. The generalization gap refers to the difference between the test set and training set performance. As low as possible is generally desired as it reflects the extent to which a model is overfitted. As overfitted models have limited practical use, it is questionable how well reported MI attack success stories can be generalized to well-trained models [14]. Despite the limitations of current MI attack strategies, it is important to study and learn from them as superior attacks might appear in the future. \n\n2) Model inversion attack: The aim of a Model inversion attack is to learn hidden sensitive attributes of a test input given knowledge about the non-sensitive attributes.",
            "score": 0.6687219940266351,
            "section_title": "B. Attacks on privacy",
            "char_start_offset": 6496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2105
                },
                {
                    "start": 2108,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 850,
                    "end": 854,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1266,
                    "end": 1270,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 1537,
                    "end": 1540,
                    "matchedPaperCorpusId": "251000069"
                },
                {
                    "start": 1540,
                    "end": 1543,
                    "matchedPaperCorpusId": "3164123"
                },
                {
                    "start": 1593,
                    "end": 1597,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 1950,
                    "end": 1954,
                    "matchedPaperCorpusId": "173990571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "165163934",
            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
            "text": "For a target machine learning model, the membership inference attacks aim to determine whether a given data point was used to train the model or not [18,32,37,41,47,64]. The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. [47] design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. [64] suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.\n\nIn this paper, we use this confidence-thresholding membership inference approach in most cases. Note that when evaluating the privacy leakage with targeted adversarial examples in Section 3.3.1 and Section 5.2.5, the confidence-thresholding approach does not apply as there are multiple prediction vectors for each data point. Instead, we follow Shokri et al. [47] to train a neural network classifier for membership inference.",
            "score": 0.6685826868609733,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 15158,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1950,
                    "end": 1954,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "10488675",
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "text": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
            "score": 0.6653039083697723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "54444175",
            "title": "Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks",
            "text": "Particularly, they show that overfitted models are more susceptible to membership inference attacks than generalized models. Such black-box attacks, however, might not be effective against deep neural networks that generalize well (having a large set of parameters). Besides, deep learning algorithms are used in a variety of settings where parameters are visible to curious adversaries, e.g., in federated learning where multiple data holders collaborate to train a global model by sharing their parameter updates with each other through a central parameter aggregator. \n\nOur contributions. In this paper, we present a comprehensive framework for the privacy analysis of deep neural networks, using white-box membership inference attacks. We go beyond membership inference attacks against fully-trained models. We take all major scenarios where deep learning is used for training and fine-tuning or updating models, with one or multiple collaborative data holders, when attacker only passively observes the model updates or actively influences the target model to extract more information, and for attackers with different types of prior knowledge. Despite differences in knowledge, observation, and actions of the adversary, the objective is the same: membership inference. \n\nA simple extension of existing black-box membership inference attacks to the white-box setting is to use the same attack on all of the activation functions of the model. Our empirical evaluations show that this will not result in accuracies higher than a black-box attacker. This is because the activation functions in the model tend to generalize much faster than the output layer. The first layers extract very simple features that are not specific to the training data. The last layers do extract complex and abstract features, but the closer they are to the final layer, the less extra information they contain about the training set, compared with the final (output) layer. \n\nWe formalize the threat model in all these settings, and exploit the privacy vulnerabilities of the stochastic gradient descent (SGD) algorithm to design our white-box inference attack. Each data point in the training set influences many of the model parameters, through the SGD algorithm, to minimize its contribution to the learning loss. The local gradient of the loss on a target data record, with respect to a given parameter, indicates how much and in which direction the parameter needs to be changed to fit the model to the data record.",
            "score": 0.6651968649153754,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2299
                },
                {
                    "start": 2300,
                    "end": 2503
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "259878257",
            "title": "FedDroidMeter: A Privacy Risk Evaluator for FL-Based Android Malware Classification Systems",
            "text": "There are four types of privacy inference attacks: attribute inference attack (AInf), model extraction attack (MExt), model inversion attack (MInv), and member inference attack (MInf).\n\nMember inference attacks are designed to determine whether candidate data belongs to the training dataset of the target model. Shokri et al. [8] proposed the first membership inference attack method for the black-box ML model. They trained multiple shadow models to simulate the target model and then use multiple attack models for reasoning. Salem et al. later relaxed several key assumptions [18], using multiple shadow models, knowledge of the structure of the target model, and having the same distribution of data sets as the auxiliary knowledge. Using new insights on how overfitting occurs in deep neural networks, Fredrikson [19] showed how the model's particular use of features provides evidence of membership for white-box attackers and demonstrates that such attacks are superior to previous black-box approaches. Shafran [20] used a new predictability error that can be calculated for each sample and does not require a training set. The membership error is used to achieve a higher precision of membership inference on many baseline models by subtracting the predictability error from the reconstruction error.\n\nThe attribute inference attack was first proposed by Ateniese [21]. An internal attacker trains a meta-classifier by collecting the intermediate results of the updated model to infer sensitive statistical attributes contained in the training dataset of other actors. Later, Melis [22] proposed the first sample-level attribute inference attack against federated learning systems. Song [23] revealed that the inference of risk attributes is caused by the overlearning characteristic inherent in ML models.\n\nModel inversion is mainly divided into data reconstruction and training class inference. Model inversion was first proposed by Fredrikson [24] in the task of drug dose classification. Fredrikson proposed a new inversion attack method in the article [25], aiming to reconstruct representative samples for each type of target model. They extend model inversion to general ML settings using the backpropagation of the target ML model parameters. Carlini [26] showed that model inversion can also be performed efficiently for natural language processing models. In the article [27], the author proposed an attack method to infer participants' private training data using the shared gradient",
            "score": 0.6642629214051974,
            "section_title": "Existing Inferential Attack Methods on Federated Learning",
            "char_start_offset": 8248,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "231925091"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "167217888"
                },
                {
                    "start": 1956,
                    "end": 1960,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 2067,
                    "end": 2071,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "221397750",
            "title": "Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries",
            "text": "In the previous section we defined the membership inference attacks and described two generic models to carry out these attacks in practice. Now, we will explore methods to defend against them. \n\nTo defend against these attacks, we need to understand what factors make these attacks possible and how we can limit and paralyze the adversary. Most of the previous work on defenses against membership inference attacks can be summarized into two categories: \n\nGeneralization-based techniques (Shokri et al., 2017) was the first to define the membership inference attacks in a machine learning setting. They also identify the overfitting of the victim model as one of the main culprits for vulnerability to membership inference attacks. They hypothesize that the victim model memorizes its training set such that the posteriors show a statistical difference between the seen and hold-out data. A more comprehensive study about the correlation of overfitting to membership inference attacks can be found in (Yeom et al., 2018). \n\nThese findings prompt a line of defense that relies on enforcing generalization on the victim model. (Shokri et al., 2017) suggest using L2 regularization of the parameters and restricting the number of training epochs. (Salem et al., 2018) use dropout and ensemble learning to train the victim model to help it generalize better. In a slightly different approach, (Nasr et al., 2018) utilizes adversarial training of the victim model in the form of a min-max game to help the model generate indistinguishable predictions on its training set and an unseen dataset. \n\nNoising-based techniques Adding randomness to different parameters of the victim model at different stages is one of the most natural ways to confuse any adversary. In fact, the first defenses against membership inference attacks on the genome data (Wang et al., 2009) proposes adding carefully-crafted noise to the published dataset. \n\nJia et al (Jia et al., 2019) suggest adding noise to the output of the victim model. They generate specially-composed noise vectors for the victim model's posteriors such that they act as adversarial examples for the attacker.",
            "score": 0.6633027621789911,
            "section_title": "Defenses for Membership Inference Attacks",
            "char_start_offset": 8510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 489,
                    "end": 510,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1002,
                    "end": 1021,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1126,
                    "end": 1147,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1245,
                    "end": 1265,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1390,
                    "end": 1409,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1841,
                    "end": 1860,
                    "matchedPaperCorpusId": "8091014"
                },
                {
                    "start": 1939,
                    "end": 1957,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "Machine learning (ML) has become a core component of many real-world applications, ranging from image classification to speech recognition. The success of ML has recently driven leading Internet companies, such as Google and Amazon, to deploy machine learning as a service (MLaaS). Under such services, a user uploads her own dataset to a server and the server returns a trained ML model to the user, typically as a black-box API. \n\nDespite being popular, ML models are vulnerable to various security and privacy attacks, such as model inversion [12], adversarial examples [15], and model extraction [43], [30], [45]. In this paper, we concentrate on one such attack, namely membership inference attack. In this setting, an adversary aims \u00a7 Corresponding author to determine whether a data item (also referred to as a data point) was used to train an ML model or not. Successful membership inference attacks can cause severe consequences. For instance, if a machine learning model is trained on the data collected from people with a certain disease, by knowing that a victim's data belong to the training data of the model, the attacker can immediately learn this victim's health status. Previously, membership inference has been successfully conducted in many other domains, such as biomedical data [2] and mobility data [35]. \n\nShokri et al. [38] present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple machine learning models (one for each prediction class), referred to as attack models, to make membership inference over the target model's output, i.e., posterior probabilities. Given that the target model is a black-box API, Shokri et al. propose to construct multiple shadow models to mimic the target model's behavior and derive the data necessary, i.e., the posteriors and the ground truth membership, to train attack models. \n\nThere are two main assumptions made by Shokri et al. [38]. First, the attacker needs to establish multiple shadow models with each one sharing the same structure as the target model. This is achieved by using the same MLaaS that trains the target model to build the shadow models.",
            "score": 0.6628653794746633,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 430
                },
                {
                    "start": 433,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 550,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 573,
                    "end": 577,
                    "matchedPaperCorpusId": "6706414"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 1344,
                    "end": 1348,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1969,
                    "end": 1973,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "49671507",
            "title": "Algorithms that remember: model inversion attacks and data protection law",
            "text": "It has been demonstrated that machine learning models are vulnerable to a range of cybersecurity attacks that cause breaches of confidentiality. Confidentiality attacks leak information to entities other than those whom designers intended to view it. In the case of machine learning systems, there are different types of attacks. The first concerns model stealing, e.g. where an attacker uses API access to replicate a model [19]. Without a further confidentiality breach, this is primarily a concern for intellectual property rather than for privacy, and is of less concern here. A second attack class, model inversion, turns the journey from training data into a machine-learned model from a one-way one to a two-way one, permitting the training data to be estimated with varying degrees of accuracy. A third attack class, membership inference, does not recover the training data, but instead recovers information about whether or not a particular individual was in the training set. Both model inversion and membership inference can be undertaken as a black-box attack, where the attack can be done with only query access (e.g. through the API business model above), or as a white-box attack, where an attacker requires full access to the model's structure and parameters [20,21]. \n\nWe will formally describe both model inversion and membership inference attacks in a manner amenable to the discussion of personal data. The set-up is as follows. A data controller holds a (a) Attack types \n\nIn the following section, we outline, at a high level and non-exhaustively, forms and work on model inversion and membership inference attacks. A visual depiction of both can be found in figure 1.",
            "score": 0.6621584069497596,
            "section_title": "Why might models be personal data?",
            "char_start_offset": 12010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1690
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1275,
                    "end": 1279,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1279,
                    "end": 1282,
                    "matchedPaperCorpusId": "20182008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "259375769",
            "title": "Scalable Membership Inference Attacks via Quantile Regression",
            "text": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.",
            "score": 0.6596820336507974,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "52901134",
            "title": "Adversarial Attacks and Defences: A Survey",
            "text": "Ateniese et al. [11] showed it is possible to gather relevant information from machine learning classifiers using a meta-classifier. Given the black box access to a model (e.g., via public APIs) and a training data, an attacker may be interested in knowing whether that data was part of the training set of the model. They experimented with a speech recognition classifier that uses Hidden Markov Models and extracted information such as accent of the users which was not supposed to be explicitly captured. Another inference attack presented by Shokri et al. [67] is membership inference, i.e., which determines whether a given data point belongs to the same distribution as the training dataset. This attack may fall under the category of non-adaptive or adaptive black box attacks. In a typical black box environment, attacker sends a query to the target model with a data point and obtains model's prediction. The output given by the model is a vector of probabilities which specifies whether the data point belongs to a certain class. For training attack model, a set of shadow models are built. Since the adversary has the knowledge of whether a given record belongs to the training set, supervised learning can be employed and corresponding output labels are then fed to attack model to train it to distinguish shadow model's outputs on members of their training data from those of non-members. \n\nFigure 7 illustrates the end-to-end attack process. The output vectors obtained from shadow model are labeled \"in\" and added to the attack model's training dataset. A test dataset is also used to query the shadow model and the outputs from this set are labeled \"out\" and also added to the attack model's training dataset. Thus a collection of target attack models is trained by utilizing the black box behaviour of the shadow models. Authors used membership attacks on classification models trained by commercial \"ML as a service\" providers such as Google and Amazon.",
            "score": 0.6586975400989404,
            "section_title": "Inference Attack",
            "char_start_offset": 20506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "251765197",
            "title": "Auditing Membership Leakages of Multi-Exit Networks",
            "text": "Currently, membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29,49,61,71,76,80,91]. Shokri et al. [76] propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. [71] propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. [76]. Later, Nasr et al. [61] focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. [80] study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang [56] and Choquette-Choo et al. [17] propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa. \n\nBesides, there exists a wide range of other attacks, defenses and applications in machine learning domain [10-12, 14-16, 20, 21, 24, 25, 28, 35, 39, 44, 50, 52, 54, 55, 63-65, 70, 74, 78, 83-86, 90, 93-95].",
            "score": 0.6585773257681268,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 48970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1463
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 123,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 789,
                    "end": 793,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 920,
                    "end": 924,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "235593386",
            "title": "Membership Inference on Word Embedding and Beyond",
            "text": "There has been a rich body of work (Mireshghallah et al., 2020;Tanuwidjaja et al., 2019;Hu et al., 2021) that investigates Machine Learning (ML) pipelines through the lens of privacy and information leakage. This body of work largely investigates the question of what information ML models capture and expose beyond the task at hand. A natural attack metric that is commonly used to understand the extent of information leakage from a ML model is Membership Inference (MI) attack. In MI attacks, an attacker is given black/white/grey box access to a ML model and aims to find out if a particular set of data was used in training the ML model. MI has been investigated in many domains such as vision (Shokri et al., 2017;He et al., 2020), generative adversarial networks (Hayes et al., 2019;Chen et al., 2020), graph neural networks (He et al., 2021;Olatunji et al., 2021) among many others.",
            "score": 0.657453076171066,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 890
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 88,
                    "matchedPaperCorpusId": "202640293"
                },
                {
                    "start": 699,
                    "end": 720,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 770,
                    "end": 790,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 790,
                    "end": 808,
                    "matchedPaperCorpusId": "221203089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "254719563",
            "title": "An Understanding of the Vulnerability of Datasets to Disparate Membership Inference Attacks",
            "text": "Membership inference attacks can be characterized based on adversarial knowledge of the model being attacked. This knowledge can be white-box, gray-box, or black boxlisted in order of increasing difficulty. This study follows the same adversarial knowledge conventions as Truex et al. [20]. White-box knowledge indicates that the adversary has access to some portion or version of the real training data, gray-box indicates that the adversary has some statistical information on the training data, and black-box indicates that the adversary has nothing more than publicly available information on the training data. \n\nThis study assumes white-box knowledge of the training data. By erring to an easier attack by the adversary, the vulnerability-classification model developed will be based on the most vulnerable type of dataset. Anything other than white-box knowledge will result in a more secure dataset. Note that these definitions refer to the adversary's training data knowledge and not their access or understanding of developed victim models. This study assumes black-box access to victim models, meaning that an adversary has access only to the inputs and results vectors of those models. This assumption is justified through the common use of black-box deployment for models when those models are open to access outside the parent organization. \n\nThe membership inference attack utilized to develop the vulnerability metric follows the shadow model methodology as developed by Shokri et al. and was chosen due to its general acceptance as a valid membership inference attack methodology as well its ability to successfully attack black-box models [65]. This attack begins with the development of a shadow model training dataset, which was developed through the utilization of some knowledge of the original training dataset-white-box knowledge of the training data-providing an easier situation for the attacker and, thus, a more reliable vulnerability metric. \n\nIn this effort, the shadow model training data was developed from a random extraction of 60% to 80% from the original training dataset, dependent on the original size of the dataset. This left 20% to 40% of the original data to be used as test data. For this study, one half of the test data was used to train the victim model and labeled as Trained. The other half of the test data was simply processed through the already trained model and labeled as Not-Trained.",
            "score": 0.6564633685545316,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 16077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2323
                },
                {
                    "start": 2324,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1657,
                    "end": 1661,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "10488675",
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "text": "Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output. \n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class. \n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members. \n\nFormally, let f target () be the target model, and let D train target be its private training dataset which contains labeled data records (x {i} , y {i} ) target . A data record x {i} target is the input to the model, and y {i} target is the true label that can take values from a set of classes of size c target . The output of the target model is a probability vector of size c target . The elements of this vector are in [0, 1] and sum up to 1. \n\nLet f attack () be the attack model. Its input x attack is composed of a correctly labeled record and a prediction vector of size c target . Since the goal of the attack is decisional membership inference, the attack model is a binary classifier with two output classes, \"in\" and \"out.\" \n\nFigure 1 illustrates our end-to-end attack process.",
            "score": 0.6556088800233637,
            "section_title": "A. Overview of the attack",
            "char_start_offset": 16269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2118
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "54444175",
            "title": "Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks",
            "text": "Investigating different privacy aspects of deep neural networks is an active field of research. Ateniese et al. [16] showed if an adversary has access to the parameters of machine learning models such as Support Vector Machines (SVM) or Hidden Markov Models (HMM) [17], she can extract valuable information about the training data (e.g., the accent of the speaker in speech recognition models). \n\nMultiple research papers address the problem of membership inference attack against model in a black-box setting [6], [7], [18]. Homer et al. [4] performed one of the first membership inference attacks on genomic data. Shokri et al. [6] showed that an ML model's output has distinguishable properties about its training data, which could be exploited by the adversary's inference model. They also introduced shadow models which mimic the behavior of the target model, and could be used by the attacker to train the attack model. Yeom et al. [7] demonstrated the relationship between overfitting and membership inference attacks. Hayes et al. [19] used generative adversarial networks to perform membership attacks on generative models. An attacker with additional information about the training data distribution can perform different types of inference attacks. Input inference [20], attribute inference [21], parameter inference [22], [23], and side-channel attacks [24] are several examples of such attacks. Hitaj et al. [25] proposed an attack on collaborative learning to generate average samples of a class. They attack an unrealistic scenario where all the data in one class is held by one participant, and the adversary knows it. It also assumes finegrained parameter updates at each mini-batch. The proposed method generates an average sample from the victim's class. This attack works on datasets where all of the members in the same classes are similar. No metric for leakage is used to measure privacy, except illustrating the generated images. \n\nGanju et al. [26] designed a new property inference attack on fully connected networks. In this attack, the attacker goal is to extract unintended properties about the target model training data from a released trained model (when attacker has full access to the trained model similar to the our whitebox attack).",
            "score": 0.6544096283747594,
            "section_title": "V. RELATED WORK",
            "char_start_offset": 48588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 515,
                    "end": 518,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1421,
                    "end": 1425,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1969,
                    "end": 1973,
                    "matchedPaperCorpusId": "52218951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "265654401",
            "title": "Preserving data privacy in machine learning systems",
            "text": "In privacy attacks, the attacker aims to gain knowledge beyond what the machine learning service offers as inference results. The attacker's visibility of the model ranges from a black-box access such as the work of Shokri et al. (2017), where the attacker has only access to the inference results produced by the model, to a white-box access (Yeom et al., 2017;Szegedy et al., 2013;Nasr et al., 2019) where the attacker has partial or full access to the models' parameters or additional information about the model, such as the adversary having access to one of the partial local models used in a federated learning setting, or knowledge about explanation vectors about the model decisions, or even the architecture of the model in the case the model was a deep neural network, etc. \n\nThere are various privacy attacks against machine learning systems such as membership inference attacks (Shokri et al., 2017;Bernau et al., 2019;Jia et al., 2019a;Li et al., 2020) and model inversion attacks (Fredrikson et al., 2015;He et al., 2019;Wu et al., 2016).",
            "score": 0.6531960949525721,
            "section_title": "Privacy risks",
            "char_start_offset": 28232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 1052
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 236,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 383,
                    "end": 401,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 890,
                    "end": 911,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 994,
                    "end": 1019,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1019,
                    "end": 1035,
                    "matchedPaperCorpusId": "208277767"
                },
                {
                    "start": 1035,
                    "end": 1051,
                    "matchedPaperCorpusId": "5921778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "218571404",
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "text": "It has been shown that machine learning models are vulnerable to various inference attacks [15,49,60,61], which enables adversaries to get useful information about the target model from only the prediction APIs. Depending on the inference goals, these inference attacks generally fall into two classes, i.e., model inference and data inference. Specifically, model inference aims at obtaining the information about the target model itself such as its parameters and architecture [37,40,54,55]. Data inference, on the contrary, focuses on extracting information about the data on which the target model operates [2,14,15,49,57,58,60,61]. In this paper, we concentrate on two of the most important and exemplary data inference attacks, notably membership inference attack [49] and model inversion attack [14,60]. In this section, we first introduce these two data inference attacks and then introduce existing defenses. Finally, we analyze the limitations of existing defense mechanisms.",
            "score": 0.6531658869843473,
            "section_title": "INFERENCE ATTACKS ON MACHINE LEARNING",
            "char_start_offset": 10277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 985
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 95,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 101,
                    "end": 104,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "54457412"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 620,
                    "end": 623,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "3920864"
                },
                {
                    "start": 626,
                    "end": 629,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 629,
                    "end": 632,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 632,
                    "end": 635,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 806,
                    "end": 809,
                    "matchedPaperCorpusId": "207941008"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "270217411",
            "title": "Probing Language Models for Pre-training Data Detection",
            "text": "Related work involves membership inference attacks in NLP and dataset contamination.\n\nMembership Inference Attacks in NLP.Membership Inference Attacks (MIAs) are designed to identify if a particular data sample was included in the training dataset of a machine learning model (Shokri et al., 2017;Yeom et al., 2018;Hu et al., 2022b).Most MIAs take a black-box setting, assuming that the adversary only has access to the model confidence or loss scores (Yeom et al., 2018;Sablayrolles et al., 2019;Jayaraman et al., 2021;Watson et al., 2021).Unlike it, similar to Leino and Fredrikson (2020), we consider a white-box setting where the adversary has access to the model weights and activations.Specifically in NLP, a lot of studies have been proposed (Carlini et al., 2021;Mireshghallah et al., 2022;Mattern et al., 2023;Shi et al., 2023).Carlini et al. (2021) (Gao et al., 2020;Brown et al., 2020;Dodge et al., 2021;Chowdhery et al., 2023;Anil et al., 2023;Touvron et al., 2023a,b).Without access to pre-training corpora, there are also some methods to detect dataset contamination.",
            "score": 0.6526110523958659,
            "section_title": "Related Work",
            "char_start_offset": 5150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 86,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1081
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 297,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 297,
                    "end": 315,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 315,
                    "end": 332,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 452,
                    "end": 471,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 471,
                    "end": 497,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 497,
                    "end": 520,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 563,
                    "end": 590,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 749,
                    "end": 771,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 771,
                    "end": 798,
                    "matchedPaperCorpusId": "247315260"
                },
                {
                    "start": 798,
                    "end": 819,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 837,
                    "end": 858,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 896,
                    "end": 915,
                    "matchedPaperCorpusId": "237568724"
                },
                {
                    "start": 915,
                    "end": 938,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "218502126",
            "title": "When Machine Unlearning Jeopardizes Privacy",
            "text": "We concentrate on machine learning classification, the most common machine learning task, and assume both original and unlearned models to be black-box, the most challenging setting for an adversary. \n\nWe first propose a novel membership inference attack in the machine unlearning setting that aims at determining whether the target sample is part of the training set of the original model. Different from classical membership inference attacks [60,64] which leverage the output (posteriors) of a single target model, our attack leverages outputs of both original and unlearned models. More concretely, we propose several aggregation methods to jointly use the two posteriors from the two models as our attack model's input, either by concatenating them or by computing their differences. Our empirical results show that the concatenation-based methods perform better in overfitted models, while the differencebased methods perform better in well-generalized models. \n\nSecond, in order to quantify the unintended privacy risks incurred by machine unlearning, we propose two novel pri-vacy metrics, namely Degradation Count and Degradation Rate. Both of them quantify how much relative privacy the target has lost due to machine unlearning. Concretely, Degradation Count calculates the proportion of cases for which the adversary's confidence about the membership status of the target sample is larger with our attack than with classical membership inference attack. Degradation Rate calculates the average confidence increase between our attack and classical membership inference. \n\nWe conduct extensive experiments to evaluate the performance of our attack over a series of ML models, ranging from logistic regression to convolutional neural networks, with multiple categorical datasets and image datasets. The experimental results show that our attack consistently degrades the membership privacy of the target sample, which indicates machine unlearning can have counterproductive effects on privacy. In particular, we observe that privacy is especially degraded because of machine unlearning in the case of well-generalized models. For example, we observe that the classical membership inference attack has an accuracy (measured by AUC) close to 0.5, or random guessing, on the well-generalized decision tree classifier.",
            "score": 0.6525274545621087,
            "section_title": "Introduction",
            "char_start_offset": 1880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "245837980",
            "title": "Trade-offs between membership privacy&adversarially robust learning",
            "text": "As far we are aware, the first work to specifically target membership inference of single inputs was Shokri et al. (2017). By training shadow models that learn the distribution of outputs of a target model, they show it is possible to infer membership of inputs on common image datasets (MNIST (LeCun et al. 1998) and CIFAR-10 (Krizhevsky et al. 2009)) and tabular data (ADULT dataset (Kohavi 1996)). Sablayrolles et al. (2019) then went on to show, under limited assumptions, black-box membership attacks are equivalent to white-box membership attacks. That is, they show that the Bayes optimal membership attack only inspects the loss of an input, and so devising a membership attack that can inspect internal parameters of a model should perform no better. However, in practice, whitebox membership attacks have been shown by Nasr, Shokri, and Houmansadr (2018) to outperform black-box membership attacks. Differential privacy (Dwork et al. 2006) has been touted as a potential remedy to privacy attacks (Carlini et al. 2019). However, recent work by Bagdasaryan, Poursaeed, and Shmatikov (2019) has shown that differentially private models have their own problems -differential privacy often increases average test error, however poorly represented subpopulations in the data distribution incur a much larger increase in test error, which in turn cases fairness concerns. \n\nAlthough many previous works have investigated the relationship between generalization error and robustness, the axis of interest in these works is usually adversarial robustness of the final model and the trade-off with standard generalization error. This is not a primary axis of interest in our work, we are concerned with how robust training relates to overfitting (and thus privacy) as a function of both the size of available training data and the size of used in robust training.",
            "score": 0.6519925079182459,
            "section_title": "Background",
            "char_start_offset": 6049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 121,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 835,
                    "end": 864,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 930,
                    "end": 949,
                    "matchedPaperCorpusId": "225366984"
                },
                {
                    "start": 1007,
                    "end": 1028,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1054,
                    "end": 1098,
                    "matchedPaperCorpusId": "168170081"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "67855651",
            "title": "When Relaxations Go Bad: \"Differentially-Private\" Machine Learning",
            "text": "The aim of a membership inference attack is to infer whether or not a given record is present in the training set. Membership inference attacks can uncover highly sensitive information from training data. An early membership inference attack showed that it is possible to identify individuals contributing DNA to studies that analyze a mixture of DNA from many individuals, using a statistical distance measure to determine if a known individual is in the mixture [28]. \n\nMembership inference attacks can either be completely black-box where an attacker only has query access to the target model [62], or can assume that the attacker has full white-box access to the target model, along with some auxillary information [76]. The first membership inference attack on machine learning was proposed by Shokri et al. [62]. They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62].",
            "score": 0.6504977520006684,
            "section_title": "Membership Inference",
            "char_start_offset": 21108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2058
                },
                {
                    "start": 2061,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "270559979",
            "title": "Do Parameters Reveal More than Loss for Membership Inference?",
            "text": "The most common disclosure auditing approach today is to conduct membership inference attacks (Kumar & Shokri, 2020) and related attacks that attempt to extract specific data (Cummings et al., 2024). While membership inference assumes the adversary already knows the full candidate record, it may still constitute a direct privacy risk when revealing the inclusion of a known record in the training data itself, which leaks sensitive information. In most scenarios, however, membership disclosure by itself is not a serious privacy risk, but rather used as a proxy for understanding information leakage that may result in more serious privacy violations. Membership inference is simple to define, relatively easy to measure, and aligns well with differential privacy. This has resulted in it being widely used as a method for auditing disclosure risks for machine learning (Kumar & Shokri, 2020;Yeom et al., 2020;Kazmi et al., 2024;Azize & Basu, 2024). \n\nPrior results on membership inference attacks have largely focused on the black-box setting, where the attacker only has input-output access to the target model. This focus has been reinforced by folklore and results demonstrating negligible gains from parameter access (known as white-box attacks) (Nasr et al., 2018;Carlini et al., 2022). A well-known theoretical result by Sablayrolles et al. (2019) proves that black-box access is sufficient for optimal membership inference under certain conditions. This result has been the basis of several subsequent works (Ye et al., 2022;Chaudhari et al., 2024). However, the assumptions made in its derivation do not hold for most models, including ones trained with stochastic gradient descent (SGD). This theoretical result has detered researchers from exploring more the potential for inference methods that utilize parameter access, even though the theoretical result does not apply to common machine learning settings.",
            "score": 0.6492393407680057,
            "section_title": "Introduction",
            "char_start_offset": 2388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 116,
                    "matchedPaperCorpusId": "220647336"
                },
                {
                    "start": 873,
                    "end": 895,
                    "matchedPaperCorpusId": "220647336"
                },
                {
                    "start": 895,
                    "end": 913,
                    "matchedPaperCorpusId": "208127526"
                },
                {
                    "start": 913,
                    "end": 932,
                    "matchedPaperCorpusId": "267682105"
                },
                {
                    "start": 1254,
                    "end": 1273,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 1273,
                    "end": 1294,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1331,
                    "end": 1357,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1519,
                    "end": 1536,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 1536,
                    "end": 1559,
                    "matchedPaperCorpusId": "249152113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "248157274",
            "title": "Optimal Membership Inference Bounds for Adaptive Composition of Sampled Gaussian Mechanisms",
            "text": "Membership Inference is the task of predicting whether a given sample was in the training set of a given model. Homer et al. [2008] showed the first proof of concept, and Shokri et al. [2017] showed that a wide variety of machine learning models are vulnerable to such attacks. Shokri et al. [2017] train neural networks to attack machine learning models, and measure the success of the attack by the percentage of correctly predicted (train/test) samples, or equivalently the advantage [Yeom et al., 2018]. While Shokri et al. [2017] trained neural networks to attack machine learning models, it was shown later that simple heuristics such as the loss [Yeom et al., 2018, Sablayrolles et al., 2019] is a more accurate and robust measure of membership inference. \n\nRecent works [Watson et al., 2022, Carlini et al., 2021, Rezaei and Liu, 2021] have proposed to evaluate membership inference by the precision/recall trade-off [Watson et al., 2022] or the precision at low levels of recall [Carlini et al., 2021]. In particular, such works show that some setups which were thought to be private because the membership accuracy is significantly less than 100% can actually reveal membership of a small group of samples with very high precision. \n\nThere is also a line of work developed to design algorithms to specifically defend against membership inference attacks . Although differential privacy would bound the membership inference, these empirical defenses are potentially able to achieve better utility while withstanding against existing membership inference attacks.",
            "score": 0.6476214178779381,
            "section_title": "Membership inference attacks",
            "char_start_offset": 9393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1571
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 131,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 171,
                    "end": 191,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 278,
                    "end": 298,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 487,
                    "end": 506,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 514,
                    "end": 534,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 653,
                    "end": 671,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 671,
                    "end": 699,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 778,
                    "end": 798,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 820,
                    "end": 843,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 925,
                    "end": 946,
                    "matchedPaperCorpusId": "244130249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "238259275",
            "title": "Information-theoretic generalization bounds for black-box learning algorithms",
            "text": "Some of these works show that having a white-box access to models allows constructing better membership inference attacks, compared to having a black-box access. This is analogous to our observation that prediction-based bounds are better than weight-based bounds. Shokri et al. [31] and Yeom et al. [39] demonstrate that even in the case of black-box access to a well-generalizing model, sometimes it is still possible to construct successful membership attacks. This is in line with our observation that the f -CMI bound can be significantly large, despite of small generalization gap (see epoch 4 of Fig. 1c). This suggests a possible direction of improving the f -CMI-based bounds. \n\n[39] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: Analyzing the connection to overfitting.",
            "score": 0.6471119825537233,
            "section_title": "Related Work",
            "char_start_offset": 31729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 818
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "221586480",
            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
            "text": "Different from the classical membership inference, there are two attack surfaces in the transfer learning setting: 1) determine whether a data sample is used to train the teacher model; 2) determine whether a data sample is used to train the student model. Besides, based on the capabilities of different adversaries, they may have access to either teacher model or student model. Considering two different attack surfaces and two different adversary capabilities, there are in total three types of attacks: \n\n\u2022 Attack-1. The adversaries have access to the teacher model and aims to infer the membership status of teacher dataset. This type of attack is similar to the classical membership inference attack where the target model is trained from scratch. \n\n\u2022 Attack-2. \n\nThe adversaries have access to the student model and aims to infer the membership status of teacher dataset. In this attack, the target model is not directly trained from the target dataset. \n\n\u2022 Attack-3. The adversaries have access to the student model and aims to infer the membership status of student dataset. Different from Attack-1, the target model of this attack is transferred from the teacher model. \n\nNotice that another combination that the adversaries have access to the teacher model and aims to infer the membership status of student model is infeasible, since the student model is never used for training the teacher model. \n\nSimilar to previous membership inference attacks [25,27], we assume the adversares have black-box access to either the teacher model and student model. This means that the adversaries can only query these models and obtain their corresponding posteriors. Compared to the white-box setting, where the adversaries have direct access to the architecture and parameters of the target model, the black-box setting is more realistic, and more challenging for the adversaries [20]. We further assume that the adversaries have a shadow dataset which can be used to train a set of shadow models to mimic the behavior of the target model. The shadow models are then used to generate another dataset to train the attack model. We will relax this assumption in the evaluation part.",
            "score": 0.6465143036319639,
            "section_title": "Attack Taxonomy and Threat Model",
            "char_start_offset": 10426,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1466,
                    "end": 1469,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "266090844",
            "title": "A Reinvent Survey on Machine Learning Attacks",
            "text": "Membership Inference Attacks Determining whether a sample of input data x has been included in the training set D is the aim of membership inference. The most well-known kind of attack is this one, and Shokri et al. [101] was the first to introduce it. The attack was conducted on supervised machine learning models and utilizes only the model output prediction vector (black-box). This kind of white-box attack is not without risk, especially in a collaborative environment where an opponent might launch both aggressive and passive attacks. Accurate execution of white-box membership inference attacks is possible if model parameters and gradients are supplied. [23]. In addition to their classification function, generative models like GAN or VAE are susceptible to membership inference attacks. \n\nFinally, from the perspective of the data owner, these attacks might be viewed from a fresh perspective. To determine whether the data were utilized without their permission in this situation, the data owner may be able to audit the black box mod-els. [24]. \n\n4.2 Reconstruction Attacks Rebuilding doses aim to duplicate the exercise sample or examples, as well as the training labels associated with each sample. Partial or complete reconstruction may occur. In earlier publications, techniques that attempt to recover sensitive characteristics or an entire sample of data have been described using terms such as attribute inference and model inversion assumed production tickets and incomplete information of certain structures. All of these attacks are considered to be part of the broader category of reconstruction attacks for the purposes of this survey. The phrase \"quality implication\" is charity in several contexts in the confidentiality poetry to refer to doses that use publicly available data to infer the private \"characterise-tics\" of a targeted user [25]. Since these attacks target an individual's data directly instead of ML models, they are not included in this research. True data reconstruction and the production of class representations or probabilistic values of sensitive qualities that aren't always included in the training dataset are two important areas of contrast among the works in this category. [26]. The second case in classification models is limited to situations where the classes consist mostly of one type of item, such as identical human faces. Alt-hough this bounds the usefulness of the dose; it can occasionally present an in-retesting circumstance.",
            "score": 0.6458317456268102,
            "section_title": "4.1",
            "char_start_offset": 17122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2386
                },
                {
                    "start": 2387,
                    "end": 2494
                }
            ],
            "ref_mentions": [
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "4528546"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "39174364"
                },
                {
                    "start": 2230,
                    "end": 2234,
                    "matchedPaperCorpusId": "114157961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "227209424",
            "title": "Use the Spear as a Shield: An Adversarial Example Based Privacy-Preserving Technique Against Membership Inference Attacks",
            "text": "Membership Inference Attacks. Earlier membership inference attacks were targeting at biomedical data or genomic data. For instance, some researches [15], [16] have shown that a specific individual can be identified or inferred from statistics of the genome-wide association studies (GWAS). Recently, membership inference attacks are applied to machine learning models. Shokri et al. [1] use the target model's prediction of member data and non-member data to train a membership inference model which acts as a binary classifier [1]. The generated membership inference model can infer whether or not a specific data was used to train the target model. Salem et al. [2] propose attack method with fewer assumptions, in which the attacker first uses a number of different models to mimic the behavior of the target model. Then, the attacker combines different model's predictions to perform membership inference attacks without the knowledge of the structure and the training algorithm [2]. Hayes et al. [17] train a Generative Adversarial Networks (GAN) model to learn the information of the target generative model and use the trained GAN model to distinguish member data or non-member data of the target model's training set [17]. Truex et al. [18] provide a generalized framework to launch the blackbox membership inference attacks. They indicate that the membership inference attacks are data-driven, and the attacks are transferable among different target models.\n\nDefenses. The defenses against membership inference attacks can be divided into two categories, as follows:\n\n-Change the confidence score in the prediction of the target model to a rough value. Shokri et al. [1] mitigate membership inference attacks by modifying the prediction of the target model in the following ways. The first method is that the prediction returned by the target model only contains the confidence scores of the top k classes [1]. The second method rounds (i.e., truncates) the confidence score in the prediction and returns an approximate prediction confidence [1]. The third method modifies the output layer of the neural network to reduce the correlation between the output and the input of the model [1]. Shokri et al. [1] demonstrate that these methods can slightly reduce the inference accuracy of the membership inference model, but cannot reduce the inference accuracy to 50%",
            "score": 0.6445019954628292,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 6719,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 528,
                    "end": 531,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 664,
                    "end": 667,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 983,
                    "end": 986,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1676,
                    "end": 1679,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1915,
                    "end": 1918,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2051,
                    "end": 2054,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "249712435",
            "title": "Reconstructing Training Data from Trained Neural Networks",
            "text": "Below we discuss several privacy attacks that have been extensively studied in recent years (see Liu et al. [2021], Jegorova et al. [2021] for surveys). \n\nMembership Inference. In membership-inference attacks [Shokri et al., 2017, Long et al., 2018, Salem et al., 2018, Yeom et al., 2018, Song and Mittal, 2021] the adversary determines whether a given data point was used to train the model or not. For example, if the model was trained on records of patients with a certain disease, the adversary might learn that an individual's record appeared in the training set and thus infer that the owner of the record has the disease with high chance. Note that membership inference attacks are significantly different from our attack, as the adversary must choose a specific data point. E.g., if the inputs are images, then the adversary must be able to guess a specific image. \n\nModel Extraction. In model-extraction attacks [Tram\u00e8r et al., 2016, Oh et al., 2019, Wang and Gong, 2018, Carlini et al., 2020b, Jagielski et al., 2020, Milli et al., 2019, Rolnick and Kording, 2020, Chen et al., 2021] the adversary aims to steal the trained model functionality. In this attack, the adversary only has black-box access with no prior knowledge of the model parameters or training data, and the outcome of the attack is a model that is approximately the same as the target model. It was shown that in certain cases the adversary can reconstruct the exact parameters of the target model. We note that such attacks might be combined with our attack in order to allow extraction of the training dataset in a black-box setting. Namely, in the first stage the model is extracted using model-extraction attacks, and in the second stage the training dataset is reconstructed using our attack. [Fredrikson et al., 2015] are perhaps the closest to our attack, as they consider reconstruction of input data given a trained model. These attacks aim to infer class features or construct class representatives, given that the adversary has some access (either black-box or white-box) to a model.",
            "score": 0.6442646367121599,
            "section_title": "A More Details on Privacy Attacks in Deep Learning",
            "char_start_offset": 23832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 229,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 268,
                    "end": 287,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 287,
                    "end": 311,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 921,
                    "end": 941,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 941,
                    "end": 958,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 958,
                    "end": 979,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 979,
                    "end": 1002,
                    "matchedPaperCorpusId": "212644655"
                },
                {
                    "start": 1002,
                    "end": 1026,
                    "matchedPaperCorpusId": "211858541"
                },
                {
                    "start": 1026,
                    "end": 1046,
                    "matchedPaperCorpusId": "49741763"
                },
                {
                    "start": 1046,
                    "end": 1073,
                    "matchedPaperCorpusId": "211259052"
                },
                {
                    "start": 1073,
                    "end": 1093,
                    "matchedPaperCorpusId": "248497917"
                },
                {
                    "start": 1776,
                    "end": 1801,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "232222684",
            "title": "On the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models",
            "text": "The model inversion or attribute inference definition from Yeom et al. [34] avoids this issue by defining the AI advantage as the difference between inferring attributes with the model and without the model (i.e., through the distribution). Indeed, our definitions of AI and AAI use the same approach, based on their work. Yeom et al. [34] are also the first to formally relate MI attacks to AI attacks. They also formalise the role of overfitting to the effectiveness of MI and AI attacks, a link which was previously experimentally identified and demonstrated in [25], [27]. As mentioned previously, they demonstrate that AI attacks are feasible on regression problems, with the accuracy of the attacks improving with the level of overfit. Although the AI attack performance is not as significant as the MI attack, it is still quite substantial reaching an advantage of up to 0.5 on one of the datasets [34]. We have shown that for classification problems, only approximate attribute inference seems to be feasible. Apart from [34], Jayamaran and Evans [12] have also experimentally evaluated attribute inference attacks on classification models. Even though the goal of their analysis is to evaluate privacy leakage from classification models treated with differential privacy, their results with lower privacy (higher values of the privacy parameter [3]) can be considered as closer to the non-private setting. These results also show low AI advantages as compared to MI attacks, although the authors do not delve into the reasons. \n\nAnother related area is the investigation of factors effecting membership inference. Sablayrolles et al. [24] seek the optimal strategy for membership inference and find that such a strategy depends only on the loss function, implying that, asymptotically, knowledge of the model parameters (white box setting) does not provide any benefits over black box access. However, their treatment does not explore distance-based impact on membership inference as is done in our work. Long et al. [17] explore the performance of membership inference focused on training data records which are more vulnerable, in contrast to looking at membership inference performance as an aggregate over the entire training dataset. They find that records which have fewer neighbors are more vulnerable, as their presence or absence has more influence on the model's output.",
            "score": 0.6434321463444034,
            "section_title": "Related Work",
            "char_start_offset": 61950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2389
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1354,
                    "end": 1357,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 1643,
                    "end": 1647,
                    "matchedPaperCorpusId": "174799799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "261638278",
            "title": "Deep Neural Network Quantization Framework for Effective Defense against Membership Inference Attacks",
            "text": "In membership inference attacks given sample data x, an adversary tries to infer whether x is a member of the training dataset. We consider a threat model that is consistent with prior work on MIA [21,[46][47][48]: \n\n\u2022 Access the target model: We assume the adversary could only access the target model output. This is referred to as black-box access [41].",
            "score": 0.6431946446637921,
            "section_title": "Threat Model",
            "char_start_offset": 14599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 356
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "221297671"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66552734375
        },
        {
            "corpus_id": "278339103",
            "title": "Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review",
            "text": "The demonstrated attack required white-box access to the target neural network parameters, activations, and gradients. The choice of a good threshold range for self-influence scores for members was crucial for this attack and was achieved by maximizing the balanced accuracy on the training set. \n\nKuppa and Le-Khac [92] used a different type of example-based explanation, namely, counterfactuals, for membership inference. The authors trained shadow models using counterfactual samples and auxiliary datasets. The use of a threshold, on the difference in predictions of the attack and target models, was employed to determine membership. Pawelczyk et al. \n\n[134] also targeted counterfactuals and proposed two types of attacks. The first relied on the distances between data points and their counterfactuals to differentiate between members and non-members. The second used a loss-based approach using a likelihood-ratio test [28] that improved the attack. \n\nInterpretable models using decision trees, and surrogate models created using Trepan algorithm [37], were evaluated for membership inference by Naretto et al. [126]. The authors also studied the effect of overfitting on the attack. The study found the success of membership inference higher on both interpretable and surrogate models in comparison to black-box models. The attack was also more successful on surrogates of overfitted models in comparison to well-regularized models. Membership inference in machine learning models has been explored extensively in existing literature [77] and attacks have exploited confidence scores and predictions [152]. However, the above attacks that exploit explanations, suggest that XAI interfaces provide a new avenue for adversaries to launch this attack. So far, the attack has targeted feature-based, example-based, and interpretable (including surrogates) XAI methods. Factors such as dataset type [151], dimension [134,151], model architecture [151] and overfitting [134], influence the effectiveness of this attack. Some demonstrated attacks are also possible without knowledge of the training dataset or target architectures [99].",
            "score": 0.6428075598333138,
            "section_title": "Membership Inference",
            "char_start_offset": 34338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "240002958"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1055,
                    "end": 1059,
                    "matchedPaperCorpusId": "6930911"
                },
                {
                    "start": 1119,
                    "end": 1124,
                    "matchedPaperCorpusId": "257516233"
                },
                {
                    "start": 1609,
                    "end": 1614,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1903,
                    "end": 1908,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 1920,
                    "end": 1925,
                    "matchedPaperCorpusId": "253446930"
                },
                {
                    "start": 1925,
                    "end": 1929,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 1950,
                    "end": 1955,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 1972,
                    "end": 1977,
                    "matchedPaperCorpusId": "253446930"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9541015625
        },
        {
            "corpus_id": "247595200",
            "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
            "text": "To evaluate a membership inference strategy, two groups of samples are needed: samples from the training set and samples outside the training set of the target model. The pre-trained models considered in this work are trained on 50k samples from the CIFAR-100 dataset. The remaining 10k samples constitute the test set. We perform membership inference attacks using nine different strategies. The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker. \n\nIn our analysis we consider a balanced evaluation set and report the AUROC score and the maximum accuracy achieved by each strategy. In this setting, a subset of 10k samples from the training set is selected uniformly as intraining data and the entire test set (10k samples) is selected as out-of-training data. Since the choice of the subset of the training set influences our results, the experiments are repeated 20 times, choosing a different subset each time. All the quantities reported are averaged over these 20 runs of the experiment and the error reported is the empirical standard deviation. The results of this analysis are reported in Tab. 1. For each target model, the best performing attack strategies are highlighted in boldface. Note that the upper part of the table corresponds to strategies that do not require to train an attack model, nor require any additional samples, while the bottom part corresponds to strategies that require training an attack model. The best performing strategy when no additional samples are available is the adversarial distance strategy. Note that this strategy performs consistently across all target models and even surpasses the more resource hungry strategies for the case of AlexNet and ResNext.",
            "score": 0.6424236491425327,
            "section_title": "Comparison of MIA strategies",
            "char_start_offset": 22071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "252438838",
            "title": "Privacy Attacks Against Biometric Models with Fewer Samples: Incorporating the Output of Multiple Models",
            "text": "We consider a secondary application to membership inference [37]. There are many different attacker postures for such attacks (see Papernot et al. [38] for an overview), in this work we make the standard assumption that the adversary has query access to the target model and part of the dataset that was used in training the target model. The goal of membership inference is to determine whether a particular sample x was used in the training of a network. That is, given f T\u03b1 (x i ) decide if x i was used to train the model f T . \n\nInferring whether a sample was used in training a network has privacy implications, allowing an attacker to infer a user's race, gender, or their ability to access a system. \n\nMembership inference attacks are usually conducted on classification networks. Mitigation strategies include regularization methods such as dropout and weight regularization [37]. Nasr et al. [39] study membership inference attacks in detail under many settings, they also consider model updates. Leino et al. [40] look at white box membership inference attacks and conclude that a small generalization error does not guarantee safety against attacks. Chen et al. [41] study membership inference attacks on GANs. Melis et al. [42] explore membership inference attacks with model updates. \n\nOur attack model for inferring membership is a simple neural network with 3 layers with the last layer predicting membership of the input vector. Even with this simple network for iris feature extractors membership inference is 99% accurate (Tables 3  and4). Membership inference is substantially harder on classification networks than feature extractors. \n\nAs before we compare the Rand, Concat, SR and SRwAL methods. Membership inference on classification networks using a single model is 52% accurate, but using SRwAL raises accuracy up to 62%. \n\nOrganization The rest of this work is organized as follows: Section 2 describes the system architecture, Section 3 reviews how feature extractors and classifiers are used in biometrics, Section 4 describes our attack model, Sections 5, 6 present evaluation methodology and results respectively. Section 7 presents both methods and results for membership inference. Section 8 concludes.",
            "score": 0.641576315860908,
            "section_title": "Application to Membership Inference",
            "char_start_offset": 7411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 64,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "44237208"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.935546875
        },
        {
            "corpus_id": "234785882",
            "title": "Protect privacy of deep classification networks by exploiting their generative power",
            "text": "One main category of privacy attacks consists of inference attacks, which contains membership inference attacks and attribute inference attacks. Membership inference attacks aim to infer whether an example was in the target model's training dataset, e.g., inferring whether a patient's record was used in medical research. Shokri et al. (2017) designed a black-box membership inference attack against machine learning models. Subsequently, researchers introduced several variants of the attack, such as attacks on GANs (Hayes et al., 2019), VAEs (Hilprecht & H\u00e4rterich, 2019), model explanations (Shokri et al., 2019), and collaborative learning models (Nasr et al., 2019). We focus on mitigating membership inference attacks on DNN classifiers in this paper. \n\nA well-known membership inference attack is the shadow-model attack (Shokri et al., 2017). It requires the attacker to train several shadow models and attack models. To attack a victim model, first, the attacker collects or synthesizes data from the same domain as the victim classifier's training and test data, and divides the data into several private training and test sets. Then, the attacker uses each of the private training set to train one shadow model f i shadow to mimic the behavior of the victim classifier. Next, the attacker sends all the examples in each private training and test set to its corresponding shadow model to create a dataset D that contains the tuple (y, f i shadow (x), I(x)) for each example x , where y is the class label of x , f i shadow (x) is a vector containing the outputs of the i th shadow model, and I(x) indicates whether x is used for training the shadow models. Finally, the attacker partitions D based on the examples' class labels and uses them to train one attack model per class to distinguish the training data from the others.",
            "score": 0.6411185513759401,
            "section_title": "Membership inference attacks",
            "char_start_offset": 6268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 343,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 519,
                    "end": 539,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 546,
                    "end": 575,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 653,
                    "end": 672,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 830,
                    "end": 851,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "249431594",
            "title": "Subject Membership Inference Attacks in Federated Learning",
            "text": "Our work is concerned with what adversaries can learn about training data given access to machine learning models, which may constitute a breach of a data subject's privacy. There are many attack surfaces for a machine learning model across its life-cycle, including after deployment [17], [18], [19]. \n\nPrivacy attacks are a common approach used to assess privacy risks in ML. The key benefit of this approach is that it grounds the privacy discussion concretely in terms of the training data whose privacy can be compromised by the model. These privacy attacks can include threats such as model inversion [20], [21] and property inference [9], [22], [23]. However, the majority of research has focused on membership inference attacks [1], [3]. Membership inference is a popular type of privacy attack that is highly relevant to our work: determining whether a particular data item was part of the training dataset. A successful membership inference attack concretely demonstrates privacy risks to individual data items from the attacked model's training dataset. \n\nWe particularly follow the line of prior work on membership inference attacks [24], [25], [26], which deliver an empirical lower bound to the risk of data leakage through a ML model. The adversary's accuracy in determining whether a data point is part of the training set gives a very real picture of whether the data was leaked. \n\nSeveral advancements in this line of work improve this lower bound by making attack models more accurate and applicable in more realistic scenarios [4]. In particular, white-box attacks that rely on the gradients sent during training show that these gradients reveal a lot of detail about the training data [27], [28], [29], [30], although the attacks may be defended against with appropriate strategies [31]. White-box attacks are quite plausible if the adversary is posing as a legitimate user in federated learning, and this opens up new avenues of risk [32], [33], [34] that are highly relevant to our work. \n\nWhile these advanced attacks are worth investigating to improve subject-level membership inference, they are overwhelmingly focused on membership inference of particular data points [35].",
            "score": 0.6411060346097666,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 10048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2010
                },
                {
                    "start": 2013,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 288,
                    "matchedPaperCorpusId": "235142501"
                },
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "235731711"
                },
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "208139345"
                },
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "237491948"
                },
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 652,
                    "end": 656,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 736,
                    "end": 739,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1157,
                    "end": 1161,
                    "matchedPaperCorpusId": "245222525"
                },
                {
                    "start": 1547,
                    "end": 1550,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "matchedPaperCorpusId": "195316471"
                },
                {
                    "start": 1718,
                    "end": 1722,
                    "matchedPaperCorpusId": "214728347"
                },
                {
                    "start": 1724,
                    "end": 1728,
                    "matchedPaperCorpusId": "256222486"
                },
                {
                    "start": 1803,
                    "end": 1807,
                    "matchedPaperCorpusId": "244773113"
                },
                {
                    "start": 1962,
                    "end": 1966,
                    "matchedPaperCorpusId": "232068854"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "5051282"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "202734167",
            "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
            "text": "Machine learning (ML) is transforming many aspects of our society. We consider a model provider deploys an ML classifier (called target classifier) as a black-box software or service, which returns a confidence score vector for a query data sample from a user. The confidence score vector is a probability distribution over the possible labels and the label of the query data sample is predicted as the one that has the largest confidence score. Multiple studies have shown that such black-box ML classifier is vulnerable to membership inference attacks [43,56,58,59]. Specifically, an attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts whether the data sample is a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to ML. In particular, in application scenarios where the training dataset is sensitive (e.g., biomedical records and location traces), successful membership inference leads to severe privacy violations. For instance, if an attacker knows her victim's data is used to train a medical diagnosis classifier, then the attacker can directly infer the victim's health status. Beyond privacy, membership inference also damages the model provider's intellectual property of the training dataset as collecting and labeling the training dataset may require lots of resources.\n\nTherefore, defending against membership inference attacks is an urgent research problem and multiple defenses [42,56,58] have been explored. A major reason why membership inference attacks succeed is that the target classifier is overfitted. As a result, the confidence score vectors predicted by the target classifier are distinguishable for members and non-members of the training dataset. Therefore, state-of-the-art defenses [42,56,58] essentially regularize the training process of the target classifier to reduce overfitting and the gaps of the confidence score vectors between members and non-members of the training dataset. For instance, L 2 regularization [58], min-max game based adversarial regularization [42], and dropout [56] have been explored to regularize the target classifier. Another line of defenses [1,6,12,24,30,60,66,70] leverage differential privacy",
            "score": 0.6409913590027976,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 561,
                    "end": 564,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 564,
                    "end": 567,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1581,
                    "end": 1584,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1896,
                    "end": 1900,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1900,
                    "end": 1903,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1903,
                    "end": 1906,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96923828125
        },
        {
            "corpus_id": "52211986",
            "title": "LOGAN: Membership Inference Attacks Against Generative Models",
            "text": "Long et al. [37] and Yeom et al. [70] investigate connections between membership inference and model inversion attacks against machine learning classifiers. In particular, [70] assumes that the adversary knows the distribution from which the training set was drawn and its size, and that the adversary colludes with the training algorithm. Their attacks are close in performance to Shokri et al.'s [57], and show that, besides overfitting, the influence of target attributes on model's outputs also correlates with successful attacks. Then, Tramer et al. [61] present a model extraction attack to infer the parameters from a trained classifier, however, it only applies to scenarios where the attacker has access to the probabilities returned for each class. \n\nSong et al. [58] attacks force a machine learning model to memorize the training data in such a way that an adversary can later extract training inputs with only black-box access to the model. Then, Carlini et al. [11] show that deep learning-based language models trained on text data can unintentionally memorize specific training inputs, which can then be extracted with black-box access, however, demonstrating it only for simple sequences of digits artificially introduced into the text. Ate-niese et al. [4] present a few attacks against SVM and HMM classifiers aimed to reconstruct properties about training sets, by exploiting knowledge of model parameters. \n\nAlso, recent work [2,23,41] present inference attacks against distributed deep learning [39,56]. In particular, Aono et al. [2] target the collaborative privacy-preserving deep learning protocol of [56], and show that an honest-but-curious server can partially recover participants' data points from the shared gradient updates. However, they operate on a simplified setting where the batch consists of a single data point. Also, Hitaj et al. [23] introduce a white-box attack against [56], which relies on GAN models to generate valid samples of a particular class from a targeted private training set, however, it cannot be extended to black-box scenarios.",
            "score": 0.6407241406139086,
            "section_title": "Attacks",
            "char_start_offset": 9300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 37,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 555,
                    "end": 559,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "2904063"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1447,
                    "end": 1450,
                    "matchedPaperCorpusId": "36622874"
                },
                {
                    "start": 1450,
                    "end": 1453,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 1521,
                    "end": 1524,
                    "matchedPaperCorpusId": "20714"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "36622874"
                },
                {
                    "start": 1627,
                    "end": 1631,
                    "matchedPaperCorpusId": "20714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "277104808",
            "title": "Zero-Knowledge Proof-Based Consensus for Blockchain-Secured Federated Learning",
            "text": "In this section, we evaluate the robustness of our approach against two prevalent types of privacy attacks: the membership inference attack and the model inversion attack. \n\n1) Membership Inference Attack: The target of the membership inference attack is to create a binary classifier that determines whether or not a given data sample is within In the attack method we used, attackers leverage the distribution of the target model gradients to approximate the distribution of the training data [41]. We conducted this experiment using the same settings described in the previous section but with the inclusion of at least one malicious client participating in the training each round. The malicious client attempts a membership inference attack on another local model during the model verification phase. The attack's effectiveness was evaluated by measuring the true positive rate of the binary classifier in identifying data samples from the original training data. As illustrated in Fig. 5, while adding Gaussian noise provides some level of protection by limiting the true positive rate of the attacker's estimation on the data distribution, our approach significantly enhances security. By completely concealing model parameters from unverified clients, our method ensures that any attack attempting to model the local training data would be reduced to mere random guessing. \n\n2) Model Inversion Attack: In model inversion attacks, attackers attempt to reconstruct the training dataset by exploiting the target model's parameters through techniques like gradient leakage [17] and numerical reconstruction [42]. These attacks can occur in either white-box or black-box scenarios. In whitebox attacks, the attackers have direct access to the target model's parameters. Conversely, in black-box scenarios, they do not have direct parameter access and can only interact with the model through inference queries. In our experiment, we allow the malicious client to conduct a white-box numerical reconstruction on another client's model. The attacker initiates the attack with a randomly generated dummy dataset and optimizes it in each iteration of the attack to maximize the cosine similarity between the synthesized gradient and the groundtruth gradient. To evaluate the quality of the reconstructed data, we employ a metric known as the inversion influence function (I 2 F ), as detailed in [43]. This metric quantifies the semantic distance between the recovered images and the original images.",
            "score": 0.63815171258082,
            "section_title": "C. Privacy Analysis of ZKPoT",
            "char_start_offset": 36012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2400
                },
                {
                    "start": 2401,
                    "end": 2499
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "259137851"
                },
                {
                    "start": 1577,
                    "end": 1581,
                    "matchedPaperCorpusId": "195316471"
                },
                {
                    "start": 1611,
                    "end": 1615,
                    "matchedPaperCorpusId": "214728347"
                },
                {
                    "start": 2395,
                    "end": 2399,
                    "matchedPaperCorpusId": "262217093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "195346528",
            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
            "text": "Shokri et al. [7] develop a novel and intriguing membership inference attack, and apply it to popular machine learning as-a-service APIs. Their attacks are based on so-called \"shadow models\" that approximate the behavior of the model under attack. The shadow models are used to build another machine learning model called the \"attack model\", which is trained to distinguish points in training data from otherwise based on the output they induce on the original model (under attack). As we discussed in Section 6.2, our simple threshold adversary comes surprisingly close to the accuracy of their attack, given the differences in complexity and requisite adversarial assumptions between the attacks. \n\nBecause the attack proposed by Shokri et al. itself relies on machine learning to find a function that separates training and non-training points, it is not immediately clear why the attack works, but the authors hypothesize that it is related to overfitting and the \"diversity\" of the training data. They graph generalization error against the precision of their attack and find some evidence of a relationship, although they also find that the relationship is not perfect, and conclude that model structure must also be relevant. The results presented in this paper make the connection to overfitting precise in many settings, and the colluding trainer we give in Section 6.4 demonstrates exactly how model structure can be exploited to create membership inference vulnerability. \n\nLi et al. [6] explore membership inference, distinguishing between \"positive\" and \"negative\" membership privacy. They show how this framework defines a family of related privacy definitions that are parameterized on distributions of the adversary's prior knowledge, and show that a number of previous definitions can be instantiated in this way.",
            "score": 0.6379706682709989,
            "section_title": "Privacy and machine learning",
            "char_start_offset": 70802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 17,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1495,
                    "end": 1498,
                    "matchedPaperCorpusId": "17295025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "227152059",
            "title": "When Machine Learning Meets Privacy",
            "text": "Membership inference attack refers to acquiring the knowledge about whether a certain data record ( \u00ec  \u2605 ,  \u2605 ) belongs to the model's training dataset  or not [98,141]. An illustration of such an attack can be found in Fig. 5(c). \n\nShokri et al. [141] introduced a \"black-box membership inference\" that used a shadow training technique to imitate the behavior of the target model. The trained inference model is used \"to recognize differences in the target model's predictions\" on training and non-training inputs. They also found that overfitting, the structure and type of the model are the main factors that cause a model to be vulnerable to membership inference attack. Long et al. [89] and Yeom et al. [173] investigated \"the relationship between overfitting and privacy leakage\". Salem et al. [134] proposed a membership inference attack method using an unsupervised binary classification, \"which does not need to train any shadow model and does not assume knowledge of model or data distribution\". \n\nMembership inference attacks are also studied in Generative Adversarial Networks (GANs). For example, Liu et al. [84] trained an attacker network to launch membership attacks against Variational Autoencoders (VAEs) and GANs. Hayes et al. [52] focused on \"generative models in ML-as-a-service applications and train GANs to recognize training inputs\". \n\nMelis et al. [98] studied membership inference in collaborative learning. The attack is achieved by \"analyzing periodic updates to the shared model during training\". The reason that this attack is effective is that the gradients in neural networks are based on features, \"thus observations of the participants' gradient updates can be used to infer the feature values, which are in turn based on these participants' private training data\". Wang et al. [163] considered membership inference attack \"against the user-level privacy on the federated learning framework by the attack from a malicious server. The proposed attack framework exploits GAN with a multi-task discriminator, which simultaneously discriminates category, reality and client identity of input samples, and doing so recovers user-specific private data\". \n\n3.1.4 Model Memorization Attack.",
            "score": 0.6374766360025526,
            "section_title": "Membership Inference Attack.",
            "char_start_offset": 21788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2182
                },
                {
                    "start": 2185,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 247,
                    "end": 252,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 708,
                    "end": 713,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 800,
                    "end": 805,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "53099247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "We use two membership inference attack methods, namely black-box attack and white-box attack, to attack the target model. For our PPGF, we leverage the generated new data on the three databases to train the target classification model, and utilize the two kinds of membership inference attacks to test. Note that, we choose ResNet-18 model for MNIST, FashionMNIST, CelebA as for the limitation of GPU memory.",
            "score": 0.6370529982614997,
            "section_title": "Security of PPGF",
            "char_start_offset": 35895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "257378365",
            "title": "Can Membership Inferencing be Refuted?",
            "text": "Membership inference (MI) attack is currently the most popular test for measuring privacy leakage in machine learning models. Given a machine learning model, a data point and some auxiliary information, the goal of an MI attack is to determine whether the data point was used to train the model. In this work, we study the reliability of membership inference attacks in practice. Specifically, we show that a model owner can plausibly refute the result of a membership inference test on a data point $x$ by constructing a proof of repudiation that proves that the model was trained without $x$. We design efficient algorithms to construct proofs of repudiation for all data points of the training dataset. Our empirical evaluation demonstrates the practical feasibility of our algorithm by constructing proofs of repudiation for popular machine learning models on MNIST and CIFAR-10. Consequently, our results call for a re-evaluation of the implications of membership inference attacks in practice.",
            "score": 0.636007128939238,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "Machine learning has enabled a wide range of applications in areas like computer vision, machine translation, health analytics, and advertising, among others. The fact that many compelling applications of this technology involve the collection and processing of sensitive personal data has given rise to concerns about privacy [2,4,7,10,11,25,32,37,45,46]. In particular, when machine learning algorithms are applied to private training data, the resulting models might unwittingly leak information about that data through either their behavior or the details of their structure and parameters. \n\nTwo particular attacks have emerged as concrete threats: model inversion and membership inference. In a model inversion attack, the adversary uses a machine learning model and incomplete information about a data point to infer the missing information for that point. For example, the adversary might be given partial information about an individual's medical record, and attempt to infer the individual's genotype by using a model trained on similar medical records [11]. \n\nTraining data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25]. \n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data.",
            "score": 0.6358516350948444,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 330,
                    "end": 332,
                    "matchedPaperCorpusId": "14444919"
                },
                {
                    "start": 332,
                    "end": 334,
                    "matchedPaperCorpusId": "8921975"
                },
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 337,
                    "end": 340,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1467,
                    "end": 1471,
                    "matchedPaperCorpusId": "17295025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "270559979",
            "title": "Do Parameters Reveal More than Loss for Membership Inference?",
            "text": "Membership inference attacks are used as a key tool for disclosure auditing. They aim to infer whether an individual record was used to train a model. While such evaluations are useful to demonstrate risk, they are computationally expensive and often make strong assumptions about potential adversaries' access to models and training environments, and thus do not provide tight bounds on leakage from potential attacks. We show how prior claims around black-box access being sufficient for optimal membership inference do not hold for stochastic gradient descent, and that optimal membership inference indeed requires white-box access. Our theoretical results lead to a new white-box inference attack, IHA (Inverse Hessian Attack), that explicitly uses model parameters by taking advantage of computing inverse-Hessian vector products. Our results show that both auditors and adversaries may be able to benefit from access to model parameters, and we advocate for further research into white-box methods for membership inference.",
            "score": 0.6355190022991516,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "244345608",
            "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
            "text": "Machine learning algorithms are under increasing scrutiny from regulatory authorities, due to their usage of large amount of sensitive data. In particular, vulnerability to membership inference attacks [11,29] -which allow determining if a specific data instance was part of the model's training set -is highlighted as a potential confidentiality violation and privacy threat to training data by organizations such as the ICO (UK) and NIST (US) [21]. Also, there is a tight connection between the underlying notion of privacy risk in membership inference and differential privacy [6,13,24,37]. Therefore, using membership inference analysis for privacy auditing and data protection impact assessment (DPIA) in machine learning systems is gaining traction. However, despite the plethora of membership inference attacks in the literature, and regardless of their reported empirical performance, the attacks and their associated risk measurement approaches are fundamentally limited.\n\nLack of consistency/comparability: The focus in the literature is in celebrating the empirically evaluated success of new attacks, even though different attacks could be incomparable as the meaning of their success rates is dependent on specific assumptions about the adversary's uncertainty about the target, instead of (or in addition to) the inherent leakage of the model. As we show in Section 4.6, attacks that are seemingly similar (e.g., they all test the loss or output of model on data) can be actually measuring different notions of leakage. Also, oversimplifying the notions of leakage to average-case and worst-case leakage, without formally specifying the random experiments in the membership indistinguishability games, can lead to more confusion. Thus, it is crucial to formalize attacks and their uncertainty in a consistent framework to develop meaningful metrics for their comparisons (Section 5. 3",
            "score": 0.6354406705517127,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 580,
                    "end": 583,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "219686822"
                },
                {
                    "start": 586,
                    "end": 589,
                    "matchedPaperCorpusId": "231583084"
                },
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1897,
                    "end": 1898,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "251881571",
            "title": "SNAP: Efficient Extraction of Private Properties with Poisoning",
            "text": "instance, poisoning of private models enables auditing of private machine learning to infer lower bounds on the privacy budget [35], [36]. Also, the most recent membership inference attack [37], with higher success rate than previous works [22], [23], is constructed with the help of data poisoning. Property Inference. Property inference attacks aim to learn global information of the training data distribution from an ML model, in contrast to attacks that leak information about individuals, such as reconstruction or membership inference attacks. Introduced by Ateniese et al. [4], these attacks were formalized as a distinguishing game between two worlds, where different fractions, t 0 and t 1 , of the sensitive data were used to train an ML model [5]. Property inference attacks can either be classified as white-box attacks [1], [4], [5], in which the adversary has knowledge of the model architecture and parameters, or black-box attacks [2], [3], in which the attacker can query the trained ML model to receive either model confidences or labels. Initial property inference attacks were designed for Hidden Markov Models and Support Vector Machines [4], while most of the recent papers propose attacks on deep neural networks, including feed-forward neural networks [1]- [3], convolutional neural networks [5], federated learning models [38], generative adversarial networks (GAN) [39], and graph neural networks [40]. Mahloujifar et al. [3] showed that data poisoning can help property inference attacks achieve higher success.\n\nWe address this limitation by proposing SNAP, a more efficient property inference attack. SNAP uses a distinguishing test designed by following rigorous theoretical analysis to achieve higher success than previous work while also being more efficient.",
            "score": 0.6354141017445019,
            "section_title": "Related Work",
            "char_start_offset": 9178,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "219686822"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "231583084"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "247922814"
                },
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 755,
                    "end": 758,
                    "matchedPaperCorpusId": "237491948"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 838,
                    "end": 841,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "237491948"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "235485024"
                },
                {
                    "start": 953,
                    "end": 956,
                    "matchedPaperCorpusId": "231718884"
                },
                {
                    "start": 1160,
                    "end": 1163,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 1277,
                    "end": 1280,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1282,
                    "end": 1285,
                    "matchedPaperCorpusId": "231718884"
                },
                {
                    "start": 1317,
                    "end": 1320,
                    "matchedPaperCorpusId": "237491948"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "244117091"
                },
                {
                    "start": 1449,
                    "end": 1452,
                    "matchedPaperCorpusId": "231718884"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83544921875
        },
        {
            "corpus_id": "218571404",
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "text": "Inference Attacks. The inference attacks against machine learning can be divided into model inference and data inference attacks. In model inference attacks, an attacker could infer the parameters [54], hyper-parameters [55], architecture [37] and functionality [40] of a target model. In data inference attacks, the attacker aims at inferring information about the data that the target model operates on. These attacks include membership inference attack [49], model inversion attack (input inference) [14,60], attribute inference [15,58], statistics inference [2] and side-channel attack [57]. \n\nIn this paper, we concentrate on data inference attacks, notably membership inference attack and model inversion attack. Existing defense methods mainly focused on membership inference attacks [24,28,35,49]. Little has been studied about the model inversion attack. Xiao et al. [59] studied the adversarial reconstruction problem where they aim to prevent the latent representations from being decoded into the original input data. To this end, they regularized the encoder with an adversarial loss from a decoder. They studied the face attribute prediction model which outputs 40 binary facial attributes. Our paper, on the contrary, studies black-box classification models whose output is constrained by a probability distribution wherein the values sum up to 1. Moreover, they did not consider the adversarial scenario where the attacker has no access to the same data distribution as the original training data. \n\nGeneral Membership Inference Attack. Membership inference attack is performed to determine whether a given data sample is part of a target dataset. It is not limited to machine learning models. Homer et al. [21] proposed one of the first membership inference attacks in the biomedical setting on genomic data. Some studies also performed membership inference attacks on other biomedical data such as MicroRNA [3] and DNA methylation [17]. Pyrgelis et al. [45,46] further showed that it is possible to perform membership inference attack on location datasets as well. Shokri et al. [49] performed membership inference attack in the machine learning setting which is the same setting in this paper. \n\nSecure & Privacy-Preserving Machine Learning.",
            "score": 0.6353622692549203,
            "section_title": "RELATED WORK",
            "char_start_offset": 55003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2212
                },
                {
                    "start": 2215,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "3423242"
                },
                {
                    "start": 239,
                    "end": 243,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "54457412"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 562,
                    "end": 565,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "3920864"
                },
                {
                    "start": 791,
                    "end": 795,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 798,
                    "end": 801,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1723,
                    "end": 1727,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 1925,
                    "end": 1928,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 1949,
                    "end": 1953,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 1971,
                    "end": 1975,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 2097,
                    "end": 2101,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "189928475",
            "title": "Reconciling Utility and Membership Privacy via Knowledge Distillation",
            "text": "Membership inference is a serious privacy concern for machine learning models [15], [28], [40], [25]. Consider a machine learning model \u03b8 and a data sample (x, y). The goal of a membership inference adversary is to infer whether (x, y) belongs to the dataset used to train the model \u03b8. The membership inference attack exploits the memorization of training data by large neural networks by inspecting various features of the target trained model. Therefore, the standard approach for the membership inference adversary is to train an inference model, h, whose goal is to classify data samples into members and non-members. \n\nLet \u03b8 be the target model and h : F(X, Y, \u03b8) \u2192 [0, 1] be the inference model. For a given data sample (x, y), the inference adversary evaluates F(X, Y, \u03b8), which is a combination of different features of \u03b8 related to (x, y), for instance, \u03b8's prediction on the record [40], [30], [28], the loss function on the record, the gradients of the loss [31], [28], etc. Based on this input feature vector F(X, Y, \u03b8), the output of h is the probability that (x, y) has been a member of \u03b8's training set. Let Pr D (X, Y) and Pr \\D (X, Y) be the conditional probabilities of the members and non-members, respectively. For the above setting, the expected gain of the inference model can be computed as: \n\n[log(h(F(x, y, \u03b8)))] \n\nIn practice [15], [28], [40], [25], [31], [30], the inference adversary only knows a (small) subset of the members D, i.e., she only knows D A \u2282 D and has access to enough nonmembers D A required to train h. Therefore, the adversary computes an empirical gain as: \n\nwhich is used to get the inference model: \n\nIn (5), the two summations compute the empirical gain of inference model on the subset of members and non-members that the adversary has.",
            "score": 0.6344607347889766,
            "section_title": "C. Membership inference attack setting",
            "char_start_offset": 12450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 84,
                    "end": 88,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 90,
                    "end": 94,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 96,
                    "end": 100,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 975,
                    "end": 979,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1358,
                    "end": 1362,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 1376,
                    "end": 1380,
                    "matchedPaperCorpusId": "54444175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "253654618",
            "title": "Membership Inference Attacks on Machine Learning Model",
            "text": "Machine learning(ML) models today are vulnerable to several types of attacks. In this work, we will study a category of attack known as membership inference attack and show how ML models are susceptible to leaking secure information under such attacks. Given a data record and a black box access to a ML model, we present a framework to deduce whether the data record was part of the model\u2019s training dataset or not. We achieve this objective by creating an attack ML model which learns to differentiate the target model\u2019s predictions on its training data from target model\u2019s predictions on data not part of its training data. In other words, we solve this membership inference problem by converting it into a binary classification problem. We also study mitigation strategies to defend the ML models against the attacks discussed in this work. In this paper evaluation method on real world datasets: (1) CIFAR-10 and (2) UCI Adult (Census Income) using classification as the task performed by the target ML models built on these datasets.",
            "score": 0.633327813147104,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "226300898",
            "title": "Securing Machine Learning in the Cloud: A Systematic Review of Cloud Machine Learning Security",
            "text": "In a typical membership inference attack, for given input data and black box access to the ML model, an attacker attempts to figure out if the given input sample was the part of the training set or not. To realize a membership inference attack against a target model, a classification model is trained for distinguishing between the predictions of the target model against the inputs on which it was trained and that those on which it was not trained (Shokri et al., 2017).",
            "score": 0.631969265564104,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 23190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 473
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 472,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "208158041",
            "title": "Privacy Leakage Avoidance with Switching Ensembles",
            "text": "The spectacular successes of machine learning in recent years have also brought significant scrutiny in its highprofile security vulnerabilities. Although the focus has been mostly on adversarial attacks, which were successfully constructed for a variety of applications [3,4,18,21,22,16,23,13,25], another group of security issues was recently identified in machine learning concerning various forms of privacy attacks. It was shown that it is possible to extract information about the training data by analyzing or querying a machine learning model [1,8,7,29,2,28]. \n\nIn this paper, we focus on a type of attack commonly referred to as membership inference attack. A successful membership inference attack can determine, with better than random success rate, if a given sample belongs to the training set when given access to a machine learning model [27]. This type of attack was introduced in [27], and later [17] proposed a method of providing privacy guarantees while taking into account the accuracy of the defended classifier. The attack [27] leverages the differences in the target model's predictions (confidence values) on the samples it was trained on versus the samples which were outside of training data for detecting if a sample was part of the training data. Even with such limited information, this attack can be effective and remains effective for a variety of privacy defense methods [27]. \n\nThere have been many recent research advances in the area of membership inference attacks. While [27] makes several assumptions that weaken the scope of membership attacks such as knowledge of the type of model and knowledge of certain aspects of the training data, [24] is able to generate successful attacks with weaker assumptions. In [26], membership attacks exploit interpretable machine learning output to get extra information about the training data. Even unsupervised learning is susceptible as [9] has described successful membership attacks in case of generative adversarial networks. Perhaps the most interesting research has been applying differential privacy [6] as a way to guarantee a defense against membership attacks. The PATE algorithm [5,20] uses a teacher-student framework to protect any machine learning algorithm from a broad range of privacy attacks, including membership attacks, with differential privacy guarantees.",
            "score": 0.6307713480057482,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "18716873"
                },
                {
                    "start": 274,
                    "end": 276,
                    "matchedPaperCorpusId": "10022042"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "654993"
                },
                {
                    "start": 554,
                    "end": 556,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 556,
                    "end": 558,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2085,
                    "end": 2088,
                    "matchedPaperCorpusId": "207178262"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90771484375
        },
        {
            "corpus_id": "250048543",
            "title": "Debiasing Learning for Membership Inference Attacks Against Recommender Systems",
            "text": "Recently, membership inference attacks (MIAs) have achieved promising performance in various domains, such as biomedical data [2,13,18] and mobility traces [37]. The goal of membership inference attacks is to infer the membership of individual training samples for a target model. Shokri et al. [46] specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. [42] further relax several key assumptions from [46], including knowledge of the target model architecture and target dataset distribution. Yeom et al. [52] explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. [35] study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks [9,29] assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage. In addition, Zhang et al. [53] investigate MIA against recommender systems, leveraging the differences between user history behaviors and output items from recommenders. \n\nTo mitigate the attacks, some defense mechanisms, including model stacking [42], dropout [42], adversarial training [34], differential privacy [9,29], regularization [9,29], and jointly maximizing privacy and prediction accuracy [19], have been proposed. To protect membership privacy of recommender systems, Zhang et al. [53] design a defense mechanism, named Popularity Randomization, and randomly recommend popular items to non-member users.",
            "score": 0.6307494461875687,
            "section_title": "Membership inference attacks",
            "char_start_offset": 5381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "52205573"
                },
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1097,
                    "end": 1100,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1100,
                    "end": 1103,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1507,
                    "end": 1511,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1534,
                    "end": 1538,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1561,
                    "end": 1564,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1564,
                    "end": 1567,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1587,
                    "end": 1590,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1647,
                    "end": 1651,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "237532289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "273962909",
            "title": "On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models",
            "text": "The aim of membership inference attacks is to determine if a specific target sample was included in the training dataset of a designated model [5,10,20,40,50]. Such attacks represent a notable risk to data privacy and have consequently attracted significant attention [17,37,51,58,59]. \n\nApproach. MIAs can be categorized based on the design approach of the attack model into two types: classifier-based techniques and metric-based techniques. Classifier-based methods aim to determine if a particular sample being analyzed is part of the training set of the target model by creating a binary classifier. \n\nFrequently employed methods include the use of shadow model training [50]. Conversely, metric-based approaches distinguish between member and non-member instances by examining differences in various specific metrics. These metrics encompass prediction loss [57], confidence vectors [49], among others [52]. \n\nAdversarial Knowledge. Adversarial knowledge refers to the information an attacker has about a target model, which greatly impacts the effectiveness and complexity of the attack. In the majority of attacks, an attacker is typically restricted to the model's output data. However, in scenarios where the attacker is the model publisher, such as during privacy evaluations in the model training phase, the attacker may possess full knowledge of the model. Attacks are categorized based on the attacker's knowledge level into black-box and white-box attacks [20,42]. In black-box attacks, the attacker can only access the model's confidence scores [50] or just the hard labels [10,31,48]. Conversely, in white-box attacks, the attacker has access to detailed computational data regarding a particular sample [40]. This encompasses examining the model's internal architecture and operations, intermediate computation, gradient details, model loss, as well as all the information accessible in black-box situations.",
            "score": 0.6298178826782951,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 8186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "119302127"
                },
                {
                    "start": 272,
                    "end": 275,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 676,
                    "end": 680,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1471,
                    "end": 1475,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 1475,
                    "end": 1478,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1594,
                    "end": 1597,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "matchedPaperCorpusId": "54444175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "220525609",
            "title": "A Survey of Privacy Attacks in Machine Learning",
            "text": "To summarize the attacks proposed against machine learning privacy, Table 1 presents the 42 papers analyzed in terms of adversarial knowledge, model under attack, attack type, and timing of the attack. \n\nIn terms of model types, 83.3% of the papers dealt with attacks against neural networks, with decision trees being the second most popular model to attack at 11.9% (some papers covered attacks against multiple model types). The concept of neural networks groups together both shallow and deep models, as well as multiple architectures, such as convolutional neural networks, recurrent neural networks, while under SVMs we group together both linear and nonlinear versions. \n\nThe most popular attack types are membership inference and reconstruction attacks (35.7% of the papers, respectively), with model extraction the next most popular (31%). The majority of the proposed attacks are performed during the inference phase (88%). Attacks during training are mainly on distributed forms of learning. Black-box and white-box attacks were studied in 66.7% and 54.8% of the papers, respectively (some papers covered both settings). In the white-box category, we also include partial white-box attacks. \n\nThe focus on neural networks in the existing literature as well as the focus on supervised learning is also apparent in Figure 4. The figure depicts types of machine learning algorithms versus the types of attacks that have been studied so far based on the existing literature. The list of algorithms is indicative and not exhaustive, but it contains the most popular ones in terms of research and Table 1. Summary of papers on privacy attacks on machine learning systems, including information of their assumptions about adversarial knowledge (black / white-box), the type of model(s) under attack, the attack type, and the timing of the attack (during training or during inference). The transparent circle in the Knowledge column indicates partial white-box attacks.",
            "score": 0.6290891115311549,
            "section_title": "Summary of Attacks",
            "char_start_offset": 59760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "254125728",
            "title": "Purifier: Defending Data Inference Attacks via Transforming Confidence Scores",
            "text": "Machine learning has been provided as a service by many platforms, transforming various aspects of daily life such as handling users' sensitive data. Users access these models through prediction APIs which return a confidence score or a label. Many studies have indicated that the prediction information of a sample could be exploited to perform data inference attacks to get information about this sample (Shokri et al. 2017;Yang et al. 2019;Song and Shmatikov 2020). Data inference attacks could be largely divided into two categories. The first kind of attack aims at inferring distributional information about a class by observing the prediction changes of different samples (An et al. 2022;Mehnaz et al. 2022), while the second kind of attack is to infer the individual information of a sample by observing its specific output such as the membership inference attacks (Nasr, Shokri, and Houmansadr 2018;Salem et al. 2018;Hui et al. 2021;Yeom et al. 2018;Li and Zhang 2021;Li, Li, and Ribeiro 2021), adversarial model inversion attacks (Yang et al. 2019) and attribute inference attacks (Song and Shmatikov 2020). In this paper, we focus on the second type of data inference attack. \n\nAmong these data inference attacks, membership inference attack (Shokri et al. 2017) is one of the most important and exemplary attacks, where the adversary is asked to determine whether a sample is in the target model's training set. Many studies acknowledge that the confidence scores tell more prediction information beyond the label and thus they should be provided in the output. Therefore, a number of approaches have been proposed to defend the membership inference attack while preserving the confidence scores (Shokri et al. 2017;Salem et al. 2018;Nasr, Shokri, and Houmansadr 2018;Abadi et al. 2016;Jia et al. 2019;Tang et al. 2022). On the other hand, some studies believe that removing the confidence information in the output is a way of defending the membership inference attack.",
            "score": 0.6281914745471429,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 406,
                    "end": 426,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 426,
                    "end": 443,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 443,
                    "end": 467,
                    "matchedPaperCorpusId": "167217888"
                },
                {
                    "start": 679,
                    "end": 695,
                    "matchedPaperCorpusId": "247313542"
                },
                {
                    "start": 695,
                    "end": 714,
                    "matchedPaperCorpusId": "246241051"
                },
                {
                    "start": 873,
                    "end": 908,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 908,
                    "end": 926,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 926,
                    "end": 942,
                    "matchedPaperCorpusId": "44108074"
                },
                {
                    "start": 959,
                    "end": 977,
                    "matchedPaperCorpusId": "3617652"
                },
                {
                    "start": 977,
                    "end": 1002,
                    "matchedPaperCorpusId": "221297671"
                },
                {
                    "start": 1040,
                    "end": 1057,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 1091,
                    "end": 1116,
                    "matchedPaperCorpusId": "167217888"
                },
                {
                    "start": 1253,
                    "end": 1272,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1708,
                    "end": 1728,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1728,
                    "end": 1746,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1746,
                    "end": 1780,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1780,
                    "end": 1798,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1798,
                    "end": 1814,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "233783448",
            "title": "A survey on adversarial attacks and defences",
            "text": "Ateniese et al. [40] elaborated on gathering relevant information from machine learning classifiers using a meta-classifier. Given the black-box access to a model (e.g. via public APIs) and a training data, an attacker may be interested in knowing whether that data was part of the training set of the model. They experimented with a speech recognition classifier that uses hidden Markov models and extracted information such as accent of the users which was not supposed to be explicitly captured. \n\nAnother inference attack presented by Shokri et al. [37] is membership inference attack, which determines if a data point belongs to the same distribution as the training dataset. This attack may fall under the category of non-adaptive or adaptive black-box attacks. In a typical black-box environment, the attacker uses a datapoint to query the target model and obtains its output which is a vector of probabilities that specifies whether the data point belongs to a certain class. \n\nFor training the attack model, a set of shadow models is built. Since the adversary has the knowledge of whether a given record belongs to the training set or not, supervised learning can be employed and the corresponding output labels are then fed to attack model to train it to distinguish shadow model's outputs. \n\nFigure 7 illustrates the end-to-end attack process. The output vectors obtained from shadow model are added to the training dataset of the attack model. The shadow model is also queried using a test dataset and added to the training dataset of the attack model. Thus a collection of target attack models is trained by utilizing the black-box behaviour of the shadow models. The authors used membership attacks on classification models trained by commercial players like Amazon and Google who provide 'ML as a service'.",
            "score": 0.6280260439767849,
            "section_title": "| Inference attack",
            "char_start_offset": 21454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 557,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "244345608",
            "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
            "text": "In the membership inference game, the adversary can observe the output of a target machine learning model (trained on unknown private dataset ) and a precise target data point as input, and is expected to output 0 or 1 to guess whether the sample is in the dataset or not. In this section, we construct and evaluate attacks under the following assumptions.\n\n(1) Black-box access: the adversary could only access the output function of the target model instead of white-box parameters. In terms of signal function for the black-box membership inference attack, in this paper, we solely consider loss values. This is because, under binary hypothesis test formulation of membership inference attack game on general targets (Definition 3.1), we observe that the most powerful criterion for choosing among hypothesesthe likelihood ratio test (LRT), is approximately comparing the loss of target to a constant threshold (under certain assumptions and approximation steps). . . .\n\nThen under certain assumptions (in Appendix A) the Likelihood Ratio Test (LRT) approximately equals\n\nwhere could be an arbitrarily constant threshold.\n\nWe provide a detailed proof in Appendix A. It should be noted that Lemma 4.1 is not an optimality guarantee, because the assumptions may not hold in practical settings, and the approximations are only accurate when the dataset is large enough. Instead, Lemma 4.1 is an explanation of why (under certain assumptions and approximations) the loss-threshold based attacks are powerful in principle, which is also the reason that we focus on loss-based attacks. 1 As discussed in Section 3, attack success on general targets is not enough for auditing more specific types of privacy loss, where we actually want attacks that succeed on a specific target model or a worst-case data record. To this end, the general attack equation 3 with constant threshold is overly general and may have suboptimal performance, as it fails to capture the individual patterns for atypical targets. This motivates us to design attacks with model-dependent and record-dependent thresholds, such that the thresholds are optimized under more specific target-dependent inference games (Definition 3.2, 3.3 or 3.4). These target-dependent thresholds and inference games model reduced uncertainty of the attacker about its target (which is reflected by the fixed random seeds",
            "score": 0.6279503140493454,
            "section_title": "CONSTRUCTING BLACK-BOX MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 22981,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1583,
                    "end": 1584,
                    "matchedPaperCorpusId": "1154437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "245423308",
            "title": "Evaluating Differentially Private Generative Adversarial Networks Over Membership Inference Attack",
            "text": "The final inferences for specific data are made by arranging the data according to the results of the shadow discriminator. That is, the attacker can determine that data was included in the training dataset of the target generative model if the data is ranked at the upper position (relatively high probability) in the results of the shadow discriminator. In [8], the authors showed that the ideal white-box attacker who can access the discriminator of the target model can perfectly infer membership (100% attack accuracy). In the case of a black-box scenario, they showed that a general attacker can achieve up to 63% attack accuracy, and an informed attacker who has auxiliary knowledge of the training dataset can improve the attack success rate (these outcomes are the experimental results on a facial dataset, and we deal with the same dataset as our experimental data in section 5). \n\nAs an extended approach of the attack against the generative model, Hilprecht et al. [9] proposed a Monte Carlo integration-based membership inference attack in terms of distance metric between training and generated data. The intuition of this approach is that the overfitted generator tends to output synthetic data close to the training data that the target model has learned. In [9], the authors considered blackbox scenarios and showed that an attacker can conduct membership inference on specific data in the Monte Carlo method by comparing the distance with synthetic data obtained from the target model. \n\nFrom the perspective of distance metric, Chen et al. [10] proposed a more sophisticated method of attack by subdividing the attacker's knowledge about the generator of the target GAN model. They assumed black-box and white-box scenarios for the generator, and considered that the adversary can access the latent code that is the input of the model. With these assumptions, they realized a more sophisticated distance-based membership inference attack by reconstructing the synthetic data as close as possible to the target data that the attacker wants to know about membership. Note that in the case of the white-box scenario for the generator, an attacker can obtain data closer to the target data through an optimization process on the input space of the generator (e.g., gradient descent).",
            "score": 0.6279143778837002,
            "section_title": "A. MEMBERSHIP INFERENCE ATTACK AGAINST GENERATIVE MODEL",
            "char_start_offset": 19613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "221203089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9453125
        },
        {
            "corpus_id": "218571404",
            "title": "Defending Model Inversion and Membership Inference Attacks via Prediction Purification",
            "text": "The third network operates on the concatenation of the output of the first two networks and predicts the membership. They assume that the attacker has a subset of the members and non-members of the target model's training data, and thus they do not train shadow models. \n\nMore settings of membership inference attack have been studied in the literature. For examples, Nasr et al. [36] proposed membership inference attack in the white-box setting, where the attacker computes the gradients of the white-box target model with respect to the given data sample as features for membership inference. There are also research efforts on membership inference attack in federated learning [32,36] and against generative models [18]. In this paper, we consider membership inference attack in the blackbox setting against standalone centralized classification models. \n\nModel Inversion Attack. Model inversion aims to reconstruct the input data from its confidence scores predicted by the target model. Fredrikson et al. [14] proposed a method to infer a representative sample of a training class against a white-box target model. It casts the inversion task as an optimization problem in the input domain to find the best representative for a given class. \n\nYang et al. [60] recently proposed a model inversion attack in the black-box setting. Specifically, they train a separate inversion model on an auxiliary dataset which acts as the inverse of the target model. The inversion model takes the confidence scores of the target model as input and tries to reconstruct the original input data. Their experimental results showed significant improvement of the inversion accuracy over previous works. They also performed accurate inversion attacks against real-world commercial facial recognition services. Some other methods were proposed to infer sensitive attributes [15,19,58,63] or statistical information [2] about the training data instead of reconstructing the specific input data. There are also studies [20] of inversion attack in the setting of federated learning where the attacker has white-box access to the global model. In this paper, we concentrate on model inversion attack that aims to reconstruct the input data in the black-box setting against standalone centralized classification models.",
            "score": 0.6270515809082542,
            "section_title": "Data Inference Attacks",
            "char_start_offset": 13563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 685,
                    "end": 688,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "207941008"
                },
                {
                    "start": 1859,
                    "end": 1863,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1863,
                    "end": 1866,
                    "matchedPaperCorpusId": "52916090"
                },
                {
                    "start": 1866,
                    "end": 1869,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 1900,
                    "end": 1903,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 2002,
                    "end": 2006,
                    "matchedPaperCorpusId": "5051282"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "214623088",
            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
            "text": "A recent thread of research has shown that machine learning (ML) models memorize sensitive information of training data, indicating serious privacy risks [4,9,10,15,35,39,40]. In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not [39,47]. It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3,33]. For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. [39] conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models [7,12,14,45], to differentially private models [18,34], to decentralized settings where the models are trained across multiple users without sharing their data [28,30], and to white-box settings where the adversary also has the access to the target model's architecture and weights [30]. \n\nTo mitigate such privacy risks, several defenses against membership inference attacks have been proposed. Nasr et al. [29] propose to include membership inference attacks during the training process: they train the target model to simultaneously achieve correct predictions and low membership inference attack accuracy by adding the inference attack as an adversarial regularization term. Jia et al. [19] propose a defense method called MemGuard which does not require retraining the model: the model prediction outputs are obfuscated with noisy perturbations such that the adversary cannot distinguish between members and non-members based on the perturbed outputs. Both papers show that their defenses greatly mitigate membership inference privacy risks, resulting in attack performance that is close to random guessing.",
            "score": 0.6269022402962346,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 157,
                    "end": 159,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "91184074"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "2904063"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "3570809"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1063,
                    "end": 1066,
                    "matchedPaperCorpusId": "202539472"
                },
                {
                    "start": 1066,
                    "end": 1069,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1069,
                    "end": 1072,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1072,
                    "end": 1075,
                    "matchedPaperCorpusId": "201126909"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1114,
                    "end": 1117,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1227,
                    "end": 1230,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1345,
                    "end": 1349,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1471,
                    "end": 1475,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "259064156",
            "title": "TMI! Finetuned Models Leak Private Information from their Pretraining Data",
            "text": "Membership-inference attacks [34] aim to determine whether or not a given individual's data record was present in a machine learning model's training dataset. These attacks represent a fundamental privacy violation that has a direct connection to differential privacy. Mounting these attacks and learning whether or not an individual was part of a sensitive dataset can serve as the basis for more powerful attacks. For example, prior work has used MI as a step in extracting training data [15]. Because of their simplicity, MI attacks are also a popular way to audit machine learning models for privacy leakage [35,54,62]. \n\nThese attacks been extensively studied with two types of adversarial access: black-box query access and white-box access to the machine learning model's parameters [40]. The query access setting has been more thoroughly studied, with attacks spanning several different machine learning domains, such as classification [13,53,58,62,63], natural language generation [13,58], and federated learning [47]. Despite there being extensive work on black-box attacks and prior work on MI attacks on pretrained encoders [43], continuously updated models [38], and distilled models [36], there are few works that explore MI in the transfer learning setting where a pretrained model is finetuned on a new task. Zou et al. [69] study MI attacks that target individuals in the finetuning dataset, and Hidano et al. [33] explore ways in which an adversary can leverage control over the transfer learning process to amplify the success of MI attacks on the original model. No works have studied black-box MI attacks on the pretraining dataset of a finetuned machine learning model.",
            "score": 0.6266560105020397,
            "section_title": "Membership-Inference Attacks",
            "char_start_offset": 14040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1691
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 619,
                    "end": 622,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "247922814"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 994,
                    "end": 997,
                    "matchedPaperCorpusId": "247922814"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1136,
                    "end": 1140,
                    "matchedPaperCorpusId": "237290083"
                },
                {
                    "start": 1170,
                    "end": 1174,
                    "matchedPaperCorpusId": "258721801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "249152113",
            "title": "Membership Inference Attack Using Self Influence Functions",
            "text": "Machine learning (ML) algorithms have advanced tremendously over the past decade and have been commonly used for a variety of tasks, including privacy sensitive application, such as medical imaging [21,2], conversations [8], face recognition [34], and financial information [7]. Most of these models are trained using sensitive user data which can be leaked later by an adversary from the models' parameters [32]. \n\nMembership inference (MI) attacks aim to infer whether or not a specific sample was used to train a target ML model. This information can be detrimental if it falls to the wrong hands. For example, consider an ML model trained on blood tests of HIV patients, for predicting their reaction to a corona vaccine. If an adversary somehow obtains a patient's medical record, she can only observe the patient's blood reading and query the model for the predicted reaction, but she cannot deduce if the patient has HIV. However, if the adversary infers that the record was used to train the model, then she would know the patient has HIV. If this adversary is a health insurance company, it might increase the patient's insurance premium. \n\nMany MI attacks make use of the class probability vector (or logits) at the output of the target model [19,29,32,36], since deep neural networks (DNNs) often tend to exhibit over-confidence for samples from their training set [26], a phenomenon that is largely attributed to overfitting [36]. More recent studies do not assume access to model probability vectors and still achieve state-of-the-art (SOTA) MI accuracy by relying on the final predicted labels at the model output [4,20]. MI attacks can operate under two threat model settings: white-box or black-box. The white-box setting assumes that the adversary has full information on the target model's architecture, parameters, activations, training process, and training data distribution. On the other hand, the black-box setting is more restrictive, allowing the adversary access only to the target model's input and outputs. All the aforementioned MI attacks use the black-box setting.",
            "score": 0.626009243402941,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "53753063"
                },
                {
                    "start": 202,
                    "end": 204,
                    "matchedPaperCorpusId": "232240293"
                },
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "2814088"
                },
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "65600212"
                },
                {
                    "start": 1263,
                    "end": 1266,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1628,
                    "end": 1631,
                    "matchedPaperCorpusId": "220831381"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "278130013",
            "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics",
            "text": "An active field of research exists on empirical inference attacks against machine learning models. Attacks typically rely on the loss of the target model [50] and shadow models [42,41,10]. The first inference attacks against machine learning models were attribute inference attacks (also referred to in the literature as model inversion attacks [41]). Fredrikson et al. [20] proposed the first methodology and AIA against an ML model, inferring a patient's genetic marker from a model trained to predict the dosage of a drug. The attack was later extended from a black-box to a white-box attack by Mehnaz et al. [32]. The correlation between the genetic marker and the dosage of the drug has, however, raised concerns about the validity of some of the results and the extent to which it proved that an information leakage was happening [30]. Jayaraman et al. [26] formalized the issue and showed that popular black-box attacks often do not perform better than a baseline that uses these correlations to impute the values of the sensitive attribute. We refer to this as the imputation issue. Recent work on AIA against ML models, often image models, addressed this issue by evaluating the attack performance inferring the sensitive attributes with the model and without the model [52]. In line with AIA against tabular datasets protected by query-based system (below) we address this issue by uniformly randomizing the sensitive attributes. \n\nSoon after the first attribute inference attack was proposed, Shokri et al. proposed the first membership inference attack against machine learning models [42] and coined the term shadow models. This has become an active field of research with attacks such as RMIA [51] and LiRA [8] and a focus on evaluating the performance of inference attacks on the most vulnerable users by reporting TPR at low FPR [8] which we follow. \n\nEmpirical attacks are often used to argue that models are not privacy-preserving by default, emphasizing the need for privacy-preserving measures, and to estimate the privacy risk in practice by models trained with DP privacy guarantees, typically using DP-SGD [2]. DP-SGD theoretical guarantees are indeed typically not tight, often overestimating the strength of the attacker [36,33,45].",
            "score": 0.6259713163705013,
            "section_title": "Attacks Against Machine Learning Models",
            "char_start_offset": 37676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "246241051"
                },
                {
                    "start": 859,
                    "end": 863,
                    "matchedPaperCorpusId": "252089942"
                },
                {
                    "start": 1279,
                    "end": 1283,
                    "matchedPaperCorpusId": "232222684"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1845,
                    "end": 1848,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 2129,
                    "end": 2132,
                    "matchedPaperCorpusId": "207241585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "269756904",
            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
            "text": "Membership inference attack (MIA) [4] is the common name for the methods to determine whether or not a particular example was presented in the training dataset of the given target model.In the original work, the authors train multiple shadow models to mimic the behavior of the target model and use its outputs to train an auxiliary classifier to predict the membership status of the data samples.The black-box methods for membership inference use the information about the loss of the target model [19], [5], [11], [15], labels [6], [20], or the functions of the loss value [13], [9].Among the other approaches, there are ones using quantile regression [16], knowledge distillation [15], [21], neighbourhood comparison [22] and parameter regularization [23].\n\nIn [9], the authors argue that a method to MIA as a classification problem should be evaluated by computing the values of true positive rate (TPR) at low values of false positive rate (FPR) instead of classic average-case metrics (e.g., accuracy or the area under the ROC curve).They introduce Likelihood Ratio Attack (LiRA), an approach to membership inference attacks via statistical hypothesis testing.Namely, given the target model f trained on an unknown dataset D and a sample of interest (x, y), they perform hypothesis testing, H 0 vs H 1 .\n\nHere H 0 : (x, y) \u2208 D and H 1 : (x, y) / \u2208 D. Specifically, using the shadow models, they estimate the distribution of the models' confidence to assign x to its ground truth class y.The authors improve their method by querying on multiple augmented versions of the data point x.\n\nAfter being shaped in [17], knowledge distillation has been successfully integrated in different areas [24], [25], [26], [27], [28], [29].However, in the field of membership inference attack, only the specific direction of knowledge distillation, called self-distillation [30], has been utilized so far.In this setting, the teacher model and student model(s) have identical architectures.",
            "score": 0.6255070029401238,
            "section_title": "III. RELATED WORK",
            "char_start_offset": 8753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 759
                },
                {
                    "start": 761,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1166
                },
                {
                    "start": 1166,
                    "end": 1309
                },
                {
                    "start": 1311,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1589
                },
                {
                    "start": 1591,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 37,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "259375769"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "257378537"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "258967264"
                },
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "258170547"
                },
                {
                    "start": 764,
                    "end": 767,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1613,
                    "end": 1617,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1718,
                    "end": 1722,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1863,
                    "end": 1867,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "250089283",
            "title": "On the amplification of security and privacy risks by post-hoc explanations in machine learning models",
            "text": "In this attack, the goal of an adversary is to determine if a given data sample was used to train an ML model. Successful membership inference attacks can compromise user's privacy. For instance, if a machine learning model is trained on the data collected from people with certain characteristics, by knowing that a victim's data sample belongs to the training data of the model, the attacker can immediately learn sensitive information about the victim [23], [24]. Notation: Let x \u2208 R n be the input, and F (x) denote the victim model. D train is the dataset used to train F (x). D out contains the set of x that are not in D train . Adversary Access: The attacker can provide x to the blackbox model and obtain the prediction F (x) and its explanation E(x) in a black-box setting. Table V summarizes the additional requirements to perform membership inference attacks for existing attack methods [7], [24]- [26]. Attack Objective: Given a sample x, the attacker aims to infer whether the sample belongs to the dataset D train or D out . \n\nA. Our Attack: Revisiting MIA Using Explanations For data points that are in the training set, the victim model is certain about the prediction as the training algorithms are designed to push the points away from the decision boundary. In a white-box setting, this is indicated by a relatively low loss score on training samples, which is used to determine membership in the training data. Consequently, the gradient values computed using the loss should also be fairly small for the training samples. This intuition is used in OPT-var [7], to mount MIA in black-box settings when explanations are available. They show that variance of the explanation E(F, x) produced from the logit score F (x) can indicate membership. \n\nWe build on the intuition in OPT-var [7] but note that the logit score does not completely capture the victim model's prediction confidence. The reason is that the logit score of a particular class does not take into account the logit scores of other classes. On the other hand, the softmax function of the logit consider the interaction of classes and the corresponding gradient may contain richer membership information.",
            "score": 0.6247164672655472,
            "section_title": "IV. MEMBERSHIP INFERENCE ATTACK (MIA)",
            "char_start_offset": 16664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1762
                },
                {
                    "start": 1765,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2187
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1578,
                    "end": 1581,
                    "matchedPaperCorpusId": "220514835"
                },
                {
                    "start": 1802,
                    "end": 1805,
                    "matchedPaperCorpusId": "220514835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "276902979",
            "title": "Privacy Auditing of Large Language Models",
            "text": "Membership inference attacks are also used in auditing the privacy of differentially private models. The effectiveness of privacy auditing hinges on the selection of optimal \"canaries\". We introduce new methods for generating easy-to-memorize input space canaries, and use these to improve the performance of existing privacy auditing methods and obtain tighter empirical bounds on privacy leakage. We provide the first privacy audit for the black-box setting for LLMs. Our audit achieves a non-trivial lower bound of \u03b5 \u2248 1 for a model trained to an upper bound of \u03b5 = 4. \n\nPrivacy Attacks in Machine Learning. Membership Inference (Shokri et al., 2017;Choquette-Choo et al., 2021;Carlini et al., 2022a;Jagielski et al., 2023a), attribute inference (Yeom et al., 2018;Fredrikson et al., 2015), and data extraction (Carlini et al., 2019;2023a;b;Biderman et al., 2023a;Tirumala et al., 2022;Mireshghallah et al., 2022;Huang et al., 2022;Lukas et al., 2023;Jagielski et al., 2023b;Ippolito et al., 2023;Anil et al., 2023;Kudugunta et al., 2023;Panda et al., 2024a) are the three main attacks on privacy in machine learning. Our attacks are based on membership inference, and require the logprobs of the model to compute the loss. Morris et al. (2024); Carlini et al. (2024) show that it is still possible for the attacker to access the logprobs when the logprobs are not directly available. Although we do not consider data extraction in this work, membership inference can lead to data extraction by using knowledge of the \"outlier\" token to iteratively guide decoding. We believe that using our method to improve existing data extraction attacks is an interesting future direction. \n\nMembership Inference Attacks on LLMs.",
            "score": 0.6235916224077502,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1720
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 653,
                    "end": 681,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 681,
                    "end": 703,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 703,
                    "end": 727,
                    "matchedPaperCorpusId": "257378537"
                },
                {
                    "start": 768,
                    "end": 792,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 814,
                    "end": 836,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 844,
                    "end": 867,
                    "matchedPaperCorpusId": "258291763"
                },
                {
                    "start": 867,
                    "end": 889,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 889,
                    "end": 916,
                    "matchedPaperCorpusId": "256461422"
                },
                {
                    "start": 916,
                    "end": 935,
                    "matchedPaperCorpusId": "249063119"
                },
                {
                    "start": 935,
                    "end": 954,
                    "matchedPaperCorpusId": "256459554"
                },
                {
                    "start": 954,
                    "end": 978,
                    "matchedPaperCorpusId": "250243645"
                },
                {
                    "start": 978,
                    "end": 1000,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 1018,
                    "end": 1041,
                    "matchedPaperCorpusId": "261682406"
                },
                {
                    "start": 1041,
                    "end": 1061,
                    "matchedPaperCorpusId": "268231023"
                },
                {
                    "start": 1227,
                    "end": 1247,
                    "matchedPaperCorpusId": "265445033"
                },
                {
                    "start": 1249,
                    "end": 1270,
                    "matchedPaperCorpusId": "268357903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "244130249",
            "title": "On the Importance of Difficulty Calibration in Membership Inference Attacks",
            "text": "Modern applications of machine learning often involve training models on sensitive data such as health records and personal information. Unfortunately, recent studies have found that these models can memorize their training data to a large extent, compromising the privacy of participants in the training dataset (Fredrikson et al., 2014;2015;Shokri et al., 2017;Carlini et al., 2019). One prominent category of privacy attacks against machine learning is the so-called membership inference attack (Shokri et al., 2017;Yeom et al., 2018), where the adversary aims to infer the participation of an individual in the target model's training set. Such an attack is undoubtedly damaging when the status of participation itself is considered sensitive, e.g., if the training dataset consists of health records of cancer patients. Moreover, the ability to infer membership can be viewed as a lower bound for the model's degree of memorization (Yeom et al., 2018), which is useful in itself as an empirical quantifier of privacy loss (Jagielski et al., 2020;Nasr et al., 2021). \n\nThe efficacy of membership inference attacks has been improved significantly since the first attempts (Salem et al., 2018;Sablayrolles et al., 2019;Leino & Fredrikson, 2020). However, the most common evaluation metric, attack accuracy, overlooks the crucial factor of the false positive rate (FPR) of non-members (Rezaei & Liu, 2021). Indeed, most attacks operate by first defining a membership score s(h, z) for the model h and a target input-label pair z = (x, y) that measures how much h memorized the sample z. The attack subsequently selects a threshold \u03c4 and predicts that z is a member if and only if s(h, z) > \u03c4 . For typical choices of the membership scoring function, there is usually a large overlap in the distribution of s(h, z) between members and non-members (see Figure 1a). As a result, an attack that determines the membership of z by thresholding on s(h, z) will have a high FPR.",
            "score": 0.6230814398508412,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 338,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 338,
                    "end": 343,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 343,
                    "end": 363,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 363,
                    "end": 384,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 498,
                    "end": 519,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1195,
                    "end": 1221,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1221,
                    "end": 1246,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1386,
                    "end": 1406,
                    "matchedPaperCorpusId": "235719329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "67856763",
            "title": "Killing Four Birds with one Gaussian Process: Analyzing Test-Time Attack Vectors on Classification",
            "text": "To conclude the background, we specify the adversary that we consider in our study. We consider three different attackers. All operate at test time, when training is completed. In general, the attacker might have full knowledge of the model (white-box), where she can access each parameter of the trained algorithm. The gray-box setting considers information about the algorithm applied, where the parameters or weights are unknown. In the black-box model, the attacker has no knowledge about the targeted model, not even which algorithm is used. \n\nAnalogously, we have to take into account the knowledge the attacker has about the data. The training data might be fully known. If access is restricted, the attacker might have learned the kind and/or the number of features. In a more difficult setting, nothing except the task is known. In the following, we summarize the attacker by setting. \n\nEvasion. The formal analysis shows a general contradiction between learning and security against evasion attacks, which is independent from a specific attacker. Given further the results from [17] and [12] for the vulnerability in white box settings, we investigate the influence of decision function curvature towards a black-box attacker concerning the model with full knowledge concerning the data. \n\nModel Stealing, Model extraction, and Membership Inference. In a GP, both attacks can be analytically formulated, allowing to determine the exact number of queries needed given differing knowledge of the attacker. In the empirical study, we investigate whether we can speed up the attacks when we leverage learning and partial knowledge about the data. Concerning the model, we assume a gray-box attacker who is only aware that a GP is applied. In case of model stealing, we further consider the training data to be known or not known (e.g., gray-box or black-box). For membership inference, we take a different approach: we assume a very strong attacker (in an effort to study a worst case scenario) who has some labeled training data from the model. We use this knowledge to study which factors affect the success of the attacker.",
            "score": 0.6229764089321294,
            "section_title": "Threat Model",
            "char_start_offset": 12438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2132
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66259765625
        },
        {
            "corpus_id": "269353759",
            "title": "Machine learning security and privacy: a review of threats and countermeasures",
            "text": "A membership inference attack is another privacy attack that infers the victim model and extracts its training data, privacy settings, and model parameters. In this type of attack, the adversary has access to query the victim model under attack and can analyze the output gathered from the queried results. The adversary can regenerate the training dataset of the targeted adversarial machine learning model by analyzing the gathered queried results. The attack surface for membership inference attacks on machine learning is highlighted in Fig. 5. \n\nOverall, machine learning model processing is at high risk of adversarial attacks. Machine learning pertains to several security and privacy vulnerabilities that exist and are exploitable at various layers of the machine learning modeling process that must be addressed adequately to mitigate adversarial attacks on machine learning models.",
            "score": 0.6221742426843477,
            "section_title": "Membership inference attack",
            "char_start_offset": 8383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 891
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "248810845",
            "title": "Evaluating Membership Inference Through Adversarial Robustness",
            "text": "Deep learning is being used in many fields [1,2,3,4], some of these areas are security and privacy sensitive, such as face recognition [5,6], medical diagnosis [7,8], code analysis [9,10,11], security incident prediction [12,13], and intrusion detection [4,14]. In the deep learning domain, a large amount of data generally leads to better performance. Therefore, it is common to use a huge amount of data to train deep learning models. Although this trait of deep learning can result in better performance, there can also be some severe consequences. For instance, the usage of data that contains private and sensitive information can ultimately lead to privacy issues. Such privacy risks associated with deep learning start to raise ethical and security concerns among the society [15,16]. \n\nOne of the most studied attacks for privacy leakage is known as the membership inference attack. The adversary's goal in this type of attack is to figure out if a certain piece of data was utilized in training or not. For example, if a model has been trained using patient data, one can use the membership inference attack to associate a particular patient with diseases of certainty if the attack reveals that the very piece of patient data has been used for training. \n\nVarious studies show that deep learning models are vulnerable to such privacy attacks [17,18,19,20,21,22,23,24]. The work in [17] first suggested using a neural network to perform membership inference as a binary classification task via the black-box access of the victim neural model. It is further demonstrated in [18] that membership inference is mainly caused by overfitting, and some metrics and thresholds were proposed for better inference. Subsequent works [19,20,21,22] further evaluated privacy risks caused by membership inference in a white-box manner. Furthermore, the work in [22] revealed that by using suitable metrics, metric-based attacks can achieve competitive performance for membership inference when compared with neural classifier [17]. In this paper, we focus on metric-based white-box membership inference attacks for classification neural models. \n\nFrom another line of research, adversarial examples of deep neural models have been extensively studied [25,26,23,24,27,28].",
            "score": 0.6217884294882968,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 46,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 46,
                    "end": 48,
                    "matchedPaperCorpusId": "3849381"
                },
                {
                    "start": 48,
                    "end": 50,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 50,
                    "end": 52,
                    "matchedPaperCorpusId": "209054093"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "7811489"
                },
                {
                    "start": 138,
                    "end": 140,
                    "matchedPaperCorpusId": "206592766"
                },
                {
                    "start": 163,
                    "end": 165,
                    "matchedPaperCorpusId": "15315839"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "230611063"
                },
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "222097224"
                },
                {
                    "start": 187,
                    "end": 190,
                    "matchedPaperCorpusId": "51974758"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "69533043"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "44107660"
                },
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "209054093"
                },
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "16774809"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1356,
                    "end": 1359,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1359,
                    "end": 1362,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1362,
                    "end": 1365,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1365,
                    "end": 1368,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1368,
                    "end": 1371,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1374,
                    "end": 1377,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1582,
                    "end": 1586,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1731,
                    "end": 1735,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1735,
                    "end": 1738,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1738,
                    "end": 1741,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1741,
                    "end": 1744,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1856,
                    "end": 1860,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 2021,
                    "end": 2025,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "255569927",
            "title": "White-box Inference Attacks against Centralized Machine Learning and Federated Learning",
            "text": "Come in for Nasr [25] et al's study for the privacy risk of deep learning, this paper used a white box member reasoning attack simulation experiment, aimed to explore in the machine deep learning environment, centralized machine learning and federal learning setting, for the level of information leakage and affect the level of the model, and will compare the two learning mechanism characteristics. This paper will be respectively for centralized machine learning and federated learning model member inference attack, using stochastic gradient descent optimizer, model gradient vector as an important attack index, the final simulation results can show the white box member inference attack model for the two target model membership inference accuracy, namely the current learning model in what leaked their own training data.Our evaluation directions include: supervised attacks and unsupervised attacks, inference attacks on the model and its newer versions, passive member reasoning attacks, and active member inference attacks. The experimental results evaluate the model performance based on the member inference attack accuracy score and the True / False positive (ROC) curve. The first part of this paper introduces the privacy rights in machine learning and the existing attack and defense strategies for machine learning. The second part of the related knowledge, the list of the discussed problems and simulation experiments of the background knowledge and the function basis and its definition. The third part, the model and algorithm, shows the workflow and fundamentals of the white-box member inference attack model used in this paper.The fourth part of white box inference attack performance evaluation introduces the experimental related work setting and process, model attack performance evaluation index and attack simulation results for centralized machine learning model and federated learning model.",
            "score": 0.6217355687089196,
            "section_title": "Study content and chapter arrangement",
            "char_start_offset": 6935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "237598322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90869140625
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "White-Box Knowledge. White-box knowledge characterizes scenarios where the training data for F t is sampled from a constrained population or in a skewed fashion such that an adversary has access to some versions of real data in the training data D of the target model F t but not the complete training set D. For example, a noisy version of the real data may be accessible which resembles D with the addition of some noise or missing values [41]. Adversaries with white-box knowledge can therefore develop or access true \"in\" samples and employ active learning techniques on these known samples to develop a very accurate dataset to mirror D. \n\nThe adversary with white-box knowledge is the most powerful adversary whereas the adversary with only black-box knowledge represents the most difficult attack environment where the adversaries are limited to (i) publicly availbale information, (ii) black-box queries to the prediction API, and (iii) the output of classification prediction from the target model F t . This is the setting we use to formulate membership inference attacks and to characterize adverse effects and divergence of membership inferences.",
            "score": 0.6213053982367209,
            "section_title": "Threat Model and Assumptions",
            "char_start_offset": 12961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1158
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "10488675",
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "text": "We have designed, implemented, and evaluated the first membership inference attack against machine learning models, notably black-box models trained in the cloud using Google Prediction API and Amazon ML. Our attack is a general, quantitative approach to understanding how machine learning models leak information about their training datasets. When choosing the type of the model to train or a machine learning service to use, our attack can be used as one of the selection metrics. \n\nOur key technical innovation is the shadow training technique that trains an attack model to distinguish the target model's outputs on members versus non-members of its training dataset. We demonstrate that shadow models used in this attack can be effectively created using synthetic or noisy data. In the case of synthetic data generated from the target model itself, the attack does not require any prior knowledge about the distribution of the target model's training data. \n\nMembership in hospital-stay and other health-care datasets is sensitive from the privacy perspective. Therefore, our results have substantial practical privacy implications.",
            "score": 0.6211462855192722,
            "section_title": "X. CONCLUSIONS",
            "char_start_offset": 64679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
            "score": 0.6211322742444431,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.978515625
        },
        {
            "corpus_id": "259224343",
            "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models",
            "text": "A membership inference attack is defined as follows. We consider an adversary A that aims to infer whether a single data point x was included in the training set D of a generative model M . The attacker has no knowledge about the dataset D and is only able to query the model M . We distinguish three scenarios according to the attacker's capabilities. \n\n\u2022 In the black-box scenario, an adversary queries a generative model with a text prompt and gets a generated image. The attacker has no knowledge about the model architecture and no access to its weights. \n\n\u2022 In the grey-box scenario an adversary has access to the visual and text encoders of the attacked model. Thus, they are able to calculate the latent representation (embedding) of a given image and text prompt. However, the attacker still has no access to the model weights. \n\n\u2022 Finally, in the white-box scenario, an adversary has full access to the model, its source code and trained weights. \n\nWe start with a baseline white-box model loss threshold attack, which is based on the fact that machine learning models learn by minimising the loss in the training samples. We extend our analysis by covering metrics related to model inference. Moreover, we introduce new white-box attack methods and show that they outperform the commonly used baseline method. We also describe and evaluate a grey and a black-box scenario. The attacks are based on the intuition that generative models tend to synthesise samples similar to their training set. For all attacks, we evaluate a variant in which the losses are obtained as the average of 5 losses (5 different passes through the model, each time with a different noise) following the findings of [3]. We explore more attack methods in the Appendix D.",
            "score": 0.6206835696494767,
            "section_title": "Threat model",
            "char_start_offset": 22697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 1702,
                    "end": 1705,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94140625
        },
        {
            "corpus_id": "234482894",
            "title": "Privacy Inference Attacks and Defenses in Cloud-based Deep Neural Network: A Survey",
            "text": "Then these computations as well as the true label y were used to train the attack inference classifier. In terms of the attack model, it was comprised of convolutional neural network (CNN) as well as the fully connected network (FCN), followed by a fully connected encoder. For supervised learning, the single value of encoder's output characterizes an encoding of the membership knowledge, i.e. P r(x, y) \u2208 D. For unsupervised learning, a decoder was deployed to reproduce the key features of the attack model. Then, the input of the target model was grounded into two clusters, i.e., member and non-member. To evaluate the validity of the proposed white-box MInf, they conducted the experiments on CIFAR-100. The extensive experimental results showed that the best model, i.e., DenseNet model with 82% classification accuracy was not brittle to black-box attacks with a 54.5% inference attack accuracy. However, it was vulnerable to the designed white-box inference attack with the accuracy of 74.3%. It demonstrated that the even well-generated classifiers might reveal a large quantities of information concerning training dataset. \n\nComparisons and Insights. Membership inference attacks are mainly performed by using two techniques: shadow model training [103], [108] and threshold-choosing method [113]. The first approach is to imitate the target classifier's behaviors by introducing shadow model training and then further develop an attack model to distinguish the member status of an individual sample. The second method is to exploit some confidence-threshold and statistics-based methods to seek a proper threshold to decide whether a given data sample is part of the target classifier's training dataset. Table V presents five current membership inference attack works from the respect of the experimental dataset, access mode, prior knowledge, and techniques. It is not difficult to find out from Table V, a majority of membership inference attacks are in the blackbox scenario [103], [108], [113]. That is to say, the attacker can only observe the model predictions.",
            "score": 0.6203163484769718,
            "section_title": "D. Membership Inference Attack",
            "char_start_offset": 56786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1135
                },
                {
                    "start": 1138,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 1268,
                    "end": 1273,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1304,
                    "end": 1309,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 2000,
                    "end": 2005,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2007,
                    "end": 2012,
                    "matchedPaperCorpusId": "165163934"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "235210365",
            "title": "Generative Adversarial Networks",
            "text": "Membership inference attacks can be launched towards a machine learning and/or deep learning model to determine if a specific data point is in the given model's training dataset or not [114]. \n\nTypically, after a model is trained, an attacker feeds data into the model and gets the corresponding prediction results that can be used as additional knowledge to perform black-box membership inference attacks. Such an attack will cause privacy leakage and even other severe consequences. \n\nFor instance, with a patient's medical records and a predictive model trained for a disease, an attacker can know whether the patient has a certain disease by implementing membership inference attacks. To defend against such attacks, the techniques of GAN, anonymization, and obfuscation have been exploited to design countermeasures [4,29].",
            "score": 0.6194525476141389,
            "section_title": "Membership Privacy",
            "char_start_offset": 34124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 828
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 190,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "13754095"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "2468323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "251719178",
            "title": "Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models",
            "text": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to infer whether an input sample was used to train the model. Over the past few years, researchers have produced many membership inference attacks and defenses. However, these attacks and defenses employ a variety of strategies and are conducted in different models and datasets. The lack of comprehensive benchmark, however, means we do not understand the strengths and weaknesses of existing attacks and defenses. We fill this gap by presenting a large-scale measurement of different membership inference attacks and defenses. We systematize membership inference through the study of nine attacks and six defenses and measure the performance of different attacks and defenses in the holistic evaluation. We then quantify the impact of the threat model on the results of these attacks. We find that some assumptions of the threat model, such as same-architecture and same-distribution between shadow and target models, are unnecessary. We are also the first to execute attacks on the real-world data collected from the Internet, instead of laboratory datasets. We further investigate what determines the performance of membership inference attacks and reveal that the commonly believed overfitting level is not sufficient for the success of the attacks. Instead, the Jensen-Shannon distance of entropy/cross-entropy between member and non-member samples correlates with attack performance much better. This gives us a new way to accurately predict membership inference risks without running the attack. Finally, we find that data augmentation degrades the performance of existing attacks to a larger extent, and we propose an adaptive attack using augmentation to train shadow and attack models that improve attack performance.",
            "score": 0.6182724782539426,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "269756904",
            "title": "GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation",
            "text": "Recent studies [1], [2], [3] have shown that machine learning models are used to memorize the training data and, hence, have a vast spectrum of data-driven vulnerabilities, such as membership inference attacks [4], [5], [6], [7], [8], [9], [10], [11], [12].Informally, a membership inference attack is a procedure to determine whether a particular data sample was a part of the training dataset of the given target neural network.Given the data sample (x, y), the majority of membership inference attacks ( [9], [13], [14], [15], [16]) are based on designing a specific statistic s((x, y)) to distinguish between the models trained on a sample (x, y) and those which were not.In other words, given H 1 as the class of models which have (x, y) in their training set, and H 2 as the class of models which did not see (x, y) during training, the statistics s ((x, y)|H 1 ) and s ((x, y)|H 2 ) are assumed to have substantially different distributions.If the difference is significant, these statistics can then be used to determine to which class the target model belongs, H 1 or H 2 .\n\nThe efficiency of such membership inference attack methods depends on the design of classes H 1 and H 2 .The most widespread approach to design H 1 and H 2 is to train shadow models [4], [5], [9], [10].Usually, shadow models are of the same architecture as the target model f , and they are trained on the random data sampled from the same distribution D, which was used to train the target model f .These shadow models are then used to compute the values of the test statistics s((x, y)|H 1 ) and s((x, y)|H 2 ), so they have to be sufficiently different from each other.To estimate the densities of the test statistics, an attacker has to train a lot of shadow models (in the work of [9], between 64 and 256 were used).",
            "score": 0.6180387944134417,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1082
                },
                {
                    "start": 1084,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1484
                },
                {
                    "start": 1484,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 18,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "231991101"
                },
                {
                    "start": 25,
                    "end": 28,
                    "matchedPaperCorpusId": "266149496"
                },
                {
                    "start": 210,
                    "end": 213,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 220,
                    "end": 223,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 240,
                    "end": 244,
                    "matchedPaperCorpusId": "252992499"
                },
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "260793490"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "244130249"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "259224343"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "244345608"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "259375769"
                },
                {
                    "start": 1266,
                    "end": 1269,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1271,
                    "end": 1274,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 1276,
                    "end": 1279,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1281,
                    "end": 1285,
                    "matchedPaperCorpusId": "252992499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "232233426",
            "title": "Membership Inference Attacks on Machine Learning: A Survey",
            "text": "In contrast, our survey on MIAs differs from them significantly in scope and depth. Instead of covering all the privacy attacks, we focus only on MIAs given that they have emerged recently and are of great interest to the research community due to their high likelihood of compromising the privacy of training data. Unlike the existing reviews that select a very limited number of publications related to MIAs, e.g., only eight references are included in [114], we conduct a comprehensive search and include more than 100 related works in this survey. Our survey work offers deeper discussion on the concepts, theories, methods, categorization with taxonomies, and visions of future research directions. The main contributions of this paper are: \n\n(1) Comprehensive Review. To the best of our knowledge, this is the first work to provide a comprehensive review of membership inference attacks and defenses on ML models. We summarize most, if not all, the published and pre-print works (over 100 papers) before September 2021. In this work, we establish novel taxonomies for membership inference attacks and defenses, respectively, according to various criteria. (2) Taxonomies of Membership Inference on ML Models. There are already over a hundred papers published in this domain. A list of all papers can help but is not good enough for readers to quickly understand the similarity and differences among membership inference attacks and among membership inference defenses. To this end, we categorize all existing works of MIAs based on different target ML models, adversarial knowledge, attack methods, training algorithms, and task domains, respectively. For membership inference defenses, we categorize all existing works based on different techniques. More details of the taxonomies are given in Fig. 8 and Fig. 9. (3) Challenges and Future Directions. Membership inference attacks on machine learning models is an active and ongoing area of research. Based on the literature reviewed, we have discussed the challenges yet to be solved and proposed several promising future directions for membership inference attacks and membership inference defenses, respectively, to inspire interested readers to explore this field in more depth. (4) Datasets and Metrics. To help researchers conduct empirical studies on membership inference attacks and defenses, we summarize most, if not all, the datasets and metrics that have been used in previous work.",
            "score": 0.6171835205437384,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2450
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "277954663",
            "title": "A synthetic dataset of French electric load curves with temperature conditioning",
            "text": "\u2022 Black-box membership inference attack: Membership inference attacks are fundamental privacy attacks of machine learning models, trying to predict if a particular sample has been seen during training. They are the foundations for stronger attacks (Carlini et al., 2022). In a black box scenario, the attacker has only access to queries to the target model under attack, and is given a set x 1 , . . . x n+m of n + m samples of which n are known to be from the training set. We perform a simple attack: the attacker decides to classify as train the samples with score below a threshold, and as test otherwise. In this blackbox scenario, the score is the minimum distance between the target sample and any synthetic sample. \n\n\u2022 White-box (loss) membership inference attack: assumes that the attacker has access to the trained machine learning model -although in our case, the model is not publicly available. \n\nThe attack is similar to blackbox, except that the score is the reconstruction error of the samples by the autoencoder (Hayes et al., 2017). \n\n\u2022 Three-sample Maximum Mean Discrepancy Test: initially, Bounliphone et al. (2016) introduced a statistical test of relative similarity to determine which of two models generates samples that are significantly closer to a real-world reference dataset of interest. The test statistics are based on differences in maximum mean discrepancies (MMDs). Here, we use the test to assess whether the train set is significantly closer to the generated samples than the test set. In other words, our three samples are: the synthetic dataset and the original train and test sets. We perform the test in the space of daily profiles instead of one-year curves because of the cost of MMD computation and since profiles are easier to discriminate. The null hypothesis is thus H 0 : the distribution of synthetic daily profiles is closer to test daily profiles than train daily profiles. See (Bounliphone et al., 2016) for more details on the implementation of the test. Rejecting H 0 would imply that there is statistical evidence that the generative model has overfitted the train set, which would be a breach of privacy.",
            "score": 0.616661546206785,
            "section_title": "Privacy metrics",
            "char_start_offset": 16191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 270,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1110,
                    "end": 1135,
                    "matchedPaperCorpusId": "3173270"
                },
                {
                    "start": 1928,
                    "end": 1954,
                    "matchedPaperCorpusId": "3173270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93310546875
        },
        {
            "corpus_id": "260237376",
            "title": "PrivMon: A Stream-Based System for Real-Time Privacy Attack Detection for Machine Learning Models",
            "text": "The goal of membership inference attacks (MFAs) is to infer whether or not a given sample is used for training a target model. While only recovering the information about membership in a training set, such attacks still trigger privacy concerns. For example, the knowledge of whether a person's data is used for training a model predicting the dosage for a certain disease can help infer whether the person has that disease. The key idea to enable MFAs is to leverage the overfitting property of ML models-they tend to exhibit different prediction behaviors for training data and those unseen during training. \n\nThe first MFA was proposed in [38] under a black-box setting, where authors used a machine learning model (referred to as an attack model) to infer membership of a target sample based on the target model's prediction confidence vector on that sample. To train the attack model, the authors assumed the knowledge of a target model architecture and training data distribution. Followup works [16,29,33,37,39] introduced various techniques to improve the attack performance and further relaxed assumptions made in [38], such as having knowledge of the target model structure or getting a dataset from the same distribution as the target model's training data, to make the attack setting more realistic. Recent studies [9,30] started to investigate MFAs with label-only access to the target model, which can be categorized into two types: the boundary-based attack and the data-augmentation-based attack. The general idea is based on the observation that if an input query is a member of the training set, it is further away from the decision boundary than a non-member query. Therefore, both studies [9,30] first estimate the distance from the decision boundary with the adversarial example generation techniques such as \"Hop-SkipJump\" [5] and \"QEBA\" [28], then decide the membership based on the distance to the decision boundary. Note that the process of finding the decision boundary is iterative. Specifically, starting from the target point, one estimates the gradient, uses it to update the point, and repeats these two steps until the point crosses the decision boundary.",
            "score": 0.6158964714481499,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 14053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2187
                }
            ],
            "ref_mentions": [
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1015,
                    "end": 1018,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1123,
                    "end": 1127,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1327,
                    "end": 1330,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1330,
                    "end": 1333,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1709,
                    "end": 1712,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1712,
                    "end": 1715,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1845,
                    "end": 1848,
                    "matchedPaperCorpusId": "173991158"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "218972155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "257636890",
            "title": "Manipulating Transfer Learning for Property Inference",
            "text": "We consider two black-box attack methods-one that directly uses model predictions, and one that leverages metaclassifiers.\n\nConfidence Score Test. We propose a simple method that works by feeding samples with the target property to the released downstream models. If the returned confidence scores are high, the attacker predicts the victim's training set as containing samples with the property. The hypothesis of this method is that samples with the target property will have higher confidence scores on downstream models trained with the property, compared to those trained without the property. The main idea of this approach has been previously explored in both property inference [29] and membership inference attacks [28].\n\nBlack-box Meta-classifier. We adapt the black-box metaclassifier proposed by Zhang et al. [41]. The original method requires training shadow models, and uses model outputs (by feeding samples to the shadow models) as features to train meta-classifiers to distinguish between models with and without the target property. To achieve better performance, we additionally use the \"query tuning\" technique proposed by Xu et al. [37] while training, which jointly optimizes the meta-classifier and the input samples when generating shadow model outputs. Figure 13 in the appendix shows the benefit of \"query tuning\".",
            "score": 0.615286740528047,
            "section_title": "Black-box API Access",
            "char_start_offset": 16781,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "237491948"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "235485024"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "203902799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "252185429",
            "title": "Robust and Lossless Fingerprinting of Deep Neural Networks via Pooled Membership Inference",
            "text": "Given a trained DNN model and a data record, membership inference [17] determines whether the data record was in the training set of the trained DNN model. This can raise privacy risks to individuals. For example, by identifying the fact that a clinical record has been used to train a DNN model associated with a certain disease, membership inference attacks can infer that the owner of the clinical record may have the disease [18].\n\nFrom a technical perspective, membership inference attacks can be performed in either white-box scenario or black-box scenario. In white-box scenario, the attacker is able to collect enough useful information such as the data distribution of the training set and/or the internal details of the target model, to attack the target model. In black-box scenario, the attacker, however, has only black-box access to the target DNN model with limited prior knowledge about the target DNN model.\n\nThrough analyzing the similarity between DNN watermarking and membership inference, it is very easy to think of extending membership inference techniques to the IP protection of DNN models. On one hand, the role of the attacker needs to be adjusted. Namely, the attacker in membership inference will become the defender in DNN watermarking. On the other hand, the ownership may be verified by analyzing the internal outputs or the final predictions given a certain number of data records belonging to the training dataset, which will raise two problems. The first one is how to collect the individual records that can be used for robust ownership verification. The second one is how to design the membership inference strategy.\n\nFor the first problem, due to the diversity between individual records, it will be not easy to collect \"robust\" data records. For the second problem, it is known that membership inference is effective for over-fitting models, but not good for models with good generalization ability. As a result, the so-called shadow models may be trained in advance to provide assistance, which consumes lots of data and time. To tackle this problem, in this paper, we propose pooled membership inference (PMI), which collects pooled evidence from individual records for inference. Compared with conventional membership inference and DNN watermarking methods, PMI possesses several advantages:\n\n-Availability: Different from black-box DNN watermarking that needs carefully crafted trigger samples, PMI",
            "score": 0.614298282533576,
            "section_title": "C. Membership Inference",
            "char_start_offset": 11145,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "232233426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "244345608",
            "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
            "text": "How much does a machine learning algorithm leak about its training data, and why? Membership inference attacks are used as an auditing tool to quantify this leakage. In this paper, we present a comprehensivehypothesis testing framework that enables us not only to formally express the prior work in a consistent way, but also to design new membership inference attacks that use reference models to achieve a significantly higher power (true positive rate) for any (false positive rate) error. More importantly, we explainwhy different attacks perform differently. We present a template for indistinguishability games, and provide an interpretation of attack success rate across different instances of the game. We discuss various uncertainties of attackers that arise from the formulation of the problem, and show how our approach tries to minimize the attack uncertainty to the one bit secret about the presence or absence of a data point in the training set. We perform adifferential analysis between all types of attacks, explain the gap between them, and show what causes data points to be vulnerable to an attack (as the reasons vary due to different granularities of memorization, from overfitting to conditional memorization). Our auditing framework is openly accessible as part of thePrivacy Meter software tool.",
            "score": 0.6136161615932285,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9130859375
        },
        {
            "corpus_id": "219177364",
            "title": "privGAN: Protecting GANs from membership inference attacks at low cost to utility",
            "text": "Much of the recent progress in machine learning and related areas has been strongly dependent on the open sharing of datasets. A recent study shows that the increase in the number of open datasets in biology has led to a strongly correlated increase in the number of data analysis papers [2]. Moreover, in many specialized application areas, the development of novel algorithms is contingent upon the public availability of relevant data. While the public availability of data is essential for reproducible science, in the case of sensitive data, this poses a possible threat to the privacy of the individuals in the dataset. \n\nOne way in which privacy of samples in a dataset can be compromised is through membership inference attacks. Membership inference attacks are adversarial attacks where the goal of the adversary is to infer whether one or more samples are a part of a dataset without having explicit access to the dataset. There has been a lot of work on developing membership inference attacks against predictive machine learning models using their outputs [3][4][5][6]. Much of this work has focused on exploiting information leakage due to overfitting in many machine learning models [7]. These approaches have been shown to be extremely effective against common machine learning models and have given rise to machine learning methods that are specifically designed to be resistant to such attacks [8][9][10][11]. \n\nThere has recently been a surge of interest in using synthetic data generated from generative models as a privacy preserving way to share datasets [12][13][14]. While this is an appealing approach, it has been shown that generative models such as GANs are also prone to memorizing their training set [15]. This has been exploited in several recent papers to explore the vulnerability of generative models to membership inference attacks [16][17][18]. [16] designed a white-box attack on the released discriminator of a GAN and showed that it can be almost 100% accurate in some cases. They also designed a black-box attack, which is comparatively a lot less accurate. [18] designed Monte-Carlo attacks on the generators which are shown to have high accuracy for set membership inference (defined later) and slightly lower accuracy for instance membership inference.",
            "score": 0.6116053053840079,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "17398269"
                },
                {
                    "start": 1068,
                    "end": 1071,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1074,
                    "end": 1077,
                    "matchedPaperCorpusId": "201598829"
                },
                {
                    "start": 1077,
                    "end": 1080,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 1197,
                    "end": 1200,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1414,
                    "end": 1417,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "43939347"
                },
                {
                    "start": 1580,
                    "end": 1584,
                    "matchedPaperCorpusId": "52305483"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "52980215"
                },
                {
                    "start": 1866,
                    "end": 1870,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1880,
                    "end": 1884,
                    "matchedPaperCorpusId": "52211986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "195346528",
            "title": "The Unintended Consequences of Overfitting: Training Data Inference Attacks",
            "text": "In this section, we consider several membership attacks that make general assumptions about the training algorithm used to build the model under attack, the adversary's knowledge of the model and its behavior, and the distribution of the training data. Importantly, the assumptions made by these attacks are consistent with many natural learning techniques widely used in practice, such as linear and logistic regression, support vector machines, and tree models (both regression and classification). \n\nIn each case, we examine the relationship between the advantage of the attack and its relationship to how well the model generalizes, and show that generalization behavior is a strong predictor for vulnerability to membership inference attacks. In Section 6.2, we demonstrate that these relationships often hold in practice on real data, when the assumptions used in our analysis do not strictly hold. \n\nBounded Loss Function We begin with a straightforward attack that makes minimal assumptions about A S . If the loss function is bounded by come constant max , then the adversary predicts that a query point z is not in S with probability proportional to the model's loss at z. The attack is formalized in Adversary 1. \n\nTheorem 3 states that the membership advantage of this approach is proportional to the generalization error of A, showing that advantage and generalization error are closely related in many common learning settings. In particular, classification settings, where the 0-1 loss function (A S , z) = 1(y = A S (x)) is commonly used, B = 1 yields membership advantage equal to the generalization error. Simply put, high generalization error necessarily results in privacy loss for classification models. \n\nAdversary 1 (Bounded loss function). Suppose (A S , z) \u2264 max for some constant max , all S \u223c D n , and all z sampled from S or D. Then, on input z = (x, y), A S , n, and D, the membership adversary A proceeds as follows: \n\n1. Query the model to get A S (x). \n\nProof. The proof is a straightforward calculation, but its simplicity serves to illustrate the general form of the remaining proofs in this section. We begin by writing advantage as in the proof of Theorem 2, and proceed by repeated substitution:",
            "score": 0.6110379276111096,
            "section_title": "Membership attacks and generalization",
            "char_start_offset": 26981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "259342605",
            "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
            "text": "Membership inference attacks. Depending on the adversary capabilities, MIAs can be divided into black-box [37], [48], [17], [3], [38], [8], [47], [27] and white-box attacks [25], [18], [30]. The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies [19], [41], [29] (and in HAMP). Such attacks can be mounted by either shadow-training [37], [29], [48] or computing statistical metrics based on the partial knowledge of the private dataset [38], [8], [27]. Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with [8] or without [27] partial knowledge of the membership information. Carlini et al. [3] introduce the LiRA attack that can succeed in inferring membership when controlled at low false positive or false negative, through a well-calibrated Gaussian likelihood estimate. \n\nIn addition to supervised classification, MIAs have also been explored in other domains, including contrastive learning [28], generative models [7], [13], federated learning [30], graph neural networks [51], and recommender systems [49]. \n\nDefenses against membership inference attacks. These defenses can be divided into provable and practical defenses. The former can provide rigorous privacy guarantee, such as DP-SGD [2], PATE [32]. Nevertheless, these defenses often incur severe accuracy drop when used with acceptable provable bounds [35], [33]. Another line of practical defenses aim to achieve empirical privacy without severely degrading accuracy. Common regularization techniques such as dropout [39], weight decay [24] are shown to be able to reduce privacy leakage, but with limited effectiveness [37], [36].",
            "score": 0.6095915645494564,
            "section_title": "VI. RELATED WORK",
            "char_start_offset": 59723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 173,
                    "end": 177,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "239016142"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "237290083"
                },
                {
                    "start": 1362,
                    "end": 1365,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1367,
                    "end": 1371,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "238407736"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "237532289"
                },
                {
                    "start": 1639,
                    "end": 1642,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1759,
                    "end": 1763,
                    "matchedPaperCorpusId": "13699042"
                },
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "220830768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.974609375
        },
        {
            "corpus_id": "202734167",
            "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
            "text": "An attacker aims to infer the proprietary training dataset of the model provider. Specifically, we consider the attacker only has black-box access to the target classifier, i.e., the attacker can send query data samples to the target classifier and obtain their confidence score vectors predicted by the target classifier. The attacker leverages black-box membership inference attacks [34,42,56,58] to infer the members of the target classifier's training dataset. Roughly speaking, in membership inference attacks, the attacker trains a binary classifier, which takes a query data sample's confidence score vector as input and predicts whether the query data sample is in the target classifier's training dataset or not. Formally, we have:\n\nwhere C is the attacker's binary classifier, s is the confidence score vector predicted by the target classifier for the query data sample x, 0 indicates that the query data sample x is not a member of the target classifier's training dataset, and 1 indicates that the query data sample x is a member of the target classifier's training dataset. For convenience, we call the attacker's binary classifier C attack classifier. We will discuss more details about how the attacker could train its attack classifier in Section 5. Note that, to consider strong attacks, we assume the attacker knows our defense mechanism, but the defender does not know the attack classifier since the attacker has many choices for the attack classifier.",
            "score": 0.60932362191621,
            "section_title": "Attacker",
            "char_start_offset": 18156,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "46933970",
            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
            "text": "In this paper, we concentrate on machine learning classification, as it is the most common ML application. An ML classifier is essentially a function M that maps a data point X (a multidimensional feature vector) to an output vector M(X ) = Y. The length of Y is equal to the number of classes considered. For most of the classification models, the output vector Y can be interpreted as a set of posterior probabilities over all classes, and the sum of all the values in Y is 1. The parameters of an ML model are learned on a training dataset (denoted by D Train ) containing multiple data points following a predefined learning object. \n\nMembership inference attack in the ML setting emerges when an adversary aims to find out whether her target data point is used to train a certain ML model. More formally, given a target data point x Target , a trained machine learning model M, and external knowledge of an adversary, denoted by K, a membership inference attack (attack model) can be defined as the following function. \n\nHere, 0 means x Target is not a member of M's training dataset D Train and 1 otherwise. The machine learning model M that the adversary targets is also referred to as the target model. As in the work of Shokri et al. [38], we assume the adversary only has black-box access to the target model, such as an MLaaS API, i.e., the adversary can submit a data point to M and then obtain the probabilistic output, i.e., M(x Target ). \n\nThe attack model A is essentially a binary classifier. Depending on the assumptions, it can be constructed in different ways, which will be presented in later sections.",
            "score": 0.6089496210234666,
            "section_title": "A. Membership Inference Against Machine Learning Models",
            "char_start_offset": 10758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "231802143",
            "title": "ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models",
            "text": "Inference attacks against Machine Learning (ML) models allow adversaries to learn sensitive information about training data, model parameters, etc. While researchers have studied, in depth, several kinds of attacks, they have done so in isolation. As a result, we lack a comprehensive picture of the risks caused by the attacks, e.g., the different scenarios they can be applied to, the common factors that influence their performance, the relationship among them, or the effectiveness of possible defenses. In this paper, we fill this gap by presenting a first-of-its-kind holistic risk assessment of different inference attacks against machine learning models. We concentrate on four attacks -- namely, membership inference, model inversion, attribute inference, and model stealing -- and establish a threat model taxonomy. Our extensive experimental evaluation, run on five model architectures and four image datasets, shows that the complexity of the training dataset plays an important role with respect to the attack's performance, while the effectiveness of model stealing and membership inference attacks are negatively correlated. We also show that defenses like DP-SGD and Knowledge Distillation can only mitigate some of the inference attacks. Our analysis relies on a modular re-usable software, ML-Doctor, which enables ML model owners to assess the risks of deploying their models, and equally serves as a benchmark tool for researchers and practitioners.",
            "score": 0.608350510012786,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "248870291",
            "title": "Black-box based limited query membership inference attack",
            "text": "In order to verify the impact of different types of target models on the attack model accuracy of membership inference, this paper selects three different types of classification models with the same training set size. In traditional machine learning algorithms, we choose Logistic regression and Xg-Boost as target models. For deep learning based algorithms, we choose convolutional neural network as the target model. \n\nIn addition, all attacks against the target model are carried out under the black-box assumption, and the attacker only has the ability to access the target model and obtain the output results of the model.",
            "score": 0.6079406027559895,
            "section_title": "B. EXPERIMENTAL SETUP 1) The target model",
            "char_start_offset": 17883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 628
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "258685782",
            "title": "A Reproducible Extraction of Training Images from Diffusion Models",
            "text": "For models like Midjourney or DALLE-2, the model parameters and training sets are unknown. \n\nIn machine learning, the process of determining which training samples were used only from model outputs, is known as membership inference and in particular the challenging \"black box\" setting. Membership inference has been widely studied against GAN generated images, with varying success, for a wide variety of different settings, for instance in [12]. Several very recent approaches have designed black box attacks specifically against diffusion models [16,9,13], but conduct studies versus relatively small datasets (<1M samples). Namely, in the recent work in [13] a loss based attack is presented which bears resemblance to our whitebox attack (See Eq. 3.1), however they do so in the unconditional case, and observe better performance later in the generation process.",
            "score": 0.6070002486953344,
            "section_title": "Related work",
            "char_start_offset": 4679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "263830573",
            "title": "From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying",
            "text": "During watermark verification or detection, they operate in a white-box setting, requiring access to the model's parameters or gradient information. As such, detection involves retrieving parameters or activation of specific layers in response to given inputs and comparing them to the embedded watermarks after decoding. On the other hand, in many real-world scenarios, model detection operates in a black-box setting as models are usually accessed only as a service, returning predictions for given inputs. Thus, detecting watermarked models can only be performed by querying the target model with a specific trigger set to reveal the watermark in its predictions. This has led to advancements in model parameter embedding techniques for black-box detection [8,12,21]. Although they can provide higher accuracy and robustness than watermarking within training samples, they require complete control over the training process. Furthermore, they often necessitate white-box access to the model during detection. Our focus, however, is on safeguarding data IP without assumptions about the attackers' training approaches, making these methods inappropriate for our specific needs. \n\nMembership Inference Attack. Membership inference attack provides a direct approach to detecting leaked data [41]. This technique assesses the likelihood that samples from the owner's dataset have been used to train the target model, thereby identifying unauthorized usage. Nonetheless, this technique encounters challenges in discerning models trained on datasets with samples that are markedly similar or even identical to those in the owner's collection. For instance, a patient may have medical records at multiple hospitals, and these records could be substantially identical. Furthermore, its accuracy may diminish when applied to shallow models [45] like -NN, Logistic Regression, or shallow Decision Trees and MLPs. Such models are less likely to exhibit significant differences in individual samples if they are trained on independent datasets drawn from the same population.",
            "score": 0.6068711968946503,
            "section_title": "RELATED WORK",
            "char_start_offset": 50981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2066
                }
            ],
            "ref_mentions": [
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "102347976"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "11008755"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1834,
                    "end": 1838,
                    "matchedPaperCorpusId": "86836429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "246822818",
            "title": "Threats to Pre-trained Language Models: Survey and Taxonomy",
            "text": "We first characterize attacks that compromise the privacy of training or inference data. Such attacks are launched by malicious DSP in the fine-tuning phase, or malicious U in the inferring phase. Recent studies [Feldman and Zhang, 2020] have demonstrated that machine learning models can memorize data, which allows adversarial service providers or users to steal information from the model behaviors. Particularly for the PTLM scenario, the key information of training or inference samples may be extracted from the embedding codes and PTLMs [Song and Raghunathan, 2020;Carlini et al., 2021]. The landscape transferability makes the victim model more transparent to the adversary, and facilitates the attacks in the black-box setting. According to the type of extracted information, we classify data privacy attacks into the following three categories. Embedding inversion attacks. PTLMs are often used as embedding models to extract embedding representations of words for downstream tasks. A malicious service provider can invert the original sentence of an inference input based on the corresponding embedding code. [Song and Raghunathan, 2020] designed such embedding inversion attacks in both white-box and black-box modes. In the white-box scenario, the embedding inversion attack is modeled as an optimization problem, and they proposed a continuous relaxation of sequential word inputs to achieve efficient optimization based on gradients. In the black-box scenario, the adversary trains an inversion model that takes a text embedding code as input and outputs the set of words as the extracted results. Attribute inference attacks. One typical example of attribute inference attacks is membership inference attacks (MIAs), which aim to infer whether a data sample is in the training set from the target machine learning model. Existing MIAs [Shokri et al., 2017] have shown that given black-box access to the victim model, the predicted confidence of the test sample can reveal its membership attribute. Such attacks usually train a shadow model to distinguish between members and non-members of the training data. [Song and Raghunathan, 2020] developed a simple and efficient MIA based on the observation that the words or sentences in the context for training have more similar scores to each other than the ones in other contexts.",
            "score": 0.6068430723446843,
            "section_title": "Data Privacy Attacks",
            "char_start_offset": 19492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 544,
                    "end": 572,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 572,
                    "end": 593,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1120,
                    "end": 1148,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1851,
                    "end": 1872,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "273850257",
            "title": "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage",
            "text": "Membership inference attacks are a typical method that steals private information of training data, aiming to infer whether a sample is a member of a target model's training set. \n\nDefinition 2 (Membership Inference Attacks [28]): Given a machine learning model M o , a training set D train and a target sample x i . Let I(\u2022) be an indicator function for membership inference attacks, used to determine the membership information of x i .",
            "score": 0.6062451212552559,
            "section_title": "C. Membership inference attacks",
            "char_start_offset": 12065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 438
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "67855651",
            "title": "When Relaxations Go Bad: \"Differentially-Private\" Machine Learning",
            "text": "This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62]. Re-  [60] proposed more generic membership inference attacks by relaxing the requirements of Shokri et al. [62]. In particular, requirements on the number of shadow models, knowledge of training data distribution and the target model architecture can be relaxed without substantially degrading the effectiveness of the attack. Yeom et al. [76] recently proposed a more computationally efficient membership inference attack when the attacker has access to the target model and knows the average training loss of the model. To test the membership of an input record, the attacker evaluates the loss of the model on the input record and then classifies it as a member if the loss is smaller than the average training loss. \n\nConnection to Differential Privacy. Differential privacy, by definition, aims to obfuscate the presence or absence of a record in the data set. On the other hand, membership inference attacks aim to identify the presence or absence of a record in the data set. Thus, intuitively these two notions are competing each other. Li et al. [42] point to this fact and provide a direct relationship between differential privacy and membership inference attacks. Moreover, Backes et al. [3] studied membership inference attacks on microRNA studies and showed that differential privacy can reduce the success of membership inference attacks, but at the cost of utility. \n\nYeom et al. [76] formally define a membership inference attack as an adversarial game where a data element is selected from the distribution, which is randomly either included in the training set or not. Then, an adversary with access to the trained model attempts to determine if that element was used in training. The membership advantage is defined as the difference between the adversary's true and false positive rates for this game. The authors prove that if the learning algorithm satisfies -differential privacy, then the adversary's advantage is bounded by e \u2212 1. Hence, it is natural to use membership inference attacks as a metric to evaluate the privacy leakage of differentially-private algorithms.",
            "score": 0.6059107579286229,
            "section_title": "Membership Inference",
            "char_start_offset": 23420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 215,
                    "end": 219,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "17295025"
                },
                {
                    "start": 1504,
                    "end": 1508,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "258298235",
            "title": "Differentially Private Synthetic Data Generation via Lipschitz-Regularised Variational Autoencoders",
            "text": "Membership inference [15], [16] is an attack on a machine learning model, where an adversary tries to infer whether a particular record was part of the training set of the attacked model . Depending on the type of ML model and the information about that model that is available to the adversary, there exist different variants of the membership inference attack (MIA). The types of MIAs that can be utilized against generative models depend on the knowledge of the attacker. In a white-box setting the attacker can access the parameters of the generative model itself and use this knowledge to craft an attack. Here we focus on a block-box setting, i.e. the model parameters of the target model are not available. \n\nIn [15] it is assumed that the attacker has access to public data that is drawn from the same distribution as the target model's training data, and the architecture and parameters of the target model are known. Several so-called shadow models are trained that mimic the behaviour of the target model. An attack model is then trained on the shadow models' outputs and used to classify the record under consideration as member or non-member of the target model's training data. Later works were able to relax these assumptions [16], [17].The most common attacks against generative models/synthetic data in a black box setting are as follows: \n\n1) Classifier trained on shadow models [15]: The MIA then proceeds with the following steps: a) Draw m datasets of size n from the public data. b) Train 2m shadow models, one for each data set with and without inserting the target record t into the set. c) Generate p synthetic data sets from each shadow model. This results in a labelled data set of size 2mp of pairs (x, y), where x is a synthetic data set and y = 0 if the target record was not in the set used to train the model that generated x, y = 1, if the target record was present. d) Train a (binary) attack classifier on the labelled set of data sets. e) Use the attack classifier to predict, whether the target record was present in the private data that was used to generate the synthetic data set. The drawback of this method is that it is computationally very expensive.",
            "score": 0.6058928312781005,
            "section_title": "C. Membership Inference Attacks",
            "char_start_offset": 7640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "251622394",
            "title": "An Empirical Study on the Membership Inference Attack against Tabular Data Synthesis Models",
            "text": "The membership inference attack (MIA) against generative models aims to determine whether a given record was used to train the victim model [11]. It can threaten privacy as not only does it disclose raw personal data used in training but also allows potential information leakage of the model. Shokri et al. [26] presented the first MIA against classification models in the black-box setting and quantified the membership information leakage. This led to increasing research on MIA in machine learning models, especially on generative models. Hayes et al. [14] proposed the first MIA against generative models for images both in the black and white-box settings. Hilprechet et al. [15] proposed two MIAs, one is for GANs in the black-box setting, and the other is for VAEs in the white-box one. Chen et al. [6] proposed a generic attack model which comprehends all scenarios from the black-box to the white-box setting, which we adopt for our experiments as follows: \n\n(1) As generative models are trained to estimate the training data distribution, attackers infer the membership   by using the probability of a given data  being generated by the victim model G  as follows: \n\nHowever, as calculating the exact probability is intractable, they approximate it using the Parzen window density estimation [10]: \n\n(2) In the black-box scenario, attackers blindly collect  fake records generated by the victim model and get a reconstructed copy of a data  as R ( |G  ) to approximate the probability in Eq. ( 2) as follows: \n\nwhere {G  (\u2022)  }  =1 is a set of  fake records collected by attackers. \n\n(3) In the white-box scenario, attackers get R ( |G  ) to compute the probability in Eq. ( 2) through optimizing  as follows: \n\nwhere \n\nHowever, all previous research has focused on generative models for image data, not for tabular data, albeit some used tabular data in their experiments by heuristically modifying the models for tabular data. However, as tabular data requires pre-processing of its complex data distribution, such heuristics can mislead the analysis. \n\nTo this end, this paper aims to target tabular data synthesis models for an accurate analysis on MIA on tabular data.",
            "score": 0.605841225659725,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 4726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "2565493"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "221203089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "259224343",
            "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models",
            "text": "In principle, loss-based membership inference attacks are based on the following simple observation [36]. A model training minimises a loss function on a training set, hence we expect the loss to be lower for training samples than for test ones. In most cases, such methods treat the attacked model a as white-box, assuming that the attacker has access to the model, its source code and trained weights. This assumption is often not met in practice, as API-based generative machine learning services such as Midjourney [32] increase in popularity. In general, methods based solely on the analysis of the loss of the model are less effective than methods utilising shadow models [3,4].",
            "score": 0.6055848825309323,
            "section_title": "Loss based attacks",
            "char_start_offset": 7524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 684
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "239016142",
            "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
            "text": "Membership inference attacks against machine learning. MIAs are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28,29]. Jayaraman et al. [20] analyze MIA in more realistic assumptions by relaxing proportion of training set size and testing set size in the MIA set up to be any positive value instead of 1. Hui et al. [18] study MIA in a practical scenario, assuming no true labels of target samples are known and utilizing differential comparison for MIAs. Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model [26,33], which can exploit model parameters to infer membership information. Membership inference defenses for machine learning. Membership inference defenses can be divided into two main categories. One category of defenses are specifically designed to mitigate such attacks. It has been shown that techniques to improve a model's generalization ability, including regularization [25] and dropout [48], can decrease the MIA success [41,43] limitedly. Several defenses [27,32] propose to add a specific constraint during training to mitigate the difference of model behavior on models and non-models. These optimization problems under multiple constraints in training are usually computationally hard to solve. Post-processing techniques on prediction vectors are also applied on membership inference defenses [21,53]. Note that these defenses which obfuscate prediction vectors can not defend against label-only attacks [8,28]. Moreover, Song et al. [46] re-evaluate two state-of-the-art defenses (adversarial regularization [32] and MemGuard [21]) and find that both of them underestimated metric-based attacks. Shejwalkar et al. [42] propose distillation of public data to protect membership privacy. However, public dataset is not usually available in many practical scenarios.",
            "score": 0.6055000261460968,
            "section_title": "Related Work",
            "char_start_offset": 53098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 479,
                    "end": 482,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 934,
                    "end": 937,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "10137788"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1367,
                    "end": 1370,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1399,
                    "end": 1403,
                    "matchedPaperCorpusId": "221297671"
                },
                {
                    "start": 1403,
                    "end": 1406,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1851,
                    "end": 1854,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 1854,
                    "end": 1857,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1956,
                    "end": 1960,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1974,
                    "end": 1978,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 2062,
                    "end": 2066,
                    "matchedPaperCorpusId": "235349092"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "235358225",
            "title": "GraphMI: Extracting Private Graph Data from Graph Neural Networks",
            "text": "Based on the attacker's goal, privacy attacks can be categorized into several types such as membership inference attack [Shokri et al., 2017], model extraction attack [Tram\u00e8r et al., 2016] and model inversion attack [Fredrikson et al., 2015]. Membership inference attack tries to determine whether one sample was used to train the machine learning model; Model extraction attack is one black-box privacy attack. It tries to extract information of model parameters and reconstruct one substitute model that behaves similarly to the target model. Model inversion attack, which is the focus of this paper, aims to reconstruct sensitive features corresponding to labels of target machine learning models. \n\nModel inversion attack was firstly presented in [Fredrikson et al., 2014] for linear regression models. [Fredrikson et al., 2015] extended model inversion attack to extract faces from shallow neural networks. They cast the model inversion as an optimization problem and solve the problem by gradient descent with modifications to the images. Furthermore, several model inversion attacks in the black-box setting or assisted with generative methods are proposed [A\u00efvodji et al., 2019;Zhang et al., 2020] in the image domain. Thus far, no existing model inversion attack has focused on the graph domain. \n\n3 Problem Formulation",
            "score": 0.6051717409796056,
            "section_title": "Related Work",
            "char_start_offset": 4657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1328
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 188,
                    "matchedPaperCorpusId": "166228294"
                },
                {
                    "start": 216,
                    "end": 241,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 751,
                    "end": 776,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 807,
                    "end": 832,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1186,
                    "end": 1205,
                    "matchedPaperCorpusId": "208139345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "229923876",
            "title": "A Differentially Private Multi-Output Deep Generative Networks Approach For Activity Diary Synthesis",
            "text": "The approach of differentially-private training promises of privacy protection for samples used as training data points. In order to evaluate the models robustness and susceptibility to adversarial attacks, Membership Inference Attacks (MIA) are implemented. The notion of Membership Inference Attack (MIA) was proposed by Shokri [6], as a privacy attack mechanism to measure the robustness of machine learning privacy protecting algorithms against adversarial attacks. (Attacks that attempt to infer the identity or uniqueness of individuals from anonymized data.) An attack evaluates the prediction score of a model when the input data point is sampled from the training set rather than the validation set. The MIA comes in two forms: black-box and white-box attacks. Black-box attacks assume an adversary can only make queries to the target model under attack but has no access to the internal parameters of the model [6]. On the other hand, white-box attacks assume the adversary has the parameters of the trained model and can make queries to the target model. We adopt a white-box attack approach in this work because it is simple to implement and efficient. In a GAN setting, the adversary is only given access to the discriminator of the trained GAN model and considers a setting where the model parameters are leaked following a data breach. The trained model determines if a record was part of the training set, consequently the attack analyzes the danger in identifying with high confidence if an observation was used in the training. Additionally, the adversary is assumed to have knowledge of the proportion of the dataset that is used for training, but no other subsequent information is known about the training set. This attack is implemented by deriving the predictive scores of the discriminator of the target model given a sample of the training set. In a non-private trained model, the output of the attack should score lower probabilities (i.e., close to 0) for validation sets and high probabilities (close to 1) for training sets. On the other hand, private trained models should not output scores that distinguish training sets from validation sets. \n\nAs a sequel to earlier discussion, population synthesis approaches suffer the drawback of scalability to high dimensional data that are computationally expensive.",
            "score": 0.6048044561824438,
            "section_title": "Membership Inference Attacks Against Generative Models",
            "char_start_offset": 17639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2173
                },
                {
                    "start": 2176,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 333,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "259854192",
            "title": "Empirical study of privacy inference attack against deep reinforcement learning models",
            "text": "Previous studies have extensively investigated the issue of privacy theft in machine learning models using training data. Most of these studies have focused on supervised learning models, including attacks like membership inference (Carlini et al., 2019;Shokri et al., 2017), attribute inference (Gong & Liu, 2016), and dataset reconstruction (Salem et al., 2020). Among these attacks, membership inference has become the most popular type, where the goal is to determine whether a given data sample was part of the training set used to build the model. White-box attacks, which leverage model parameters and gradients, have been shown to be more accurate. Although this type of attack may not appear to pose a significant privacy risk in everyday scenarios, it could be a major concern in areas such as health analytics, where differences between cases and controls could reveal sensitive conditions. Membership inference has been widely studied in the context of machine learning, with several recent works, including (Ai et al., 2021;Hou et al., 2022;Li et al., 2022;Liang et al., 2022;G. Lin et al., 2022;Yan, Hu et al., 2021;Yan, Jiang et al., 2021), all focusing on related issues. Reconstruction attacks aim to reconstruct one or more training samples or their corresponding labels, which can be partial or complete. These attacks utilise output labels and partial knowledge of specific features to recover sensitive features or complete data samples. Attribute inference (Gong & Liu, 2016), on the other hand, focuses on dataset attributes that are irrelevant to the learning task, such as the gender distribution in a patient dataset. The leakage of these properties can compromise privacy and provide additional information about the training data. While the distribution of certain attributes may differ between datasets, in reinforcement learning models, the attributes of datasets depend on the environment, making it challenging to determine if a sample point was part of the sampling process. Our recent work on attribute inference attacks in reinforcement learning models has demonstrated the potential for machine learning models to reveal private information.",
            "score": 0.6042658011245114,
            "section_title": "Machine learning privacy leakage",
            "char_start_offset": 6740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 254,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 254,
                    "end": 274,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 296,
                    "end": 314,
                    "matchedPaperCorpusId": "4948164"
                },
                {
                    "start": 343,
                    "end": 363,
                    "matchedPaperCorpusId": "91184074"
                },
                {
                    "start": 1020,
                    "end": 1037,
                    "matchedPaperCorpusId": "232432835"
                },
                {
                    "start": 1037,
                    "end": 1054,
                    "matchedPaperCorpusId": "247985284"
                },
                {
                    "start": 1054,
                    "end": 1070,
                    "matchedPaperCorpusId": "264345003"
                },
                {
                    "start": 1070,
                    "end": 1089,
                    "matchedPaperCorpusId": "225575709"
                },
                {
                    "start": 1089,
                    "end": 1109,
                    "matchedPaperCorpusId": "251854103"
                },
                {
                    "start": 1109,
                    "end": 1130,
                    "matchedPaperCorpusId": "225104439"
                },
                {
                    "start": 1130,
                    "end": 1154,
                    "matchedPaperCorpusId": "244065094"
                },
                {
                    "start": 1479,
                    "end": 1497,
                    "matchedPaperCorpusId": "4948164"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "221808247",
            "title": "The Audio Auditor: User-Level Membership Inference in Internet of Things Voice Services",
            "text": "The membership inference attack is considered as a significant privacy threat for machine learning (ML) models [22]. The attack aims to determine whether a specific data sample is within the target model's training set or not. The attack is driven by the different behaviors of the target model when making predictions on samples within or out of its training set. \n\nVarious membership inference attack methods have been recently proposed. Shokri et al. [39] train shadow models to constitute the attack model against a target ML model with black-box access. The shadow models mimic the target model's prediction behavior. To improve accuracy, Liu et al. [15] and Hayes et al. [10] leverage Generative Adversarial Networks (GAN) to generate shadow models with increasingly similar outputs to the target model. Salem et al. [32] relax the attack assumptions mentioned in the work [39], demonstrating that shadow models are not necessary to launch the membership inference attack. Instead, a threshold of the predicted confidence score can be defined to substitute the attack model. Intuitively, a large confidence score indicates the sample as a member of the training set [41]. The attacks mentioned in the work above are all performed on the record level, while Song and Shmatikov [40] study a user-level membership inference attack against text generative models. Instead of using the prediction label along with the confidence score, Song and Shmatikov [40] utilize word's rank list infor-mation of several top-ranked predictions as key features to generate the shadow model. Apart from the blackbox access, Farokhi and Kaafar [8] model the recordlevel membership inference attack under the white-box access. \n\nUnlike image recognition systems or text generative systems, ASR systems present additional challenges [5]. With strict black-box access, attacks using confidence scores cannot be applied. With limited discriminative power, features can only be extracted from the predicted transcription and its input audio to launch membership inference attacks, i.e., audio auditing in our paper.",
            "score": 0.6040993798265734,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 9979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1711
                },
                {
                    "start": 1714,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "195477472"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1456,
                    "end": 1460,
                    "matchedPaperCorpusId": "159041003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96630859375
        },
        {
            "corpus_id": "264405612",
            "title": "Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks",
            "text": "Membership Inference Attacks. In a typical black-box MI attack scenario, as per the literature (Shokri et al., 2017;Hisamoto et al., 2020), it is posited that the attacker, Bob, can access a data distribution identical to Alice's training data. This access allows Bob to train a shadow model, using the known data membership of this model as ground truth labels to train an attack classifier. Bob can then initiate the attack by sending queries to Alice's model APIs. Most previous studies leverage disparities in prediction distributions to distinguish between training and non-training samples. However, this approach is not feasible for Seq2Seq models. For each generated token in these models, the output probability over the word vocabulary often comprises tens of thousands of elements-for instance, e.g., the vocabulary size for BART is 50,265 (Lewis et al., 2020). As such, most public APIs do not offer probability vectors for each token but rather furnish an overall confidence score for the sequence, calculated based on the product of the predicted tokens' probabilities. Natural Language Privacy. An increasing body of work has been conducted on understanding privacy risk in NLP domain, (Hayes et al., 2017;Meehan et al., 2022;Chen et al., 2022;Ponomareva et al., 2022). Pioneering research has been dedicated to studying MI attacks in NLP models. The study by (Hisamoto et al., 2020) examines the black-box membership inference problem of machine translation models. They assume Bob can access both the input document and translated text and use BLEU scores as the membership signal, which is similar to our baseline attack. (Song and Shmatikov, 2019) investigate a white-box MI attack for language models, which assume Bob can obtain the probability distribution of the generated token. Different from previous work, our attack is under the black-box setting and considers a more general document-only attack in which Bob only needs input documents for membership inference.",
            "score": 0.6038420873372875,
            "section_title": "Background and Related Works",
            "char_start_offset": 4861,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 116,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 851,
                    "end": 871,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1221,
                    "end": 1241,
                    "matchedPaperCorpusId": "248665923"
                },
                {
                    "start": 1241,
                    "end": 1259,
                    "matchedPaperCorpusId": "248780015"
                },
                {
                    "start": 1259,
                    "end": 1283,
                    "matchedPaperCorpusId": "248780476"
                },
                {
                    "start": 1640,
                    "end": 1666,
                    "matchedPaperCorpusId": "159041003"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "272768481",
            "title": "Recycling Scraps: Improving Private Learning by Leveraging Checkpoints]{Recycling Scraps: Improving Private Learning by Leveraging Checkpoints",
            "text": "ML models generally require large amounts of training data to achieve good performances. This data can be of sensitive nature, e.g., medical records and personal photographs, and without proper precautions, ML models may leak sensitive information about their private training data. Multiple previous works have demonstrated this via various inference attacks, e.g., membership inference, property or attribute inference, model stealing, and model inversion. Below, we review these attacks. \n\nConsider a target model f \u03b8 trained on D tr and a target sample (x, y). Membership inference attacks Shokri et al. [2017], Sankararaman et al. [2009], Ateniese et al. [2015] aim to infer whether the target sample (x, y) was used to train the target model, i.e., whether (x, y) \u2208 D tr . Property or attribute inference attacks Melis et al. [2019], Song and Shmatikov [2019] aim to infer certain attributes of (x, y) based on model's inference time representation of (x, y). For instance, even if f \u03b8 is just a gender classifier, f \u03b8 (x) may reveal the race of the person in x. Model stealing attacks Tram\u00e8r et al. [2016], Orekondy et al. [2019] aim to reconstruct the parameters \u03b8 of the original model f \u03b8 based on black-box access to f \u03b8 , i.e., using f \u03b8 (x). Model inversion attacks Fredrikson et al. [2015] aim to reconstruct the whole training data D tr based on white-box, i.e., using \u03b8, or black-box, i.e., using f \u03b8 (x), access to model.",
            "score": 0.6028748868407057,
            "section_title": "Privacy Leakage in ML Models",
            "char_start_offset": 12175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 594,
                    "end": 614,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 644,
                    "end": 666,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 819,
                    "end": 838,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 840,
                    "end": 865,
                    "matchedPaperCorpusId": "167217888"
                },
                {
                    "start": 1092,
                    "end": 1112,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1279,
                    "end": 1303,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "251664530",
            "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors",
            "text": "However, existing member inference attacks against machine unlearning 27 are almost based on the inherent overfitting property of ML models that data items in the training set have high posterior probabilities. Accordingly, existing attacks exploit the difference between the two models caused by the deletion of the target sample, and the adversary judges whether a sample is a deleted sample by comparing the output posterior probabilities of the two models. Such attacks can be easily mitigated in the label-only settings in which the ML model only outputs the predicted labels (i.e., the final model decision) and there is no posterior probability exposed. \n\nNevertheless, such situations are more common in real life because most users only care about the final prediction result. \n\nThis motivates us to aim to enhance the feasibility of the membership inference attack and attempt to conduct membership inference in label-only settings. Our heuristic is that injected turbulence on candidate samples will present different behaviors for training and nontraining data. \n\nAccordingly, in conducting attack, the adversary queries the original model and the unlearned model for candidate data samples respectively and adds perturbations to them to change the predicted labels of the two models, and then the adversary uses the amplitude of the perturbations to distinguish whether they are deleted data samples. \n\nIn general, our contributions can be summarized as the following: \n\n\u2022 We propose a novel membership inference attack based on predicted labels, which is the first black-box membership inference attack against machine unlearning in the setting where the model only outputs predicted labels. We implemented an attack prototype, and our code is available at https://github.com/lzbuuu/MIAoMU. \u2022 We conduct experiments on a range of settings and data sets. The evaluation shows that our attack performs well, especially the attack AUC on the MNIST data set reaches 0.96, and the results also that our attack outperforms existing posterior probability-enabled attacks. \n\n\u2022 Finally, we discuss several possible defenses against our attack, such as generalization enhancement, privacy enhancement, and confidence perturbation, which are proven to be effective in most existing attacks. We investigate the impact of some defenses on our attack, and the results show that our attack can bypass them.",
            "score": 0.6028505000530784,
            "section_title": "| INTRODUCTION",
            "char_start_offset": 3408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1481
                },
                {
                    "start": 1484,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 72,
                    "matchedPaperCorpusId": "209439842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "273850257",
            "title": "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage",
            "text": "Several privacy attacks against machine learning have been proposed, such as membership inference attacks [28] and attribute inference attacks [29]. These attacks aim to steal sensitive data and model information. For example, membership inference attacks can obtain data membership information, determining whether a particular sample belongs to the training set. As mentioned in Section II-A, there are many machine unlearning methods designed to satisfy the privacy protection requirements. However, recent studies have demonstrated that machine unlearning also introduces additional privacy risks [11], [12], [14], [30]. Machine unlearning techniques modify the original model to achieve data removal, resulting in an unlearned model. The discrepancy between the original and unlearned model, such as model parameters and posteriors, which may contain the private information about unlearned data [31]. Chen et al. [12] investigated the privacy issues caused by machine unlearning. They proposed a novel membership inference attack strategy against machine unlearning, which employs the posterior differences generated by the original and unlearned models for the same samples to construct features to acquire the unlearned data's membership, providing evidence for privacy leakage caused by unlearning. Similarly, Gao et al. [14] presented a deletion inference and reconstruction framework in machine unlearning, which successfully extracts information about the unlearned data. Additionally, potential defense strategies for solving privacy issues also exist. For example, differential privacy (DP) has been proven to be effective in limiting the influence of samples on outputs [29], [32]. Therefore, using DP to train models can alleviate membership inference attacks against unlearning [12]; however, it compromises the model performance. Jia et al. [33] developed a defense scheme that adds noise to the confidence vectors to weaken membership inference attacks.",
            "score": 0.6019484477537216,
            "section_title": "B. Privacy risks in machine unlearning",
            "char_start_offset": 9229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "252917763"
                },
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 613,
                    "end": 617,
                    "matchedPaperCorpusId": "246652540"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "259899814"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "211066387"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "246652540"
                },
                {
                    "start": 1685,
                    "end": 1689,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 1691,
                    "end": 1695,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1795,
                    "end": 1799,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 1859,
                    "end": 1863,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "54444175",
            "title": "Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks",
            "text": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We perform a comprehensive analysis of white-box privacy inference attacks on deep learning models. We measure the privacy leakage by leveraging the final model parameters as well as the parameter updates during the training and fine-tuning processes. We design the attacks in the stand-alone and federated settings, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. \nWe design and evaluate our novel white-box membership inference attacks against deep learning algorithms to measure their training data membership leakage. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, widely used to train deep neural networks. We show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants of a federated learning setting can run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
            "score": 0.6018263068540299,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "237491948",
            "title": "Formalizing and Estimating Distribution Inference Risks",
            "text": "Distribution inference, sometimes called property inference, infers statistical properties about a training set from access to a model trained on that data. Distribution inference attacks can pose serious risks when models are trained on private data, but are difficult to distinguish from the intrinsic purpose of statistical machine learning\u2014namely, to produce models that capture statistical properties about a distribution. Motivated by Yeom et al.\u2019s membership inference framework, we propose a formal definition of distribution inference attacks general enough to describe a broad class of attacks distinguishing between possible training distributions. We show how our definition captures previous ratio-based inference attacks as well as new kinds of attack including revealing the average node degree or clustering coefficient of training graphs. To understand distribution inference risks, we introduce a metric that quantifies observed leakage by relating it to the leakage that would occur if samples from the training distribution were provided directly to the adversary. We report on a series of experiments across a range of different distributions using both novel black-box attacks and improved versions of the state-of-the-art white-box attacks. Our results show that inexpensive attacks are often as effective as expensive meta-classifier attacks, and that there are surprising asymmetries in the effectiveness of attacks.",
            "score": 0.6012274992565737,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8984375
        },
        {
            "corpus_id": "235358658",
            "title": "Formalizing Distribution Inference Risks",
            "text": "Property inference attacks reveal statistical properties about a training set but are difficult to distinguish from the primary purposes of statistical machine learning, which is to produce models that capture statistical properties about a distribution. Motivated by Yeom et al.'s membership inference framework, we propose a formal and generic definition of property inference attacks. The proposed notion describes attacks that can distinguish between possible training distributions, extending beyond previous property inference attacks that infer the ratio of a particular type of data in the training data set. In this paper, we show how our definition captures previous property inference attacks as well as a new attack that reveals the average degree of nodes of a training graph and report on experiments giving insight into the potential risks of property inference attacks.",
            "score": 0.6011440474498484,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "268364242",
            "title": "MPCPA: Multi-Center Privacy Computing with Predictions Aggregation based on Denoising Diffusion Probabilistic Model",
            "text": "The purpose of a membership inference attack (MIA) is to ascertain whether a given piece of data is part of the training set utilized in training a machine learning model.We evaluate membership inference with the loss threshold attack (Yeom et al., 2018).The loss threshold attack technique is based on the fact that training examples have lower loss than non-training examples because models are trained to minimize their loss on the training set.The loss threshold attack thus computes the loss l = L(x; f ) and reports \"member\" if l < \u03c4 for some chosen threshold \u03c4 and otherwise \"non-member\".\n\nTable 5 shows the MIA accuracy for each classifier model.Notably, classifiers trained on both original and generated data demonstrate a similar MIA accuracy compared to the original and federated models.This observation indicates that our method does not exacerbate privacy leaks.More results can be found in AppendixC and D.",
            "score": 0.6009597404873117,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 23952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 922
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 254,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "269293603",
            "title": "Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach",
            "text": "Membership inference2 is a task focused on determining whether a given data sample has been employed in the training of a machine learning model [17]. Figure 4 shows the pipeline of membership inference. Let  (;  * ) denote the target code completion model that is trained on a dataset D  = { 1 ,  2 , . . . ,   }, where  * is the model parameters. The fundamental concept behind membership inference is to leverage discrepancies in the behavioral patterns, specifically the predicted outputs, of the target model when presented with member and non-member data. In essence, the model is anticipated to exhibit superior performance on member data in comparison to non-member data, owing to its exposure to and memorization of the member data during the training phase. When presented with a novel data sample  = { 1 ,  2 , . . . ,  | | }, the target code completion model  (;  * ) processes it as input, producing a sequence of probability vectors denoted as \n\nHere, [EOS] denotes the end of the sequence. Each probability vector indicates the likelihood of generating the next code token. Subsequently, a membership classifier (\u210e  ;  * ) can be trained utilizing these probability vectors as input features: \n\nwhere 1 and 0 denote whether the query code sample  belongs to or does not belong to the training dataset of  (,  * ), respectively. \n\nBased on the knowledge accessible to users, membership inference approaches can be classified into two distinct categories: black-box and white-box membership inference. In the black-box setting, neither the dataset (including its underlying data distribution) nor the model architecture are accessible. When dealing with an arbitrary input , the user 3 is restricted to black-box access to the target model, from which they can extract a sequence of prediction vectors \u210e  . Conversely, in the white-box setting, the user possesses comprehensive knowledge about the target model, including both its architecture and parameters. As such, the black-box setting, with its limited knowledge availability, is considered a more realistic scenario than the white-box setting. \n\nKey Aspects and Challenges. It is worth highlighting several key aspects of this study as follows. \n\n\u2022 Black-Box Target Models. In this paper, we aim to study a more realistic scenario of code completion under the black-box setting, where the user is only allowed to access the model and obtain the predicted results.",
            "score": 0.6007839759487192,
            "section_title": "Membership Inference",
            "char_start_offset": 13391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2113
                },
                {
                    "start": 2116,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2214
                },
                {
                    "start": 2217,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "232233426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43505859375
        },
        {
            "corpus_id": "237532289",
            "title": "Membership Inference Attacks Against Recommender Systems",
            "text": "Membership Inference. The goal of membership inference is to infer whether a target data sample is used to train a machine learning model [6,22,26,28,30,39,42,43,52]. Shokri et al. [43] propose the first membership inference attack in this domain. The authors have made several key assumptions for the adversary, such as multiple shadow models and a shadow dataset which comes from the same distribution as the target model's training datasset. Salem et al. [39] gradually relax these assumptions and broaden the scenarios of membership inference attacks. Later, Nasr et al. [31] conduct a comprehensive membership privacy assessment in both centralized and federated learning setting. In particular, they propose the first membership inference when the adversary has white-box to the target model. Other research has shown that membership inference is effective under other machine learning settings, such as generative models [14], federated learning [8,29], and natural language models [45]. Besides, a plethora of other attacks have been proposed against machine learning models [3,5,7,12,20,21,33,48,49]. \n\nItem-Based Recommendation Algorithms. Item-based recommendation techniques have been applied in various scenarios [11,23,40]. Sarwar et al. [40] explore item-based collaborative filtering (CF) techniques which enhance the scalability and quality of the CF-based algorithms. Besides, Deshpande and Karypis [11,23] present item-based top-N recommendation algorithms to promote the efficiency and performance. \n\nLatent Factor Models. LFM aims to find some latent factors and is commonly implemented by Matrix Factorization (MF) [24,25,36,38]. Polat et al. [36] combine SVD-based Collaborative Filtering with privacy to achieve accurate predictions while preserving privacy. Later, Salakhutdinov et al. [38] propose the Probabilistic Matrix Factorization which scales linearly with the number of observations and performs well on very sparse and imbalanced datasets. Koren [24] presents an integrated model that combines the neighborhood and LFM, which optimizes a global cost function and integrates implicit feedback into the model.",
            "score": 0.6007159455636657,
            "section_title": "Related Work",
            "char_start_offset": 47947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 141,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "235349092"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 953,
                    "end": 956,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1083,
                    "end": 1086,
                    "matchedPaperCorpusId": "18716873"
                },
                {
                    "start": 1086,
                    "end": 1088,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1088,
                    "end": 1090,
                    "matchedPaperCorpusId": "2893830"
                },
                {
                    "start": 1093,
                    "end": 1096,
                    "matchedPaperCorpusId": "211858541"
                },
                {
                    "start": 1096,
                    "end": 1099,
                    "matchedPaperCorpusId": "4551073"
                },
                {
                    "start": 1099,
                    "end": 1102,
                    "matchedPaperCorpusId": "6726938"
                },
                {
                    "start": 1102,
                    "end": 1105,
                    "matchedPaperCorpusId": "21946795"
                },
                {
                    "start": 1105,
                    "end": 1108,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1226,
                    "end": 1230,
                    "matchedPaperCorpusId": "207650042"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "1661995"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "8047550"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "8047550"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "207650042"
                },
                {
                    "start": 1421,
                    "end": 1424,
                    "matchedPaperCorpusId": "1661995"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "207168823"
                },
                {
                    "start": 1641,
                    "end": 1644,
                    "matchedPaperCorpusId": "3022077"
                },
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "6878871"
                },
                {
                    "start": 1647,
                    "end": 1650,
                    "matchedPaperCorpusId": "467086"
                },
                {
                    "start": 1665,
                    "end": 1669,
                    "matchedPaperCorpusId": "6878871"
                },
                {
                    "start": 1811,
                    "end": 1815,
                    "matchedPaperCorpusId": "467086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "236956533",
            "title": "Membership Inference Attacks on Lottery Ticket Networks",
            "text": "Membership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021). \n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too. \n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches. Defenses against such attacks have been studied as well and mostly focus around reducing overfitting and reducing the influence of certain data points. Nasr et al. (2018) suggest using adversarial regularization training to defend against this. Shokri et al. (2017) suggested defense techniques like restricting the prediction vector to top k classes; however, highly accurate attacks are still possible even when the model reveals minimal information (Li & Zhang, 2020).",
            "score": 0.5999457847576347,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 2599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 476,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 870,
                    "end": 891,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 956,
                    "end": 978,
                    "matchedPaperCorpusId": "216652989"
                },
                {
                    "start": 1014,
                    "end": 1030,
                    "matchedPaperCorpusId": "209439842"
                },
                {
                    "start": 1445,
                    "end": 1465,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "220525609",
            "title": "A Survey of Privacy Attacks in Machine Learning",
            "text": "One of the conditions that has been shown to improve the accuracy of membership inference is the poor generalization of the model. The connection between overfitting and black-box membership inference was initially investigated by Shokri et al. [101]. This paper was the first to examine membership inference attacks on neural networks. The authors measured the effect of overfitting on the attack accuracy by training models in different MLaaS platforms using the same dataset. The authors showed experimentally that overfitting can lead to privacy leakage but also noted that it is not the only condition, since some models that had lower generalization error where more prone to membership leaks. The effect of overfitting was later corroborated formally by Yeom et al. [125]. The authors defined membership advantage as a measure of how well an attacker can distinguish whether a data sample belongs to the training set or not, given access to the model. They proved that the membership advantage is proportional to the generalization error of the model and that overfitting is a sufficient condition for performing membership inference attacks but not a necessary one. Additionally, Long et al. [67] showed that even in well-generalized models, it is possible to perform membership inference for a subset of the training data which they named vulnerable records. \n\nOther factors, such as the model architecture, model type, and dataset structure, affect the attack accuracy. Similarly to [101] but in the white-box setting, Nasr et al. [80] showed that two models with the same generalization error showed different degrees of leakage. More specifically, the most complex model in terms of number of parameters exhibited higher attack accuracy, showing that model complexity is also an important factor. \n\nTruex et al. [110] ran different types of experiments to measure the significance of the model type as well as the the number of classes present in the dataset. They found that certain model types such as Naive Bayes are less susceptible to membership inference attacks than decision trees or neural networks. They also showed that as the number of classes in the dataset increases, so does the potential of membership leaks. This finding agrees with the results in [101]. \n\nSecuring machine learning models against adversarial attacks can also have an adverse effect on the model's privacy as shown by Song et al. [105].",
            "score": 0.5994633165067913,
            "section_title": "Causes of Membership Inference Attacks",
            "char_start_offset": 29035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2283
                },
                {
                    "start": 2286,
                    "end": 2432
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 250,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 773,
                    "end": 778,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1493,
                    "end": 1498,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1541,
                    "end": 1545,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1824,
                    "end": 1829,
                    "matchedPaperCorpusId": "86836429"
                },
                {
                    "start": 2277,
                    "end": 2282,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "245423308",
            "title": "Evaluating Differentially Private Generative Adversarial Networks Over Membership Inference Attack",
            "text": "Since the concept of membership inference attack against general machine learning models was introduced, various studies have been conducted to analyze and advance the attack [8]- [10], [30]. In particular, attack methods have been proposed that focus on generative models with a different aspect from previous studies that targeted general machine learning models. Since the outputs of a generative model are synthetic data rather than predictions, it is necessary to consider a different approach from the previous methods. By capturing these points, Hayes et al. [8] proposed an attack method with the goal of membership inference against GAN models. Subsequently, inspired by Hayes et al.'s study, several attack methods have been proposed with different assumptions and attack scenarios [9], [10] in terms of distance metric. We focus on these attacks as our goal is to analyze the relationship between substantial privacy invasion attacks and privacy-preserving techniques over GAN models (a detailed analysis of each membership inference attack against GAN is covered in section 3).",
            "score": 0.5983935526056157,
            "section_title": "B. PRIVACY INVASION ATTACKS ON AI MODELS",
            "char_start_offset": 9944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 178,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 186,
                    "end": 190,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 566,
                    "end": 569,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "221203089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86572265625
        },
        {
            "corpus_id": "227209424",
            "title": "Use the Spear as a Shield: An Adversarial Example Based Privacy-Preserving Technique Against Membership Inference Attacks",
            "text": "attack without affecting the model's performance. In Section V, we have compared the performance of the proposed method with the works [1]- [3].\n\nIn general, the adversarial examples are generated to attack the deep learning models. However, adversarial examples can also be applied for privacy-preserving. Jia and Gong [19], [20] propose a defense method (named AttriGuard), which adds adversarial noise to the user's public data in order to minimize the inference accuracy of the attribute inference attacks. Different from the works [19], [20], this paper targets at defending against the membership inference attack. Membership inference attack is more powerful than attribute inference attack thus is more difficult to defeat. The reason is that attribute inference attack requires white-box access to the target model and can only infer whether the training data of the target model has specific attribute. On the contrary, the membership inference attack can be launched under black-box scenario, and can infer whether the training data of the model contains a specific data. Moreover, the proposed defense method can be applied under black-box scenarios, while the AttriGuard [19] requires white-box access to the target model.\n\nWe proposed the idea of this paper in May 2019, and applied for a Chinese patent [21] in July 2019. To date, the only concurrent work with this paper is MemGuard [8]. The method [8] formulates the defense of membership inference attacks as solving the optimization problem. First, they search for noise that does not affect the utility of the target model but can mislead the membership inference models. Then, they add the noise to the prediction of target model with a certain probability [8]. There are several differences between the proposed method and the MemGuard method [8]. First, the Mem-Guard method [8] generates adversarial perturbations by solving the optimization problem, i.e., the optimization-based method, while the proposed method crafts adversarial perturbations by calculating the gradient of the loss function, i.e., the gradient-based method. Therefore, compared with the AEPPT method, the optimization-based MemGuard method will go through much more iterations to",
            "score": 0.5983745405455656,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 12721,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 143,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "44108074"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "202660921"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "44108074"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "202660921"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "44108074"
                },
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1415,
                    "end": 1418,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1728,
                    "end": 1731,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1815,
                    "end": 1818,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1848,
                    "end": 1851,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "263834611",
            "title": "Improved Membership Inference Attacks Against Language Classification Models",
            "text": "There are several types of privacy (inference) attacks against ML models, including membership inference, attribute inference, model inversion, database reconstruction, and most recently, training data extraction from generative models. The most commonly researched and employed attack is the membership inference attack, with dozens of papers published each year [5], and implementations being made available in open-source privacy assessment frameworks [7], [12]. \n\nMI attacks attempt to distinguish between members, which were part of a target model's training data, and non-members. MI attacks have been extensively studied in the context of classification models and in the black-box setting, where the model internals are unknown to the attacker. The first MI attacks were either threshold-based [17] or employed binary classifiers trained to distinguish between members and non-members based on model outputs [15]. For example, these outputs may include class probabilities or logits (for classification models), the model's loss, and possibly also activations from internal layers of the model (in white-box attacks) [11]. To generate labeled (member/non-member) data to train the attack classifier, without knowledge of the true member samples of the attacked model, shadow models are commonly used [15]. \n\nIn the past few years, investigations have begun into MI in the context of large language models (LLM), starting with embedding models and masked language models [16], [8], [10]. [14] looked at a similar setting as ours, focusing on NLP classification models. They proposed mostly threshold-based attacks, examining different features that can be used to distinguish between members and nonmembers. [6] focused specifically on language models that were fine-tuned for the medical domain, including classification tasks such as MedNLI, employing both black-box and white-box attacks. Their black-box attack applied thresholds to the training error of samples. More recently, Likelihood Ratio Attacks (LiRA) have been proposed [2], which compare target model scores to those obtained from a reference model trained on similar data. [9] tried to relieve the assumption that an adversary has access to samples closely resembling the original training data by utilizing synthetically generated neighbor texts.",
            "score": 0.5979668777887075,
            "section_title": "Related Work",
            "char_start_offset": 4211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "220647336"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "214743021"
                },
                {
                    "start": 1495,
                    "end": 1499,
                    "matchedPaperCorpusId": "245222525"
                },
                {
                    "start": 2041,
                    "end": 2044,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9814453125
        },
        {
            "corpus_id": "270199828",
            "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
            "text": "Membership inference attacks (Shokri et al., 2017;Hu et al., 2022) are a type of privacy threat, where an attacker aims to determine whether a specific data record was used in the training set of a machine learning model. This carries significant privacy implications as it can potentially reveal sensitive information about individuals, even if the model does not directly release any personal data. \n\nFormally, an attacker aims to determine the membership of a sample x in the training data D m of a target model m, i.e., to check if x \u2208 D m . This is known as sample-level membership inference. Typically, these attacks involve calculating one or more metrics on the target model's outputs that reflect the probability of the sample being a part of the training set, such as the model outputs' entropy or log-probabilities (Carlini et al., 2022). Several metrics may be computed for each sample and then fused together using a machine learning model, known as an attack model, which in turn outputs the probability of a sample being a member of the training set. \n\nAdditionally, an attacker may also aim to determine the membership of a certain user, i.e., to check if a user's data is part of the training set, which is known as user-level membership inference (Shejwalkar et al., 2021). Throughout this paper we will address the membership inference challenge from a sample-level perspective. \n\nIn the context of RAG, membership inference can be attributed to either the membership of a sample in the training dataset of the models E or G (described in the previous subsection 2.1), or a document's membership in the retrieval dataset D. This paper focuses on the latter. Formally, the goal of the attack is to infer the membership of a target document d in the retrieval database D, i.e., to check if d \u2208 D, using only the final output of the RAG system, namely the output of the generative model G conditioned on the fetched context from the retrieval database D. \n\nTo the best of our knowledge, this is the first paper to propose such a membership inference attack tailored to RAG systems.",
            "score": 0.5976316569915974,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 6610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 50,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 50,
                    "end": 66,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 826,
                    "end": 848,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1265,
                    "end": 1290,
                    "matchedPaperCorpusId": "245222525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "251953448",
            "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
            "text": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
            "score": 0.5974900459914566,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "A particularly canonical approach, along these lines, is the attack introduced by Shokri et al. [37]. In this approach, a shadow model is trained on half of S, Sin , and an attack model is trained on the the outputs of the shadow model on its training data, Sin (labeled 1), and the remaining data S \\ Sin (labeled 0). Shadow models leverage the disparity in prediction confidences on training instances the target model has overfit to, and have been shown to be successful at membership inference on models that have sufficiently high generalization error. A few other membership inference approaches [15,34] have made use of this same technique. \n\nDespite the fact that shadow model attacks leverage more information than the naive attack, we find in our evaluation (Section 5) that often, the shadow model attack fails to outperform the naive attack. One potential reason for this finding is that the learned attack model used by this approach to distinguish between the shadow model's outputs on members and non-members may be itself subject to overfitting. This may be especially true if the attack model picks up on behavior particular to one of the shadow models rather than the true target model. Furthermore, the confidence and entropy of the target model's softmax output is likely to be closely related to whether the target model's prediction was correct or not, meaning that the softmax outputs may not provide substantially different information from that used by naive. \n\nWhite-box attacks. In some settings, it may be realistic for an attacker to have white-box access to the target model. Intuitively, while some information is leaked via the behavior of a model, the details of the structure and the parameters of the model are clear culprits for information leakage. Few prior approaches have successfully leveraged this extra information. While Hayes et al. [15] describe a white-box attack in their work on membership inference attacks applied to GANs, the attack uses access only the outputs of the discriminator portion of the GAN, rather than the learned weights of either the discriminator or the generator; thus their approach is not white-box in the same sense.",
            "score": 0.597106863834257,
            "section_title": "Related Work",
            "char_start_offset": 59410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 609,
                    "matchedPaperCorpusId": "46933970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "245502286",
            "title": "DP-UTIL: Comprehensive Utility Analysis of Differential Privacy in Machine Learning",
            "text": "In this component, we assume that multiple inference attacks can be plugged or existing attacks can be replaced with more recent when the state-of-the-art evolves. Among inference attacks are membership inference [16,25,30], attribute inference [23], model inversion [10], and model parameter inference/extraction [33]. In its current version, DP-UTIL supports the popular attack of membership inference attack [30], which we introduce next.\n\nIn membership inference, an adversary observes a model's prediction confidence on members of its training set versus non-members to determine whether a target sample is part of the model's training data. To uniformly analyze the utility of DP across the perturbation mechanisms, we use the membership inference attack introduced by Shokri et al [30]. For this attack, multiple shadow models trained on auxiliary data of the same distribution as the target model's training data are used to train an attack model. We keep our shadow models' architecture the same as the target model's architecture. The attack model is a binary classifier that predicts whether a particular sample is a member of the target model's training dataset or not. We use 10 shadow models to train our Random Forest attack model. When we attack the target model, we assume black-box access to each model, i.e., the attacker submits an input sample to a prediction API which returns prediction output of confidence score vector and a label.\n\nOur choice of membership inference attack is informed by its conceptual connection to the primary goal of DP, which is to make the presence/absence of a data sample indistinguishable in the eyes of an adversary. Membership inference essentially aims to achieve the opposite goal: determine, with high confidence, whether a given data sample is present or absent in a training set of a target model. This antagonistic setup between the two makes membership inference a natural fit to showcase DP-UTIL.",
            "score": 0.5966643552603956,
            "section_title": "Privacy Motivated Inference Attacks",
            "char_start_offset": 20911,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94677734375
        },
        {
            "corpus_id": "231846491",
            "title": "Quantifying and Mitigating Privacy Risks of Contrastive Learning",
            "text": "Membership inference attack is one of the most popular privacy attacks against ML models [7,8,10,19,28,31,33,49,52,57]. The goal of membership inference is to determine whether a data sample x is part of the training dataset of a target model T . We formally define a membership inference attack model A MemInf : x, T \u2192 {member, non-member}. \n\nHere, the target model is the contrastive model introduced in Section 2. A successful membership inference attack can cause severe privacy risks. For instance, if a model is trained on data samples collected from people with certain sensitive information, then successfully inferring a sample from a person being a member of the model can directly reveal the person's sensitive information. \n\nFollowing previous work [10,33,49,52,57], we assume that an adversary only has black-box access to the target model T , i.e., they can only query T with their data samples and obtain the outputs. In addition, the adversary also has a shadow dataset D shadow , which comes from the same distribution as the target model's training dataset. The shadow dataset D shadow is used to train a shadow model S, the goal of which is to obtain the necessary information to perform the attack. We further assume that the shadow model shares the same architecture as the target model [52]. This is realistic as the adversary can use the same machine learning service as the target model owner to train their shadow model. Alternatively, the adversary can also learn the target model's architecture first by applying model extraction attacks [40,41,60,63].",
            "score": 0.5964800583358411,
            "section_title": "Attack Definition and Threat Model",
            "char_start_offset": 11960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1579
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 100,
                    "end": 103,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 103,
                    "end": 106,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 771,
                    "end": 774,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 774,
                    "end": 777,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "3278569"
                },
                {
                    "start": 1569,
                    "end": 1572,
                    "matchedPaperCorpusId": "54457412"
                },
                {
                    "start": 1572,
                    "end": 1575,
                    "matchedPaperCorpusId": "2984526"
                },
                {
                    "start": 1575,
                    "end": 1578,
                    "matchedPaperCorpusId": "3423242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "252992499",
            "title": "Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries",
            "text": "Membership inference algorithms are designed to determine whether a target data point was present in the training set of a model. Membership inference is often studied in the context of ML privacy, as there are situations where belonging to a dataset is itself sensitive information (e.g. a model trained on a group of people with a rare disease). However, it is also relevant in other social and regulatory contexts as legislators have begun developing a slew of regulations with the intention of protecting data ownership. The right to be forgotten, which was written into the European Union's strict GDPR law, has important implications for the operation of ML-as-a-service (MLaaS) providers (Wilka et al., 2017;Truong et al., 2021). As one example, Veale et al. (2018) discuss that machine learning models could legally (in terms of the GDPR) fall into the category of \"personal data\", which equips all parties represented in the data with rights to restrict processing and to object to their inclusion. However, such rights are vacuous if enforcement agencies are unable to detect when they are violated. Membership inference could potentially be used as a legal tool against a noncompliant or malicious MLaaS provider. \n\nBecause membership inference is a difficult task, the typical setting for existing work is generous to the attacker and assumes full white-box access to model weights. In the aforementioned legal scenario this is not a realistic assumption. Organizations have an understandable interest in keeping their proprietary model weights secret and, short of a legal search warrant, often only provide blackbox querying to their clients (OpenAI, 2020). Moreover, even if a regulatory agency obtained whitebox access via an audit, for example, a malicious provider could adversarially spoof the reported weights to cover up any violations. \n\nIn this paper, we achieve state-of-the-art performance for membership inference in the black-box setting by using a new adversarial approach.",
            "score": 0.5959873167119722,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 695,
                    "end": 715,
                    "matchedPaperCorpusId": "69361196"
                },
                {
                    "start": 715,
                    "end": 735,
                    "matchedPaperCorpusId": "235142501"
                },
                {
                    "start": 753,
                    "end": 772,
                    "matchedPaperCorpusId": "49671507"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "224817947",
            "title": "Amnesiac Machine Learning",
            "text": "Machine learning models have been shown to leak information about the data they've been trained on (Homer et al. 2008;Shokri et al. 2017;Fredrikson et al. 2014;Fredrikson, Jha, and Ristenpart 2015;Hitaj, Ateniese, and Perez-Cruz 2017). Specifically, two main kinds of information leaking attacks have been studied: membership inference attacks that leak information about the presence of specific records in the training data (Yeom et al. 2018) and model inversion attacks that leak class information (Fredrikson, Jha, and Ristenpart 2015;Hitaj, Ateniese, and Perez-Cruz 2017). Membership inference attacks determine whether a particular record was present in the training data for a model. This attack was first presented in 2008 (Homer et al. 2008) and was formalized in 2015 (Dwork et al. 2015). Since then, considerable work has been done on membership inference attacks and defense mechanisms against such membership inferences (Nasr, Shokri, and Houmansadr 2018;Shokri et al. 2017;Yeom et al. 2018). Property inference attacks are a subset of membership inference attacks that determine a general property of the training data, such as the ratio of training examples in each class (Ganju et al. 2018). Model inversion attacks, introduced by Fredrikson et al. in 2014(Fredrikson et al. 2014) and expanded to vision tasks in 2015 (Fredrikson, Jha, and Ristenpart 2015), have been shown to recreate instances of records from trained ML models. Given white-box access to a trained model, examples of target classes or points near a regression value can be recreated. In this paper, we utilize both state-of-the-art membership inference attacks and model inversion attacks to evaluate how likely a model is to leak private data.\n\nThere is an abundance of literature on differential privacy which provides an upper bound on the amount of information that can be leaked from each individual data record (Dwork, Roth et al. 2014;Dwork et al. 2006;Chaudhuri, Monteleoni, and Sarwate 2011;Bassily, Smith, and Thakurta 2014). Cummings and Desai address the need for training machine learning models in a differential",
            "score": 0.5959686113201081,
            "section_title": "Related Work",
            "char_start_offset": 5463,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 118,
                    "end": 137,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 137,
                    "end": 160,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 160,
                    "end": 197,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 197,
                    "end": 234,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 426,
                    "end": 443,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 501,
                    "end": 539,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 539,
                    "end": 576,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 731,
                    "end": 750,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 778,
                    "end": 797,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 968,
                    "end": 987,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 987,
                    "end": 1004,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1187,
                    "end": 1206,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 1247,
                    "end": 1272,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1272,
                    "end": 1295,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1334,
                    "end": 1372,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "50778569",
            "title": "Towards Demystifying Membership Inference Attacks",
            "text": "Most existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28,41]. However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning. Concretely, adversarial deep learning research to date has been centered on the generation of adversarial examples by injecting minimal amount of perturbation to a benign example such that the pre-trained classification model will misclassify with high probability. Thus, adversarial example-based attacks aim at altering the output of the model prediction without being visually noticed. On the contrary, a membership inference attack does not alter the prediction output at all, and it succeeds by simply making membership inference on the prediction output. \n\nScope and Contributions of the paper. In this paper, we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model. Our research results are novel from three perspectives. First, we describe a systematic approach to construct a membership inference attack model and the general formulation of each component of the attack model generation framework. We show that generating a membership inference attack model is a complex and multi-step strategic process. Second, to understand when and how membership inference attacks work and why certain models and datasets are more vulnerable, we take a holistic approach with extensive empirical evidence to study and characterize membership inference attacks across different target model types, different types of training datasets, and different combinations of model types for generating attack training dataset and attack models. Finally, we introduce and investigate a new membership threat, insider membership inference, which is launched by a member of a federated learning system against other participants in an collaborative learning environment. As federated learning systems become more popular with promises of increased accuracy and privacy, highlighting and understanding this risk is an important part of membership inference mitigation effort.",
            "score": 0.5958440217570722,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2304
                },
                {
                    "start": 2305,
                    "end": 2508
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "221586480",
            "title": "Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning",
            "text": "In machine learning, the objective of membership inference is to determine whether a data sample was used to train the machine learning models. Knowing the membership status of individual user's data may cause severe information leakage. For example, knowing that a certain patient's clinical records were used to train a model associated with a disease (e.g., to determine the appropriate drug dosage or to discover the genetic basis of the disease) can reveal that the patient carries the associated disease. Formally, membership inference attack can be defined as the following function: \n\nHere, x target is a target data sample, M is the target model, and K is the auxiliary knowledge of adversaries. The output value equals 1 means that x target is a member of M 's training dataset D Train and 0 otherwise. The attack model A is essentially a binary classifier.",
            "score": 0.5949799476761434,
            "section_title": "Membership Inference against ML Models",
            "char_start_offset": 9379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "275306689",
            "title": "Multi-Task Federated Split Learning Across Multi-Modal Data with Privacy Preservation",
            "text": "White-Box Membership Inference (MIAs W Sh ). Similar to the black-box attack, the adversary trains a shadow model to emulate the target model. However, with white-box access, the attacker exploits additional information, such as gradient updates, to extract enhanced features for membership prediction. This approach achieves higher accuracy by leveraging insights from stochastic gradient descent. \n\nWe conducted experiments using the MNIST dataset to measure the framework's effectiveness. The dataset was divided into four subsets: target training, target testing, shadow training, and shadow testing, following a sample ratio of 2:1:2:1. Models were trained with varying privacy budgets, defined by the parameters \u03f5 and \u03b4, while maintaining consistent training configurations as outlined earlier. \n\nTable 1 summarizes the results of MIAs under varying privacy budgets (\u03f5 = 2, 4, 6, 8) and a non-private setting. The findings show that, under privacy-preserving conditions, the accuracy of MIAs remains close to random guessing, demonstrating the robustness of the proposed framework. For example, on the MNIST dataset, MIA accuracy under privacy-preserving configurations is approximately 0.50, even as \u03f5 increases, indicating effective mitigation of membership leakage. In contrast, under non-private conditions, MIA accuracy is significantly higher (e.g., 0.59 for MNIST), highlighting the vulnerability of unprotected models.",
            "score": 0.5949190754799493,
            "section_title": "\u2022",
            "char_start_offset": 24495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "270309880",
            "title": "Detecting, Explaining, and Mitigating Memorization in Diffusion Models",
            "text": "Membership Inference. The membership inference attack (Shokri et al., 2017) aims to determine if a particular data point was used in the training set of a model. Traditional studies on membership inference (Shokri et al., 2017;Yeom et al., 2018;Carlini et al., 2022;Wen et al., 2022) have predominantly focused on classifiers. An attacker can utilize losses or confidence scores as a metric. This is because data points from the training set typically exhibit lower losses or higher confidence scores than the unseen data points during inference due to overfitting. In a parallel development, recent works (Matsumoto et al., 2023;Duan et al., 2023;Wang et al., 2024;2023) have extended membership inference to diffusion models. These methodologies involve introducing noise to a target image and subsequently verifying if the predicted noise aligns closely with the induced noise. \n\nTraining Data Extraction. Somepalli et al. (2022) demonstrate that diffusion models memorize a subset of their training data, often producing the training image verbatim. Building on this fact, Carlini et al. (2023) introduce a black-box data extraction attack designed for diffusion models. This approach involves generating a multitude of images and subsequently applying a membership inference attack to assess generation density. Notably, they observe that memorized prompts tend to produce nearly identical images across different seeds, leading to high density. This strategy bears resemblance to the pipeline used by Carlini et al. (2021), who successfully extract training data from large language models with over a billion parameters. Additionally, they discover that larger models are more susceptible to data extraction attacks compared to their smaller counterparts. Daras et al. (2023) presents a method for training diffusion models using corrupted data. In their study, they demonstrate that their proposed training algorithm aids in preventing the model from overfitting to the training data. Their approach involves introducing additional corruption prior to the noising step and subsequently calculating the loss on the original input image. In a separate study, Somepalli et al. (2023b) delve into various mitigation strategies, with a focus on altering the text conditions.",
            "score": 0.5945417560973867,
            "section_title": "RELATED WORK",
            "char_start_offset": 4842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 75,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 206,
                    "end": 227,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 227,
                    "end": 245,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 245,
                    "end": 266,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 648,
                    "end": 666,
                    "matchedPaperCorpusId": "268095526"
                },
                {
                    "start": 1507,
                    "end": 1528,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "273228163",
            "title": "Defending Membership Inference Attacks via Privacy-aware Sparsity Tuning",
            "text": "Setup In this paper, we study the problem of membership inference attacks in K-class classification tasks. Let the feature space be X \u2282 R d and the label space be Y = {1, . . . , K}. Let us denote by (x, y) \u2208 (X \u00d7Y) an example containing an instance x and a real-valued label y. Given a training dataset S = {(x n , y n )} N i=1 i.i.d. sampled from the data distribution P, our goal is to learn a model h \u03b8 with trainable parameters \u03b8 \u2208 R p , that minimizes the following expected risk: \n\n(1) where E (x,y)\u223cP denotes the expectation over the data distribution P and L is a conventional loss function (such as cross-entropy loss) for classification. In modern deep learning, the neural network h \u03b8 is typically over-parameterized, allowing to easily disclose the information of training data (Tan et al., 2022a). \n\nMembership Inference Attacks Given a data point (x, y) and a trained target model h S , attackers aim to identify if (x, y) is one of the members in the training set S, which is called membership inference attacks (MIAs) (Shokri et al., 2017;Yeom et al., 2018;Salem et al., 2019). In MIAs, it is generally assumed that attackers can query the model predictions h \u03b8 (x) for any instance x. \n\nHere, we focus on standard black-box attacks (Irolla & Ch\u00e2tel, 2019), where attackers can access the knowledge of model architecture and the data distribution P. \n\nIn the process of attack, the attacker has access to a query set \n\n, where z i denotes the i-th data point (x i , y i ) and m i is the membership attribute of the given data point (x i , y i ) in the training dataset S, i.e., m i = I[(x i , y i ) \u2208 S]. In particular, the query set Q contains both member (training) and non-member samples, drawn from the data distribution P.",
            "score": 0.5944937303966011,
            "section_title": "BACKGROUND",
            "char_start_offset": 4553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 791,
                    "end": 810,
                    "matchedPaperCorpusId": "246485733"
                },
                {
                    "start": 1035,
                    "end": 1056,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1056,
                    "end": 1074,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1074,
                    "end": 1093,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1250,
                    "end": 1273,
                    "matchedPaperCorpusId": "210888275"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "272423578",
            "title": "Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?",
            "text": "Membership Inference Attacks (MIA) [20,31,50,55,65,74] have emerged as a significant threat to privacy in the context of machine learning models. These attacks aim to reveal the membership status of a target sample, i.e., whether the sam-ple was part of the training dataset or not, thereby directly breaching privacy. \n\nThe seminal work by Shokri et al. [55] introduced MIA against machine learning models, wherein multiple shadow models were trained to mimic the behavior of the target model. This attack originally required access to data from the same distribution as the training dataset. However, Salem et al. [50] relaxed this assumption by demonstrating the effectiveness of using only a single shadow model, substantially reducing the computational cost involved. \n\nSubsequent research [12,31] has explored more challenging settings for MIA. In these scenarios, the adversary only has access to hard-label predictions from the target model. Li and Zhang [31] proposed a method that approximates the distance between the target sample and its decision boundary using adversarial examples, enabling the attacker to make decisions based on this distance. \n\nRecent advancements in MIA have focused on enhancing attack performance. Carlini et al. [7] leveraged the discrepancy between models trained with and without the target sample to improve attack effectiveness. Liu et al. [34] demonstrated the utility of loss trajectory analysis in MIA. Furthermore, Tram\u00e8r et al. [61] highlighted the potential of data poisoning, showing that even with access to a small fraction of the training dataset, the attacker can significantly boost the performance of membership inference attacks.",
            "score": 0.5936351305361321,
            "section_title": "K.1 Membership Inference Attack",
            "char_start_offset": 65364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 42,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 42,
                    "end": 45,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 45,
                    "end": 48,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "272367776"
                },
                {
                    "start": 51,
                    "end": 54,
                    "matchedPaperCorpusId": "264820351"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "251953448"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "247922814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "259129905",
            "title": "10 Security and Privacy Problems in Large Foundation Models",
            "text": "In the past several years, membership inference attacks have been studied for image classifiers [18][19][20][21][22][23][24][25], image generative models [26,27], language models [28], and graph neural networks [29]. For instance, Shokri et al. [18] extended membership inference attack [30,31] to machine learning classifier, where an attacker trains a binary attack classifier to infer whether a data sample is in the training dataset of a target classifier by leveraging the confidence scores predicted by the target classifier for the data sample. Carlini et al. [28] showed that an attacker can infer whether a text is used to train a language model, e.g., a text is likely to be a member of a language model's training data if the language model assigns high likelihood for the text. He et al. [29] showed that an attacker can infer whether there exists an edge between two nodes in a graph via black-box access to a graph neural network trained on the graph. \n\nMany methods [18,20,32,33] have been proposed to defend against membership inference attacks such as regularization [18,20], ensemble method [20], differential privacy [34], game-theoretic method [32], adversarial example based method [33], and so on [35]. For instance, Jia et al. [33] proposed MemGuard, which leverages adversarial examples to defend against membership inference attacks. In particular, the idea is to add carefully crafted noise to the confidence scores predicted by a target classifier for a data sample such that an attacker's classifier cannot correctly predict whether the data sample is a member of the target classifier's training dataset. \n\nRecently, two concurrent works [17,36] studied membership inference attacks to contrastive learning. In particular, Liu et al. [17] developed the first membership inference attack, namely EncoderMI, to image encoders pre-trained by contrastive learning.",
            "score": 0.5924709440150739,
            "section_title": "Six Problems on Confidentiality",
            "char_start_offset": 22337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 104,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 112,
                    "end": 116,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "218502486"
                },
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "218502486"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1136,
                    "end": 1140,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 1164,
                    "end": 1168,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "202734167"
                },
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "202734167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "244345608",
            "title": "Enhanced Membership Inference Attacks against Machine Learning Models",
            "text": "Privacy Risk Analysis with Membership Inference Attacks. [11] performed the first membership inference attack on genome data to identify the presence of an individual's genome in a mixture of genomes. [1,28] provided a formal analysis of this risk of detecting the presence of an individual from aggregate statistics computed on independent (binary or continuous) attributes. [22] extended this analysis to the case of releasing discrete Bayesian networks learned from data with dependent attributes. [8] provide a more extensive analysis when the released statistics are noisy and the attacker has only one reference sample to perform the attack. These works establish the privacy risk of releasing aggregate statistics by quantifying the success of membership inference attacks as a function of the number of statistics released and individuals in the dataset. We refer the reader to [7] for a survey.\n\nDifferential Privacy and Membership Inference: The definitions of differential privacy [6] and membership inference [8,11,29] are very closely connected. By definition, differentially private algorithms bound the success of membership inference attacks for distinguishing between two neighboring datasets. Multiple works [9,12,31,37], each improving on the previous work, have provided upper bounds on the average success of membership inference attacks over general targets as a function of the parameters in differential privacy. [14,25] evaluated the performance of membership inference attacks on machine learning models trained with differentially private algorithms. Moreover, the empirical performance of membership inference attacks has also been used to provide lower bounds on the privacy guarantees achieved by various differentially private algorithms [13,19,24]. These works call for stronger membership inference attacks that could measure the leakage through the model about a particular point of interest, which is what we are trying to do in this paper.\n\nMembership Inference Attack for Machine Learning Models. [29] demonstrated the vulnerability of machine learning models to membership inference attacks in the black-box setting, where the adversary has only query access to the target model. The attack algorithm is based on the concept of shadow models, which are models trained on some attacker dataset that is similar to that of the training data. A substantial body of literature followed this work extending the shadow model attacks to different setting such as white box analysis [15,23,26], label-only access [5,16], and federated learning [20,23].",
            "score": 0.5924639696215864,
            "section_title": "RELATED WORK",
            "char_start_offset": 8883,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "1154437"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "231950082"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "26766335"
                },
                {
                    "start": 992,
                    "end": 995,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 1021,
                    "end": 1024,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 1024,
                    "end": 1027,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 1027,
                    "end": 1030,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "173990571"
                },
                {
                    "start": 1769,
                    "end": 1773,
                    "matchedPaperCorpusId": "219686822"
                },
                {
                    "start": 1773,
                    "end": 1776,
                    "matchedPaperCorpusId": "235358770"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "231583084"
                },
                {
                    "start": 2034,
                    "end": 2038,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96142578125
        },
        {
            "corpus_id": "231418955",
            "title": "Towards a Robust and Trustworthy Machine Learning System Development",
            "text": "Membership inference attacks attempt to determine if a sample was a member of the training dataset [6]. Shokri et al. [102] quantitatively explore how machine learning models leak information about the individual data records. Given a data record and black-box access to a model, they try to determine if the record was in the model's training dataset. To perform membership inference against a target model, they trained their own model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not.\n\nOverfitting is an important reason why machine learning models leak information about their training datasets. Regularization techniques such as dropout can help defeat overfitting and also strengthen privacy guarantees in neural networks. Regularization is also used for objective perturbation in differentially private machine learning.",
            "score": 0.5922554384554022,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 58500,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "13754095"
                },
                {
                    "start": 118,
                    "end": 123,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "201660180",
            "title": "On Inferring Training Data Attributes in Machine Learning Models",
            "text": "The introduction of low-cost machine learning APIs from Google, Microsoft, Amazon, IBM, etc., has enabled many companies to monetize advanced machine learning models trained on private datasets by exposing them as a service. This has also caught the interest of the privacy community who has shown that these models may leak information about the records of the training dataset via membership inference attacks (MIA). In MIA, the adversary (a user of the service) may have simple black-box access to these models, submitting inputs (records) and obtaining outputs (class labels and probability/confidence for each label), can infer whether its target input was part of the training dataset or not. This can be a serious privacy breach when the underlying dataset is sensitive, e.g., medical data. \n\nThe main reason for the success of an MIA is attributed to the observation that machine learning algorithms tend to return higher confidence scores for examples that they have seen (i.e., records in the training dataset) versus those that they encounter for the first time [4,5]. Overfitted models are shown to be more prone to MIA [5,9], although the attack has also been observed on wellgeneralized models [2]. It has also been observed that the attack's success rate varies on different sub-groups of the training dataset [7]. Some researchers have also investigated a related, and perhaps a more likely attack in practice, where the adversary only knows a partial record of a target in the training dataset and seeks to complete its knowledge of the missing attributes by accessing the machine learning model. This is known as the Attribute Inference Attack (AIA) [9]. Samuel et al. [9] provide a formal definition of an AIA, and argue that an attribute inference adversary can infer the missing attribute value by using a membership inference adversary as a subroutine. Slightly more precisely, for a missing attribute with t possible values, the AI adversary constructs t different input (feature) vectors, gives them as input to the MI adversary, and outputs the attribute value which corresponds to the output from the MI adversary with the highest confidence [9, \u00a75, p. 277].",
            "score": 0.5922088173528818,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 1076,
                    "end": 1078,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1135,
                    "end": 1137,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1687,
                    "end": 1690,
                    "matchedPaperCorpusId": "2656445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "The threat model determines what information is available to the adversary in aux(g) for making a membership determination. Prior work [37,47] has focused primarily on the so-called black-box model where the adversary has access to \u03b8 * , the learning algorithm used to produce \u011d (including hyperparameters), the size of the training set, and the ability to query \u011d arbitrarily on new points. In practice, having access to \u03b8 * amounts to knowing a finite data set, S (distinct from S), sampled i.i.d. from \u03b8 * . \n\nIn this work, we replace black-box access to \u011d with whitebox access. Rather than only being able to query the target model, the attacker has access to the exact representation of \u011d that was produced by the learning algorithm and used by the model owner to make inferences on new data. For the target models commonly used in practice, e.g. neural networks and linear classifiers, this amounts to a set of floating-point weight matrices and biases, in addition to the linear operators and activation functions used at each layer. \n\nThis threat model reflects the growing number of publiclyavailable models on websites like Model Zoo [20], as well as the fact that white box representations may be obtained from black box APIs through other attacks [43].",
            "score": 0.5921534748714089,
            "section_title": "Threat Model",
            "char_start_offset": 14974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1264
                }
            ],
            "ref_mentions": [
                {
                    "start": 1259,
                    "end": 1263,
                    "matchedPaperCorpusId": "2984526"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "270878039",
            "title": "A Method to Facilitate Membership Inference Attacks in Deep Learning Models",
            "text": "Modern machine learning (ML) ecosystems offer a surging number of ML frameworks and code repositories that can greatly facilitate the development of ML models. Today, even ordinary data holders who are not ML experts can apply off-the-shelf codebase to build high-performance ML models on their data, many of which are sensitive in nature (e.g., clinical records). In this work, we consider a malicious ML provider who supplies model-training code to the data holders, does not have access to the training process, and has only black-box query access to the resulting model. In this setting, we demonstrate a new form of membership inference attack that is strictly more powerful than prior art. Our attack empowers the adversary to reliably de-identify all the training samples (average>99% attack TPR@0.1% FPR), and the compromised models still maintain competitive performance as their uncorrupted counterparts (average<1% accuracy drop). Moreover, we show that the poisoned models can effectively disguise the amplified membership leakage under common membership privacy auditing, which can only be revealed by a set of secret samples known by the adversary. Overall, our study not only points to the worst-case membership privacy leakage, but also unveils a common pitfall underlying existing privacy auditing methods, which calls for future efforts to rethink the current practice of auditing membership privacy in machine learning models.",
            "score": 0.5919212272764866,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "189928475",
            "title": "Reconciling Utility and Membership Privacy via Knowledge Distillation",
            "text": "The recent breakthroughs in deep learning and computing infrastructure, and the availability of large amounts of data have facilitated the adoption of machine learning (ML) in various domains ranging from recommendation systems to critical health-care management. The quality and quantity of data plays an instrumental role in the performance of machine learning models. Many companies providing MLas-a-Service computing platforms (e.g., Google API, Amazon AWS, etc.) enable novice data owners to train ML models for different applications. Such models are then released either as a prediction API and accessed in a blackbox fashion, or as a set of parameters and accessed in a whitebox fashion. \n\nThe data used for training ML models often contains sensitive user information such as clinical records, location traces, personal photos, etc. [8], [9], [42]; therefore, an ML model trained using sensitive data may pose privacy threats to the data owners by leaking the sensitive training information. This has been demonstrated through various inference attacks [19], [39], [13], [18], [6], [27], most notably the membership inference attack [40] which is the focus of our work. An adversary with just a blackbox access to the target model can mount successful membership inference attack to determine if a given target sample belonged to the training set of the target model or not [30]. The attack performance significantly improves with a whitebox access to the trained models [31]. Membership inference attacks are able to distinguish the members from non-members by learning the behavior of the target model on member versus non-member inputs. They use different features of the target model for this classification, including the entropy of the predictions [40], the input loss, and gradients of the input loss with respect to the model parameters [31]. Membership inference attacks are particularly more impactful against large neural networks [40], [28], [13], [14], [38] because such models can better memorize their training samples. \n\nRecent work has investigated several defenses against membership inference attacks. Early defenses suggest to limit information disclosure by releasing only a subset of the predictions [40], [13]. Such defenses, however, are shown to be ineffective against membership inference attacks [40] and can not be deployed in the whitebox setting.",
            "score": 0.5914376003025268,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2385
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 845,
                    "matchedPaperCorpusId": "8842268"
                },
                {
                    "start": 847,
                    "end": 850,
                    "matchedPaperCorpusId": "11854018"
                },
                {
                    "start": 852,
                    "end": 856,
                    "matchedPaperCorpusId": "2814088"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "916355"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1086,
                    "end": 1089,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1480,
                    "end": 1484,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1854,
                    "end": 1858,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1957,
                    "end": 1961,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 1969,
                    "end": 1973,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1975,
                    "end": 1979,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 2231,
                    "end": 2235,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 2237,
                    "end": 2241,
                    "matchedPaperCorpusId": "207229839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "265455913",
            "title": "Confidence Is All You Need for MI Attacks",
            "text": "In this evolving era of machine learning security, membership inference attacks have emerged as a potent threat to the confidentiality of sensitive data. In this attack, adversaries aim to determine whether a particular point was used during the training of a target model. This paper proposes a new method to gauge a data point\u2019s membership in a model\u2019s training set. Instead of correlating loss with membership, as is traditionally done, we have leveraged the fact that training examples generally exhibit higher confidence values when classified into their actual class. During training, the model is essentially being \u2019fit\u2019 to the training data and might face particular difficulties in generalization to unseen data. This asymmetry leads to the model achieving higher confidence on the training data as it exploits the specific patterns and noise present in the training data. Our proposed approach leverages the confidence values generated by the machine-learning model. These confidence values provide a probabilistic measure of the model\u2019s certainty in its predictions and can further be used to infer the membership of a given data point. Additionally, we also introduce another variant of our method that allows us to carry out this attack without knowing the ground truth(true class) of a given data point, thus offering an edge over existing label-dependent attack methods.",
            "score": 0.5912182337026002,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "195699554",
            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
            "text": "While Hayes et al. [15] describe a white-box attack in their work on membership inference attacks applied to GANs, the attack uses access only the outputs of the discriminator portion of the GAN, rather than the learned weights of either the discriminator or the generator; thus their approach is not white-box in the same sense. Meanwhile, Nasr et al. [32] demonstrated that a simple extension of the black-box shadow model approach to utilize internal activations does not result in higher membership inference accuracies than the original black-box approach. This is perhaps unsurprising, as the internal units of the shadow models are not likely to have any relation to those of the target model. \n\nRecently, Nasr et al. [32] provided a white-box attack that leverages the gradients of the target model's loss function with respect to its weights, which SGD approximately brings to zero on the training points at convergence. In contrast to our work, Nasr et al. use a further relaxed threat model, in which the attacker has access to as much as half of the target model's training data. We suggest an approach that is quite different from that of Nasr et al.. Our approach does not require this extra knowledge for the attacker, and thus falls under a more restrictive threat model, in which, to our knowledge, no other effective white-box attacks have been proposed.",
            "score": 0.5910875909396263,
            "section_title": "Related Work",
            "char_start_offset": 61269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "246823353",
            "title": "Privacy-preserving Generative Framework Against Membership Inference Attacks",
            "text": "We utilize the white-box attack method from the paper [27]. As is shown in this paper, the whitebox attack is more effective than the black-box one and considered to be the most effective attack. The adversary controls the target model and could test target models locally, learning a two-class model based on the difference between the model's layer output, loss values, label values, and layer gradients for member data and non-member data. In this part, we use this two-class model in [27] to determine whether the data is used for training the target model. The evaluation indicators include the accuracy, precision, recall, fscore and AUC value of the attack. Specifically, we only select the gradients of last layer, the model's output, loss values and label values to train this two-class model. We use 60000 training data to train target model. And we set training size as x and test size as x \u2032 . We randomly select x data from all raw training data and x \u2032 data from all raw test data to test this white-box attack. As shown in the TABLE 6, our framework with \u01eb = 0.5 reduces the inference accuracy to the target model to be close to 0.5 and the AUC of the attack model to be close to 0.5, which means our target model could prevent this white-box attack.",
            "score": 0.5910711952782082,
            "section_title": "White-box Attacks",
            "char_start_offset": 38202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 58,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "203591531",
            "title": "Alleviating Privacy Attacks via Causal Learning",
            "text": "Deep learning models have shown to memorize or overfit on the training data during the learning process (Carlini et al., 2018). Such overfitted models are susceptible to membership inference attacks that can accurately predict whether a target input belongs to the training dataset or not (Shokri et al., 2017). There are multiple variants of the attack depending on the information accessible to the adversary. An adversary with access to a black-box model only sees the confidence scores for the predicted output whereas one with the white-box has access to the model parameters and observe the output at each layer in the model (Nasr et al., 2018a). In the black-box setting, a membership attack is possible whenever the distribution of output scores for training data is different from the test data, and has been connected to model overfitting (Yeom et al., 2018). For the white-box setting, if an adversary knows the true label for the target input, then they may guess the input to be a member of the training set whenever the loss is lower, and vice-versa. Alternatively, if the adversary knows the distribution of the training inputs, they may learn a \"shadow\" model based on synthetic inputs and use the shadow model's output to build a membership classifier for any new input (Salem et al., 2018). \n\nMost of the existing membership inference attacks have been demonstrated for test inputs from the same data distribution as the training set. When test inputs are expected from the same distribution, methods to reduce overfitting (such as adversarial regularization) can help reduce privacy risks (Nasr et al., 2018b). However, in practice, this is seldom the case. For instance, in our example of a model trained to detect HIV, the test inputs may come from different hospitals. Models trained to reduce the generalization error for a specific test distribution are still susceptible to membership inference when the distribution of features is changed. This is due to the problem of covariate shift that introduces a domain adaptation error term (Mansour et al., 2009).",
            "score": 0.5907141488696773,
            "section_title": "ROBUSTNESS TO MEMBERSHIP ATTACKS",
            "char_start_offset": 14876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 310,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 849,
                    "end": 868,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1608,
                    "end": 1628,
                    "matchedPaperCorpusId": "49863840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "221808247",
            "title": "The Audio Auditor: User-Level Membership Inference in Internet of Things Voice Services",
            "text": "Membership Inference Attacks. The attack distinguishes whether a particular data sample is a member of the target model's training set or not. Traditional membership inference attacks against ML models under black-box access leverage numerous shadow models to mimic the target model's behavior [10,17,39]. Salem et al. [32] revealed that membership inference attacks could be launched by directly utilizing the prediction probabilities and thresholds of the target model. Both works [17] and [49] prove that overfitting of a model is sufficient but not a necessity to the success of a membership inference attack. Yeom et al. [49] as well as Farokhi and Kaafar [8] formalize the membership inference attack with black-box and white-box access. All previously mentioned works consider record-level inference; however, Song and Shmatikov [40] deploy a user-level membership inference attack in text generative models, with only the top-n predictions known. Trustworthiness of ASR Systems. The ASR systems are often deployed on voice-controlled devices [5], voice personal assistants [37], and machine translation services [6]. Tung and Shin [45] propose SafeChat to utilize a masking sound to distinguish authorized audios from unauthorized recording to protect any information leakage. Recent works [18] and [44] propose an audio cloning attack and audio reply attack against the speech recognition system to impersonate a legitimate user or inject unintended voices. Voice masquerading to impersonate users on the voice personal assistants has been studied [50]. Whereas Zhang et al. [50] propose another attack, namely voice squatting, to hijack the user's voice command, producing a sentence similar to the legal command. Du et al. [6] generate the adversarial audio samples to deceive the end-to-end ASR systems. Auditing ML Models. Many of the current proposed auditing services also seek to audit the bias and fairness of a given model [31]. Works have also been presented to audit the ML model to learn and check the model's prediction reliability [1,12,36].",
            "score": 0.5899250620875272,
            "section_title": "Related Work",
            "char_start_offset": 53322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 836,
                    "end": 840,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "211145382"
                },
                {
                    "start": 1139,
                    "end": 1143,
                    "matchedPaperCorpusId": "195848849"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "matchedPaperCorpusId": "67751855"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "52187155"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "53309633"
                },
                {
                    "start": 1584,
                    "end": 1588,
                    "matchedPaperCorpusId": "53309633"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96044921875
        },
        {
            "corpus_id": "54444175",
            "title": "Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks",
            "text": "The local gradient of the loss on a target data record, with respect to a given parameter, indicates how much and in which direction the parameter needs to be changed to fit the model to the data record. To minimize the expected loss of the model, the SGD algorithm repeatedly updates model parameters in a direction that the gradient of the loss over the whole training dataset leans to zero. Therefore, each training data sample will leave a distinguishable footprint on the local gradients of the loss function over the model's parameters. \n\nWe design our inference attack by using the gradient vector, on the target data point over all parameters, as the main feature for the attack model. We design an architecture for our attack model that processes extracted features from different layers separately, and then aggregates them to extract membership information. In the cases where the adversary does not have samples from the target training set to train its inference model, we train the attack model in an unsupervised manner. We train auto-encoders to compute a membership information embedding for any data. We then use a clustering algorithm, on the target dataset, to separate members from non-members based on their membership information embedding. \n\nTo show the effectiveness of our white-box inference attack, we evaluate the privacy of pre-trained and publicly available state-of-the-art models on the CIFAR100 dataset. We had no influence on training these models. Our results show that the DenseNet model-which is the best model on CIFAR100 with 82% test accuracy-is not much vulnerable to black-box attacks (with a 54.5% inference attack accuracy, where 50% is the baseline for random guess). However, the white-box membership inference attack obtains a considerably higher accuracy of 74.3%. This shows that even well-generalized deep models leak significant amount of information about their training data, and are vulnerable to white-box membership inference attacks. \n\nIn federated learning, we show that a curious parameter server or even a participant can perform alarmingly accurate membership inference attacks against other participants. For the DenseNet model on CIFAR100, a local participant can achieve a membership inference accuracy of 72.2%, even though it only observes aggregate updates through the parameter server.",
            "score": 0.5898340002768127,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1991
                },
                {
                    "start": 1994,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "269293603",
            "title": "Does Your Neural Code Completion Model Use My Code? A Membership Inference Approach",
            "text": "Membership inference attacks (MIAs) against machine learning models endeavor to discern whether a given data sample was utilized in the training of a target model or not. Membership inference methods can be broadly divided into two categories, i.e., shadow model training and metric-based techniques [17]. Shadow model training [42] aims to train a binary attack classifier by creating multiple shadow models to mimic the behavior of the target model. Metric-based techniques [40,44,57] proposed to infer the membership of a given data record by comparing the metric value (e.g., prediction loss [57]) calculated on the prediction vector of the given record to a preset threshold. Since both techniques are formulated from the attackers' standpoint, they assume the presence of abundant prior information, including knowledge of training data distribution and target model architectures. This assumption renders the direct application of these techniques unfeasible for data owners aiming to implement membership inference. Recently, Hu et al. [16] leveraged backdoor techniques to allow the data owner to achieve membership inference effectively without such prior information. Their approach enables individuals to carry out membership inference solely by utilizing black-box query access to the target model. One related work to ours is [55], which also investigates the membership leakage risks of code models through membership inference and has been released as a preprint. However, our work diverges from this study in three key aspects. First, in experiments, we extend the scope beyond small code models, delving into the feasibility of implementing membership inference on LLMs such as CodeGen and StarCoder to safeguard code intellectual property. Second, instead of relying on prediction probabilities, we utilize the distribution of ranks of ground truth in the model output as the feature for the membership classifier to learn. Third, we conduct a comprehensive analysis to scrutinize the factors contributing to the effectiveness of our membership inference method.",
            "score": 0.589496949205247,
            "section_title": "Membership Inference",
            "char_start_offset": 48445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2081
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "232233426"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1044,
                    "end": 1048,
                    "matchedPaperCorpusId": "249605407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "252368180",
            "title": "Distribution Inference Risks: Identifying and Mitigating Sources of Leakage",
            "text": "The problem of distribution inference has first been described by Ateniese et al. [4] as a problem of binary inference: there exist two possible training distributions D 0 and D 1 . The adversary knows that the target model was trained on one of them, and tries to figure out on which one. Typically, these two distributions differ in the proportion of a binary attribute that may or may not be one of the features used during training. The binary setting is the one considered in most of the prior work, with the exception of the work by Zhang et al. [39], which also considers a finite set of possible training distributions, and the works by Zhou et al. [40] and Suri and Evans [33], which perform regression attacks on continuous properties of the training distribution. To also cover these more general settings, we will index the training distribution D r by a variable r \u2208 R, which in the regression setting denotes the regression target and in the binary setting can take the value 0 or 1. \n\nMany attacks rely on meta-classifiers whose output is a guess for r. In the white-box setting, the input to these metaclassifier are the weights of the target model. In the blackbox setting, the input are outputs of the target model. To generate training data for the meta-classifiers, the adversary trains so-called shadow models. These are models with the same architecture and training parameters as the target model that are trained on samples from D r for different values of r, and labeled by that value, akin to the shadow models used for membership inference attacks [31]. A white-box metaclassifier can then be trained by using the weights of the shadow models as training features, and a black-box metaclassifier by using outputs of the shadow models.",
            "score": 0.5893393281743555,
            "section_title": "A. Distribution inference attacks",
            "char_start_offset": 6909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "14757739"
                },
                {
                    "start": 552,
                    "end": 556,
                    "matchedPaperCorpusId": "235485024"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "244117091"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "237491948"
                },
                {
                    "start": 1575,
                    "end": 1579,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "233864706",
            "title": "Membership Inference Attacks on Deep Regression Models for Neuroimaging",
            "text": "Membership inference attacks are one of the most popular attacks to evaluate privacy leakage in practice (Jayaraman and Evans, 2019). The malicious use of trained models to infer which subjects participated in the training set by having access to some or all attributes of the subject is termed as membership inference attack (Shokri et al., 2017;Nasr et al., 2019). These attacks aim to infer if a record (a person's MRI scan in our case) was used to train the model, revealing information about the subject's participation in the study, which could have legal implications. These attacks are often distinguished by the access to the information that the adversary has (Nasr et al., 2019). Most successful membership inference attacks in the deep neural network literature require access to some parts of the training data or at least some samples from the training data distribution (Salem et al., 2019;Pyrgelis et al., 2017;Truex et al., 2018). White-box attacks assume that the attacker is also aware of the training procedure and has access to the trained model parameters, whereas Black-box attacks only assume unlimited access to an API that provides the output of the model (Leino and Fredrikson, 2020;Nasr et al., 2019). \n\nCreating efficient membership inference attacks with minimal assumptions and information is an active area of research (Choo et al., 2020;Jayaraman et al., 2020;Song and Mittal, 2020). However, our work is focused on demonstrating the vulnerability of deep neural networks to membership inference attacks in the federated as well as non-federated setup. Therefore, we make straightforward assumptions and assume somewhat lenient access to information. Our attack models are inspired by Nasr et al. (2019); Shokri et al. (2017), and we use similar features such as gradients of parameters, activations, predictions, and labels to simulate membership inference attacks. In particular, we learn deep binary classifiers to distinguish training samples from unseen samples using these features. \n\nIn the case of federated learning, each learner receives model parameters and has some private training data. Thus, any learner is capable of launching white-box attacks.",
            "score": 0.5893122281327591,
            "section_title": "Membership Inference Attacks",
            "char_start_offset": 6862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 347,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 347,
                    "end": 365,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 670,
                    "end": 689,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 885,
                    "end": 905,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 1182,
                    "end": 1210,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 1210,
                    "end": 1228,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1718,
                    "end": 1736,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1738,
                    "end": 1758,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "9517641",
            "title": "Towards Measuring Membership Privacy",
            "text": "The goal of the adversary is to perform a membership inference attack by querying the machine learning model. That is, given a particular target record t \u2208 D, he wants to infer whether t is used to train the model he queries. Notations. Let T be the training set of the model. Since T is randomly sampled from D, we call D the candidate set for T. \n\nWe define X m to be a set of all possible features and Y to be the set of all possible class labels: {y 1 , y 2 , . . . , y k }. Each record z \u2208 D can be divided into two parts: the feature vector x \u2208 X m and the class label y \u2208 Y . Let A be the classification algorithm and c = A(T ) be the output classifier. We assume that for each query x, c(x) returns a vector of conditional probability of all class labels y \u2208 {y 1 , y 2 , . . . , y k } given feature vector x. We use p c (y | x) to represent the conditional probability of class label y given feature x, predicted by classifier c. That is, c( \n\nFor classifiers that do not directly provide predicted probabilities, these can be obtained through normalization over the class labels. \n\nIn membership inferences, the adversary wants to infer whether a specific record t = (x (t ) , y (t ) ) \u2208 D is part of the training set T . We call t the target record of the attack. \n\nThere are two approaches that an adversary can take to perform a membership inference attack on a target record t. He can launch a direct attack by querying the features of the target record x (t ) . Or, he can perform an indirect attack by querying some feature vector x x (t ) . Intuitively, a direct attack should have better performance than an indirect attack because querying the features of t should give more information about t compared to querying other features. In this paper, we first study the membership privacy risk under this assumption. In section 6, we test this assumption by analyzing some commonly used classifiers.",
            "score": 0.5886246344164905,
            "section_title": "PROBLEM STATEMENT",
            "char_start_offset": 8449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1914
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "219177364",
            "title": "privGAN: Protecting GANs from membership inference attacks at low cost to utility",
            "text": "A membership attacker aims to infer the membership of samples to a training set. To formally understand the adversary, let us first assume there exists a training set X train drawn from some data distribution D, a machine learning model M trained on X train and an adversary A that has access to samples X adv (also drawn from D). The adversary is assumed to have some query access to M such that, given a sample x \u2208 X adv , it can compute some function Q(M, x). The goal of the adversary is to then estimate P r(x \u2208 X train ) as a function of Q(M, x). [3] demonstrated one of the first membership attacks against discriminative models with only black-box access to the confidence scores for each input data sample. Their approach involved first training many 'shadow' machine learning models. Then an adversary model was trained to identify whether a sample was in the training set of any of the shadow models. This MIA approach was shown to be surprisingly effective against a variety of CNNs. Since then, there have been many such empirical membership attacks designed against machine learning models. Recently, [7,19] quantified membership privacy leakage as an adversarial game and showed a connection to the generalization error of machine learning algorithms and differential privacy. \n\nThere have been several recent papers proposing successful membership inference adversaries against generative models [16,18]. Both of these works were motivated by the close connection of information leakage to overfitting. More specifically, the generative models tend to memorize the training samples. The success of such adversaries greatly increases the risk to publish even synthetic datasets. To solve this concern, We propose privGAN, and will show the effectiveness of our method against all of these attacks in Section 6.",
            "score": 0.5885686990025626,
            "section_title": "Membership inference attacks against machine learning models",
            "char_start_offset": 3926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 1416,
                    "end": 1419,
                    "matchedPaperCorpusId": "199546273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "199546273",
            "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
            "text": "The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model.",
            "score": 0.5884829424681519,
            "section_title": "Background of Membership Inference",
            "char_start_offset": 6352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 759
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.970703125
        },
        {
            "corpus_id": "232233426",
            "title": "Membership Inference Attacks on Machine Learning: A Survey",
            "text": "In both MIAs, the training procedure of the attack models is the same as shown in Fig. 4. However, because adversarial knowledge available for an attacker of black-box and white-box MIAs is different, the attacker can collect different amounts of information about the training members and non-members under the different settings. In the black-box setting, the attacker only has black-box access to the target model, which means the attacker can only receive the prediction vector of an arbitrary input record when querying the target model. Thus, when querying the shadow models using their own shadow training datasets and test datasets, the attacker only collects the prediction vectors of each data record. However, in the white-box setting, the attacker has full access to the target model, which means the attacker can observe the intermediate computations at hidden layers and the prediction vector of an arbitrary input record. Thus, in the white-box setting, when querying the shadow models, the attacker can collect prediction vectors in addition to the intermediate computations of each data record. Compared to black-box MIAs, the attacker of white-box MIAs gets much more information to build the attack model. Next, we show more details of how an attacker constructs the attack model in both settings. Binary Classifier Based MIA in Black-box Setting. Datasets  m 1 , \u2022 \u2022 \u2022 ,  m  are \"member\" datasets which contain prediction vectors of the data records in the shadow training datasets. Datasets In the membership inference phase, the black-box attack model only takes the prediction vector p ( | ) as input and outputs the membership status of the data record. However, the white-box attack model can take the flat vector  containing much more information of the data record as input and outputs its membership status.",
            "score": 0.5870845350446696,
            "section_title": "Membership Inference Attack Approaches",
            "char_start_offset": 21981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "216553611",
            "title": "Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning",
            "text": "Machine Learning (ML) algorithms are vulnerable to various forms of inference attacks, including membership inference [38,42,44,48], property inference [10,24,31,35,38,54], and feature inference (also known as model inversion) [9,17,22,32,33,55,57,70,72]. Membership inference attacks determine whether a specific sample is part of the training dataset. [48] introduces a shadow model training technique for membership inference, which is later improved by [44] under loose assumptions, and by [42] in the setting of federated learning. Property inference attacks aim to infer aggregate statistics of the training dataset, such as the statistics of software execution traces [10] and the age distribution of a bank's customer data [24]. Feature inference attacks attempt to reconstruct the original inputs from ML applications. [9] explores model inversion attacks in decision trees and neural networks using labels and black-box access to the target model. [33] proposes to infer the training records in federated learning via model predictions. [70] restores the private training data using only model gradients. In federated learning, recent studies exploit GAN-based feature inference attacks. [17] proposes a GAN-based threat model where an adversary tries to extract representatives of a class not owned by him. By taking the shared global model as the discriminator of GAN, the adversary can further mislabel the pseudo-samples generated by the generator to trick other participants into leaking more private information. [55] assumes a malicious server and utilizes GANs to explore user-level privacy leakage, similar to the method in [17]. Note that in the literature, GANs have also been used in other types of attacks, such as membership inference attacks [7,29], backdoor attacks [18,37], and adversarial attacks [41,71]. \n\nIn this paper, we focus on GAN-based feature inference attacks.",
            "score": 0.5868538151234957,
            "section_title": "Inference Attacks on Machine Learning",
            "char_start_offset": 45307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1899
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 122,
                    "end": 125,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 128,
                    "end": 131,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "266852384"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "268063402"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "231718884"
                },
                {
                    "start": 165,
                    "end": 168,
                    "matchedPaperCorpusId": "53099247"
                },
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "251429277"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "252089942"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "253371229"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "224803115"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "54436587"
                },
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "5921778"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "195316471"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 457,
                    "end": 461,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "266852384"
                },
                {
                    "start": 828,
                    "end": 831,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "224803115"
                },
                {
                    "start": 1047,
                    "end": 1051,
                    "matchedPaperCorpusId": "195316471"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1529,
                    "end": 1533,
                    "matchedPaperCorpusId": "54436587"
                },
                {
                    "start": 1643,
                    "end": 1647,
                    "matchedPaperCorpusId": "5051282"
                },
                {
                    "start": 1767,
                    "end": 1770,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 1770,
                    "end": 1773,
                    "matchedPaperCorpusId": "233237159"
                },
                {
                    "start": 1792,
                    "end": 1796,
                    "matchedPaperCorpusId": "4316147"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "259165569"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "251765450"
                },
                {
                    "start": 1829,
                    "end": 1832,
                    "matchedPaperCorpusId": "267024773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "278338824",
            "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey",
            "text": "Shokri et al. [76] laid the groundwork for Membership Inference Attacks. They employed a shadow model approach: first, they trained several 'shadow models' to simulate the behavior of a target model trained on images. Then, they used the outputs of the shadow models to train an \"attack model\" that would determine whether a provided image was in the target model's training set. ML-Leaks [77], built on top of this by using just the target model predictions. They argued that models provide higher confidence scores if they have seen the data in training so that they could use simple statistics instead of multiple shadow models. LOGAN [78] built on top of these principles and used GAN architecture to demonstrate this attack. The attack was able to recover 100% of the training data in the white-box setting and 80% in the black-box setting. Tao and Shokri [79] presented this in the multimodal world specifically for text, image, and tabular forms of data.",
            "score": 0.5867453873360498,
            "section_title": "C. Membership Inference Attacks in Images",
            "char_start_offset": 21020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 961
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "244809678",
            "title": "Is Homomorphic Encryption-Based Deep Learning Secure Enough?",
            "text": "A reconstruction attack aims to restore target information given partial information such as insensitive information or label. Attacks of this category were first introduced by Fredrikson et al. and can be grouped into two sub-categories, attribute inference(partial data reconstruction) [4] and model inversion (full data reconstruction) [5]. Attribute inference attack infers sensitive information using probabilistic technique such as Maximum A Posteriori (MAP) when distribution of sensitive information and insensitive information of target data are known. Model inversion attack reconstructs training samples respective their labels when iterative queries on target model are possible. Since intuition of model inversion attack is based on gradient descent technique, the majority of model inversion attacks are implemented on white-box setting to use gradients information of target model [16][17][18]. To handle the limitation of a white-box setting, non gradient based attacks using generative network or Generative Adversarial Network (GAN) are also proposed [19,20]. \n\nMembership inference attack aims to determine whether an input data x was used during the training phase of the target model. Shokri et al. [6] proposed the first membership inference attack which uses output vectors of target model only (black-box setting). After [6], Nasr et al. [21] showed that membership inference attack can be performed more effectively in collaborative setting where model parameters and gradients are known(whitebox setting).",
            "score": 0.5861541457599577,
            "section_title": "Attacks on Deep Learning Based Services",
            "char_start_offset": 6200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1531
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "207229839"
                },
                {
                    "start": 896,
                    "end": 900,
                    "matchedPaperCorpusId": "195316471"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "195316471"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "233241017"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "208277767"
                },
                {
                    "start": 1220,
                    "end": 1223,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1345,
                    "end": 1348,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "133091488"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "254275235",
            "title": "On the Discredibility of Membership Inference Attacks",
            "text": "The wide-spread deployment of machine learning in various applications that deal with sensitive data, such as health records and personal information, has raised concerns about the leakage of sensitive training data post-deployment. Recently, a few studies suggest that machine learning models memorize the training data [41] and, consequently, various attacks, called membership inference (MI), have been proposed to identify the training samples [3, 9, 15, 17-19, 29, 31-34, 36, 37]. Due to its simplicity, membership inference attacks have become a standard way to evaluate the privacy risk of machine learning models [3,24]. \n\nRecent studies have shown that the evaluation of such models using average-case success metrics is misleading [30]. Specifically, a trivial random guess adjusted using the generalization gap, called gap attack [5] or naive attack [14,30], has shown to achieve similar performance as many membership inference attacks. Moreover, as argued in [3] and [20], privacy is not an average case metric and a pragmatic approach should avoid relying on such metrics. To better demonstrate the privacy risk of a model, true positive rate at low false positive rate is suggested in [3] as used in various areas of computer security [8,10,11,22]. Using the true positive rate at low false positive rate has revealed that many de facto membership inference attacks, such as [9,34,41], catastrophically fail. Only the state-of-the-art MI attacks that use some form of sample difficulty calibration [38], such as [3,29,32,38], can identify some training samples at low false positive rates. \n\nContributions. In this paper, we aim to answer the following question: Can membership inference attacks with low false positive be reliably used in practice? We show that despite their low false positive when evaluated on the entire dataset, they have a very high false positive when evaluated on nonmember samples belonging to the exact subpopulations that identified member samples are coming from.",
            "score": 0.5861178922226229,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 861,
                    "end": 865,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "235719329"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "226266600"
                },
                {
                    "start": 1200,
                    "end": 1203,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1250,
                    "end": 1253,
                    "matchedPaperCorpusId": "38169998"
                },
                {
                    "start": 1253,
                    "end": 1256,
                    "matchedPaperCorpusId": "11303098"
                },
                {
                    "start": 1256,
                    "end": 1259,
                    "matchedPaperCorpusId": "12767870"
                },
                {
                    "start": 1259,
                    "end": 1262,
                    "matchedPaperCorpusId": "18749628"
                },
                {
                    "start": 1390,
                    "end": 1393,
                    "matchedPaperCorpusId": "218862728"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1396,
                    "end": 1399,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 1527,
                    "end": 1530,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1533,
                    "end": 1536,
                    "matchedPaperCorpusId": "174799799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "261241592",
            "title": "A Probabilistic Fluctuation based Membership Inference Attack for Diffusion Models",
            "text": "Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.",
            "score": 0.5860237599178906,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "251664530",
            "title": "Label\u2010only membership inference attacks on machine unlearning without dependence of posteriors",
            "text": "During the ML model training process, the model is over-parameterized due to limited available training data and superabundant training epochs, which will cause the ML model to behave differently to training data and nontraining data. For example, an ML model classifies the training data with a high probability, while classifying the nontraining data with a relatively low probability. The adversary speculates whether a given data sample is in the training data set of an ML model, which is the membership inference attack. 28 Membership inference attack is a privacy attack against machine learning models, which exposes users' data. Formally, given a data sample x and a trained model \uf04d, the adversary \uf041 can calculate the membership state according to the additional knowledge \u03a9: \n\nwhere 1 means that x belongs to the training set of \uf04d, otherwise not. Membership inference attacks have been extensively studied in various fields, such as generative adversarial networks, 13,14,23 distributed recommender systems, 12 natural language processing, 29 and computer vision segmentation. 27",
            "score": 0.5854980749073806,
            "section_title": "| Membership inference",
            "char_start_offset": 6435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 527,
                    "end": 529,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 982,
                    "end": 984,
                    "matchedPaperCorpusId": "199546273"
                },
                {
                    "start": 1050,
                    "end": 1052,
                    "matchedPaperCorpusId": "159041003"
                },
                {
                    "start": 1087,
                    "end": 1089,
                    "matchedPaperCorpusId": "209439842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93701171875
        },
        {
            "corpus_id": "254854601",
            "title": "Fine-Tuning Is All You Need to Mitigate Backdoor Attacks",
            "text": "We first explore whether the model is more vulnerable to membership inference attacks [47] or not after fine-tuning. Membership inference attacks aim to infer whether a given sample is in the training set of a target model or not. A successful membership inference attack can cause severe privacy leakage. Normally, there are three different ways to conduct membership inference attacks: neural networkbased attacks [38,47], metric-based attacks [27,49,50,59], and query-based attacks [11,32]. In this work, we use the neural network-based attack due to its popularity. \n\nThreat Model. We first assume that the adversary only has black-box access to the target model, which means they can only query the model and obtain the output. Then, following previous works [36,39], we further assume that the adversary has part of the target model's training data (treated as members) and testing data (non-members). The adversary can use them for training an attack model and inferring the membership status for other data samples. Note that we adopt the strongest attacker assumption defined in [36] to estimate the worst-case scenario for membership leakage. \n\nMethodology. Our method can be described in two steps: \n\n1. The adversary first queries the target model with both the target model's (partial) training and testing samples, and they label the corresponding outputs as members and non-members. \n\n2. Second, the adversary uses the outputs and the corresponding labels to train their attack model, which is a three-layer neural network model. \n\nThe evaluations are conducted on both the backdoored model and the super-fine-tuned model to see whether finetuning will increase or decrease membership inference risks. \n\nExperimental Settings. We evaluate the membership inference attack in the standalone scenario, which means that the fine-tuning dataset is the same as the pre-training dataset. For each dataset, we randomly sample half of its testing samples and the same number of training samples as the attack training dataset. Then, we select the other half of its testing samples (serving as non-members) and the same number of training samples (serving as members with no overlap on the attack training dataset) to evaluate the attack performance.",
            "score": 0.5850611972219599,
            "section_title": "Membership Inference Attack",
            "char_start_offset": 39552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1397
                },
                {
                    "start": 1400,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "49863840"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "195699554"
                },
                {
                    "start": 450,
                    "end": 453,
                    "matchedPaperCorpusId": "214623088"
                },
                {
                    "start": 453,
                    "end": 456,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 456,
                    "end": 459,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 485,
                    "end": 489,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "237563320"
                },
                {
                    "start": 764,
                    "end": 768,
                    "matchedPaperCorpusId": "231802143"
                },
                {
                    "start": 768,
                    "end": 771,
                    "matchedPaperCorpusId": "133091488"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "231802143"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "248157274",
            "title": "Optimal Membership Inference Bounds for Adaptive Composition of Sampled Gaussian Mechanisms",
            "text": "The recent success of machine learning models make them the go-to approach to solve a variety of problems, ranging from computer vision [Krizhevsky et al., 2012] to NLP [Sutskever et al., 2014], including applications to sensitive data such as health records or chatbots. Access to a trained machine learning model, through a black-box API or a white-box access to a published model, can leak traces of information [Dwork et al., 2015] from the training data. Researchers have tried to measure this information leakage through metrics such as membership inference [Shokri et al., 2017]. Membership inference is the task of guessing, from a trained model, whether it includes a given sample or not. This task is both interesting in its own right, as the participation of an individual in a data collection can be a sensitive information. It also serves as the \"most significant bit\" of information: if membership inference fails, attacks revealing more information such as reconstruction attacks [Fredrikson et al., 2014, Carlini et al., 2020] will also fail. In other words, defending against membership inference attacks would also defend against attacks such as reconstruction attacks that aim at reconstructing training examples. \n\nThe standard approach to provably defeat these membership privacy attacks is differential privacy [Dwork et al., 2006]. Differential privacy defines a class of training algorithms that respect a privacy budget and a probability of failure \u03b4. These quantities quantify how much information about each individual training example is revealed by the output of the algorithm. Most algorithms obtaining differential privacy need to inject noise somewhere in their process. The amount of injected noise then creates a trade-off between privacy utility of the trained model. To measure the privacy of a given algorithms, researchers have developed advanced mathematical tools and notions such as Renyi differential privacy Mironov [2017], Abadi et al. [2016] and advanced composition theorems Dwork et al. [2010], Kairouz et al. [2015]. These tools allow us to calculate ( , \u03b4) values for carefully designed algorithms. \n\nPrevious work has shown that any deferentially private algorithm will provably bound the accuracy of any membership inference adversary.",
            "score": 0.5840010140563935,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2147
                },
                {
                    "start": 2150,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 161,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 169,
                    "end": 193,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 415,
                    "end": 435,
                    "matchedPaperCorpusId": "217900441"
                },
                {
                    "start": 564,
                    "end": 585,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 995,
                    "end": 1019,
                    "matchedPaperCorpusId": "2148083"
                },
                {
                    "start": 1333,
                    "end": 1353,
                    "matchedPaperCorpusId": "2468323"
                },
                {
                    "start": 1951,
                    "end": 1965,
                    "matchedPaperCorpusId": "9386213"
                },
                {
                    "start": 1967,
                    "end": 1986,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 2021,
                    "end": 2040,
                    "matchedPaperCorpusId": "9132611"
                },
                {
                    "start": 2042,
                    "end": 2063,
                    "matchedPaperCorpusId": "1721827"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7607421875
        },
        {
            "corpus_id": "220831381",
            "title": "Label-Only Membership Inference Attacks",
            "text": "The membership inference threat model originally introduced by Shokri et al. (Shokri et al., 2016), and used in many subsequent works (Long et al., 2017;Truex et al., 2018;Salem et al., 2018;Song et al., 2019;Nasr et al., 2018b), assumes that the adversary has black-box access to the model h (i.e., they can only query the model for its prediction and confidence but not inspect its learned parameters ). Our work also assumes black-box model access, with the extra restriction (see Section C.2 for more details) that the model only returns (hard) labels to queries. Though studying membership inference attacks with whitebox model access (Leino & Fredrikson, 2019) has merits (e.g., for upper-bounding the membership leakage), our label-only restriction inherently presumes a black-box setting (as otherwise, the adversary could just run h locally to obtain confidence scores). Although we are focused on the label-only domain, our attack methodologies can be applied for analysis in the white-box domain.\n\nAssuming a black-box query interface, there are a number of other dimensions to the adversary's assumed knowledge of the trained model:\n\nTask Knowledge refers to global information about the model's prediction task and, therefore, of its prediction API. Examples of task knowledge include the total number of classes, the class-labels (dog, cat, etc.), and the input format (32 \u00d7 32 RGB or grayscale images, etc.). Task knowledge is always assumed to be known to the adversary, as it is necessary for the classifier service to be useful to a user.\n\nTraining Knowledge refers to knowledge about the model architecture (e.g., the type of neural network, its number of layers, etc.) and how it was trained (the training  Table 3. Survey of membership inference threat models. L is the model's loss function, \u03c4 is a calibration term reflecting the difficulty of the sample, \u03b8 are the model parameters centered around \u03b8 * , \u03b80 are the parameters on all other datapoints (other than x), aug(x) is a data augmentation of x (e.g., image",
            "score": 0.583680926053045,
            "section_title": "C.1. Adversarial Knowledge",
            "char_start_offset": 47969,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "247922814",
            "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
            "text": "Membership inference (MI) captures one of the most generic notions of privacy leakage in machine learning. Indeed, any form of data leakage from a model's training set (e.g., attribute inference or data extraction) implies the ability to infer membership of some training examples. As a result, membership inference is a natural target for evaluating the impact of poisoning attacks on data privacy.\n\nIn this section, we introduce and analyze data poisoning attacks that improve membership inference by one to two orders of magnitude. Section 4.2 describes a targeted attack that increases leakage of a specific sample ( , ), and Section 4.3 contains an analysis of this attack's success. Section 4.4 explores untargeted attacks that increase privacy leakage on all training points simultaneously.",
            "score": 0.5835505826492117,
            "section_title": "MEMBERSHIP INFERENCE ATTACKS",
            "char_start_offset": 23042,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "254275235",
            "title": "On the Discredibility of Membership Inference Attacks",
            "text": "With the wide-spread application of machine learning models, it has become critical to study the potential data leakage of models trained on sensitive data. Recently, various membership inference (MI) attacks are proposed to determine if a sample was part of the training set or not. The question is whether these attacks can be reliably used in practice. We show that MI models frequently misclassify neighboring nonmember samples of a member sample as members. In other words, they have a high false positive rate on the subpopulations of the exact member samples that they can identify. We then showcase a practical application of MI attacks where this issue has a real-world repercussion. Here, MI attacks are used by an external auditor (investigator) to show to a judge/jury that an auditee unlawfully used sensitive data. Due to the high false positive rate of MI attacks on member's subpopulations, auditee challenges the credibility of the auditor by revealing the performance of the MI attacks on these subpopulations. We argue that current membership inference attacks can identify memorized subpopulations, but they cannot reliably identify which exact sample in the subpopulation was used during the training.",
            "score": 0.5833695988170969,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "230770439",
            "title": "Model Extraction and Defenses on Generative Adversarial Networks",
            "text": "In addition to model extract attacks, there are other types of attacks in relation to privacy and security [9,21,67], such as membership inference attacks [12,19,55,58,59] and property inference attacks [17]. Some efforts have been also made to investigate membership inference attacks against GANs, where queries to a GAN model can reveal information about the training dataset [12,19,23]. Overall, these studies mainly focus on privacy on the training dataset, while model extraction attacks in our paper concentrate on machine learning model itself.",
            "score": 0.5826028671815526,
            "section_title": "RELATED WORK",
            "char_start_offset": 11018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 552
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 168,
                    "end": 170,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "52218951"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "221203089"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "52211986"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "199546273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "237563320",
            "title": "Membership Leakage in Label-Only Exposures",
            "text": "Machine learning (ML) has witnessed tremendous progress over the past decade and has been applied across a wide range of privacy-critical applications, such as face recognition [28,61] and medical image analysis [9,29,51]. Such developments rely on not only novel training algorithms and architectures, but also access to sensitive and private data, such as health data. Various recent research [23,25,31,35,36,45,46,48,49,54,57,60] has shown that ML models are vulnerable to privacy attacks. One major attack in this domain is membership inference: An adversary aims to determine whether or not a data sample is used to train a target ML model. \n\nExisting membership inference attacks [25, 31, 35, 46, 48,   1 Our code is available at https://github.com/zhenglisec/Decisionbased-MIA. 49, 57] rely on the confidence scores (e.g. class probabilities or logits) returned by a target ML model as their inputs. The success of membership inference is due to the inherent overfitting property of ML models, i.e., an ML model is more confident facing a data sample it was trained on, and this confidence is reflected in the model's output scores. See Figure 1 for an illustration of accessible components of an ML model for such score-based threat model. A major drawback for these score-based attacks is that they can be trivially mitigated if the model only exposes the predicted label, i.e., the final model decision, instead of confidence scores. The fact that score-based attacks can be easily averted makes it more difficult to evaluate whether a model is truly vulnerable to membership inference or not, which may lead to premature claims of privacy for ML models.",
            "score": 0.5822917924971601,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1664
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "7811489"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "4941890"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "15315839"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "14434529"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "230523638"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "211532510"
                },
                {
                    "start": 411,
                    "end": 414,
                    "matchedPaperCorpusId": "174799799"
                },
                {
                    "start": 414,
                    "end": 417,
                    "matchedPaperCorpusId": "46933970"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 420,
                    "end": 423,
                    "matchedPaperCorpusId": "165163934"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "2656445"
                },
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "208139345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "paperId": "25de9e07d1b1d0f09995347bb6dd193e1ef52e91",
            "corpusId": 264179931,
            "title": "Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models",
            "venue": "IEEE Transactions on Information Forensics and Security",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 11,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIFS.2023.3324772?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIFS.2023.3324772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2019143562",
                    "name": "Gaoyang Liu"
                },
                {
                    "authorId": "2260601109",
                    "name": "Tianlong Xu"
                },
                {
                    "authorId": "2118403448",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2259065760",
                    "name": "Zixiong Wang"
                },
                {
                    "authorId": "40614774",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2213648984",
                    "name": "Ling Liu"
                }
            ],
            "abstract": "Machine Learning (ML) techniques have been applied to many real-world applications to perform a wide range of tasks. In practice, ML models are typically deployed as the black-box APIs to protect the model owner\u2019s benefits and/or defend against various privacy attacks. In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model\u2019s prediction behavior. By extracting the membership information of the given record from the gradient of the substituted local model using an intentionally modified autoencoder, Gradient-Leaks can thus breach the membership privacy of the target model\u2019s training data in an unsupervised manner, without any priori knowledge about the target model\u2019s internals or its training data. Extensive experiments on different types of ML models with real-world datasets have shown that Gradient-Leaks can achieve a better performance compared with state-of-the-art attacks.",
            "corpus_id": "264179931",
            "text": "Machine Learning (ML) techniques have been applied to many real-world applications to perform a wide range of tasks. In practice, ML models are typically deployed as the black-box APIs to protect the model owner\u2019s benefits and/or defend against various privacy attacks. In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model\u2019s prediction behavior. By extracting the membership information of the given record from the gradient of the substituted local model using an intentionally modified autoencoder, Gradient-Leaks can thus breach the membership privacy of the target model\u2019s training data in an unsupervised manner, without any priori knowledge about the target model\u2019s internals or its training data. Extensive experiments on different types of ML models with real-world datasets have shown that Gradient-Leaks can achieve a better performance compared with state-of-the-art attacks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96728515625
        },
        {
            "paperId": "01a1a3ab47ac24f688d9021d41d334324d26adc7",
            "corpusId": 276675131,
            "title": "Evaluating Differential Privacy in Federated Learning Based on Membership Inference Attacks",
            "venue": "International Conference on Big Data Computing and Communications",
            "year": 2024,
            "referenceCount": 26,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BIGCOM65357.2024.00035?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BIGCOM65357.2024.00035, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302840226",
                    "name": "Peng He"
                },
                {
                    "authorId": "2265045062",
                    "name": "Xinyu Wang"
                },
                {
                    "authorId": "2327694194",
                    "name": "Weijiao Zhang"
                },
                {
                    "authorId": "2327724227",
                    "name": "Zhongkai Wang"
                },
                {
                    "authorId": "2280180727",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2280146926",
                    "name": "Chuangxin Ou"
                },
                {
                    "authorId": "2280135392",
                    "name": "Guozheng Li"
                }
            ],
            "abstract": "While machine learning has achieved tremendous success, the privacy security of its models faces challenges. To protect the privacy of machine learning models, researchers have proposed methods such as federated learning and differential privacy. However, the effectiveness of these methods in defending against attacks on model privacy at the practical level has not been comprehensively evaluated. In this paper, we focus on membership inference attacks targeting the privacy of machine learning models. By employing classical black-box membership inference attacks and a white-box membership inference attack proposed in this paper, we evaluate the privacy performance of the federated differential privacy framework in scenarios where privacy attacks are actively defended. Experimental results demonstrate that compared to centralized learning with differential privacy methods, models trained using the federated differential privacy framework exhibit stronger privacy performance and higher utility. We investigate the impact of differential privacy implementation mechanisms and privacy budgets on the privacy performance of federated learning models, providing insights and guidance for selecting critical privacy mechanisms and parameters in the practical application of the federated differential privacy framework.",
            "corpus_id": "276675131",
            "text": "While machine learning has achieved tremendous success, the privacy security of its models faces challenges. To protect the privacy of machine learning models, researchers have proposed methods such as federated learning and differential privacy. However, the effectiveness of these methods in defending against attacks on model privacy at the practical level has not been comprehensively evaluated. In this paper, we focus on membership inference attacks targeting the privacy of machine learning models. By employing classical black-box membership inference attacks and a white-box membership inference attack proposed in this paper, we evaluate the privacy performance of the federated differential privacy framework in scenarios where privacy attacks are actively defended. Experimental results demonstrate that compared to centralized learning with differential privacy methods, models trained using the federated differential privacy framework exhibit stronger privacy performance and higher utility. We investigate the impact of differential privacy implementation mechanisms and privacy budgets on the privacy performance of federated learning models, providing insights and guidance for selecting critical privacy mechanisms and parameters in the practical application of the federated differential privacy framework.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92333984375
        },
        {
            "paperId": "62749fbe7bfb0cd3abe0a8897efd3989ef867941",
            "corpusId": 249595286,
            "title": "Membership Inference Attacks Against Machine Learning Models via Prediction Sensitivity",
            "venue": "IEEE Transactions on Dependable and Secure Computing",
            "year": 2023,
            "referenceCount": 23,
            "citationCount": 53,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TDSC.2022.3180828?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TDSC.2022.3180828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116205870",
                    "name": "Lan Liu"
                },
                {
                    "authorId": "2154458502",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2019143562",
                    "name": "Gaoyang Liu"
                },
                {
                    "authorId": "47205946",
                    "name": "Kai Peng"
                },
                {
                    "authorId": "40614774",
                    "name": "Chen Wang"
                }
            ],
            "abstract": "Machine learning (ML) has achieved huge success in recent years, but is also vulnerable to various attacks. In this article, we concentrate on membership inference attacks and propose Aster, which merely requires the target model's black-box API and a data sample to determine whether this sample was used to train the given ML model or not. The key idea of Aster is that the training data of a fully trained ML model usually has lower prediction sensitivities compared with that of the non-training data (i.e., testing data). Less sensitivity means that when perturbing a training sample's feature value in the corresponding feature space, the prediction of the perturbed sample obtained from the target model tends to be consistent with the original prediction. In this article, we quantify the prediction sensitivity with the Jacobian matrix which could reflect the relationship between each feature's perturbation and the corresponding prediction's change. Then we regard the samples with a lower as training data. Aster can breach the membership privacy of the target model's training data with no prior knowledge about the target model or its training data. The experiment results on four datasets show that our method outperforms three state-of-the-art inference attacks.",
            "corpus_id": "249595286",
            "text": "Machine learning (ML) has achieved huge success in recent years, but is also vulnerable to various attacks. In this article, we concentrate on membership inference attacks and propose Aster, which merely requires the target model's black-box API and a data sample to determine whether this sample was used to train the given ML model or not. The key idea of Aster is that the training data of a fully trained ML model usually has lower prediction sensitivities compared with that of the non-training data (i.e., testing data). Less sensitivity means that when perturbing a training sample's feature value in the corresponding feature space, the prediction of the perturbed sample obtained from the target model tends to be consistent with the original prediction. In this article, we quantify the prediction sensitivity with the Jacobian matrix which could reflect the relationship between each feature's perturbation and the corresponding prediction's change. Then we regard the samples with a lower as training data. Aster can breach the membership privacy of the target model's training data with no prior knowledge about the target model or its training data. The experiment results on four datasets show that our method outperforms three state-of-the-art inference attacks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9111328125
        },
        {
            "paperId": "cdc76e6023fc6cd40c6451c42e3fc8193e8b3a4b",
            "corpusId": 264483281,
            "title": "Resisting Membership Inference Attacks by Dynamically Adjusting Loss Targets",
            "venue": "International Conference on Networking and Network Applications",
            "year": 2023,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/NaNA60121.2023.00100?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/NaNA60121.2023.00100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261691048",
                    "name": "Xihua Ma"
                },
                {
                    "authorId": "2261674621",
                    "name": "Youliang Tian"
                },
                {
                    "authorId": "2261900206",
                    "name": "Zehua Ding"
                }
            ],
            "abstract": "Machine learning (ML) models are susceptible to membership inference attacks (MIAs), which aim to infer whether a particular sample was involved in model training. Previous research suggests that the difference in loss distribution between member and non-member sample is an essential factor of MIAs. In the latest mitigation strategies to reduce the loss distribution discrepancy, the model owner must manually set a loss target for the training task. However, this can be challenging due to differences in datasets and model structures. We propose a new mitigation strategy based on existing studies, which can dynamically adjust the loss target during the training process according to the model structure and dataset characteristics, to achieve a reduced loss gap. We extensively evaluated our strategy in white-box and black-box environments, respectively. Our experimental results show that our approach avoids the problem of setting loss targets and even improves the model's resistance to attacks in most cases. Specifically, the accuracy of the attacks is reduced by an average of 4.92% and 11.3% in the black-box and white-box environments, respectively.",
            "corpus_id": "264483281",
            "text": "Machine learning (ML) models are susceptible to membership inference attacks (MIAs), which aim to infer whether a particular sample was involved in model training. Previous research suggests that the difference in loss distribution between member and non-member sample is an essential factor of MIAs. In the latest mitigation strategies to reduce the loss distribution discrepancy, the model owner must manually set a loss target for the training task. However, this can be challenging due to differences in datasets and model structures. We propose a new mitigation strategy based on existing studies, which can dynamically adjust the loss target during the training process according to the model structure and dataset characteristics, to achieve a reduced loss gap. We extensively evaluated our strategy in white-box and black-box environments, respectively. Our experimental results show that our approach avoids the problem of setting loss targets and even improves the model's resistance to attacks in most cases. Specifically, the accuracy of the attacks is reduced by an average of 4.92% and 11.3% in the black-box and white-box environments, respectively.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.900390625
        },
        {
            "paperId": "bfb0f44e42ab7f25e6365f5badb70c5cd5717a99",
            "corpusId": 278064051,
            "title": "Efficient Detection of Black Box Adversarial Attacks in Machine Learning Systems Using Cluster-Based and Class-Based Input Patterns",
            "venue": "International Conference on Computing: Theory and Applications",
            "year": 2024,
            "referenceCount": 12,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCTA64612.2024.10974889?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCTA64612.2024.10974889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357280935",
                    "name": "Rghda Salah"
                },
                {
                    "authorId": "2357281265",
                    "name": "Tarek Sharaf"
                },
                {
                    "authorId": "2357295102",
                    "name": "Nada Tayel"
                },
                {
                    "authorId": "2357295151",
                    "name": "Doaa Shawky"
                }
            ],
            "abstract": "Adversarial attacks pose a significant challenge to the security and reliability of machine learning models. This paper focuses on detecting black-box adversarial attacks, where attackers operate without access to the model's internal parameters or architecture\u2014a scenario more relevant to real-world threats than white-box settings, where such knowledge is assumed. Two novel detection methodologies are introduced: (1) a clustering-based approach that groups input data to identify anomalies and (2) a class-centric method that uses statistical properties of labeled data to compute class centers and boundaries for detecting deviations. These methods utilize thresholds derived from dynamic statistical metrics to ensure robust, efficient, and scalable detection.Extensive experiments validate the effectiveness of these approaches using the Iris dataset. Results demonstrate that the clustering-based method excels in detecting adversarial samples, achieving high accuracy, while the class-based method offers a conservative alternative by minimizing false positives. Both methodologies provide practical solutions for enhancing model security, particularly in scenarios where model details are unavailable, thereby addressing the limitations of existing defense mechanisms. Future work will explore scalability and application to higher-dimensional datasets.",
            "corpus_id": "278064051",
            "text": "Adversarial attacks pose a significant challenge to the security and reliability of machine learning models. This paper focuses on detecting black-box adversarial attacks, where attackers operate without access to the model's internal parameters or architecture\u2014a scenario more relevant to real-world threats than white-box settings, where such knowledge is assumed. Two novel detection methodologies are introduced: (1) a clustering-based approach that groups input data to identify anomalies and (2) a class-centric method that uses statistical properties of labeled data to compute class centers and boundaries for detecting deviations. These methods utilize thresholds derived from dynamic statistical metrics to ensure robust, efficient, and scalable detection.Extensive experiments validate the effectiveness of these approaches using the Iris dataset. Results demonstrate that the clustering-based method excels in detecting adversarial samples, achieving high accuracy, while the class-based method offers a conservative alternative by minimizing false positives. Both methodologies provide practical solutions for enhancing model security, particularly in scenarios where model details are unavailable, thereby addressing the limitations of existing defense mechanisms. Future work will explore scalability and application to higher-dimensional datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5927734375
        },
        {
            "paperId": "60e53d9f34e71cbbfe87ab75ca3a909233a6b356",
            "corpusId": 260887617,
            "title": "White-box Membership Inference Attacks against Diffusion Models",
            "venue": "Proceedings on Privacy Enhancing Technologies",
            "year": 2023,
            "referenceCount": 75,
            "citationCount": 24,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.06405",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2057013801",
                    "name": "Yan Pang"
                },
                {
                    "authorId": "49980880",
                    "name": "Tianhao Wang"
                },
                {
                    "authorId": "2152320435",
                    "name": "Xu Kang"
                },
                {
                    "authorId": "2925985",
                    "name": "Mengdi Huai"
                },
                {
                    "authorId": "2145954003",
                    "name": "Yang Zhang"
                }
            ],
            "abstract": "Diffusion models have begun to overshadow GANs and other generative models in industrial applications due to their superior image generation performance. The complex architecture of these models furnishes an extensive array of attack features. In light of this, we aim to design membership inference attacks (MIAs) catered to diffusion models. We first conduct an exhaustive analysis of existing MIAs on diffusion models, taking into account factors such as black-box/white-box models and the selection of attack features. We found that white-box attacks are highly applicable in real-world scenarios, and the most effective attacks presently are white-box. Departing from earlier research, which employs model loss as the attack feature for white-box MIAs, we employ model gradients in our attack, leveraging the fact that these gradients provide a more profound understanding of model responses to various samples. We subject these models to rigorous testing across a range of parameters, including training steps, timestep sampling frequency, diffusion steps, and data variance. Across all experimental settings, our method consistently demonstrated near-flawless attack performance, with attack success rate approaching 100% and attack AUCROC near 1.0. We also evaluated our attack against common defense mechanisms, and observed our attacks continue to exhibit commendable performance.",
            "corpus_id": "260887617",
            "text": "Diffusion models have begun to overshadow GANs and other generative models in industrial applications due to their superior image generation performance. The complex architecture of these models furnishes an extensive array of attack features. In light of this, we aim to design membership inference attacks (MIAs) catered to diffusion models. We first conduct an exhaustive analysis of existing MIAs on diffusion models, taking into account factors such as black-box/white-box models and the selection of attack features. We found that white-box attacks are highly applicable in real-world scenarios, and the most effective attacks presently are white-box. Departing from earlier research, which employs model loss as the attack feature for white-box MIAs, we employ model gradients in our attack, leveraging the fact that these gradients provide a more profound understanding of model responses to various samples. We subject these models to rigorous testing across a range of parameters, including training steps, timestep sampling frequency, diffusion steps, and data variance. Across all experimental settings, our method consistently demonstrated near-flawless attack performance, with attack success rate approaching 100% and attack AUCROC near 1.0. We also evaluated our attack against common defense mechanisms, and observed our attacks continue to exhibit commendable performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.94775390625
        },
        {
            "paperId": "2991af1063b4a9500d24fdd3c1052e2a670ecab9",
            "corpusId": 275587728,
            "title": "Assessing Membership Inference Attacks under Distribution Shifts",
            "venue": "BigData Congress [Services Society]",
            "year": 2024,
            "referenceCount": 10,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigData62323.2024.10825580?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigData62323.2024.10825580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287761558",
                    "name": "Yichuan Shi"
                },
                {
                    "authorId": "7731413",
                    "name": "O. Kotevska"
                },
                {
                    "authorId": "2247773985",
                    "name": "Viktor Reshniak"
                },
                {
                    "authorId": "2307300006",
                    "name": "Amir Sadovnik"
                }
            ],
            "abstract": "Membership inference attacks (MIAs) exploit machine learning models to infer whether a data point was in the training set, posing significant privacy risks even with limited black-box access. These attacks rely on the attacker approximating the target model\u2019s training distribution, yet the impact of distribution shifts between target and shadow models on MIA success remains underexplored. We systematically evaluate five types of distribution shifts \u2014-cutout, jitter, Gaussian noise, label shift, and attribute shift \u2014- at varying intensities. Our results reveal that these shifts affect MIA effectiveness in nuanced ways, with some reducing attack success while others exacerbate vulnerabilities, and the same shift can have opposite effects depending on the type of MIA. This highlights the complex interplay between distributional differences and attack performance, offering critical insights for improving model defenses against MIAs.",
            "corpus_id": "275587728",
            "text": "Membership inference attacks (MIAs) exploit machine learning models to infer whether a data point was in the training set, posing significant privacy risks even with limited black-box access. These attacks rely on the attacker approximating the target model\u2019s training distribution, yet the impact of distribution shifts between target and shadow models on MIA success remains underexplored. We systematically evaluate five types of distribution shifts \u2014-cutout, jitter, Gaussian noise, label shift, and attribute shift \u2014- at varying intensities. Our results reveal that these shifts affect MIA effectiveness in nuanced ways, with some reducing attack success while others exacerbate vulnerabilities, and the same shift can have opposite effects depending on the type of MIA. This highlights the complex interplay between distributional differences and attack performance, offering critical insights for improving model defenses against MIAs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.931640625
        },
        {
            "paperId": "f58bfd8363ce3ceb21e7c1ea45665e861be39653",
            "corpusId": 261311443,
            "title": "Mitigating Membership Inference Attacks in Machine Learning as a Service",
            "venue": "Computer Science Symposium in Russia",
            "year": 2023,
            "referenceCount": 26,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CSR57506.2023.10224960?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CSR57506.2023.10224960, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "11785189",
                    "name": "Myria Bouhaddi"
                },
                {
                    "authorId": "1833485",
                    "name": "K. Adi"
                }
            ],
            "abstract": "The increasing use of Machine Learning as a Service (MLaaS) has raised privacy and security issues due to membership inference attacks. These attacks can extract sensitive information such as the identification of an individual's participation in a training dataset, by exploiting a binary classifier with limited access. The attacks exploit weaknesses in the decision boundaries of the model, and can lead to the disclosure of private information. However, the current defenses against such attacks, such as those based on differential privacy or regularization, have significant limitations. Therefore, further research is needed to develop effective defenses that maintain the utility of machine learning models while providing formal guarantees, even in the presence of strategic adversaries. In this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models as a service. We propose a defense mechanism that brings the attacker's inference classifier into a zone of uncertainty, rendering it unable to classify a data point as a member or non-member. This mechanism takes into account the attacker's behavior by modeling the interaction between defense and attacker as a game, considering potential gains in confidentiality and costs. Our experiments on two datasets demonstrate the effectiveness of our approach in mitigating membership inference attacks. Furthermore, our defense mechanism outperforms existing defenses by offering superior privacy-utility-performance tradeoffs.",
            "corpus_id": "261311443",
            "text": "The increasing use of Machine Learning as a Service (MLaaS) has raised privacy and security issues due to membership inference attacks. These attacks can extract sensitive information such as the identification of an individual's participation in a training dataset, by exploiting a binary classifier with limited access. The attacks exploit weaknesses in the decision boundaries of the model, and can lead to the disclosure of private information. However, the current defenses against such attacks, such as those based on differential privacy or regularization, have significant limitations. Therefore, further research is needed to develop effective defenses that maintain the utility of machine learning models while providing formal guarantees, even in the presence of strategic adversaries. In this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models as a service. We propose a defense mechanism that brings the attacker's inference classifier into a zone of uncertainty, rendering it unable to classify a data point as a member or non-member. This mechanism takes into account the attacker's behavior by modeling the interaction between defense and attacker as a game, considering potential gains in confidentiality and costs. Our experiments on two datasets demonstrate the effectiveness of our approach in mitigating membership inference attacks. Furthermore, our defense mechanism outperforms existing defenses by offering superior privacy-utility-performance tradeoffs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.83984375
        },
        {
            "paperId": "7b278178f524160b16fa6ead5fdadfd4a143d35e",
            "corpusId": 273507277,
            "title": "Dual-Model Defense: Safeguarding Diffusion Models from Membership Inference Attacks through Disjoint Data Splitting",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327043379",
                    "name": "Bao Q. Tran"
                },
                {
                    "authorId": "2275329323",
                    "name": "Viet Nguyen"
                },
                {
                    "authorId": "2327046351",
                    "name": "Anh Tran"
                },
                {
                    "authorId": "2275127531",
                    "name": "Toan Tran"
                }
            ],
            "abstract": "Diffusion models have demonstrated remarkable capabilities in image synthesis, but their recently proven vulnerability to Membership Inference Attacks (MIAs) poses a critical privacy concern. This paper introduces two novel and efficient approaches (DualMD and DistillMD) to protect diffusion models against MIAs while maintaining high utility. Both methods are based on training two separate diffusion models on disjoint subsets of the original dataset. DualMD then employs a private inference pipeline that utilizes both models. This strategy significantly reduces the risk of black-box MIAs by limiting the information any single model contains about individual training samples. The dual models can also generate\"soft targets\"to train a private student model in DistillMD, enhancing privacy guarantees against all types of MIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art MIAs across various datasets in white-box and black-box settings demonstrate their effectiveness in substantially reducing MIA success rates while preserving competitive image generation performance. Notably, our experiments reveal that DistillMD not only defends against MIAs but also mitigates model memorization, indicating that both vulnerabilities stem from overfitting and can be addressed simultaneously with our unified approach.",
            "corpus_id": "273507277",
            "text": "Diffusion models have demonstrated remarkable capabilities in image synthesis, but their recently proven vulnerability to Membership Inference Attacks (MIAs) poses a critical privacy concern. This paper introduces two novel and efficient approaches (DualMD and DistillMD) to protect diffusion models against MIAs while maintaining high utility. Both methods are based on training two separate diffusion models on disjoint subsets of the original dataset. DualMD then employs a private inference pipeline that utilizes both models. This strategy significantly reduces the risk of black-box MIAs by limiting the information any single model contains about individual training samples. The dual models can also generate\"soft targets\"to train a private student model in DistillMD, enhancing privacy guarantees against all types of MIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art MIAs across various datasets in white-box and black-box settings demonstrate their effectiveness in substantially reducing MIA success rates while preserving competitive image generation performance. Notably, our experiments reveal that DistillMD not only defends against MIAs but also mitigates model memorization, indicating that both vulnerabilities stem from overfitting and can be addressed simultaneously with our unified approach.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.89111328125
        },
        {
            "paperId": "7698ad870dc0a74001ca579fab61233cdc23e1cd",
            "corpusId": 270461470,
            "title": "Rethinking Membership Inference Attacks Against Transfer Learning",
            "venue": "IEEE Transactions on Information Forensics and Security",
            "year": 2025,
            "referenceCount": 97,
            "citationCount": 34,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.11577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2129404435",
                    "name": "Cong Wu"
                },
                {
                    "authorId": "2288607261",
                    "name": "Jing Chen"
                },
                {
                    "authorId": "2306165408",
                    "name": "Qianru Fang"
                },
                {
                    "authorId": "145905327",
                    "name": "Kun He"
                },
                {
                    "authorId": "2306588978",
                    "name": "Ziming Zhao"
                },
                {
                    "authorId": "2298588825",
                    "name": "Hao Ren"
                },
                {
                    "authorId": "2298340442",
                    "name": "Guowen Xu"
                },
                {
                    "authorId": "2152803226",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2068340077",
                    "name": "Yang Xiang"
                }
            ],
            "abstract": "Transfer learning, successful in knowledge translation across related tasks, faces a substantial privacy threat from membership inference attacks (MIAs). These attacks, despite posing significant risk to ML model\u2019s training data, remain limited-explored in transfer learning. The interaction between teacher and student models in transfer learning has not been thoroughly explored in MIAs, potentially resulting in an under-examined aspect of privacy vulnerabilities within transfer learning. In this paper, we propose a new MIA vector against transfer learning, to determine whether a specific data point was used to train the teacher model while only accessing the student model in a white-box setting. Our method delves into the intricate relationship between teacher and student models, analyzing the discrepancies in hidden layer representations between the student model and its shadow counterpart. These identified differences are then adeptly utilized to refine the shadow model\u2019s training process and to inform membership inference decisions effectively. Our method, evaluated across four datasets in diverse transfer learning tasks, reveals that even when an attacker only has access to the student model, the teacher model\u2019s training data remains susceptible to MIAs. We believe our work unveils the unexplored risk of membership inference in transfer learning.",
            "corpus_id": "270461470",
            "text": "Transfer learning, successful in knowledge translation across related tasks, faces a substantial privacy threat from membership inference attacks (MIAs). These attacks, despite posing significant risk to ML model\u2019s training data, remain limited-explored in transfer learning. The interaction between teacher and student models in transfer learning has not been thoroughly explored in MIAs, potentially resulting in an under-examined aspect of privacy vulnerabilities within transfer learning. In this paper, we propose a new MIA vector against transfer learning, to determine whether a specific data point was used to train the teacher model while only accessing the student model in a white-box setting. Our method delves into the intricate relationship between teacher and student models, analyzing the discrepancies in hidden layer representations between the student model and its shadow counterpart. These identified differences are then adeptly utilized to refine the shadow model\u2019s training process and to inform membership inference decisions effectively. Our method, evaluated across four datasets in diverse transfer learning tasks, reveals that even when an attacker only has access to the student model, the teacher model\u2019s training data remains susceptible to MIAs. We believe our work unveils the unexplored risk of membership inference in transfer learning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8701171875
        },
        {
            "paperId": "c3843b5a1511d9fc131433c7993f74e3012c7d25",
            "corpusId": 252283966,
            "title": "CLIPping Privacy: Identity Inference Attacks on Multi-Modal Machine Learning Models",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 66,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.07341",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2209.07341?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2209.07341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140396795",
                    "name": "Dominik Hintersdorf"
                },
                {
                    "authorId": "2140415179",
                    "name": "Lukas Struppek"
                },
                {
                    "authorId": "2066493115",
                    "name": "K. Kersting"
                }
            ],
            "abstract": "As deep learning is now used in many real-world applications, research has focused increasingly on the privacy of deep learning models and how to prevent attackers from obtaining sensitive information about the training data. However, image-text models like CLIP have not yet been looked at in the context of privacy attacks. While membership inference attacks aim to tell whether a speci\ufb01c data point was used for training, we introduce a new type of privacy attack, named identity inference attack (IDIA), designed for multi-modal image-text models like CLIP. Using IDIAs, an attacker can reveal whether a particular person, was part of the training data by querying the model in a black-box fashion with different images of the same person. Letting the model choose from a wide variety of possible text labels, the attacker can probe the model whether it recognizes the person and, therefore, was used for training. Through several experiments on CLIP, we show that the attacker can identify individuals used for training with very high accuracy and that the model learns to connect the names with the depicted people. Our experiments show that a multi-modal image-text model indeed leaks sensitive information about its training data and, therefore, should be handled with care.",
            "corpus_id": "252283966",
            "text": "As deep learning is now used in many real-world applications, research has focused increasingly on the privacy of deep learning models and how to prevent attackers from obtaining sensitive information about the training data. However, image-text models like CLIP have not yet been looked at in the context of privacy attacks. While membership inference attacks aim to tell whether a speci\ufb01c data point was used for training, we introduce a new type of privacy attack, named identity inference attack (IDIA), designed for multi-modal image-text models like CLIP. Using IDIAs, an attacker can reveal whether a particular person, was part of the training data by querying the model in a black-box fashion with different images of the same person. Letting the model choose from a wide variety of possible text labels, the attacker can probe the model whether it recognizes the person and, therefore, was used for training. Through several experiments on CLIP, we show that the attacker can identify individuals used for training with very high accuracy and that the model learns to connect the names with the depicted people. Our experiments show that a multi-modal image-text model indeed leaks sensitive information about its training data and, therefore, should be handled with care.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.52099609375
        },
        {
            "paperId": "305631707498c5602a7329d7c9fbfcada4cbf7b9",
            "corpusId": 249461647,
            "title": "White-box Membership Attack Against Machine Learning Based Retinopathy Classification",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 6,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.03584",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.03584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35477938",
                    "name": "Mounia Hamidouche"
                },
                {
                    "authorId": "1996509",
                    "name": "R. Bellafqira"
                },
                {
                    "authorId": "1684052",
                    "name": "G. Quellec"
                },
                {
                    "authorId": "1734213",
                    "name": "G. Coatrieux"
                }
            ],
            "abstract": "The advances in machine learning (ML) have greatly improved AI-based diagnosis aid systems in medical imaging. However, being based on collecting medical data specific to individuals induces several security issues, especially in terms of privacy. Even though the owner of the images like a hospital put in place strict privacy protection provisions at the level of its information system, the model trained over his images still holds disclosure potential. The trained model may be accessible to an attacker as: 1) White-box: accessing to the model architecture and parameters; 2) Black-box: where he can only query the model with his own inputs through an appropriate interface. Existing attack methods include: feature estimation attacks (FEA), membership inference attack (MIA), model memorization attack (MMA) and identification attacks (IA) [1]. In this work we focus on MIA against a model that has been trained to detect diabetic retinopathy from retinal images. Diabetic retinopathy is a condition that can cause vision loss and blindness in the people who have diabetes. MIA is the process of determining whether a data sample comes from the training data set of a trained ML model or not. From a privacy perspective in our use case where a diabetic retinopathy classification model is given to partners that have at their disposal images along with patients\u2019 identifiers, inferring the membership status of a data sample can help to state if a patient has contributed or not to the training of the model.",
            "corpus_id": "249461647",
            "text": "The advances in machine learning (ML) have greatly improved AI-based diagnosis aid systems in medical imaging. However, being based on collecting medical data specific to individuals induces several security issues, especially in terms of privacy. Even though the owner of the images like a hospital put in place strict privacy protection provisions at the level of its information system, the model trained over his images still holds disclosure potential. The trained model may be accessible to an attacker as: 1) White-box: accessing to the model architecture and parameters; 2) Black-box: where he can only query the model with his own inputs through an appropriate interface. Existing attack methods include: feature estimation attacks (FEA), membership inference attack (MIA), model memorization attack (MMA) and identification attacks (IA) [1]. In this work we focus on MIA against a model that has been trained to detect diabetic retinopathy from retinal images. Diabetic retinopathy is a condition that can cause vision loss and blindness in the people who have diabetes. MIA is the process of determining whether a data sample comes from the training data set of a trained ML model or not. From a privacy perspective in our use case where a diabetic retinopathy classification model is given to partners that have at their disposal images along with patients\u2019 identifiers, inferring the membership status of a data sample can help to state if a patient has contributed or not to the training of the model.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92138671875
        },
        {
            "paperId": "639f7181bbf8ee96310533aa4a329edde1178fcb",
            "corpusId": 274445730,
            "title": "Comparative Analysis of Black-Box and White-Box Machine Learning Model in Phishing Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.02084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66227929",
                    "name": "Abdullah Fajar"
                },
                {
                    "authorId": "3460095",
                    "name": "S. Yazid"
                },
                {
                    "authorId": "2330190121",
                    "name": "Indra Budi"
                }
            ],
            "abstract": "Background: Explainability in phishing detection model can support a further solution of phishing attack mitigation by increasing trust and understanding how phishing can be detected. Objective: The aims of this study to determine and best recommendation to apply an approach which has several components with abilities to fulfil the critical needs Methods: A methodology starting with analyzing both black-box and white-box models to get the pros and cons specifically in phishing detection. The conclusion of the analysis will be validated by experiment using a set of well-known algorithms and public phishing datasets. Experimental metrics covers 3 measurements such as predictive accuracy and explainability metrics. Conclusion: Both models are comparable in terms of interpretability and consistency, with room for improvement in diverse datasets. EBM as an example of white-box model is generally better suited for applications requiring explainability and actionable insights. Finally, each model, white-box and black-box model has positive and negative aspects both for performance metric and for explainable metric. It is important to consider the objective of model usage.",
            "corpus_id": "274445730",
            "text": "Background: Explainability in phishing detection model can support a further solution of phishing attack mitigation by increasing trust and understanding how phishing can be detected. Objective: The aims of this study to determine and best recommendation to apply an approach which has several components with abilities to fulfil the critical needs Methods: A methodology starting with analyzing both black-box and white-box models to get the pros and cons specifically in phishing detection. The conclusion of the analysis will be validated by experiment using a set of well-known algorithms and public phishing datasets. Experimental metrics covers 3 measurements such as predictive accuracy and explainability metrics. Conclusion: Both models are comparable in terms of interpretability and consistency, with room for improvement in diverse datasets. EBM as an example of white-box model is generally better suited for applications requiring explainability and actionable insights. Finally, each model, white-box and black-box model has positive and negative aspects both for performance metric and for explainable metric. It is important to consider the objective of model usage.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01934814453125
        },
        {
            "paperId": "3205117afbc1412f0a18c92e99ce6ba9cbde9382",
            "corpusId": 269033223,
            "title": "Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks",
            "venue": "Decision and Game Theory for Security",
            "year": 2024,
            "referenceCount": 69,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2070363314",
                    "name": "Kavita Kumari"
                },
                {
                    "authorId": "2235825",
                    "name": "Murtuza Jadliwala"
                },
                {
                    "authorId": "39058278",
                    "name": "S. Jha"
                },
                {
                    "authorId": "34645101",
                    "name": "Anindya Maiti"
                }
            ],
            "abstract": "Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA). Existing works have only analyzed MIA in a single\"what if\"interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings. Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members. In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method. We model such interactions by employing a continuous-time stochastic signaling game framework. In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately. First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA. Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system. By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings.",
            "corpus_id": "269033223",
            "text": "Model explanations improve the transparency of black-box machine learning (ML) models and their decisions; however, they can also be exploited to carry out privacy threats such as membership inference attacks (MIA). Existing works have only analyzed MIA in a single\"what if\"interaction scenario between an adversary and the target ML model; thus, it does not discern the factors impacting the capabilities of an adversary in launching MIA in repeated interaction settings. Additionally, these works rely on assumptions about the adversary's knowledge of the target model's structure and, thus, do not guarantee the optimality of the predefined threshold required to distinguish the members from non-members. In this paper, we delve into the domain of explanation-based threshold attacks, where the adversary endeavors to carry out MIA attacks by leveraging the variance of explanations through iterative interactions with the system comprising of the target ML model and its corresponding explanation method. We model such interactions by employing a continuous-time stochastic signaling game framework. In our framework, an adversary plays a stopping game, interacting with the system (having imperfect information about the type of an adversary, i.e., honest or malicious) to obtain explanation variance information and computing an optimal threshold to determine the membership of a datapoint accurately. First, we propose a sound mathematical formulation to prove that such an optimal threshold exists, which can be used to launch MIA. Then, we characterize the conditions under which a unique Markov perfect equilibrium (or steady state) exists in this dynamic system. By means of a comprehensive set of simulations of the proposed game model, we assess different factors that can impact the capability of an adversary to launch MIA in such repeated interaction settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.912109375
        }
    ],
    "quotes": {
        "cost": 0.31304399999999993,
        "quotes": [
            {
                "idx": 0,
                "key": "[10488675 | Shokri et al. | 2016 | Citations: 4160]",
                "snippets": "Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output.\n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class.\n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Overview of the attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1323,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 200
                            },
                            {
                                "start": 201,
                                "end": 271
                            },
                            {
                                "start": 272,
                                "end": 535
                            },
                            {
                                "start": 538,
                                "end": 628
                            },
                            {
                                "start": 629,
                                "end": 785
                            },
                            {
                                "start": 788,
                                "end": 898
                            },
                            {
                                "start": 899,
                                "end": 1044
                            },
                            {
                                "start": 1045,
                                "end": 1325
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output.\n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class.\n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[165163934 | Song et al. | 2019 | Citations: 244]",
                "snippets": "Membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 41,
                        "end": 1588,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "52211986",
                            "133091488",
                            "46933970",
                            "10488675",
                            "2656445",
                            "10488675",
                            "2656445"
                        ],
                        "quote": "Membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[195699554 | Leino et al. | 2019 | Citations: 272]",
                "snippets": "Training data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25].\n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 864,
                        "end": 2311,
                        "sentence_offsets": [
                            {
                                "start": 864,
                                "end": 1068
                            },
                            {
                                "start": 1071,
                                "end": 1213
                            },
                            {
                                "start": 1214,
                                "end": 1472
                            },
                            {
                                "start": 1475,
                                "end": 1577
                            },
                            {
                                "start": 1578,
                                "end": 1806
                            },
                            {
                                "start": 1807,
                                "end": 2093
                            },
                            {
                                "start": 2094,
                                "end": 2276
                            },
                            {
                                "start": 2277,
                                "end": 2519
                            }
                        ],
                        "ref_mentions": [
                            "2148083",
                            "17295025"
                        ],
                        "quote": "Training data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25].\n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[199546273 | Hilprecht et al. | 2019 | Citations: 191]",
                "snippets": "The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background of Membership Inference",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 759,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 172
                            },
                            {
                                "start": 173,
                                "end": 282
                            },
                            {
                                "start": 283,
                                "end": 403
                            },
                            {
                                "start": 404,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 615
                            },
                            {
                                "start": 616,
                                "end": 759
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[202734167 | Jia et al. | 2019 | Citations: 394]",
                "snippets": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset.\n\nMore recently, Nasr et al. [43] proposed membership inference attacks against white-box ML models. For a data sample, they calculate the corresponding gradients over the white-box target classifier's parameters and use these gradients as the data sample's feature for membership inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset.\n\nMore recently, Nasr et al. [43] proposed membership inference attacks against white-box ML models. For a data sample, they calculate the corresponding gradients over the white-box target classifier's parameters and use these gradients as the data sample's feature for membership inference.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[202888587 | Aivodji et al. | 2019 | Citations: 42]",
                "snippets": "Membership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov (Shokri et al., 2016). Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov (Melis et al., 2018) in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[53099247 | Melis et al. | 2018 | Citations: 1482]": "Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."
                },
                "metadata": [
                    {
                        "section_title": "Inference attacks against machine learning models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1448,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 116
                            },
                            {
                                "start": 117,
                                "end": 283
                            },
                            {
                                "start": 284,
                                "end": 531
                            },
                            {
                                "start": 532,
                                "end": 756
                            },
                            {
                                "start": 757,
                                "end": 942
                            },
                            {
                                "start": 943,
                                "end": 1057
                            },
                            {
                                "start": 1058,
                                "end": 1135
                            },
                            {
                                "start": 1136,
                                "end": 1448
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "53099247"
                        ],
                        "quote": "Membership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov (Shokri et al., 2016). Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov (Melis et al., 2018) in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[214623088 | Song et al. | 2020 | Citations: 375]",
                "snippets": "In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not (Shokri et al., 2016)(Yeom et al., 2017). It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3](Pyrgelis et al., 2017). For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. (Shokri et al., 2016) conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models (Chen et al., 2019)(Hayes et al., 2017)(Hilprecht et al., 2019)(Wu et al., 2019), to differentially private models (Jayaraman et al., 2019)(Rahman et al., 2018), to decentralized settings where the models are trained across multiple users without sharing their data (Melis et al., 2018)(Nasr et al., 2018), and to white-box settings where the adversary also has the access to the target model's architecture and weights (Nasr et al., 2018).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[201126909 | Wu et al. | 2019 | Citations: 45]": "In this paper, we aim to understand the generalization properties of generative adversarial networks (GANs) from a new perspective of privacy protection. Theoretically, we prove that a differentially private learning algorithm used for training the GAN does not overfit to a certain degree, i.e., the generalization gap can be bounded. Moreover, some recent works, such as the Bayesian GAN, can be re-interpreted based on our theoretical insight from privacy protection. Quantitatively, to evaluate the information leakage of well-trained GAN models, we perform various membership attacks on these models. The results show that previous Lipschitz regularization techniques are effective in not only reducing the generalization gap but also alleviating the information leakage of the training dataset.",
                    "[202539472 | Chen et al. | 2019 | Citations: 52]": "In recent years, the success of deep learning has carried over from discriminative models to generative models. In particular, generative adversarial networks (GANs) have facilitated a new level of performance ranging from media manipulation to dataset re-generation. Despite the success, the potential risks of privacy breach stemming from GANs are less well explored. In this paper, we focus on membership inference attack against GANs that has the potential to reveal information about victim models' training data. Specifically, we present the first taxonomy of membership inference attacks, which encompasses not only existing attacks but also our novel ones. We also propose the first generic attack model that can be instantiated in various settings according to adversary's knowledge about the victim model. We complement our systematic analysis of attack vectors with a comprehensive experimental study, that investigates the effectiveness of these attacks w.r.t. model type, training configurations, and attack type across three diverse application scenarios ranging from images, over medical data to location data. We show consistent effectiveness in all the setups, which bridges the assumption gap and performance gap in previous study with a complete spectrum of performance across settings. We conclusively remind users to think over before publicizing any part of their models.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[3570809 | Pyrgelis et al. | 2017 | Citations: 271]": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. \nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
                    "[53099247 | Melis et al. | 2018 | Citations: 1482]": "Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 176,
                        "end": 1350,
                        "sentence_offsets": [
                            {
                                "start": 176,
                                "end": 361
                            },
                            {
                                "start": 362,
                                "end": 467
                            },
                            {
                                "start": 468,
                                "end": 601
                            },
                            {
                                "start": 602,
                                "end": 795
                            },
                            {
                                "start": 796,
                                "end": 958
                            },
                            {
                                "start": 959,
                                "end": 1350
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "2656445",
                            "3570809",
                            "10488675",
                            "202539472",
                            "52211986",
                            "199546273",
                            "201126909",
                            "173990571",
                            "13699042",
                            "53099247",
                            "133091488",
                            "133091488"
                        ],
                        "quote": "In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not (Shokri et al., 2016)(Yeom et al., 2017). It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3](Pyrgelis et al., 2017). For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. (Shokri et al., 2016) conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models (Chen et al., 2019)(Hayes et al., 2017)(Hilprecht et al., 2019)(Wu et al., 2019), to differentially private models (Jayaraman et al., 2019)(Rahman et al., 2018), to decentralized settings where the models are trained across multiple users without sharing their data (Melis et al., 2018)(Nasr et al., 2018), and to white-box settings where the adversary also has the access to the target model's architecture and weights (Nasr et al., 2018)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[216553034 | Mirshghallah et al. | 2020 | Citations: 140]",
                "snippets": "Given a data instance and (black-box or white-box) access to a pre-trained target model, a membership inference attack speculates whether or not the given data instance has contributed to the training step of the target model. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack on machine learning where they consider an attacker who has black-box query access to the target model and can obtain confidence scores (probability vector) for the queried input. The attacker uses this confidence score to deduce the participation of given data in training. They first train shadow models on a labeled dataset that can be generated using three methods: model inversion attack (we will see next), statistics-based synthesis (through assumptions about the underlying distribution of training set), or noisy real data. Using these shadow models, the attacker trains an \"attack model\" that distinguishes the participation of a data instance in the training set of the shadow models. Lastly, for the main inference attack, the attacker makes queries to the target deployed model to receive confidence scores for each given input data instance and infers whether or not the input was part of the target training data. This attack is built on the assumption that if a record was used in the training of a model, it would yield a higher confidence score, than a record which was not seen before by the model. Some studies [46](Truex et al., 2018)(Yeom et al., 2017) attribute membership inference attacks to the generalization gap, the over-fitting of the model, and data memorization capabilities of neural networks. Deep neural networks have been shown to memorize the training data (Arpit et al., 2017)(Meehan et al., 2020)(Izzo et al., 2020), rather than learning the latent properties of the data, which means they often tend to over-fit to the training data. Long et al. (Long et al., 2017) propose an approach which more accurately tests the membership of a given instance. They train the shadow models with and without this given instance, and then at inference time the attacker tests to see if the instance was used for training the target model, similar to Shokri et al.'s approach.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[50778569 | Truex et al. | 2018 | Citations: 112]": "Membership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. Aiming at providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning in federated systems exposes vulnerabilities to membership inference risks when the adversary is a participant in the federation. We also discuss countermeasure and mitigation strategies.",
                    "[11455421 | Arpit et al. | 2017 | Citations: 1827]": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.",
                    "[215745042 | Meehan et al. | 2020 | Citations: 65]": "Detecting overfitting in generative models is an important challenge in machine learning. In this work, we formalize a form of overfitting that we call {\\em{data-copying}} -- where the generative model memorizes and outputs training samples or small variations thereof. We provide a three sample non-parametric test for detecting data-copying that uses the training set, a separate sample from the target distribution, and a generated sample from the model, and study the performance of our test on several canonical models and datasets. \nFor code \\& examples, visit this https URL",
                    "[232033672 | Izzo et al. | 2020 | Citations: 264]": "Deleting data from a trained machine learning (ML) model is a critical task in many applications. For example, we may want to remove the influence of training points that might be out of date or outliers. Regulations such as EU's General Data Protection Regulation also stipulate that individuals can request to have their data deleted. The naive approach to data deletion is to retrain the ML model on the remaining data, but this is too time consuming. Moreover there is no known efficient algorithm that exactly deletes data from most ML models. In this work, we evaluate several approaches for approximate data deletion from trained models. For the case of linear regression, we propose a new method with linear dependence on the feature dimension $d$, a significant gain over all existing methods which all have superlinear time dependence on the dimension. We also provide a new test for evaluating data deletion from linear models.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[9517641 | Long et al. | 2017 | Citations: 85]": "Machine learning models are increasingly made available to the masses through public query interfaces. Recent academic work has demonstrated that malicious users who can query such models are able to infer sensitive information about records within the training data. Differential privacy can thwart such attacks, but not all models can be readily trained to achieve this guarantee or to achieve it with acceptable utility loss. As a result, if a model is trained without differential privacy guarantee, little is known or can be said about the privacy risk of releasing it. In this work, we investigate and analyze membership attacks to understand why and how they succeed. Based on this understanding, we propose Differential Training Privacy (DTP), an empirical metric to estimate the privacy risk of publishing a classier when methods such as differential privacy cannot be applied. DTP is a measure of a classier with respect to its training dataset, and we show that calculating DTP is efficient in many practical cases. We empirically validate DTP using state-of-the-art machine learning models such as neural networks trained on real-world datasets. Our results show that DTP is highly predictive of the success of membership attacks and therefore reducing DTP also reduces the privacy risk. We advocate for DTP to be used as part of the decision-making process when considering publishing a classifier. To this end, we also suggest adopting the DTP-1 hypothesis: if a classifier has a DTP value above 1, it should not be published."
                },
                "metadata": [
                    {
                        "section_title": "MEMBERSHIP INFERENCE",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2096,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 226
                            },
                            {
                                "start": 227,
                                "end": 471
                            },
                            {
                                "start": 472,
                                "end": 566
                            },
                            {
                                "start": 567,
                                "end": 824
                            },
                            {
                                "start": 825,
                                "end": 987
                            },
                            {
                                "start": 988,
                                "end": 1220
                            },
                            {
                                "start": 1221,
                                "end": 1409
                            },
                            {
                                "start": 1410,
                                "end": 1585
                            },
                            {
                                "start": 1586,
                                "end": 1782
                            },
                            {
                                "start": 1783,
                                "end": 1883
                            },
                            {
                                "start": 1884,
                                "end": 2096
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "50778569",
                            "2656445",
                            "11455421",
                            "215745042",
                            "232033672",
                            "9517641"
                        ],
                        "quote": "Given a data instance and (black-box or white-box) access to a pre-trained target model, a membership inference attack speculates whether or not the given data instance has contributed to the training step of the target model. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack on machine learning where they consider an attacker who has black-box query access to the target model and can obtain confidence scores (probability vector) for the queried input. The attacker uses this confidence score to deduce the participation of given data in training. They first train shadow models on a labeled dataset that can be generated using three methods: model inversion attack (we will see next), statistics-based synthesis (through assumptions about the underlying distribution of training set), or noisy real data. Using these shadow models, the attacker trains an \"attack model\" that distinguishes the participation of a data instance in the training set of the shadow models. Lastly, for the main inference attack, the attacker makes queries to the target deployed model to receive confidence scores for each given input data instance and infers whether or not the input was part of the target training data. This attack is built on the assumption that if a record was used in the training of a model, it would yield a higher confidence score, than a record which was not seen before by the model. Some studies [46](Truex et al., 2018)(Yeom et al., 2017) attribute membership inference attacks to the generalization gap, the over-fitting of the model, and data memorization capabilities of neural networks. Deep neural networks have been shown to memorize the training data (Arpit et al., 2017)(Meehan et al., 2020)(Izzo et al., 2020), rather than learning the latent properties of the data, which means they often tend to over-fit to the training data. Long et al. (Long et al., 2017) propose an approach which more accurately tests the membership of a given instance. They train the shadow models with and without this given instance, and then at inference time the attacker tests to see if the instance was used for training the target model, similar to Shokri et al.'s approach."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[218571404 | Yang et al. | 2020 | Citations: 71]",
                "snippets": "Membership inference and model inversion attacks are two types of data inference attacks that threaten the security and privacy of machine learning. They differ in their inference goals Membership Inference Attack. In the membership inference attack, the attacker is asked to determine whether a given data record is part of the training data of the target model [24,30,31,36,47,49].\n\nConfidence-based Attack [47,49]. Shokri et al. [49] introduced membership inference against black-box models, where the attacker has access only to the prediction scores of the target model. To infer the membership, the attacker trains a binary classifier (also referred to as attack model) which takes as input the confidence scores of the target model on a given data sample and predicts the data sample to be a member or non-member of the training dataset of the target model. Prior to training the attack model, the attacker trains a set of shadow models on an auxiliary dataset drawn from the same data distribution as the target model's training data to replicate the target model. The attack model is then trained on the confidence scores predicted by the shadow models instead of the target model on the members and non-members of the shadow models' training data. Salem et al. [47] further showed that it is sufficient to train only one shadow model to replicate the target model for membership inference attack. Besides, they also showed that ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy. For example, their experimental results show that only the top one/three highest values in the confidence vector are sufficient to result in effective membership inference. These results indicate that the assumptions of membership inference attack could be largely relaxed which might lead to more practical threats.\n\nConfidence & Label-based Attack [35]. Nasr et al. [35] extended the attack model by also taking the label information as input. Their attack model is composed of three neural networks. The first two networks operate on the confidence score vector and the onehot encoded label respectively. They have the same size of input dimension, i.e., the number of classes of the target model. The third network operates on the concatenation of the output of the first two networks and predicts the membership.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[202734167 | Jia et al. | 2019 | Citations: 394]": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                },
                "metadata": [
                    {
                        "section_title": "Data Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2388,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 214
                            },
                            {
                                "start": 215,
                                "end": 383
                            },
                            {
                                "start": 386,
                                "end": 418
                            },
                            {
                                "start": 419,
                                "end": 576
                            },
                            {
                                "start": 577,
                                "end": 865
                            },
                            {
                                "start": 866,
                                "end": 1073
                            },
                            {
                                "start": 1074,
                                "end": 1258
                            },
                            {
                                "start": 1259,
                                "end": 1407
                            },
                            {
                                "start": 1408,
                                "end": 1571
                            },
                            {
                                "start": 1572,
                                "end": 1744
                            },
                            {
                                "start": 1745,
                                "end": 1888
                            },
                            {
                                "start": 1891,
                                "end": 1928
                            },
                            {
                                "start": 1929,
                                "end": 2018
                            },
                            {
                                "start": 2019,
                                "end": 2075
                            },
                            {
                                "start": 2076,
                                "end": 2180
                            },
                            {
                                "start": 2181,
                                "end": 2273
                            },
                            {
                                "start": 2274,
                                "end": 2390
                            }
                        ],
                        "ref_mentions": [
                            "202734167",
                            "133091488",
                            "46933970",
                            "10488675",
                            "46933970",
                            "10488675",
                            "10488675",
                            "46933970",
                            "49863840",
                            "49863840"
                        ],
                        "quote": "Membership inference and model inversion attacks are two types of data inference attacks that threaten the security and privacy of machine learning. They differ in their inference goals Membership Inference Attack. In the membership inference attack, the attacker is asked to determine whether a given data record is part of the training data of the target model [24,30,31,36,47,49].\n\nConfidence-based Attack [47,49]. Shokri et al. [49] introduced membership inference against black-box models, where the attacker has access only to the prediction scores of the target model. To infer the membership, the attacker trains a binary classifier (also referred to as attack model) which takes as input the confidence scores of the target model on a given data sample and predicts the data sample to be a member or non-member of the training dataset of the target model. Prior to training the attack model, the attacker trains a set of shadow models on an auxiliary dataset drawn from the same data distribution as the target model's training data to replicate the target model. The attack model is then trained on the confidence scores predicted by the shadow models instead of the target model on the members and non-members of the shadow models' training data. Salem et al. [47] further showed that it is sufficient to train only one shadow model to replicate the target model for membership inference attack. Besides, they also showed that ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy. For example, their experimental results show that only the top one/three highest values in the confidence vector are sufficient to result in effective membership inference. These results indicate that the assumptions of membership inference attack could be largely relaxed which might lead to more practical threats.\n\nConfidence & Label-based Attack [35]. Nasr et al. [35] extended the attack model by also taking the label information as input. Their attack model is composed of three neural networks. The first two networks operate on the confidence score vector and the onehot encoded label respectively. They have the same size of input dimension, i.e., the number of classes of the target model. The third network operates on the concatenation of the output of the first two networks and predicts the membership."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[221808247 | Miao et al. | 2019 | Citations: 36]",
                "snippets": "The membership inference attack is considered as a significant privacy threat for machine learning (ML) models [22]. The attack aims to determine whether a specific data sample is within the target model's training set or not. The attack is driven by the different behaviors of the target model when making predictions on samples within or out of its training set.\n\nVarious membership inference attack methods have been recently proposed. Shokri et al. [39] train shadow models to constitute the attack model against a target ML model with black-box access. The shadow models mimic the target model's prediction behavior. To improve accuracy, Liu et al. [15] and Hayes et al. [10] leverage Generative Adversarial Networks (GAN) to generate shadow models with increasingly similar outputs to the target model. Salem et al. [32] relax the attack assumptions mentioned in the work [39], demonstrating that shadow models are not necessary to launch the membership inference attack. Instead, a threshold of the predicted confidence score can be defined to substitute the attack model. Intuitively, a large confidence score indicates the sample as a member of the training set [41]...Apart from the blackbox access, Farokhi and Kaafar [8] model the recordlevel membership inference attack under the white-box access.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195477472 | Liu et al. | 2019 | Citations: 82]": "Social media networks have shown rapid growth in the past, and massive social data are generated which can reveal behavior or emotion propensities of users. Numerous social researchers leverage machine learning technology to build social media analytic models which can detect the abnormal behaviors or mental illnesses from the social media data effectively. Although the researchers only public the prediction interfaces of the machine learning models, in general, these interfaces may leak information about the individual data records on which the models were trained. Knowing a certain user\u2019s social media record was used to train a model can breach user privacy. In this paper, we present SocInf and focus on the fundamental problem known as membership inference. The key idea of SocInf is to construct a mimic model which has a similar prediction behavior with the public model, and then we can disclose the prediction differences between the training and testing data set by abusing the mimic model. With elaborated analytics on the predictions of the mimic model, SocInf can thus infer whether a given record is in the victim model\u2019s training set or not. We empirically evaluate the attack performance of SocInf on machine learning models trained by Xgboost, logistics, and online cloud platform. Using the realistic data, the experiment results show that SocInf can achieve an inference accuracy and precision of 73% and 84%, respectively, in average, and of 83% and 91% at best.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%).",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1175,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 116
                            },
                            {
                                "start": 117,
                                "end": 226
                            },
                            {
                                "start": 227,
                                "end": 364
                            },
                            {
                                "start": 367,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 558
                            },
                            {
                                "start": 559,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 809
                            },
                            {
                                "start": 810,
                                "end": 978
                            },
                            {
                                "start": 979,
                                "end": 1080
                            },
                            {
                                "start": 1081,
                                "end": 1177
                            }
                        ],
                        "ref_mentions": [
                            "49863840",
                            "10488675",
                            "195477472",
                            "52211986",
                            "46933970",
                            "10488675"
                        ],
                        "quote": "The membership inference attack is considered as a significant privacy threat for machine learning (ML) models [22]. The attack aims to determine whether a specific data sample is within the target model's training set or not. The attack is driven by the different behaviors of the target model when making predictions on samples within or out of its training set.\n\nVarious membership inference attack methods have been recently proposed. Shokri et al. [39] train shadow models to constitute the attack model against a target ML model with black-box access. The shadow models mimic the target model's prediction behavior. To improve accuracy, Liu et al. [15] and Hayes et al. [10] leverage Generative Adversarial Networks (GAN) to generate shadow models with increasingly similar outputs to the target model. Salem et al. [32] relax the attack assumptions mentioned in the work [39], demonstrating that shadow models are not necessary to launch the membership inference attack. Instead, a threshold of the predicted confidence score can be defined to substitute the attack model. Intuitively, a large confidence score indicates the sample as a member of the training set [41]"
                    },
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 1579,
                        "end": 1712,
                        "sentence_offsets": [
                            {
                                "start": 1579,
                                "end": 1711
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Apart from the blackbox access, Farokhi and Kaafar [8] model the recordlevel membership inference attack under the white-box access."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[227227868 | Hidano et al. | 2020 | Citations: 13]",
                "snippets": "We deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train.\n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f. To build an adversary A against f, we attempt to learn some statistical relevance between the distribution f(x) of confidence values and the membership (x, y) \u2208 D train.\n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train. An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f(x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train.\n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f. To build an adversary A against f, we attempt to learn some statistical relevance between the distribution f(x) of confidence values and the membership (x, y) \u2208 D train.\n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train. An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f(x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network).",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[231861713 | He et al. | 2021 | Citations: 98]",
                "snippets": "Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API (Carlini et al., 2018)9,19,28,(Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)...Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack...Nasr et al. (Nasr et al., 2018) conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[170076423 | Carlini et al. | 2018 | Citations: 1148]": "This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. \nIn experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.",
                    "[203593891 | Song et al. | 2019 | Citations: 3]": "Machine learning as a service (MLaaS), and algorithm marketplaces are on a rise. Data holders can easily train complex models on their data using third party provided learning codes. Training accurate ML models requires massive labeled data and advanced learning algorithms. The resulting models are considered as intellectual property of the model owners and their copyright should be protected. Also, MLaaS needs to be trusted not to embed secret information about the training data into the model, such that it could be later retrieved when the model is deployed. In this paper, we present membership encoding for training deep neural networks and encoding the membership information, i.e. whether a data point is used for training, for a subset of training data. Membership encoding has several applications in different scenarios, including robust watermarking for model copyright protection, and also the risk analysis of stealthy data embedding privacy attacks. Our encoding algorithm can determine the membership of significantly redacted data points, and is also robust to model compression and fine-tuning. It also enables encoding a significant fraction of the training set, with negligible drop in the model's prediction accuracy.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 29,
                        "end": 233,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "170076423",
                            "49863840",
                            "133091488",
                            "46933970",
                            "10488675",
                            "203593891",
                            "2656445"
                        ],
                        "quote": "Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API (Carlini et al., 2018)9,19,28,(Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 381,
                        "end": 772,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1021,
                        "end": 1142,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "133091488"
                        ],
                        "quote": "Nasr et al. (Nasr et al., 2018) conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[234785882 | Chen et al. | 2021 | Citations: 8]",
                "snippets": "A well-known membership inference attack is the shadow-model attack (Shokri et al., 2016). It requires the attacker to train several shadow models and attack models. To attack a victim model, first, the attacker collects or synthesizes data from the same domain as the victim classifier's training and test data, and divides the data into several private training and test sets. Then, the attacker uses each of the private training set to train one shadow model f i shadow to mimic the behavior of the victim classifier. Next, the attacker sends all the examples in each private training and test set to its corresponding shadow model to create a dataset D that contains the tuple (y, f i shadow (x), I(x)) for each example x , where y is the class label of x , f i shadow (x) is a vector containing the outputs of the i th shadow model, and I(x) indicates whether x is used for training the shadow models. Finally, the attacker partitions D based on the examples' class labels and uses them to train one attack model per class to distinguish the training data from the others.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "Membership inference attacks",
                        "pdf_hash": "",
                        "start": 762,
                        "end": 1839,
                        "sentence_offsets": [
                            {
                                "start": 762,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 927
                            },
                            {
                                "start": 928,
                                "end": 1140
                            },
                            {
                                "start": 1141,
                                "end": 1282
                            },
                            {
                                "start": 1283,
                                "end": 1668
                            },
                            {
                                "start": 1669,
                                "end": 1839
                            }
                        ],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "A well-known membership inference attack is the shadow-model attack (Shokri et al., 2016). It requires the attacker to train several shadow models and attack models. To attack a victim model, first, the attacker collects or synthesizes data from the same domain as the victim classifier's training and test data, and divides the data into several private training and test sets. Then, the attacker uses each of the private training set to train one shadow model f i shadow to mimic the behavior of the victim classifier. Next, the attacker sends all the examples in each private training and test set to its corresponding shadow model to create a dataset D that contains the tuple (y, f i shadow (x), I(x)) for each example x , where y is the class label of x , f i shadow (x) is a vector containing the outputs of the i th shadow model, and I(x) indicates whether x is used for training the shadow models. Finally, the attacker partitions D based on the examples' class labels and uses them to train one attack model per class to distinguish the training data from the others."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[236956533 | Bagmar et al. | 2021 | Citations: 5]",
                "snippets": "Membership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021).\n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too.\n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[209439842 | He et al. | 2019 | Citations: 51]": "Today's success of state of the art methods for semantic segmentation is driven by large datasets. Data is considered an important asset that needs to be protected, as the collection and annotation of such datasets comes at significant efforts and associated costs. In addition, visual data might contain private or sensitive information, that makes it equally unsuited for public release. Unfortunately, recent work on membership inference in the broader area of adversarial machine learning and inference attacks on machine learning models has shown that even black box classifiers leak information on the dataset that they were trained on. We show that such membership inference attacks can be successfully carried out on complex, state of the art models for semantic segmentation. In order to mitigate the associated risks, we also study a series of defenses against such membership inference attacks and find effective counter measures against the existing risks with little effect on the utility of the segmentation method. Finally, we extensively evaluate our attacks and defenses on a range of relevant real-world datasets: Cityscapes, BDD100K, and Mapillary Vistas.",
                    "[216652989 | Danhier et al. | 2020 | Citations: 2]": "We apply membership inference attacks in the context of preference data exploited by recommendation systems and show that they can lead to \u201cfidelity leakages\u201d. These leakages allow one service provider to determine whether or not its users are faithful. We first provide experimental results based on real-world data made available by Spotify that confirm the feasibility of such attacks and allow us to put forward their influencing parameters. We then discuss the challenges for interpreting and mitigating fidelity leakages."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1197,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 123
                            },
                            {
                                "start": 124,
                                "end": 321
                            },
                            {
                                "start": 324,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 584
                            },
                            {
                                "start": 585,
                                "end": 733
                            },
                            {
                                "start": 734,
                                "end": 808
                            },
                            {
                                "start": 811,
                                "end": 1031
                            },
                            {
                                "start": 1032,
                                "end": 1199
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "10488675",
                            "216652989",
                            "209439842"
                        ],
                        "quote": "Membership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021).\n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too.\n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[237563320 | Li et al. | 2020 | Citations: 246]",
                "snippets": "Shokri et al. (Shokri et al., 2016) present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. (Salem et al., 2018) later present another attack by gradually relaxing the assumptions made by Shokri et al. (Shokri et al., 2016) achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks (Hui et al., 2021)(Li et al., 2020)35,(Song et al., 2019)(Yeom et al., 2017)...Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[165163934 | Song et al. | 2019 | Citations: 244]": "The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.",
                    "[211532510 | Li et al. | 2020 | Citations: 35]": "This work studies membership inference (MI) attack against classifiers, where the attacker's goal is to determine whether a data instance was used for training the classifier. While it is known that overfitting makes classifiers susceptible to MI attacks, we showcase a simple numerical relationship between the generalization gap---the difference between training and test accuracies---and the classifier's vulnerability to MI attacks---as measured by an MI attack's accuracy gain over a random guess. We then propose to close the gap by matching the training and validation accuracies during training, by means of a new {\\em set regularizer} using the Maximum Mean Discrepancy between the softmax output empirical distributions of the training and validation sets. Our experimental results show that combining this approach with another simple defense (mix-up training) significantly improves state-of-the-art defense against MI attacks, with minimal impact on testing accuracy.",
                    "[230523638 | Hui et al. | 2021 | Citations: 122]": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 353,
                        "end": 1003,
                        "sentence_offsets": [
                            {
                                "start": 353,
                                "end": 450
                            },
                            {
                                "start": 451,
                                "end": 590
                            },
                            {
                                "start": 591,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 897
                            },
                            {
                                "start": 898,
                                "end": 1004
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970",
                            "10488675",
                            "230523638",
                            "211532510",
                            "165163934",
                            "2656445"
                        ],
                        "quote": "Shokri et al. (Shokri et al., 2016) present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. (Salem et al., 2018) later present another attack by gradually relaxing the assumptions made by Shokri et al. (Shokri et al., 2016) achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks (Hui et al., 2021)(Li et al., 2020)35,(Song et al., 2019)(Yeom et al., 2017)"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 419,
                        "end": 937,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[239016142 | Tang et al. | 2021 | Citations: 80]",
                "snippets": "Membership inference attacks are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28]29]...Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model (Leino et al., 2019)(Nasr et al., 2018), which can exploit model parameters to infer membership information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                },
                "metadata": [
                    {
                        "quote": "Membership inference attacks are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28]29]",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 824,
                        "end": 1007,
                        "sentence_offsets": [
                            {
                                "start": 824,
                                "end": 1006
                            }
                        ],
                        "ref_mentions": [
                            "195699554",
                            "133091488"
                        ],
                        "quote": "Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model (Leino et al., 2019)(Nasr et al., 2018), which can exploit model parameters to infer membership information."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[246706163 | Zhou et al. | 2022 | Citations: 23]",
                "snippets": "The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[244077745 | Hu et al. | 2021 | Citations: 28]": "Generative adversarial networks (GANs) have made unprecedented performance in image synthesis and play a key role in various downstream applications of computer vision. However, GAN models trained on sensitive data also pose a distinct threat to privacy. In this poster, we present a novel over-representation based membership inference attack. Unlike prior attacks against GANs which focus on the overall metrics, such as the attack accuracy, our attack aims to make inference from the high-precision perspective, which allows the adversary to concentrate on inferring a sample as a member confidently. Initial experimental results demonstrate that the adversary can achieve a high precision attack even if the overall attack accuracy is about 50% for a well-trained GAN model. Our work will raise awareness of the importance of precision when GAN owners evaluate the privacy risks of their models.",
                    "[3570809 | Pyrgelis et al. | 2017 | Citations: 271]": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. \nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "B. Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1332,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 268
                            },
                            {
                                "start": 269,
                                "end": 529
                            },
                            {
                                "start": 530,
                                "end": 611
                            },
                            {
                                "start": 612,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 992
                            },
                            {
                                "start": 993,
                                "end": 1212
                            },
                            {
                                "start": 1213,
                                "end": 1332
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970",
                            "133091488",
                            "221203089",
                            "195699554",
                            "3570809",
                            "244077745",
                            "52211986",
                            "199546273"
                        ],
                        "quote": "The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[247595200 | Grosso et al. | 2022 | Citations: 22]",
                "snippets": "The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Comparison of MIA strategies",
                        "pdf_hash": "",
                        "start": 393,
                        "end": 1147,
                        "sentence_offsets": [
                            {
                                "start": 393,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1040
                            },
                            {
                                "start": 1041,
                                "end": 1108
                            },
                            {
                                "start": 1109,
                                "end": 1147
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[248810845 | Zhang et al. | 2022 | Citations: 16]",
                "snippets": "There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings.\n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18]22].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[174799799 | Sablayrolles et al. | 2019 | Citations: 368]": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                },
                "metadata": [
                    {
                        "section_title": "Privacy Leakage and Membership Inference",
                        "pdf_hash": "",
                        "start": 107,
                        "end": 1626,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 130
                            },
                            {
                                "start": 131,
                                "end": 194
                            },
                            {
                                "start": 195,
                                "end": 274
                            },
                            {
                                "start": 275,
                                "end": 486
                            },
                            {
                                "start": 487,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 668
                            },
                            {
                                "start": 669,
                                "end": 791
                            },
                            {
                                "start": 792,
                                "end": 904
                            },
                            {
                                "start": 905,
                                "end": 1034
                            },
                            {
                                "start": 1037,
                                "end": 1182
                            },
                            {
                                "start": 1183,
                                "end": 1329
                            },
                            {
                                "start": 1330,
                                "end": 1463
                            },
                            {
                                "start": 1464,
                                "end": 1651
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "133091488",
                            "195699554",
                            "174799799",
                            "214623088",
                            "214623088",
                            "2656445"
                        ],
                        "quote": "There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings.\n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18]22]."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[248870291 | Zhang et al. | 2022 | Citations: 3]",
                "snippets": "Membership inference attacks are divided into white-box attacks [11](Nasr et al., 2018)(Leino et al., 2019)(Hayes et al., 2017) and blackbox attacks (Yurtsever et al., 2019)(Shokri et al., 2016)(Chen et al., 2019) according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[186206717 | Yurtsever et al. | 2019 | Citations: 1388]": "Automated driving systems (ADSs) promise a safe, comfortable and efficient driving experience. However, fatalities involving vehicles equipped with ADSs are on the rise. The full potential of ADSs cannot be realized unless the robustness of state-of-the-art is improved further. This paper discusses unsolved problems and surveys the technical aspect of automated driving. Studies regarding present challenges, high-level system architectures, emerging methodologies and core functions including localization, mapping, perception, planning, and human machine interfaces, were thoroughly reviewed. Furthermore, many state-of-the-art algorithms were implemented and compared on our own platform in a real-world driving setting. The paper concludes with an overview of available datasets and tools for ADS development.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                },
                "metadata": [
                    {
                        "section_title": "A. MEMBERSHIP INFERENCE ATTACK",
                        "pdf_hash": "",
                        "start": 386,
                        "end": 913,
                        "sentence_offsets": [
                            {
                                "start": 386,
                                "end": 531
                            },
                            {
                                "start": 532,
                                "end": 742
                            },
                            {
                                "start": 743,
                                "end": 913
                            }
                        ],
                        "ref_mentions": [
                            "133091488",
                            "195699554",
                            "52211986",
                            "186206717",
                            "10488675",
                            "221203089"
                        ],
                        "quote": "Membership inference attacks are divided into white-box attacks [11](Nasr et al., 2018)(Leino et al., 2019)(Hayes et al., 2017) and blackbox attacks (Yurtsever et al., 2019)(Shokri et al., 2016)(Chen et al., 2019) according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[250048543 | Wang et al. | 2022 | Citations: 14]",
                "snippets": "Shokri et al. (Shokri et al., 2016) specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. (Salem et al., 2018) further relax several key assumptions from (Shokri et al., 2016), including knowledge of the target model architecture and target dataset distribution. Yeom et al. (Yeom et al., 2017) explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. (Nasr et al., 2018) study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks (Choquette-Choo et al., 2020)(Li et al., 2020) assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Membership inference attacks",
                        "pdf_hash": "",
                        "start": 281,
                        "end": 1245,
                        "sentence_offsets": [
                            {
                                "start": 281,
                                "end": 378
                            },
                            {
                                "start": 379,
                                "end": 558
                            },
                            {
                                "start": 559,
                                "end": 653
                            },
                            {
                                "start": 654,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 936
                            },
                            {
                                "start": 937,
                                "end": 1030
                            },
                            {
                                "start": 1031,
                                "end": 1245
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970",
                            "10488675",
                            "2656445",
                            "133091488",
                            "220831381",
                            "237563320"
                        ],
                        "quote": "Shokri et al. (Shokri et al., 2016) specify the first membership inference attack against machine learning models. The authors propose a general formulation of membership inference attack against machine learning models, and train multiple shadow models to simulate the target model's behavior. In that case, the training sets for multiple attack models (one for each class) are generated. Salem et al. (Salem et al., 2018) further relax several key assumptions from (Shokri et al., 2016), including knowledge of the target model architecture and target dataset distribution. Yeom et al. (Yeom et al., 2017) explore the relationship between attack performance and overfitting, and propose the first decisionbased attack. Nasr et al. (Nasr et al., 2018) study membership inference attacks in both black-box and white-box settings. Instead of using output scores, several recent membership attacks (Choquette-Choo et al., 2020)(Li et al., 2020) assume only predicted hard labels of models are exposed, and demonstrate that label-only exposures are also vulnerable to membership leakage."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[251066729 | He et al. | 2022 | Citations: 16]",
                "snippets": "Membership inference attack (Shokri et al., 2016)(Salem et al., 2018)(Nasr et al., 2018)(Pyrgelis et al., 2017)(Chen et al., 2019)(He et al., 2019)(Rahimian et al., 2021)(Hui et al., 2021)(Liu et al., 2021)(He et al., 2021)(Song et al., 2020)9,(Li et al., 2020)34,4] aims to determine whether a given data sample is used to train a target model. Multiple works studied the membership inference attacks against the supervised learning (Shokri et al., 2016)(Salem et al., 2018)(Nasr et al., 2018)(Li et al., 2020)(Choquette-Choo et al., 2020)[9]. Shokri et al. (Shokri et al., 2016) proposed the first black-box membership inference attack against machine learning models by leveraging multiple shadow models and attack models. The attack model takes a sample's posteriors generated from the target model as the input and predicts whether it is a member or not. Salem et al. (Salem et al., 2018) relaxed the assumption from Shokri et al. (Shokri et al., 2016) and proposed novel model-independent and dataset-independent membership inference attacks. Nasr et al. (Nasr et al., 2018) studied the white-box membership inference attacks in both centralized and federated learning settings. Li and Zhang (Li et al., 2020) and Choo et al. (Choquette-Choo et al., 2020) concentrated on a more restricted attack scenario (called label-only attack) where the target model only returns the predicted labels instead of posteriors when the adversary queries the target model with given samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[209439842 | He et al. | 2019 | Citations: 51]": "Today's success of state of the art methods for semantic segmentation is driven by large datasets. Data is considered an important asset that needs to be protected, as the collection and annotation of such datasets comes at significant efforts and associated costs. In addition, visual data might contain private or sensitive information, that makes it equally unsuited for public release. Unfortunately, recent work on membership inference in the broader area of adversarial machine learning and inference attacks on machine learning models has shown that even black box classifiers leak information on the dataset that they were trained on. We show that such membership inference attacks can be successfully carried out on complex, state of the art models for semantic segmentation. In order to mitigate the associated risks, we also study a series of defenses against such membership inference attacks and find effective counter measures against the existing risks with little effect on the utility of the segmentation method. Finally, we extensively evaluate our attacks and defenses on a range of relevant real-world datasets: Cityscapes, BDD100K, and Mapillary Vistas.",
                    "[211201635 | Rahimian et al. | 2021 | Citations: 22]": "Machine learning models are commonly trained on sensitive and personal data such as pictures, medical records, financial records, etc. A serious breach of the privacy of this training set occurs when an adversary is able to decide whether or not a specific data point in her possession was used to train a model. While all previous membership inference attacks rely on access to the posterior probabilities, we present the first attack which only relies on the predicted class label - yet shows high success rate.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[230523638 | Hui et al. | 2021 | Citations: 122]": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
                    "[231846491 | He et al. | 2021 | Citations: 52]": "Data is the key factor to drive the development of machine learning (ML) during the past decade. However, high-quality data, in particular labeled data, is often hard and expensive to collect. To leverage large-scale unlabeled data, self-supervised learning, represented by contrastive learning, is introduced. The objective of contrastive learning is to map different views derived from a training sample (e.g., through data augmentation) closer in their representation space, while different views derived from different samples more distant. In this way, a contrastive model learns to generate informative representations for data samples, which are then used to perform downstream ML tasks. Recent research has shown that machine learning models are vulnerable to various privacy attacks. However, most of the current efforts concentrate on models trained with supervised learning. Meanwhile, data samples' informative representations learned with contrastive learning may cause severe privacy risks as well. In this paper, we perform the first privacy analysis of contrastive learning through the lens of membership inference and attribute inference. Our experimental results show that contrastive models trained on image datasets are less vulnerable to membership inference attacks but more vulnerable to attribute inference attacks compared to supervised models. The former is due to the fact that contrastive models are less prone to overfitting, while the latter is caused by contrastive models' capability of representing data samples expressively. To remedy this situation, we propose the first privacy-preserving contrastive learning mechanism, Talos, relying on adversarial training. Empirical results show that Talos can successfully mitigate attribute inference risks for contrastive models while maintaining their membership privacy and model utility.",
                    "[237290083 | Liu et al. | 2021 | Citations: 99]": "Given a set of unlabeled images or (image, text) pairs, contrastive learning aims to pre-train an image encoder that can be used as a feature extractor for many downstream tasks. In this work, we propose EncoderMI, the first membership inference method against image encoders pre-trained by contrastive learning. In particular, given an input and a black-box access to an image encoder, EncoderMI aims to infer whether the input is in the training dataset of the image encoder. EncoderMI can be used 1) by a data owner to audit whether its (public) data was used to pre-train an image encoder without its authorization or 2) by an attacker to compromise privacy of the training data when it is private/sensitive. Our EncoderMI exploits the overfitting of the image encoder towards its training data. In particular, an overfitted image encoder is more likely to output more (or less) similar feature vectors for two augmented versions of an input in (or not in) its training dataset. We evaluate EncoderMI on image encoders pre-trained on multiple datasets by ourselves as well as the Contrastive Language-Image Pre-training (CLIP) image encoder, which is pre-trained on 400 million (image, text) pairs collected from the Internet and released by OpenAI. Our results show that EncoderMI can achieve high accuracy, precision, and recall. We also explore a countermeasure against EncoderMI via preventing overfitting through early stopping. Our results show that it achieves trade-offs between accuracy of EncoderMI and utility of the image encoder, i.e., it can reduce the accuracy of EncoderMI, but it also incurs classification accuracy loss of the downstream classifiers built based on the image encoder.",
                    "[3570809 | Pyrgelis et al. | 2017 | Citations: 271]": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. \nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1090,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 421
                            },
                            {
                                "start": 422,
                                "end": 555
                            },
                            {
                                "start": 556,
                                "end": 711
                            },
                            {
                                "start": 712,
                                "end": 832
                            },
                            {
                                "start": 833,
                                "end": 1090
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970",
                            "49863840",
                            "3570809",
                            "221203089",
                            "209439842",
                            "211201635",
                            "230523638",
                            "237290083",
                            "231846491",
                            "214623088",
                            "237563320",
                            "10488675",
                            "46933970",
                            "133091488",
                            "237563320",
                            "220831381",
                            "10488675",
                            "46933970",
                            "10488675",
                            "133091488",
                            "237563320",
                            "220831381"
                        ],
                        "quote": "Membership inference attack (Shokri et al., 2016)(Salem et al., 2018)(Nasr et al., 2018)(Pyrgelis et al., 2017)(Chen et al., 2019)(He et al., 2019)(Rahimian et al., 2021)(Hui et al., 2021)(Liu et al., 2021)(He et al., 2021)(Song et al., 2020)9,(Li et al., 2020)34,4] aims to determine whether a given data sample is used to train a target model. Multiple works studied the membership inference attacks against the supervised learning (Shokri et al., 2016)(Salem et al., 2018)(Nasr et al., 2018)(Li et al., 2020)(Choquette-Choo et al., 2020)[9]. Shokri et al. (Shokri et al., 2016) proposed the first black-box membership inference attack against machine learning models by leveraging multiple shadow models and attack models. The attack model takes a sample's posteriors generated from the target model as the input and predicts whether it is a member or not. Salem et al. (Salem et al., 2018) relaxed the assumption from Shokri et al. (Shokri et al., 2016) and proposed novel model-independent and dataset-independent membership inference attacks. Nasr et al. (Nasr et al., 2018) studied the white-box membership inference attacks in both centralized and federated learning settings. Li and Zhang (Li et al., 2020) and Choo et al. (Choquette-Choo et al., 2020) concentrated on a more restricted attack scenario (called label-only attack) where the target model only returns the predicted labels instead of posteriors when the adversary queries the target model with given samples."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[251719178 | He et al. | 2022 | Citations: 25]",
                "snippets": "In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models (Hayes et al., 2017)19,(Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017). Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. (Salem et al., 2018) propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. (Shokri et al., 2016). Later, Nasr et al. (Nasr et al., 2018) focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. (Song et al., 2019) study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang (Li et al., 2020) and Choquette-Choo et al. (Choquette-Choo et al., 2020) propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[165163934 | Song et al. | 2019 | Citations: 244]": "The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[3052834 | Hayes et al. | 2017 | Citations: 104]": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 30,
                        "end": 1567,
                        "sentence_offsets": [
                            {
                                "start": 30,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 390
                            },
                            {
                                "start": 391,
                                "end": 524
                            },
                            {
                                "start": 525,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 809
                            },
                            {
                                "start": 810,
                                "end": 987
                            },
                            {
                                "start": 988,
                                "end": 1232
                            },
                            {
                                "start": 1233,
                                "end": 1364
                            },
                            {
                                "start": 1365,
                                "end": 1567
                            }
                        ],
                        "ref_mentions": [
                            "3052834",
                            "195699554",
                            "133091488",
                            "46933970",
                            "10488675",
                            "165163934",
                            "2656445",
                            "10488675",
                            "46933970",
                            "10488675",
                            "133091488",
                            "165163934",
                            "237563320",
                            "220831381"
                        ],
                        "quote": "In membership inference, the adversary's goal is to infer whether a given data sample is used to train a target model. Currently, membership inference is one of the major methods to evaluate the privacy risks of machine learning models (Hayes et al., 2017)19,(Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017). Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. They train multiple attack models using a dataset constructed from multiple shadow models, where each attack model is for each class. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. (Salem et al., 2018) propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. (Shokri et al., 2016). Later, Nasr et al. (Nasr et al., 2018) focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and white-box settings. Song et al. (Song et al., 2019) study the relationship between adversarial examples and the privacy risk caused by membership inference attacks and find that the latter increases when model builders take measures to defend against adversarial example attacks. Recently, researchers consider a more challenging and realistic scenario where the adversary can only receive the predicted labels. Li and Zhang (Li et al., 2020) and Choquette-Choo et al. (Choquette-Choo et al., 2020) propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[251765197 | Li et al. | 2022 | Citations: 34]",
                "snippets": "Membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29](Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017). Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. (Salem et al., 2018) propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. (Shokri et al., 2016). Later, Nasr et al. (Nasr et al., 2018) focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. (Song et al., 2019) study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang (Li et al., 2020) and Choquette-Choo et al. (Choquette-Choo et al., 2020) propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[165163934 | Song et al. | 2019 | Citations: 244]": "The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 11,
                        "end": 1254,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 223
                            },
                            {
                                "start": 224,
                                "end": 314
                            },
                            {
                                "start": 315,
                                "end": 446
                            },
                            {
                                "start": 447,
                                "end": 599
                            },
                            {
                                "start": 600,
                                "end": 776
                            },
                            {
                                "start": 777,
                                "end": 906
                            },
                            {
                                "start": 907,
                                "end": 1110
                            },
                            {
                                "start": 1111,
                                "end": 1254
                            }
                        ],
                        "ref_mentions": [
                            "195699554",
                            "133091488",
                            "46933970",
                            "10488675",
                            "165163934",
                            "2656445",
                            "10488675",
                            "46933970",
                            "10488675",
                            "133091488",
                            "165163934",
                            "237563320",
                            "220831381"
                        ],
                        "quote": "Membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29](Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017). Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. (Salem et al., 2018) propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. (Shokri et al., 2016). Later, Nasr et al. (Nasr et al., 2018) focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. (Song et al., 2019) study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang (Li et al., 2020) and Choquette-Choo et al. (Choquette-Choo et al., 2020) propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[253244424 | Chen et al. | 2022 | Citations: 32]",
                "snippets": "Based on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs (Hui et al., 2021)(Shokri et al., 2020)(Shokri et al., 2016). This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased (Shokri et al., 2016), where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased (Salem et al., 2018)(Song et al., 2020), where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker (Leino et al., 2019)(Nasr et al., 2018)(Sablayrolles et al., 2019). Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[174799799 | Sablayrolles et al. | 2019 | Citations: 368]": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                    "[230523638 | Hui et al. | 2021 | Citations: 122]": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 776,
                        "end": 2044,
                        "sentence_offsets": [
                            {
                                "start": 776,
                                "end": 876
                            },
                            {
                                "start": 879,
                                "end": 912
                            },
                            {
                                "start": 913,
                                "end": 1014
                            },
                            {
                                "start": 1015,
                                "end": 1070
                            },
                            {
                                "start": 1071,
                                "end": 1114
                            },
                            {
                                "start": 1115,
                                "end": 1371
                            },
                            {
                                "start": 1372,
                                "end": 1585
                            },
                            {
                                "start": 1588,
                                "end": 1621
                            },
                            {
                                "start": 1622,
                                "end": 1765
                            },
                            {
                                "start": 1766,
                                "end": 1907
                            },
                            {
                                "start": 1908,
                                "end": 2044
                            }
                        ],
                        "ref_mentions": [
                            "230523638",
                            "218514464",
                            "10488675",
                            "10488675",
                            "46933970",
                            "214623088",
                            "195699554",
                            "133091488",
                            "174799799"
                        ],
                        "quote": "Based on the attacker's capability, membership inference attacks can be grouped into two categories: \n\n\u2022 Black-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs (Hui et al., 2021)(Shokri et al., 2020)(Shokri et al., 2016). This case is generic to most machine learning contexts. There are two attack strategies in general. The first strategy is modelbased (Shokri et al., 2016), where the attacker builds multiple shadow models to mimic the victim model, then utilizes them to construct a dataset with member/non-member labels, and finally trains a binary classifier to predict member/non-member. The second strategy is metricbased (Salem et al., 2018)(Song et al., 2020), where the attacker compares a designed metric M mem , such as correctness or entropy, with a predefined threshold \u03c4 to infer if a sample belongs to the training dataset. \n\n\u2022 White-box Membership Inference. In this case, model parameters \u03b8 * or even intermediate training information such as gradients \u2202L \u2202\u03b8 are observable by the attacker (Leino et al., 2019)(Nasr et al., 2018)(Sablayrolles et al., 2019). Such capability provides additional information supporting inference attacks, which is usually achievable in collaborative learning settings. Most white-box attacks are model-based, as the attacker needs to access the internals of deep models to extract model-specific features."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[256868849 | Zhu et al. | 2023 | Citations: 14]",
                "snippets": "Membership Inference Attack (MIA) was first introduced by Shokri et al. (Shokri et al., 2016). It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box (Nasr et al., 2018), [28] as well as black-box (Shokri et al., 2016), (Sablayrolles et al., 2019), (Yeom et al., 2017), (Salem et al., 2018), (Song et al., 2020) classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members (Yeom et al., 2017). Sablayrolles et al. (Sablayrolles et al., 2019) further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[174799799 | Sablayrolles et al. | 2019 | Citations: 368]": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1103,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 77
                            },
                            {
                                "start": 78,
                                "end": 294
                            },
                            {
                                "start": 295,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 647
                            },
                            {
                                "start": 648,
                                "end": 779
                            },
                            {
                                "start": 780,
                                "end": 928
                            },
                            {
                                "start": 929,
                                "end": 1103
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "133091488",
                            "10488675",
                            "174799799",
                            "2656445",
                            "46933970",
                            "214623088",
                            "2656445",
                            "174799799"
                        ],
                        "quote": "Membership Inference Attack (MIA) was first introduced by Shokri et al. (Shokri et al., 2016). It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box (Nasr et al., 2018), [28] as well as black-box (Shokri et al., 2016), (Sablayrolles et al., 2019), (Yeom et al., 2017), (Salem et al., 2018), (Song et al., 2020) classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members (Yeom et al., 2017). Sablayrolles et al. (Sablayrolles et al., 2019) further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[258298235 | Gross et al. | 2023 | Citations: 2]",
                "snippets": "Membership inference [15], [16] is an attack on a machine learning model, where an adversary tries to infer whether a particular record was part of the training set of the attacked model. Depending on the type of ML model and the information about that model that is available to the adversary, there exist different variants of the membership inference attack (MIA). The types of MIAs that can be utilized against generative models depend on the knowledge of the attacker. In a white-box setting the attacker can access the parameters of the generative model itself and use this knowledge to craft an attack. Here we focus on a block-box setting, i.e. the model parameters of the target model are not available.\n\nIn [15] it is assumed that the attacker has access to public data that is drawn from the same distribution as the target model's training data, and the architecture and parameters of the target model are known. Several so-called shadow models are trained that mimic the behaviour of the target model. An attack model is then trained on the shadow models' outputs and used to classify the record under consideration as member or non-member of the target model's training data. Later works were able to relax these assumptions [16], [17].The most common attacks against generative models/synthetic data in a black box setting are as follows:\n\n1) Classifier trained on shadow models [15]: The MIA then proceeds with the following steps: a) Draw m datasets of size n from the public data. b) Train 2m shadow models, one for each data set with and without inserting the target record t into the set. c) Generate p synthetic data sets from each shadow model. This results in a labelled data set of size 2mp of pairs (x, y), where x is a synthetic data set and y = 0 if the target record was not in the set used to train the model that generated x, y = 1, if the target record was present. d) Train a (binary) attack classifier on the labelled set of data sets. e) Use the attack classifier to predict, whether the target record was present in the private data that was used to generate the synthetic data set. The drawback of this method is that it is computationally very expensive.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples."
                },
                "metadata": [
                    {
                        "section_title": "C. Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2191,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 188
                            },
                            {
                                "start": 189,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 474
                            },
                            {
                                "start": 475,
                                "end": 610
                            },
                            {
                                "start": 611,
                                "end": 713
                            },
                            {
                                "start": 716,
                                "end": 926
                            },
                            {
                                "start": 927,
                                "end": 1016
                            },
                            {
                                "start": 1017,
                                "end": 1191
                            },
                            {
                                "start": 1192,
                                "end": 1355
                            },
                            {
                                "start": 1358,
                                "end": 1501
                            },
                            {
                                "start": 1502,
                                "end": 1611
                            },
                            {
                                "start": 1612,
                                "end": 1669
                            },
                            {
                                "start": 1670,
                                "end": 1899
                            },
                            {
                                "start": 1900,
                                "end": 1971
                            },
                            {
                                "start": 1972,
                                "end": 2120
                            },
                            {
                                "start": 2121,
                                "end": 2194
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "10488675",
                            "199546273",
                            "10488675"
                        ],
                        "quote": "Membership inference [15], [16] is an attack on a machine learning model, where an adversary tries to infer whether a particular record was part of the training set of the attacked model. Depending on the type of ML model and the information about that model that is available to the adversary, there exist different variants of the membership inference attack (MIA). The types of MIAs that can be utilized against generative models depend on the knowledge of the attacker. In a white-box setting the attacker can access the parameters of the generative model itself and use this knowledge to craft an attack. Here we focus on a block-box setting, i.e. the model parameters of the target model are not available.\n\nIn [15] it is assumed that the attacker has access to public data that is drawn from the same distribution as the target model's training data, and the architecture and parameters of the target model are known. Several so-called shadow models are trained that mimic the behaviour of the target model. An attack model is then trained on the shadow models' outputs and used to classify the record under consideration as member or non-member of the target model's training data. Later works were able to relax these assumptions [16], [17].The most common attacks against generative models/synthetic data in a black box setting are as follows:\n\n1) Classifier trained on shadow models [15]: The MIA then proceeds with the following steps: a) Draw m datasets of size n from the public data. b) Train 2m shadow models, one for each data set with and without inserting the target record t into the set. c) Generate p synthetic data sets from each shadow model. This results in a labelled data set of size 2mp of pairs (x, y), where x is a synthetic data set and y = 0 if the target record was not in the set used to train the model that generated x, y = 1, if the target record was present. d) Train a (binary) attack classifier on the labelled set of data sets. e) Use the attack classifier to predict, whether the target record was present in the private data that was used to generate the synthetic data set. The drawback of this method is that it is computationally very expensive."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[259342605 | Chen et al. | 2023 | Citations: 24]",
                "snippets": "Depending on the adversary capabilities, MIAs can be divided into black-box (Shokri et al., 2016), (Yeom et al., 2017), [17], (Carlini et al., 2021), (Song et al., 2020), (Choquette-Choo et al., 2020), [47], (Li et al., 2020) and white-box attacks (Leino et al., 2019), (Jayaraman et al., 2020), (Nasr et al., 2018). The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies (Jia et al., 2019), (Tang et al., 2021), (Nasr et al., 2018) (and in HAMP). Such attacks can be mounted by either shadow-training (Shokri et al., 2016), (Nasr et al., 2018), (Yeom et al., 2017) or computing statistical metrics based on the partial knowledge of the private dataset (Song et al., 2020), (Choquette-Choo et al., 2020), (Li et al., 2020). Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with (Choquette-Choo et al., 2020) or without (Li et al., 2020) partial knowledge of the membership information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[202734167 | Jia et al. | 2019 | Citations: 394]": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[239016142 | Tang et al. | 2021 | Citations: 80]": "Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML) models. These attacks aim to distinguish training members from non-members by exploiting differential behavior of the models on member and non-member inputs. The goal of this work is to train ML models that have high membership privacy while largely preserving their utility; we therefore aim for an empirical membership privacy guarantee as opposed to the provable privacy guarantees provided by techniques like differential privacy, as such techniques are shown to deteriorate model utility. Specifically, we propose a new framework to train privacy-preserving models that induces similar behavior on member and non-member inputs to mitigate membership inference attacks. Our framework, called SELENA, has two major components. The first component and the core of our defense is a novel ensemble architecture for training. This architecture, which we call Split-AI, splits the training data into random subsets, and trains a model on each subset of the data. We use an adaptive inference strategy at test time: our ensemble architecture aggregates the outputs of only those models that did not contain the input sample in their training data. We prove that our Split-AI architecture defends against a large family of membership inference attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component in our framework called Self-Distillation to protect against such stronger attacks. The Self-Distillation component (self-)distills the training dataset through our Split-AI ensemble, without using any external public datasets. Through extensive experiments on major benchmark datasets we show that SELENA presents a superior trade-off between membership privacy and utility compared to the state of the art.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[218862728 | Jayaraman et al. | 2020 | Citations: 151]": "Abstract We study membership inference in settings where assumptions commonly used in previous research are relaxed. First, we consider cases where only a small fraction of the candidate pool targeted by the adversary are members and develop a PPV-based metric suitable for this setting. This skewed prior setting is more realistic than the balanced prior setting typically considered. Second, we consider adversaries that select inference thresholds according to their attack goals, such as identifying as many members as possible with a given false positive tolerance. We develop a threshold selection designed for achieving particular attack goals. Since previous inference attacks fail in imbalanced prior settings, we develop new inference attacks based on the intuition that inputs corresponding to training set members will be near a local minimum in the loss function. An attack that combines this with thresholds on the per-instance loss can achieve high PPV even in settings where other attacks are ineffective.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[244920593 | Carlini et al. | 2021 | Citations: 708]": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%)."
                },
                "metadata": [
                    {
                        "section_title": "VI. RELATED WORK",
                        "pdf_hash": "",
                        "start": 30,
                        "end": 1016,
                        "sentence_offsets": [
                            {
                                "start": 30,
                                "end": 190
                            },
                            {
                                "start": 191,
                                "end": 375
                            },
                            {
                                "start": 376,
                                "end": 516
                            },
                            {
                                "start": 517,
                                "end": 691
                            },
                            {
                                "start": 692,
                                "end": 847
                            },
                            {
                                "start": 848,
                                "end": 1016
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "2656445",
                            "244920593",
                            "214623088",
                            "220831381",
                            "237563320",
                            "195699554",
                            "218862728",
                            "133091488",
                            "202734167",
                            "239016142",
                            "49863840",
                            "10488675",
                            "49863840",
                            "2656445",
                            "214623088",
                            "220831381",
                            "237563320",
                            "220831381",
                            "237563320"
                        ],
                        "quote": "Depending on the adversary capabilities, MIAs can be divided into black-box (Shokri et al., 2016), (Yeom et al., 2017), [17], (Carlini et al., 2021), (Song et al., 2020), (Choquette-Choo et al., 2020), [47], (Li et al., 2020) and white-box attacks (Leino et al., 2019), (Jayaraman et al., 2020), (Nasr et al., 2018). The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies (Jia et al., 2019), (Tang et al., 2021), (Nasr et al., 2018) (and in HAMP). Such attacks can be mounted by either shadow-training (Shokri et al., 2016), (Nasr et al., 2018), (Yeom et al., 2017) or computing statistical metrics based on the partial knowledge of the private dataset (Song et al., 2020), (Choquette-Choo et al., 2020), (Li et al., 2020). Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with (Choquette-Choo et al., 2020) or without (Li et al., 2020) partial knowledge of the membership information."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[259837436 | Niu et al. | 2023 | Citations: 3]",
                "snippets": "Membership Inference Attacks. Homer et al. (Homer et al., 2008) first proposed an MI attack on biological data. Shokri et al. (Shokri et al., 2016) proposed the first black-box MI attack against ML. Huge literature followed these works to different scenarios (e.g., location data (Pyrgelis et al., 2019), language models (Carlini et al., 2020), sentence embeddings (Song et al., 2020), speech recognition models (Shah et al., 2021), federated learning (Jere et al., 2021), transfer learning [63], generative models (Chen et al., 2019), white box access (Sablayrolles et al., 2019), (Nasr et al., 2018), (Leino et al., 2019)). Categories of Membership Inference Attacks. There are main three categories 1) Binary classifier-based MI attacks, which utilize the output predictions of shadow models to train a binary classifier to launch the MI attacks (Shokri et al., 2016), [9]. 2) Evaluation metric-based MI attacks, which used the defined evaluation metrics to distinguish members and nonmembers [9], (Yeom et al., 2017), (Hui et al., 2021), (Choquette-Choo et al., 2020), (Li et al., 2020), (Long et al., 2020). 3) Differential Comparisons-based MI attacks (BlindMI-Diff), which mainly utilized the differences between two datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[174799799 | Sablayrolles et al. | 2019 | Citations: 368]": "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet.",
                    "[214743021 | Song et al. | 2020 | Citations: 273]": "Embeddings are functions that map raw input data to low-dimensional vector representations, while preserving important semantic information about the inputs. Pre-training embeddings on a large amount of unlabeled data and fine-tuning them for downstream tasks is now a de facto standard in achieving state of the art learning in many domains. We demonstrate that embeddings, in addition to encoding generic semantics, often also present a vector that leaks sensitive information about the input data. We develop three classes of attacks to systematically study information that might be leaked by embeddings. First, embedding vectors can be inverted to partially recover some of the input data. As an example, we show that our attacks on popular sentence embeddings recover between 50%--70% of the input words (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive attributes inherent in inputs and independent of the underlying semantic task at hand. Attributes such as authorship of text can be easily extracted by training an inference model on just a handful of labeled embedding vectors. Third, embedding models leak moderate amount of membership information for infrequent training data inputs. We extensively evaluate our attacks on various state-of-the-art embedding models in the text domain. We also propose and evaluate defenses that can prevent the leakage to some extent at a minor cost in utility.",
                    "[216553356 | Pyrgelis et al. | 2019 | Citations: 23]": "While location data is extremely valuable for various applications, disclosing it prompts serious threats to individuals' privacy. To limit such concerns, organizations often provide analysts with aggregate time-series that indicate, e.g., how many people are in a location at a time interval, rather than raw individual traces. In this paper, we perform a measurement study to understand Membership Inference Attacks (MIAs) on aggregate location time-series, where an adversary tries to infer whether a specific user contributed to the aggregates. We find that the volume of contributed data, as well as the regularity and particularity of users' mobility patterns, play a crucial role in the attack's success. We experiment with a wide range of defenses based on generalization, hiding, and perturbation, and evaluate their ability to thwart the attack vis-\u00e0-vis the utility loss they introduce for various mobility analytics tasks. Our results show that some defenses fail across the board, while others work for specific tasks on aggregate location time-series. For instance, suppressing small counts can be used for ranking hotspots, data generalization for forecasting traffic, hotspot discovery, and map inference, while sampling is effective for location labeling and anomaly detection when the dataset is sparse. Differentially private techniques provide reasonable accuracy only in very specific settings, e.g., discovering hotspots and forecasting their traffic, and more so when using weaker privacy notions like crowd-blending privacy. Overall, our measurements show that there does not exist a unique generic defense that can preserve the utility of the analytics for arbitrary applications, and provide useful insights regarding the disclosure of sanitized aggregate location time-series.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[226266600 | Long et al. | 2020 | Citations: 83]": "Membership Inference Attacks (MIAs) aim to determine the presence of a record in a machine learning model's training data by querying the model. Recent work has demonstrated the effectiveness of MIA on various machine learning models and corresponding defenses have been proposed. However, both attacks and defenses have focused on an adversary that indiscriminately attacks all the records without regard to the cost of false positives or negatives. In this work, we revisit membership inference attacks from the perspective of a pragmatic adversary who carefully selects targets and make predictions conservatively. We design a new evaluation methodology that allows us to evaluate the membership privacy risk at the level of individuals and not only in aggregate. We experimentally demonstrate that highly vulnerable records exist even when the aggregate attack precision is close to 50% (baseline). Specifically, on the MNIST dataset, our pragmatic adversary achieves a precision of 95.05% whereas the prior attack only achieves a precision of 51.7%.",
                    "[229156229 | Carlini et al. | 2020 | Citations: 1950]": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                    "[230523638 | Hui et al. | 2021 | Citations: 122]": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
                    "[232372747 | Jere et al. | 2021 | Citations: 166]": "Federated learning is a privacy-by-design framework that enables training deep neural networks from decentralized sources of data, but it is fraught with innumerable attack surfaces. We provide a taxonomy of recent attacks on federated learning systems and detail the need for more robust threat modeling in federated learning environments.",
                    "[236880971 | Shah et al. | 2021 | Citations: 15]": "Recent studies have shown that it may be possible to determine if a machine learning model was trained on a given data sample, using Membership Inference Attacks (MIA). In this paper we evaluate the vulnerability of state-of-the-art speech recognition models to MIA under black-box access. Using models trained with standard methods and public datasets, we demonstrate that without any knowledge of the target model\u2019s parameters or training data a MIA can successfully infer membership with precision and recall more than 60%. Furthermore, for utterances from about 39% of the speakers the precision is more than 75%, indicating that training data membership can be inferred more precisely for some speakers than others. While strong regularization reduces the overall accuracy of MIA to almost 50%, the attacker can still infer membership for utterances from 25% of the speakers with high precision. These results indicate that (1) speaker-level MIA success should be reported, along with overall accuracy, to provide a holistic view of the model\u2019s vulnerability and (2) conventional regularization is an inadequate defense against MIA. We believe that the insights gleaned from this study can direct future work towards more effective defenses.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[916355 | Homer et al. | 2008 | Citations: 1283]": "We use high-density single nucleotide polymorphism (SNP) genotyping microarrays to demonstrate the ability to accurately and robustly determine whether individuals are in a complex genomic DNA mixture. We first develop a theoretical framework for detecting an individual's presence within a mixture, then show, through simulations, the limits associated with our method, and finally demonstrate experimentally the identification of the presence of genomic DNA of specific individuals within a series of highly complex genomic mixtures, including mixtures where an individual contributes less than 0.1% of the total genomic DNA. These findings shift the perceived utility of SNPs for identifying individual trace contributors within a forensics mixture, and suggest future research efforts into assessing the viability of previously sub-optimal DNA sources due to sample contamination. These findings also suggest that composite statistics across cohorts, such as allele frequency or genotype counts, do not mask identity within genome-wide association studies. The implications of these findings are discussed."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 947,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 29
                            },
                            {
                                "start": 30,
                                "end": 95
                            },
                            {
                                "start": 96,
                                "end": 164
                            },
                            {
                                "start": 165,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 483
                            },
                            {
                                "start": 484,
                                "end": 672
                            },
                            {
                                "start": 673,
                                "end": 826
                            },
                            {
                                "start": 827,
                                "end": 947
                            }
                        ],
                        "ref_mentions": [
                            "916355",
                            "10488675",
                            "216553356",
                            "229156229",
                            "214743021",
                            "236880971",
                            "232372747",
                            "221203089",
                            "174799799",
                            "133091488",
                            "195699554",
                            "10488675",
                            "2656445",
                            "230523638",
                            "220831381",
                            "237563320",
                            "226266600"
                        ],
                        "quote": "Membership Inference Attacks. Homer et al. (Homer et al., 2008) first proposed an MI attack on biological data. Shokri et al. (Shokri et al., 2016) proposed the first black-box MI attack against ML. Huge literature followed these works to different scenarios (e.g., location data (Pyrgelis et al., 2019), language models (Carlini et al., 2020), sentence embeddings (Song et al., 2020), speech recognition models (Shah et al., 2021), federated learning (Jere et al., 2021), transfer learning [63], generative models (Chen et al., 2019), white box access (Sablayrolles et al., 2019), (Nasr et al., 2018), (Leino et al., 2019)). Categories of Membership Inference Attacks. There are main three categories 1) Binary classifier-based MI attacks, which utilize the output predictions of shadow models to train a binary classifier to launch the MI attacks (Shokri et al., 2016), [9]. 2) Evaluation metric-based MI attacks, which used the defined evaluation metrics to distinguish members and nonmembers [9], (Yeom et al., 2017), (Hui et al., 2021), (Choquette-Choo et al., 2020), (Li et al., 2020), (Long et al., 2020). 3) Differential Comparisons-based MI attacks (BlindMI-Diff), which mainly utilized the differences between two datasets."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[259878257 | Jiang et al. | 2023 | Citations: 1]",
                "snippets": "Member inference attacks are designed to determine whether candidate data belongs to the training dataset of the target model. Shokri et al. (Shokri et al., 2016) proposed the first membership inference attack method for the black-box ML model. They trained multiple shadow models to simulate the target model and then use multiple attack models for reasoning. Salem et al. later relaxed several key assumptions [18], using multiple shadow models, knowledge of the structure of the target model, and having the same distribution of data sets as the auxiliary knowledge. Using new insights on how overfitting occurs in deep neural networks, Fredrikson (Leino et al., 2019) showed how the model's particular use of features provides evidence of membership for white-box attackers and demonstrates that such attacks are superior to previous black-box approaches. Shafran (Shafran et al., 2021) used a new predictability error that can be calculated for each sample and does not require a training set. The membership error is used to achieve a higher precision of membership inference on many baseline models by subtracting the predictability error from the reconstruction error.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[231925091 | Shafran et al. | 2021 | Citations: 17]": "Membership inference attacks (MIA) try to detect if data samples were used to train a neural network model, e.g. to detect copyright abuses. We show that models with higher dimensional input and output are more vulnerable to MIA, and address in more detail models for image translation and semantic segmentation, including medical image segmentation. We show that reconstruction-errors can lead to very effective MIA attacks as they are indicative of memorization. Unfortunately, reconstruction error alone is less effective at discriminating between non-predictable images used in training and easy to predict images that were never seen before. To overcome this, we propose using a novel predictability error that can be computed for each sample, and its computation does not require a training set. Our membership error, obtained by subtracting the predictability error from the reconstruction error, is shown to achieve high MIA accuracy on an extensive number of benchmarks. 1"
                },
                "metadata": [
                    {
                        "section_title": "Existing Inferential Attack Methods on Federated Learning",
                        "pdf_hash": "",
                        "start": 186,
                        "end": 1310,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "10488675",
                            "195699554",
                            "231925091"
                        ],
                        "quote": "Member inference attacks are designed to determine whether candidate data belongs to the training dataset of the target model. Shokri et al. (Shokri et al., 2016) proposed the first membership inference attack method for the black-box ML model. They trained multiple shadow models to simulate the target model and then use multiple attack models for reasoning. Salem et al. later relaxed several key assumptions [18], using multiple shadow models, knowledge of the structure of the target model, and having the same distribution of data sets as the auxiliary knowledge. Using new insights on how overfitting occurs in deep neural networks, Fredrikson (Leino et al., 2019) showed how the model's particular use of features provides evidence of membership for white-box attackers and demonstrates that such attacks are superior to previous black-box approaches. Shafran (Shafran et al., 2021) used a new predictability error that can be calculated for each sample and does not require a training set. The membership error is used to achieve a higher precision of membership inference on many baseline models by subtracting the predictability error from the reconstruction error."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[263834611 | Shachor et al. | 2023 | Citations: 5]",
                "snippets": "MI attacks attempt to distinguish between members, which were part of a target model's training data, and non-members. MI attacks have been extensively studied in the context of classification models and in the black-box setting, where the model internals are unknown to the attacker. The first MI attacks were either threshold-based (Yeom et al., 2017) or employed binary classifiers trained to distinguish between members and non-members based on model outputs (Shokri et al., 2016). For example, these outputs may include class probabilities or logits (for classification models), the model's loss, and possibly also activations from internal layers of the model (in white-box attacks) (Nasr et al., 2018). To generate labeled (member/non-member) data to train the attack classifier, without knowledge of the true member samples of the attacked model, shadow models are commonly used (Shokri et al., 2016).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 468,
                        "end": 1313,
                        "sentence_offsets": [
                            {
                                "start": 468,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 752
                            },
                            {
                                "start": 753,
                                "end": 921
                            },
                            {
                                "start": 922,
                                "end": 1130
                            },
                            {
                                "start": 1131,
                                "end": 1313
                            }
                        ],
                        "ref_mentions": [
                            "2656445",
                            "10488675",
                            "133091488",
                            "10488675"
                        ],
                        "quote": "MI attacks attempt to distinguish between members, which were part of a target model's training data, and non-members. MI attacks have been extensively studied in the context of classification models and in the black-box setting, where the model internals are unknown to the attacker. The first MI attacks were either threshold-based (Yeom et al., 2017) or employed binary classifiers trained to distinguish between members and non-members based on model outputs (Shokri et al., 2016). For example, these outputs may include class probabilities or logits (for classification models), the model's loss, and possibly also activations from internal layers of the model (in white-box attacks) (Nasr et al., 2018). To generate labeled (member/non-member) data to train the attack classifier, without knowledge of the true member samples of the attacked model, shadow models are commonly used (Shokri et al., 2016)."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[264179931 | Liu et al. | 2024 | Citations: 11]",
                "snippets": "In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model's prediction behavior.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model's prediction behavior.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[268819379 | Alshantti et al. | 2024 | Citations: 4]",
                "snippets": "Membership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting. In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model. Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set. It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019). Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019). Federated learning can however introduce data leakages which add up to the privacy concerns. In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017; Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models. Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications. The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models. Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[199546273 | Hilprecht et al. | 2019 | Citations: 191]": "Abstract We present two information leakage attacks that outperform previous work on membership inference against generative models. The first attack allows membership inference without assumptions on the type of the generative model. Contrary to previous evaluation metrics for generative models, like Kernel Density Estimation, it only considers samples of the model which are close to training data records. The second attack specifically targets Variational Autoencoders, achieving high membership inference accuracy. Furthermore, previous work mostly considers membership inference adversaries who perform single record membership inference. We argue for considering regulatory actors who perform set membership inference to identify the use of specific datasets for training. The attacks are evaluated on two generative model architectures, Generative Adversarial Networks (GANs) and Variational Autoen-coders (VAEs), trained on standard image datasets. Our results show that the two attacks yield success rates superior to previous work on most data sets while at the same time having only very mild assumptions. We envision the two attacks in combination with the membership inference attack type formalization as especially useful. For example, to enforce data privacy standards and automatically assessing model quality in machine learning as a service setups. In practice, our work motivates the use of GANs since they prove less vulnerable against information leakage attacks while producing detailed samples.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[221203089 | Chen et al. | 2019 | Citations: 406]": "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).",
                    "[52211986 | Hayes et al. | 2017 | Citations: 515]": "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
                    "[53099247 | Melis et al. | 2018 | Citations: 1482]": "Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2121,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 147
                            },
                            {
                                "start": 147,
                                "end": 318
                            },
                            {
                                "start": 318,
                                "end": 546
                            },
                            {
                                "start": 546,
                                "end": 729
                            },
                            {
                                "start": 729,
                                "end": 912
                            },
                            {
                                "start": 912,
                                "end": 1004
                            },
                            {
                                "start": 1004,
                                "end": 1337
                            },
                            {
                                "start": 1339,
                                "end": 1446
                            },
                            {
                                "start": 1446,
                                "end": 1715
                            },
                            {
                                "start": 1715,
                                "end": 1910
                            },
                            {
                                "start": 1910,
                                "end": 2111
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "133091488",
                            "53099247",
                            "10488675",
                            "52211986",
                            "199546273",
                            "221203089"
                        ],
                        "quote": "Membership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting. In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model. Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set. It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019). Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019). Federated learning can however introduce data leakages which add up to the privacy concerns. In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017; Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models. Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications. The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models. Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[269293603 | Wan et al. | 2024 | Citations: 3]",
                "snippets": "Membership inference methods can be broadly divided into two categories, i.e., shadow model training and metric-based techniques (Hu et al., 2021). Shadow model training (Shokri et al., 2016) aims to train a binary attack classifier by creating multiple shadow models to mimic the behavior of the target model. Metric-based techniques (Salem et al., 2018)(Song et al., 2020)(Yeom et al., 2017) proposed to infer the membership of a given data record by comparing the metric value (e.g., prediction loss (Yeom et al., 2017)) calculated on the prediction vector of the given record to a preset threshold. Since both techniques are formulated from the attackers' standpoint, they assume the presence of abundant prior information, including knowledge of training data distribution and target model architectures. This assumption renders the direct application of these techniques unfeasible for data owners aiming to implement membership inference. Recently, Hu et al. (Hu et al., 2022) leveraged backdoor techniques to allow the data owner to achieve membership inference effectively without such prior information. Their approach enables individuals to carry out membership inference solely by utilizing black-box query access to the target model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[232233426 | Hu et al. | 2021 | Citations: 440]": "Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature.",
                    "[249605407 | Hu et al. | 2022 | Citations: 31]": "Recently issued data privacy regulations like GDPR (General Data Protection Regulation) grant individuals the right to be forgotten. In the context of machine learning, this requires a model to forget about a training data sample if requested by the data owner (i.e., machine unlearning). As an essential step prior to machine unlearning, it is still a challenge for a data owner to tell whether or not her data have been used by an unauthorized party to train a machine learning model. Membership inference is a recently emerging technique to identify whether a data sample was used to train a target model, and seems to be a promising solution to this challenge. However, straightforward adoption of existing membership inference approaches fails to address the challenge effectively due to being originally designed for attacking membership privacy and suffering from several severe limitations such as low inference accuracy on well-generalized models. In this paper, we propose a novel membership inference approach inspired by the backdoor technology to address the said challenge. Specifically, our approach of Membership Inference via Backdooring (MIB) leverages the key observation that a backdoored model behaves very differently from a clean model when predicting on deliberately marked samples created by a data owner. Appealingly, MIB requires data owners' marking a small number of samples for membership inference and only black-box access to the target model, with theoretical guarantees for inference results. We perform extensive experiments on various datasets and deep neural network architectures, and the results validate the efficacy of our approach, e.g., marking only 0.1% of the training dataset is practically sufficient for effective membership inference.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference",
                        "pdf_hash": "",
                        "start": 171,
                        "end": 1311,
                        "sentence_offsets": [
                            {
                                "start": 171,
                                "end": 305
                            },
                            {
                                "start": 306,
                                "end": 451
                            },
                            {
                                "start": 452,
                                "end": 680
                            },
                            {
                                "start": 681,
                                "end": 887
                            },
                            {
                                "start": 888,
                                "end": 1023
                            },
                            {
                                "start": 1024,
                                "end": 1178
                            },
                            {
                                "start": 1179,
                                "end": 1311
                            }
                        ],
                        "ref_mentions": [
                            "232233426",
                            "10488675",
                            "46933970",
                            "214623088",
                            "2656445",
                            "2656445",
                            "249605407"
                        ],
                        "quote": "Membership inference methods can be broadly divided into two categories, i.e., shadow model training and metric-based techniques (Hu et al., 2021). Shadow model training (Shokri et al., 2016) aims to train a binary attack classifier by creating multiple shadow models to mimic the behavior of the target model. Metric-based techniques (Salem et al., 2018)(Song et al., 2020)(Yeom et al., 2017) proposed to infer the membership of a given data record by comparing the metric value (e.g., prediction loss (Yeom et al., 2017)) calculated on the prediction vector of the given record to a preset threshold. Since both techniques are formulated from the attackers' standpoint, they assume the presence of abundant prior information, including knowledge of training data distribution and target model architectures. This assumption renders the direct application of these techniques unfeasible for data owners aiming to implement membership inference. Recently, Hu et al. (Hu et al., 2022) leveraged backdoor techniques to allow the data owner to achieve membership inference effectively without such prior information. Their approach enables individuals to carry out membership inference solely by utilizing black-box query access to the target model."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[271328769 | Li et al. | 2024 | Citations: 9]",
                "snippets": "Membership inference attacks have been successfully performed in various settings about the adversary's knowledge, including white-box (Leino et al., 2019)(Nasr et al., 2018), black-box (Chen et al., 2020)(Hisamoto et al., 2019)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2020)(Zhang et al., 2021), and label-only [10,(Li et al., 2020) settings...Specifically, Shokri et al. (Shokri et al., 2016) and Salem et al. (Salem et al., 2018) proposed a shadow training technique that employs shadow models to acquire the membership signals...Song et al. (Song et al., 2020) and Yeom et al. (Yeom et al., 2017) proposed the metric-based attack that directly compares losses or other metric values of samples with a predefined threshold...some membership signals obtained in the white-box scenario are incorporated to improve the attack performance (Grosso et al., 2022)(Nasr et al., 2018)...label-only attacks [10,(Li et al., 2020)[60] solely rely on the predicted labels to acquire the membership signals...Liu et al. (Liu et al., 2022) presented Trajec-toryMIA, which utilizes the membership signals generated during the training of the target model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[195699554 | Leino et al. | 2019 | Citations: 272]": "Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model. In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work. Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers---even when the model's black-box behavior appears to generalize well---and demonstrate that this attack outperforms prior black-box methods. Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision. Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-$\\epsilon$-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger $\\epsilon$ that are sometimes used in practice (e.g., $\\epsilon=16$), the attack can achieve nearly the same accuracy as on the unprotected model.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[247595200 | Grosso et al. | 2022 | Citations: 22]": "The use of personal data for training machine learning systems comes with a privacy threat and measuring the level of privacy of a model is one of the major challenges in machine learning today. Identifying training data based on a trained model is a standard way of measuring the privacy risks induced by the model. We develop a novel approach to address the problem of membership inference in pattern recognition models, relying on information provided by adversarial examples. The strategy we propose consists of measuring the magnitude of a perturbation necessary to build an adversarial example. Indeed, we argue that this quantity reflects the likelihood of belonging to the training data. Extensive numerical experiments on multivariate data and an array of state-of-the-art target models show that our method performs comparable or even outperforms state-of-the-art strategies, but without requiring any additional training samples.",
                    "[251953448 | Liu et al. | 2022 | Citations: 111]": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
                    "[119302127 | Hisamoto et al. | 2019 | Citations: 107]": "Data privacy is an important issue for \u201cmachine learning as a service\u201d providers. We focus on the problem of membership inference attacks: Given a data sample and black-box access to a model\u2019s API, determine whether the sample existed in the model\u2019s training data. Our contribution is an investigation of this problem in the context of sequence-to-sequence models, which are important in applications such as machine translation and video captioning. We define the membership inference problem for sequence generation, provide an open dataset based on state-of-the-art machine translation models, and report initial results on whether these models leak private information against several kinds of membership inference attacks.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[218502126 | Chen et al. | 2020 | Citations: 232]": "The right to be forgotten states that a data owner has the right to erase their data from an entity storing it. In the context of machine learning (ML), the right to be forgotten requires an ML model owner to remove the data owner's data from the training set used to build the ML model, a process known asmachine unlearning. While originally designed to protect the privacy of the data owner, we argue that machine unlearning may leave some imprint of the data in the ML model and thus create unintended privacy risks. In this paper, we perform the first study on investigating the unintended information leakage caused by machine unlearning. We propose a novel membership inference attack that leverages the different outputs of an ML model's two versions to infer whether a target sample is part of the training set of the original model but out of the training set of the corresponding unlearned model. Our experiments demonstrate that the proposed membership inference attack achieves strong performance. More importantly, we show that our attack in multiple cases outperforms the classical membership inference attack on the original ML model, which indicates that machine unlearning can have counterproductive effects on privacy. We notice that the privacy degradation is especially significant for well-generalized ML models where classical membership inference does not perform well. We further investigate four mechanisms to mitigate the newly discovered privacy risks and show that releasing the predicted label only, temperature scaling, and differential privacy are effective. We believe that our results can help improve privacy protection in practical implementations of machine unlearning. \\footnoteOur code is available at \\urlhttps://github.com/MinChen00/UnlearningLeaks.",
                    "[237532289 | Zhang et al. | 2021 | Citations: 88]": "Recently, recommender systems have achieved promising performances and become one of the most widely used web applications. However, recommender systems are often trained on highly sensitive user data, thus potential data leakage from recommender systems may lead to severe privacy problems. In this paper, we make the first attempt on quantifying the privacy leakage of recommender systems through the lens of membership inference. In contrast with traditional membership inference against machine learning classifiers, our attack faces two main differences. First, our attack is on the user-level but not on the data sample-level. Second, the adversary can only observe the ordered recommended items from a recommender system instead of prediction results in the form of posterior probabilities. To address the above challenges, we propose a novel method by representing users from relevant items. Moreover, a shadow recommender is established to derive the labeled training data for training the attack model. Extensive experimental results show that our attack framework achieves a strong performance. In addition, we design a defense mechanism to effectively mitigate the membership inference threat of recommender systems.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 234,
                        "end": 439,
                        "sentence_offsets": [
                            {
                                "start": 234,
                                "end": 440
                            }
                        ],
                        "ref_mentions": [
                            "195699554",
                            "133091488",
                            "218502126",
                            "119302127",
                            "46933970",
                            "10488675",
                            "214623088",
                            "237532289",
                            "237563320"
                        ],
                        "quote": "Membership inference attacks have been successfully performed in various settings about the adversary's knowledge, including white-box (Leino et al., 2019)(Nasr et al., 2018), black-box (Chen et al., 2020)(Hisamoto et al., 2019)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2020)(Zhang et al., 2021), and label-only [10,(Li et al., 2020) settings"
                    },
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 573,
                        "end": 725,
                        "sentence_offsets": [
                            {
                                "start": 573,
                                "end": 726
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "46933970"
                        ],
                        "quote": "Specifically, Shokri et al. (Shokri et al., 2016) and Salem et al. (Salem et al., 2018) proposed a shadow training technique that employs shadow models to acquire the membership signals"
                    },
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 736,
                        "end": 898,
                        "sentence_offsets": [
                            {
                                "start": 726,
                                "end": 899
                            }
                        ],
                        "ref_mentions": [
                            "214623088",
                            "2656445"
                        ],
                        "quote": "Song et al. (Song et al., 2020) and Yeom et al. (Yeom et al., 2017) proposed the metric-based attack that directly compares losses or other metric values of samples with a predefined threshold"
                    },
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 912,
                        "end": 1029,
                        "sentence_offsets": [
                            {
                                "start": 899,
                                "end": 1030
                            }
                        ],
                        "ref_mentions": [
                            "247595200",
                            "133091488"
                        ],
                        "quote": "some membership signals obtained in the white-box scenario are incorporated to improve the attack performance (Grosso et al., 2022)(Nasr et al., 2018)"
                    },
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 1039,
                        "end": 1138,
                        "sentence_offsets": [
                            {
                                "start": 1030,
                                "end": 1139
                            }
                        ],
                        "ref_mentions": [
                            "237563320"
                        ],
                        "quote": "label-only attacks [10,(Li et al., 2020)[60] solely rely on the predicted labels to acquire the membership signals"
                    },
                    {
                        "section_title": "Membership Inference Attacks",
                        "pdf_hash": "",
                        "start": 1473,
                        "end": 1603,
                        "sentence_offsets": [
                            {
                                "start": 1463,
                                "end": 1603
                            }
                        ],
                        "ref_mentions": [
                            "251953448"
                        ],
                        "quote": "Liu et al. (Liu et al., 2022) presented Trajec-toryMIA, which utilizes the membership signals generated during the training of the target model."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[272423578 | Wen et al. | 2024 | Citations: 2]",
                "snippets": "Membership Inference Attacks (MIA) [20](Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016)(Wen et al., 2024)(Zhang et al., 2023) have emerged as a significant threat to privacy in the context of machine learning models. These attacks aim to reveal the membership status of a target sample, i.e., whether the sam-ple was part of the training dataset or not, thereby directly breaching privacy. \n\nThe seminal work by Shokri et al. (Shokri et al., 2016) introduced MIA against machine learning models, wherein multiple shadow models were trained to mimic the behavior of the target model. This attack originally required access to data from the same distribution as the training dataset. However, Salem et al. (Salem et al., 2018) relaxed this assumption by demonstrating the effectiveness of using only a single shadow model, substantially reducing the computational cost involved. \n\nSubsequent research (Choquette-Choo et al., 2020)(Li et al., 2020) has explored more challenging settings for MIA. In these scenarios, the adversary only has access to hard-label predictions from the target model. Li and Zhang (Li et al., 2020) proposed a method that approximates the distance between the target sample and its decision boundary using adversarial examples, enabling the attacker to make decisions based on this distance. \n\nRecent advancements in MIA have focused on enhancing attack performance. Carlini et al. (Carlini et al., 2021) leveraged the discrepancy between models trained with and without the target sample to improve attack effectiveness. Liu et al. (Liu et al., 2022) demonstrated the utility of loss trajectory analysis in MIA. Furthermore, Tram\u00e8r et al. (Tram\u00e8r et al., 2022) highlighted the potential of data poisoning, showing that even with access to a small fraction of the training dataset, the attacker can significantly boost the performance of membership inference attacks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[237563320 | Li et al. | 2020 | Citations: 246]": "Machine learning (ML) has been widely adopted in various privacy-critical applications, e.g., face recognition and medical image analysis. However, recent research has shown that ML models are vulnerable to attacks against their training data. Membership inference is one major attack in this domain: Given a data sample and model, an adversary aims to determine whether the sample is part of the model's training set. Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack. Empirical evaluation shows that our decision-based attacks can achieve remarkable performance, and even outperform the previous score-based attacks in some cases. We further present new insights on the success of membership inference based on quantitative and qualitative analysis, i.e., member samples of a model are more distant to the model's decision boundary than non-member samples. Finally, we evaluate multiple defense mechanisms against our decision-based attacks and show that our two types of attacks can bypass most of these defenses.",
                    "[251953448 | Liu et al. | 2022 | Citations: 111]": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
                    "[220831381 | Choquette-Choo et al. | 2020 | Citations: 516]": "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.",
                    "[244920593 | Carlini et al. | 2021 | Citations: 708]": "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \u201caccuracy\u201d metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics.",
                    "[247922814 | Tramer et al. | 2022 | Citations: 123]": "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data. Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8\u00d7 more precise inference on all other users' otherwise-private data points. Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.",
                    "[264820351 | Zhang et al. | 2023 | Citations: 18]": "Generative models have demonstrated revolutionary success in various visual creation tasks, but in the meantime, they have been exposed to the threat of leaking private information of their training data. Several membership inference attacks (MIAs) have been proposed to exhibit the privacy vulnerability of generative models by classifying a query image as a training dataset member or nonmember. However, these attacks suffer from major limitations, such as requiring shadow models and white-box access, and either ignoring or only focusing on the unique property of diffusion models, which block their generalization to multiple generative models. In contrast, we propose the first generalized membership inference attack against a variety of generative models such as generative adversarial networks, [variational] autoencoders, implicit functions, and the emerging diffusion models. We leverage only generated distributions from target generators and auxiliary nonmember datasets, therefore regarding target generators as black boxes and agnostic to their architectures or application scenarios. Experiments validate that all the generative models are vulnerable to our attack. For instance, our work achieves attack AUC > 0.99 against DDPM, DDIM, and FastDPM trained on CIFAR-10 and CelebA. And the attack against VQGAN, LDM (for the text-conditional generation), and LIIF achieves AUC > 0.90. As a result, we appeal to our community to be aware of such privacy leakage risks when designing and publishing generative models.1",
                    "[272367776 | Wen et al. | 2024 | Citations: 14]": "Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "K.1 Membership Inference Attack",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1686,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 145
                            },
                            {
                                "start": 146,
                                "end": 318
                            },
                            {
                                "start": 321,
                                "end": 494
                            },
                            {
                                "start": 495,
                                "end": 593
                            },
                            {
                                "start": 594,
                                "end": 772
                            },
                            {
                                "start": 775,
                                "end": 850
                            },
                            {
                                "start": 851,
                                "end": 949
                            },
                            {
                                "start": 950,
                                "end": 1160
                            },
                            {
                                "start": 1163,
                                "end": 1235
                            },
                            {
                                "start": 1236,
                                "end": 1371
                            },
                            {
                                "start": 1372,
                                "end": 1448
                            },
                            {
                                "start": 1449,
                                "end": 1686
                            }
                        ],
                        "ref_mentions": [
                            "237563320",
                            "46933970",
                            "10488675",
                            "272367776",
                            "264820351",
                            "10488675",
                            "46933970",
                            "220831381",
                            "237563320",
                            "237563320",
                            "244920593",
                            "251953448",
                            "247922814"
                        ],
                        "quote": "Membership Inference Attacks (MIA) [20](Li et al., 2020)(Salem et al., 2018)(Shokri et al., 2016)(Wen et al., 2024)(Zhang et al., 2023) have emerged as a significant threat to privacy in the context of machine learning models. These attacks aim to reveal the membership status of a target sample, i.e., whether the sam-ple was part of the training dataset or not, thereby directly breaching privacy. \n\nThe seminal work by Shokri et al. (Shokri et al., 2016) introduced MIA against machine learning models, wherein multiple shadow models were trained to mimic the behavior of the target model. This attack originally required access to data from the same distribution as the training dataset. However, Salem et al. (Salem et al., 2018) relaxed this assumption by demonstrating the effectiveness of using only a single shadow model, substantially reducing the computational cost involved. \n\nSubsequent research (Choquette-Choo et al., 2020)(Li et al., 2020) has explored more challenging settings for MIA. In these scenarios, the adversary only has access to hard-label predictions from the target model. Li and Zhang (Li et al., 2020) proposed a method that approximates the distance between the target sample and its decision boundary using adversarial examples, enabling the attacker to make decisions based on this distance. \n\nRecent advancements in MIA have focused on enhancing attack performance. Carlini et al. (Carlini et al., 2021) leveraged the discrepancy between models trained with and without the target sample to improve attack effectiveness. Liu et al. (Liu et al., 2022) demonstrated the utility of loss trajectory analysis in MIA. Furthermore, Tram\u00e8r et al. (Tram\u00e8r et al., 2022) highlighted the potential of data poisoning, showing that even with access to a small fraction of the training dataset, the attacker can significantly boost the performance of membership inference attacks."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[274281228 | Chen et al. | 2024 | Citations: 0]",
                "snippets": "Membership inference attack aims to infer whether a specific sample (, ) belongs to the training data set   of the target model. According to the attacker's capabilities, MIAs can be roughly divided into two categories: white-box attacks and black-box attacks: \n\nBlack-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs (Hui et al., 2021)(Shokri et al., 2020)(Shokri et al., 2016). There are generally two strategies in black-box settings: model-based attacks and metric-based attacks. \n\n\u2022 Model-based Attacks: Shokri et al. (Shokri et al., 2016) introduced the first MIA against machine learning models, where the attacker has black-box access to the target model. The attacker builds multiple shadow models to mimic the target model, constructs a dataset of membership labels, and trains a binary classifier to predict membership status. However, this method requires extensive resources to train multiple shadow models with the same architecture as the target model, necessitating access to a shadow dataset with a distribution similar to that of the target's training set. To mitigate these issues, Salem et al. (Salem et al., 2018) proposed using a single shadow model, allowing for effective attacks while relaxing some adversarial assumptions. \u2022 Metric-based Attacks: Song et al. (Song et al., 2020) developed a metricbased attack where the attacker compares a calculated metric  (such as entropy) to a predefined threshold to infer membership. This method, however, is limited when the target model only provides predicted labels without prediction vectors. In contrast, our approach effectively utilizes predicted labels for membership inference. Bertran et al. (Bertr\u00e1n et al., 2023) introduced a novel method that distinguishes between members and non-members using quantiles, eliminating the need for architectural knowledge and representing a true \"black box\" approach. Liu et al. (Liu et al., 2022) leverage the training process of the target model in their MIA, called TrajectoryMIA, utilizing knowledge distillation to extract membership information from loss records at various training epochs. These methods, while innovative, typically involve complex training processes and significant costs, particularly with knowledge distillation. \n\nWhite-box Membership Inference. In white-box settings, attackers gain access to model parameters  * and potentially intermediate training information, such as gradients \u2112  [16]23].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[214623088 | Song et al. | 2020 | Citations: 375]": "Machine learning models are prone to memorizing sensitive data, making them vulnerable to membership inference attacks in which an adversary aims to guess if an input sample was used to train the model. In this paper, we show that prior work on membership inference attacks may severely underestimate the privacy risks by relying solely on training custom neural network classifiers to perform attacks and focusing only on the aggregate results over data samples, such as the attack accuracy. To overcome these limitations, we first propose to benchmark membership inference privacy risks by improving existing non-neural network based inference attacks and proposing a new inference attack method based on a modification of prediction entropy. We also propose benchmarks for defense mechanisms by accounting for adaptive adversaries with knowledge of the defense and also accounting for the trade-off between model accuracy and privacy risks. Using our benchmark attacks, we demonstrate that existing defense approaches are not as effective as previously reported. \nNext, we introduce a new approach for fine-grained privacy analysis by formulating and deriving a new metric called the privacy risk score. Our privacy risk score metric measures an individual sample's likelihood of being a training member, which allows an adversary to perform membership inference attacks with high confidence. We experimentally validate the effectiveness of the privacy risk score metric and demonstrate that the distribution of the privacy risk score across individual samples is heterogeneous. Finally, we perform an in-depth investigation for understanding why certain samples have high privacy risk scores, including correlations with model sensitivity, generalization error, and feature embeddings. Our work emphasizes the importance of a systematic and rigorous evaluation of privacy risks of machine learning models.",
                    "[251953448 | Liu et al. | 2022 | Citations: 111]": "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios.",
                    "[259375769 | Bertran et al. | 2023 | Citations: 50]": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models} -- i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures.",
                    "[230523638 | Hui et al. | 2021 | Citations: 122]": "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK 6.1 Membership Inference",
                        "pdf_hash": "",
                        "start": 26,
                        "end": 2397,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 286
                            },
                            {
                                "start": 289,
                                "end": 320
                            },
                            {
                                "start": 321,
                                "end": 422
                            },
                            {
                                "start": 423,
                                "end": 526
                            },
                            {
                                "start": 529,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 863
                            },
                            {
                                "start": 864,
                                "end": 1100
                            },
                            {
                                "start": 1101,
                                "end": 1258
                            },
                            {
                                "start": 1259,
                                "end": 1444
                            },
                            {
                                "start": 1445,
                                "end": 1558
                            },
                            {
                                "start": 1559,
                                "end": 1648
                            },
                            {
                                "start": 1649,
                                "end": 1856
                            },
                            {
                                "start": 1857,
                                "end": 2071
                            },
                            {
                                "start": 2072,
                                "end": 2214
                            },
                            {
                                "start": 2217,
                                "end": 2248
                            },
                            {
                                "start": 2249,
                                "end": 2397
                            }
                        ],
                        "ref_mentions": [
                            "230523638",
                            "218514464",
                            "10488675",
                            "10488675",
                            "46933970",
                            "214623088",
                            "259375769",
                            "251953448"
                        ],
                        "quote": "Membership inference attack aims to infer whether a specific sample (, ) belongs to the training data set   of the target model. According to the attacker's capabilities, MIAs can be roughly divided into two categories: white-box attacks and black-box attacks: \n\nBlack-box Membership Inference. In this case, the attacker distinguishes members and non-members only using model outputs (Hui et al., 2021)(Shokri et al., 2020)(Shokri et al., 2016). There are generally two strategies in black-box settings: model-based attacks and metric-based attacks. \n\n\u2022 Model-based Attacks: Shokri et al. (Shokri et al., 2016) introduced the first MIA against machine learning models, where the attacker has black-box access to the target model. The attacker builds multiple shadow models to mimic the target model, constructs a dataset of membership labels, and trains a binary classifier to predict membership status. However, this method requires extensive resources to train multiple shadow models with the same architecture as the target model, necessitating access to a shadow dataset with a distribution similar to that of the target's training set. To mitigate these issues, Salem et al. (Salem et al., 2018) proposed using a single shadow model, allowing for effective attacks while relaxing some adversarial assumptions. \u2022 Metric-based Attacks: Song et al. (Song et al., 2020) developed a metricbased attack where the attacker compares a calculated metric  (such as entropy) to a predefined threshold to infer membership. This method, however, is limited when the target model only provides predicted labels without prediction vectors. In contrast, our approach effectively utilizes predicted labels for membership inference. Bertran et al. (Bertr\u00e1n et al., 2023) introduced a novel method that distinguishes between members and non-members using quantiles, eliminating the need for architectural knowledge and representing a true \"black box\" approach. Liu et al. (Liu et al., 2022) leverage the training process of the target model in their MIA, called TrajectoryMIA, utilizing knowledge distillation to extract membership information from loss records at various training epochs. These methods, while innovative, typically involve complex training processes and significant costs, particularly with knowledge distillation. \n\nWhite-box Membership Inference. In white-box settings, attackers gain access to model parameters  * and potentially intermediate training information, such as gradients \u2112  [16]23]."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[50778569 | Truex et al. | 2018 | Citations: 112]",
                "snippets": "Most existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28](Shokri et al., 2016). However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning...we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 495,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 202
                            },
                            {
                                "start": 203,
                                "end": 496
                            }
                        ],
                        "ref_mentions": [
                            "10488675"
                        ],
                        "quote": "Most existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28](Shokri et al., 2016). However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning"
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1113,
                        "end": 1323,
                        "sentence_offsets": [
                            {
                                "start": 1098,
                                "end": 1322
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[53172224 | Song et al. | 2018 | Citations: 18]",
                "snippets": "Membership inference against ML models was studied in [13], [22], (Rahman et al., 2018), (Shokri et al., 2016). \n\nShokri et al. (Shokri et al., 2016) developed membership inference techniques against black-box models, exploiting the differences in the models' outputs on training and non-training inputs. Their attack performs best when target model is overfitted to the training data. Truex et al. [37] extend and generalize this work, including white-box and federated-learning setting.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "VII. RELATED WORK",
                        "pdf_hash": "",
                        "start": 166,
                        "end": 603,
                        "sentence_offsets": [
                            {
                                "start": 166,
                                "end": 243
                            },
                            {
                                "start": 246,
                                "end": 419
                            },
                            {
                                "start": 420,
                                "end": 500
                            },
                            {
                                "start": 501,
                                "end": 603
                            }
                        ],
                        "ref_mentions": [
                            "13699042",
                            "10488675",
                            "10488675"
                        ],
                        "quote": "Membership inference against ML models was studied in [13], [22], (Rahman et al., 2018), (Shokri et al., 2016). \n\nShokri et al. (Shokri et al., 2016) developed membership inference techniques against black-box models, exploiting the differences in the models' outputs on training and non-training inputs. Their attack performs best when target model is overfitted to the training data. Truex et al. [37] extend and generalize this work, including white-box and federated-learning setting."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[67855651 | Jayaraman et al. | 2019 | Citations: 7]",
                "snippets": "Membership inference attacks can either be completely black-box where an attacker only has query access to the target model (Shokri et al., 2016), or can assume that the attacker has full white-box access to the target model, along with some auxillary information (Yeom et al., 2017). The first membership inference attack on machine learning was proposed by Shokri et al. (Shokri et al., 2016). They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                },
                "metadata": [
                    {
                        "section_title": "Membership Inference",
                        "pdf_hash": "",
                        "start": 472,
                        "end": 2419,
                        "sentence_offsets": [
                            {
                                "start": 472,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 818
                            },
                            {
                                "start": 819,
                                "end": 945
                            },
                            {
                                "start": 946,
                                "end": 1067
                            },
                            {
                                "start": 1068,
                                "end": 1295
                            },
                            {
                                "start": 1296,
                                "end": 1437
                            },
                            {
                                "start": 1438,
                                "end": 1632
                            },
                            {
                                "start": 1633,
                                "end": 1776
                            },
                            {
                                "start": 1777,
                                "end": 1955
                            },
                            {
                                "start": 1956,
                                "end": 2058
                            },
                            {
                                "start": 2061,
                                "end": 2198
                            },
                            {
                                "start": 2199,
                                "end": 2311
                            },
                            {
                                "start": 2312,
                                "end": 2419
                            }
                        ],
                        "ref_mentions": [
                            "10488675",
                            "2656445",
                            "10488675"
                        ],
                        "quote": "Membership inference attacks can either be completely black-box where an attacker only has query access to the target model (Shokri et al., 2016), or can assume that the attacker has full white-box access to the target model, along with some auxillary information (Yeom et al., 2017). The first membership inference attack on machine learning was proposed by Shokri et al. (Shokri et al., 2016). They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62]."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[91184074 | Salem et al. | 2019 | Citations: 257]",
                "snippets": "Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data (Hagestedt et al., 2019)[21] and location data (Pyrgelis et al., 2017)[37]. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28](Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)42,(Yeom et al., 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[10488675 | Shokri et al. | 2016 | Citations: 4160]": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
                    "[133091488 | Nasr et al. | 2018 | Citations: 1452]": "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.",
                    "[2656445 | Yeom et al. | 2017 | Citations: 1133]": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
                    "[3570809 | Pyrgelis et al. | 2017 | Citations: 271]": "Aggregate location data is often used to support smart services and applications, e.g., generating live traffic maps or predicting visits to businesses. In this paper, we present the first study on the feasibility of membership inference attacks on aggregate location time-series. We introduce a game-based definition of the adversarial task, and cast it as a classification problem where machine learning can be used to distinguish whether or not a target user is part of the aggregates. \nWe empirically evaluate the power of these attacks on both raw and differentially private aggregates using two mobility datasets. We find that membership inference is a serious privacy threat, and show how its effectiveness depends on the adversary's prior knowledge, the characteristics of the underlying location data, as well as the number of users and the timeframe on which aggregation is performed. Although differentially private mechanisms can indeed reduce the extent of the attacks, they also yield a significant loss in utility. Moreover, a strategic adversary mimicking the behavior of the defense mechanism can greatly limit the protection they provide. Overall, our work presents a novel methodology geared to evaluate membership inference on aggregate location data in real-world settings and can be used by providers to assess the quality of privacy protection before data release or by regulators to detect violations.",
                    "[46933970 | Salem et al. | 2018 | Citations: 950]": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.",
                    "[49863840 | Nasr et al. | 2018 | Citations: 474]": "Machine learning models leak significant amount of information about their training sets, through their predictions. This is a serious privacy concern for the users of machine learning as a service. To address this concern, in this paper, we focus on mitigating the risks of black-box inference attacks against machine learning models. We introduce a mechanism to train models with membership privacy, which ensures indistinguishability between the predictions of a model on its training data and other data points (from the same distribution). This requires minimizing the accuracy of the best black-box membership inference attack against the model. We formalize this as a min-max game, and design an adversarial training algorithm that minimizes the prediction loss of the model as well as the maximum gain of the inference attacks. This strategy, which can guarantee membership privacy (as prediction indistinguishability), acts also as a strong regularizer and helps generalizing the model. We evaluate the practical feasibility of our privacy mechanism on training deep neural networks using benchmark datasets. We show that the min-max strategy can mitigate the risks of membership inference attacks (near random guess), and can achieve this with a negligible drop in the model's prediction accuracy (less than 4%).",
                    "[52205573 | Hagestedt et al. | 2019 | Citations: 42]": "The advancement of molecular profiling techniques \nfuels biomedical research with a deluge of data. To facilitate \ndata sharing, the Global Alliance for Genomics and Health \nestablished the Beacon system, a search engine designed to help \nresearchers find datasets of interest. While the current Beacon \nsystem only supports genomic data, other types of biomedical \ndata, such as DNA methylation, are also essential for advancing \nour understanding in the field. In this paper, we propose the first \nBeacon system for DNA methylation data sharing: MBeacon. As \nthe current genomic Beacon is vulnerable to privacy attacks, such \nas membership inference, and DNA methylation data is highly \nsensitive, we take a privacy-by-design approach to construct \nMBeacon. \nFirst, we demonstrate the privacy threat, by proposing a \nmembership inference attack tailored specifically to unprotected \nmethylation Beacons. Our experimental results show that 100 \nqueries are sufficient to achieve a successful attack with AUC \n(area under the ROC curve) above 0.9. To remedy this situation, \nwe propose a novel differential privacy mechanism, namely SVT2 \n, \nwhich is the core component of MBeacon. Extensive experiments \nover multiple datasets show that SVT2 \ncan successfully mitigate \nmembership privacy risks without significantly harming utility. \nWe further implement a fully functional prototype of MBeacon \nwhich we make available to the research community"
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 22,
                        "end": 780,
                        "sentence_offsets": [
                            {
                                "start": 22,
                                "end": 105
                            },
                            {
                                "start": 106,
                                "end": 220
                            },
                            {
                                "start": 221,
                                "end": 318
                            },
                            {
                                "start": 319,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 780
                            }
                        ],
                        "ref_mentions": [
                            "52205573",
                            "3570809",
                            "10488675",
                            "49863840",
                            "133091488",
                            "46933970",
                            "2656445"
                        ],
                        "quote": "Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data (Hagestedt et al., 2019)[21] and location data (Pyrgelis et al., 2017)[37]. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28](Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)42,(Yeom et al., 2017)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.049071,
        "cot": "In analyzing the query about membership inference attacks (MIAs) on machine learning models and the differences between black-box and white-box approaches, I need to create a clear structure that covers the main methodologies and distinguishes between these two approaches.\n\nBased on the quotes, I'll organize the information into the following dimensions:\n\n1. \"Introduction to Membership Inference Attacks\" - This will provide background information about what MIAs are, their purpose, and why they're considered privacy threats. This is necessary to establish a foundation before diving into the methodologies.\n\n2. \"Black-Box Methodologies\" - This will cover approaches used when attackers only have access to the model outputs but not the internal structure. Several quotes discuss shadow model techniques, metric-based approaches, and other black-box strategies.\n\n3. \"White-Box Methodologies\" - This will address the approaches used when attackers have access to the model's internal structure, parameters, or gradients. Several quotes mention specific white-box techniques that leverage this additional information.\n\n4. \"Comparing Black-Box and White-Box Approaches\" - This will directly address the second part of the query by highlighting the key differences, advantages, and limitations of each approach.\n\n5. \"Recent Developments and Variants\" - This will cover newer approaches and variants of MIAs that have been developed, including label-only attacks, trajectory-based approaches, and others mentioned in the quotes.\n\nFor each dimension, I'll choose the most appropriate format:\n- Introduction, White-Box vs. Black-Box Comparison, and Recent Developments will use \"synthesis\" format since they require integrating information and explaining concepts coherently.\n- Black-Box Methodologies and White-Box Methodologies will use \"list\" format to clearly enumerate and explain different techniques in these categories.\n\nThis organization will ensure all relevant information from the quotes is included while directly addressing the user's query about the main methodologies and differences between approaches.",
        "plan": {
            "Introduction to Membership Inference Attacks (synthesis)": [
                0,
                1,
                3,
                5,
                8,
                10,
                11,
                13,
                23,
                31,
                37,
                39
            ],
            "Black-Box Methodologies (list)": [
                0,
                1,
                2,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                12,
                13,
                14,
                15,
                18,
                20,
                21,
                22,
                24,
                25,
                26,
                27,
                28,
                30,
                33,
                34,
                36,
                40
            ],
            "White-Box Methodologies (list)": [
                2,
                4,
                6,
                8,
                10,
                11,
                13,
                15,
                16,
                17,
                18,
                19,
                21,
                22,
                23,
                24,
                25,
                28,
                29,
                32,
                34,
                35,
                36,
                38
            ],
            "Comparing Black-Box and White-Box Approaches (synthesis)": [
                2,
                11,
                13,
                17,
                18,
                19,
                25,
                27,
                28,
                32,
                34,
                38
            ],
            "Recent Developments and Variants (synthesis)": [
                14,
                15,
                20,
                22,
                23,
                27,
                28,
                32,
                34,
                35,
                39
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Membership Inference Attacks",
                "tldr": "Membership inference attacks aim to determine whether a specific data record was used to train a machine learning model, posing significant privacy risks. These attacks exploit differences in model behavior when processing seen versus unseen data, with approaches ranging from shadow model training to confidence score analysis. (15 sources)",
                "text": "\nMembership inference attacks (MIAs) represent a significant privacy concern in machine learning, as they allow adversaries to determine whether a specific data record was part of a model's training dataset <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper>. These attacks exploit a fundamental observation: machine learning models often behave differently on data they were trained on versus data they encounter for the first time, with overfitting being a common but not the only reason for this difference <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>.\n\nThe core objective of a membership inference attack is to recognize distinctions in a target model's behavior that can differentiate between members and non-members of the training dataset <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. This type of attack can be conducted in either black-box or white-box settings. In the black-box setting, the adversary only has query access to the model through a prediction API, while in the white-box setting, the attacker has full access to the model's internal structure <Paper corpusId=\"227227868\" paperTitle=\"(Hidano et al., 2020)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper> <Paper corpusId=\"67855651\" paperTitle=\"(Jayaraman et al., 2019)\" isShortName></Paper>.\n\nThe most established approach to membership inference, introduced by Shokri et al., involves training multiple \"shadow models\" designed to mimic the target model's behavior <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. These shadow models are used to generate labeled examples (member vs. non-member), which are then used to train an \"attack model\" - a classifier that can distinguish between the predictions made on training data versus predictions on unseen data <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"231861713\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>. Later research by Salem et al. demonstrated that even a single shadow model can be sufficient for an effective attack <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper> <Paper corpusId=\"236956533\" paperTitle=\"(Bagmar et al., 2021)\" isShortName></Paper>.\n\nAlternative approaches include confidence-thresholding methods, where the prediction confidence of the target model is compared to a threshold - with higher confidence potentially indicating membership <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper>. More recent work has explored label-only membership inference attacks that don't require confidence scores, instead relying on the robustness of model predictions under perturbations <Paper corpusId=\"251765197\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\nThe vulnerability to membership inference attacks is often linked to overfitting, as models that memorize their training data rather than learning generalizable patterns create distinguishable differences in their behavior on seen versus unseen examples <Paper corpusId=\"199546273\" paperTitle=\"(Hilprecht et al., 2019)\" isShortName></Paper>. However, research has shown that even well-generalized models can be susceptible to these attacks <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>.\n\nThe potential privacy risks posed by membership inference attacks are particularly concerning in domains involving sensitive data, such as healthcare analytics, where revealing that an individual's data was used to train a model could expose private information <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"202888587\" paperTitle=\"(Aivodji et al., 2019)\" isShortName></Paper>. Recent advances have even demonstrated the possibility of performing membership inference attacks with only black-box access to target models, further highlighting the breadth of this privacy concern <Paper corpusId=\"264179931\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"50778569\" paperTitle=\"(Truex et al., 2018)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Shokri et al., 2016)",
                        "snippets": [
                            "Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output.\n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class.\n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members."
                        ],
                        "paper": {
                            "corpus_id": 10488675,
                            "title": "Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "34828439",
                                    "name": "M. Stronati"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 4160
                        },
                        "score": 0.96923828125
                    },
                    {
                        "id": "(Yeom et al., 2017)",
                        "snippets": [
                            "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                        ],
                        "paper": {
                            "corpus_id": 2656445,
                            "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                            "authors": [
                                {
                                    "authorId": "26378728",
                                    "name": "Samuel Yeom"
                                },
                                {
                                    "authorId": "3025831",
                                    "name": "Irene Giacomelli"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                },
                                {
                                    "authorId": "1680133",
                                    "name": "S. Jha"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE Computer Security Foundations Symposium",
                            "n_citations": 1133
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hidano et al., 2020)",
                        "snippets": [
                            "We deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train.\n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f. To build an adversary A against f, we attempt to learn some statistical relevance between the distribution f(x) of confidence values and the membership (x, y) \u2208 D train.\n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train. An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f(x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network)."
                        ],
                        "paper": {
                            "corpus_id": 227227868,
                            "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training",
                            "authors": [
                                {
                                    "authorId": "3222644",
                                    "name": "Seira Hidano"
                                },
                                {
                                    "authorId": "49224245",
                                    "name": "Yusuke Kawamoto"
                                },
                                {
                                    "authorId": "2116295",
                                    "name": "Takao Murakami"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE International Joint Conference on Neural Network",
                            "n_citations": 13
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Nasr et al., 2018)",
                        "snippets": [
                            "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                        ],
                        "paper": {
                            "corpus_id": 133091488,
                            "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jayaraman et al., 2019)",
                        "snippets": [
                            "Membership inference attacks can either be completely black-box where an attacker only has query access to the target model (Shokri et al., 2016), or can assume that the attacker has full white-box access to the target model, along with some auxillary information (Yeom et al., 2017). The first membership inference attack on machine learning was proposed by Shokri et al. (Shokri et al., 2016). They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62]."
                        ],
                        "paper": {
                            "corpus_id": 67855651,
                            "title": "When Relaxations Go Bad: \"Differentially-Private\" Machine Learning",
                            "authors": [
                                {
                                    "authorId": "2348109",
                                    "name": "Bargav Jayaraman"
                                },
                                {
                                    "authorId": "145685504",
                                    "name": "David Evans"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Song et al., 2019)",
                        "snippets": [
                            "Membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training."
                        ],
                        "paper": {
                            "corpus_id": 165163934,
                            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
                            "authors": [
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 244
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(He et al., 2021)",
                        "snippets": [
                            "Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API (Carlini et al., 2018)9,19,28,(Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)",
                            "Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack",
                            "Nasr et al. (Nasr et al., 2018) conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings."
                        ],
                        "paper": {
                            "corpus_id": 231861713,
                            "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "2054749404",
                                    "name": "Rui Wen"
                                },
                                {
                                    "authorId": "2127727861",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "2117688523",
                                    "name": "Yun Shen"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 98
                        },
                        "score": 0.9794921875
                    },
                    {
                        "id": "(Salem et al., 2018)",
                        "snippets": [
                            "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                        ],
                        "paper": {
                            "corpus_id": 46933970,
                            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "66697271",
                                    "name": "A. Salem"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "144887171",
                                    "name": "Mathias Humbert"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                }
                            ],
                            "year": 2018,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bagmar et al., 2021)",
                        "snippets": [
                            "Membership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021).\n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too.\n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches."
                        ],
                        "paper": {
                            "corpus_id": 236956533,
                            "title": "Membership Inference Attacks on Lottery Ticket Networks",
                            "authors": [
                                {
                                    "authorId": "1693718757",
                                    "name": "Aadesh Bagmar"
                                },
                                {
                                    "authorId": "51469126",
                                    "name": "Shishira R. Maiya"
                                },
                                {
                                    "authorId": "2122929492",
                                    "name": "Shruti Bidwalka"
                                },
                                {
                                    "authorId": "144520191",
                                    "name": "A. Deshpande"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.96728515625
                    },
                    {
                        "id": "(Li et al., 2022)",
                        "snippets": [
                            "Membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29](Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017). Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. (Salem et al., 2018) propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. (Shokri et al., 2016). Later, Nasr et al. (Nasr et al., 2018) focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. (Song et al., 2019) study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang (Li et al., 2020) and Choquette-Choo et al. (Choquette-Choo et al., 2020) propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa."
                        ],
                        "paper": {
                            "corpus_id": 251765197,
                            "title": "Auditing Membership Leakages of Multi-Exit Networks",
                            "authors": [
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2182511319",
                                    "name": "Yiyong Liu"
                                },
                                {
                                    "authorId": "2116553732",
                                    "name": "Xinlei He"
                                },
                                {
                                    "authorId": "145648201",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "1698138",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 34
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Choquette-Choo et al., 2020)",
                        "snippets": [
                            "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees."
                        ],
                        "paper": {
                            "corpus_id": 220831381,
                            "title": "Label-Only Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "1415982317",
                                    "name": "Christopher A. Choquette-Choo"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "1967156",
                                    "name": "Nicolas Papernot"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 516
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hilprecht et al., 2019)",
                        "snippets": [
                            "The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model."
                        ],
                        "paper": {
                            "corpus_id": 199546273,
                            "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models",
                            "authors": [
                                {
                                    "authorId": "81786870",
                                    "name": "Benjamin Hilprecht"
                                },
                                {
                                    "authorId": "2736329",
                                    "name": "Martin H\u00e4rterich"
                                },
                                {
                                    "authorId": "13047311",
                                    "name": "Daniel Bernau"
                                }
                            ],
                            "year": 2019,
                            "venue": "Proceedings on Privacy Enhancing Technologies",
                            "n_citations": 191
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Aivodji et al., 2019)",
                        "snippets": [
                            "Membership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov (Shokri et al., 2016). Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov (Melis et al., 2018) in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets."
                        ],
                        "paper": {
                            "corpus_id": 202888587,
                            "title": "GAMIN: An Adversarial Approach to Black-Box Model Inversion",
                            "authors": [
                                {
                                    "authorId": "40907220",
                                    "name": "U. A\u00efvodji"
                                },
                                {
                                    "authorId": "1777382",
                                    "name": "S. Gambs"
                                },
                                {
                                    "authorId": "1388769197",
                                    "name": "Timon Ther"
                                }
                            ],
                            "year": 2019,
                            "venue": "arXiv.org",
                            "n_citations": 42
                        },
                        "score": 0.96875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model's prediction behavior."
                        ],
                        "paper": {
                            "corpus_id": 264179931,
                            "title": "Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2019143562",
                                    "name": "Gaoyang Liu"
                                },
                                {
                                    "authorId": "2260601109",
                                    "name": "Tianlong Xu"
                                },
                                {
                                    "authorId": "2118403448",
                                    "name": "Rui Zhang"
                                },
                                {
                                    "authorId": "2259065760",
                                    "name": "Zixiong Wang"
                                },
                                {
                                    "authorId": "40614774",
                                    "name": "Chen Wang"
                                },
                                {
                                    "authorId": "2213648984",
                                    "name": "Ling Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Transactions on Information Forensics and Security",
                            "n_citations": 11
                        },
                        "score": 0.96728515625
                    },
                    {
                        "id": "(Truex et al., 2018)",
                        "snippets": [
                            "Most existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28](Shokri et al., 2016). However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning",
                            "we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model."
                        ],
                        "paper": {
                            "corpus_id": 50778569,
                            "title": "Towards Demystifying Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "25121568",
                                    "name": "Stacey Truex"
                                },
                                {
                                    "authorId": "46458150",
                                    "name": "Ling Liu"
                                },
                                {
                                    "authorId": "2327300",
                                    "name": "M. E. Gursoy"
                                },
                                {
                                    "authorId": "2112532900",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "47747953",
                                    "name": "Wenqi Wei"
                                }
                            ],
                            "year": 2018,
                            "venue": "arXiv.org",
                            "n_citations": 112
                        },
                        "score": 0.96875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Black-Box Methodologies",
                "tldr": "Black-box membership inference attacks operate without access to the target model's internal structure, using only the model's outputs to determine membership status. These methodologies range from shadow model training approaches to metric-based techniques that analyze prediction confidence, with recent innovations focusing on minimizing assumptions and resource requirements. (10 sources)",
                "text": "\nBlack-box membership inference attacks operate in settings where the adversary has limited access to the target model, typically only being able to query the model and observe its outputs without knowledge of its internal parameters or architecture. Several key methodologies have been developed for this scenario:\n\n## Shadow Model Training Approaches\n\nThe most established black-box methodology, introduced by Shokri et al., uses multiple \"shadow models\" to mimic the behavior of the target model <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. In this approach:\n\n1. The attacker trains several shadow models on datasets that attempt to replicate the distribution of the target model's training data\n2. For each shadow model, the attacker collects outputs on both training data (members) and test data (non-members)\n3. These outputs, labeled as either member or non-member, are used to train an \"attack model\" (typically a binary classifier)\n4. The attack model then predicts membership status for any data point queried against the target model\n\nThis approach was later refined by Salem et al., who demonstrated that even a single shadow model can be sufficient to mount effective attacks, significantly reducing the computational burden <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>. This relaxation of assumptions makes membership inference attacks more broadly applicable and less resource-intensive <Paper corpusId=\"91184074\" paperTitle=\"(Salem et al., 2019)\" isShortName></Paper>.\n\n## Metric-Based Approaches\n\nAs an alternative to training shadow models, researchers have developed simpler techniques that rely on analyzing specific metrics from model outputs:\n\n1. **Confidence thresholding**: This approach compares the prediction confidence of the target model to a threshold, with higher confidence potentially indicating membership <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper>. This method can achieve membership inference accuracy similar to more complex approaches.\n\n2. **Entropy-based methods**: These techniques use the prediction entropy as a measure to distinguish between members and non-members <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\n3. **Ranking-based methods**: Salem et al. demonstrated that simply ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>.\n\n## Data-Independent Approaches\n\nMore recent research has focused on making black-box attacks less dependent on assumptions about data and model architecture:\n\n1. **BlindMI**: Hui et al. developed an approach that extracts membership semantics without shadow models through \"differential comparison,\" where a dataset of non-members is generated through transformations, and target samples are moved between datasets to detect membership status <Paper corpusId=\"230523638\" paperTitle=\"(Hui et al., 2021)\" isShortName></Paper>.\n\n2. **Metric benchmarking**: Song et al. proposed a systematic way to benchmark membership inference risks by comparing different non-neural network based inference attacks and introduced a \"privacy risk score\" to measure individual samples' likelihood of being training members <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\n## Label-Only Attacks\n\nA significant advancement in black-box methodologies are attacks that work with only the predicted labels rather than confidence scores:\n\n1. **Decision-based attacks**: Li et al. demonstrated that even when a model only exposes the final predicted label (not confidence scores), membership can still be inferred through techniques like transfer attacks and boundary attacks <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>.\n\n2. **Robustness-based approaches**: Choquette-Choo et al. introduced attacks that evaluate the robustness of a model's predicted labels under perturbations to obtain membership signals <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\nThese various black-box methodologies highlight how membership information can leak through a model's outputs in multiple ways, even with minimal access to the model itself <Paper corpusId=\"232233426\" paperTitle=\"(Hu et al., 2021)\" isShortName></Paper>. The evolution of these techniques has progressively relaxed the assumptions required for successful attacks, making them increasingly practical and concerning from a privacy perspective.",
                "citations": [
                    {
                        "id": "(Shokri et al., 2016)",
                        "snippets": [
                            "Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output.\n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class.\n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members."
                        ],
                        "paper": {
                            "corpus_id": 10488675,
                            "title": "Membership Inference Attacks Against Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "34828439",
                                    "name": "M. Stronati"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 4160
                        },
                        "score": 0.96923828125
                    },
                    {
                        "id": "(Salem et al., 2018)",
                        "snippets": [
                            "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."
                        ],
                        "paper": {
                            "corpus_id": 46933970,
                            "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "66697271",
                                    "name": "A. Salem"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "144887171",
                                    "name": "Mathias Humbert"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                }
                            ],
                            "year": 2018,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Salem et al., 2019)",
                        "snippets": [
                            "Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data (Hagestedt et al., 2019)[21] and location data (Pyrgelis et al., 2017)[37]. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28](Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)42,(Yeom et al., 2017)."
                        ],
                        "paper": {
                            "corpus_id": 91184074,
                            "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
                            "authors": [
                                {
                                    "authorId": "66697271",
                                    "name": "A. Salem"
                                },
                                {
                                    "authorId": "3407762",
                                    "name": "Apratim Bhattacharyya"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2019,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 257
                        },
                        "score": 0.97216796875
                    },
                    {
                        "id": "(Song et al., 2019)",
                        "snippets": [
                            "Membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training."
                        ],
                        "paper": {
                            "corpus_id": 165163934,
                            "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
                            "authors": [
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 244
                        },
                        "score": 0.96826171875
                    },
                    {
                        "id": "(Yeom et al., 2017)",
                        "snippets": [
                            "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                        ],
                        "paper": {
                            "corpus_id": 2656445,
                            "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                            "authors": [
                                {
                                    "authorId": "26378728",
                                    "name": "Samuel Yeom"
                                },
                                {
                                    "authorId": "3025831",
                                    "name": "Irene Giacomelli"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                },
                                {
                                    "authorId": "1680133",
                                    "name": "S. Jha"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE Computer Security Foundations Symposium",
                            "n_citations": 1133
                        },
                        "score": 0
                    },
                    {
                        "id": "(Song et al., 2020)",
                        "snippets": [
                            "In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not (Shokri et al., 2016)(Yeom et al., 2017). It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3](Pyrgelis et al., 2017). For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. (Shokri et al., 2016) conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models (Chen et al., 2019)(Hayes et al., 2017)(Hilprecht et al., 2019)(Wu et al., 2019), to differentially private models (Jayaraman et al., 2019)(Rahman et al., 2018), to decentralized settings where the models are trained across multiple users without sharing their data (Melis et al., 2018)(Nasr et al., 2018), and to white-box settings where the adversary also has the access to the target model's architecture and weights (Nasr et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 214623088,
                            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 375
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Hui et al., 2021)",
                        "snippets": [
                            "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses."
                        ],
                        "paper": {
                            "corpus_id": 230523638,
                            "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
                            "authors": [
                                {
                                    "authorId": "2000495031",
                                    "name": "Bo Hui"
                                },
                                {
                                    "authorId": "46285766",
                                    "name": "Yuchen Yang"
                                },
                                {
                                    "authorId": "2114128628",
                                    "name": "Haolin Yuan"
                                },
                                {
                                    "authorId": "1765936",
                                    "name": "P. Burlina"
                                },
                                {
                                    "authorId": "144516687",
                                    "name": "N. Gong"
                                },
                                {
                                    "authorId": "3139121",
                                    "name": "Yinzhi Cao"
                                }
                            ],
                            "year": 2021,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 122
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Shokri et al. (Shokri et al., 2016) present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. (Salem et al., 2018) later present another attack by gradually relaxing the assumptions made by Shokri et al. (Shokri et al., 2016) achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks (Hui et al., 2021)(Li et al., 2020)35,(Song et al., 2019)(Yeom et al., 2017)",
                            "Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack."
                        ],
                        "paper": {
                            "corpus_id": 237563320,
                            "title": "Membership Leakage in Label-Only Exposures",
                            "authors": [
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 246
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Choquette-Choo et al., 2020)",
                        "snippets": [
                            "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees."
                        ],
                        "paper": {
                            "corpus_id": 220831381,
                            "title": "Label-Only Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "1415982317",
                                    "name": "Christopher A. Choquette-Choo"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "1967156",
                                    "name": "Nicolas Papernot"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 516
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hu et al., 2021)",
                        "snippets": [
                            "Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature."
                        ],
                        "paper": {
                            "corpus_id": 232233426,
                            "title": "Membership Inference Attacks on Machine Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2109102414",
                                    "name": "Hongsheng Hu"
                                },
                                {
                                    "authorId": "2175708",
                                    "name": "Z. Salcic"
                                },
                                {
                                    "authorId": "46732871",
                                    "name": "Lichao Sun"
                                },
                                {
                                    "authorId": "152945656",
                                    "name": "G. Dobbie"
                                },
                                {
                                    "authorId": "2721708",
                                    "name": "P. Yu"
                                },
                                {
                                    "authorId": "2356672360",
                                    "name": "Xuyun Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 440
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "White-Box Methodologies",
                "tldr": "White-box membership inference attacks leverage full access to the target model's internal architecture and parameters to determine training data membership. These methodologies exploit gradient information, activation patterns, and loss characteristics to achieve higher precision than black-box approaches, particularly when models appear to generalize well. (11 sources)",
                "text": "\nWhite-box membership inference attacks operate with full access to a target model's internal structure, enabling more powerful attacks compared to black-box approaches. These methodologies leverage various internal signals to determine whether a specific data point was used during training:\n\n## Gradient-Based Approaches\n\nWhite-box attacks commonly exploit the gradients of the model with respect to its parameters:\n\n1. **Gradient analysis**: Nasr et al. introduced techniques that analyze how the stochastic gradient descent algorithm behaves differently on training versus non-training data <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. The authors showed that a straightforward extension of black-box attacks to the white-box setting (by analyzing activation function outputs) is ineffective, and instead developed algorithms that exploit specific gradient-based vulnerability patterns.\n\n2. **Loss gradient exploitation**: Leino et al. demonstrated how analyzing gradients reveals that models use features in idiosyncratic ways for training data, providing strong membership signals <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>. This approach can detect membership even when a model's black-box behavior appears to generalize well.\n\n3. **Parameter-based membership scoring**: Sablayrolles et al. developed theoretical foundations showing that optimal membership inference can be approximated through model parameters' distribution analysis <Paper corpusId=\"174799799\" paperTitle=\"(Sablayrolles et al., 2019)\" isShortName></Paper>.\n\n## Feature Representation Analysis\n\nWhite-box attacks can also exploit how models process and represent features:\n\n1. **Intermediate layer analysis**: Multiple researchers have investigated using the outputs of intermediate layers (feature representations) to detect differences in how models process training versus non-training data <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248810845\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>.\n\n2. **Adversarial distance**: Some attacks measure the distance needed to create adversarial examples for input samples, with training samples typically requiring larger perturbations <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper>.\n\n## Loss-Based Mechanisms\n\nThe loss function provides particularly strong signals in white-box settings:\n\n1. **Direct loss comparison**: Yeom et al. showed that simply comparing the loss of a model on a given sample against a threshold can effectively distinguish between members and non-members <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper>. While this can be done in black-box settings too, white-box access provides additional loss information.\n\n2. **Training trajectory analysis**: Recent approaches by Liu et al. analyze how the loss of samples evolves throughout training, providing an enhanced membership signal <Paper corpusId=\"251953448\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\n## Advantages of White-Box Approaches\n\nWhite-box attacks consistently demonstrate several advantages over black-box methods:\n\n1. **Higher precision**: Leino et al. showed that white-box attacks can be calibrated for high precision, making them effective at confidently identifying positive membership when black-box attacks cannot <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n2. **Effectiveness on well-generalized models**: Nasr et al. demonstrated that even well-generalized models that resist black-box attacks remain vulnerable to white-box methods <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. This suggests that model memorization occurs even without overt overfitting.\n\n3. **Attack strength**: Multiple studies confirm that white-box attacks generally outperform black-box attacks in terms of accuracy and confidence <Paper corpusId=\"239016142\" paperTitle=\"(Tang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper>.\n\nWhile some research suggests that under certain assumptions about model parameter distribution, theoretical optimal black-box attacks can match white-box performance <Paper corpusId=\"174799799\" paperTitle=\"(Sablayrolles et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256868849\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>, empirical results consistently show that white-box attacks remain more powerful in practical scenarios, particularly when models appear to generalize well or when high-confidence membership inference is required.",
                "citations": [
                    {
                        "id": "(Nasr et al., 2018)",
                        "snippets": [
                            "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                        ],
                        "paper": {
                            "corpus_id": 133091488,
                            "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Leino et al., 2019)",
                        "snippets": [
                            "Training data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25].\n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data."
                        ],
                        "paper": {
                            "corpus_id": 195699554,
                            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                            "authors": [
                                {
                                    "authorId": "35802340",
                                    "name": "Klas Leino"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                }
                            ],
                            "year": 2019,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 272
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Sablayrolles et al., 2019)",
                        "snippets": [
                            "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet."
                        ],
                        "paper": {
                            "corpus_id": 174799799,
                            "title": "White-box vs Black-box: Bayes Optimal Strategies for Membership Inference",
                            "authors": [
                                {
                                    "authorId": "3469062",
                                    "name": "Alexandre Sablayrolles"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                },
                                {
                                    "authorId": "2462253",
                                    "name": "C. Schmid"
                                },
                                {
                                    "authorId": "1734570",
                                    "name": "Y. Ollivier"
                                },
                                {
                                    "authorId": "1681054",
                                    "name": "H. J\u00e9gou"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 368
                        },
                        "score": 0
                    },
                    {
                        "id": "(Grosso et al., 2022)",
                        "snippets": [
                            "The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."
                        ],
                        "paper": {
                            "corpus_id": 247595200,
                            "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
                            "authors": [
                                {
                                    "authorId": "2033372042",
                                    "name": "Ganesh Del Grosso"
                                },
                                {
                                    "authorId": "52195298",
                                    "name": "Hamid Jalalzai"
                                },
                                {
                                    "authorId": "134026950",
                                    "name": "Georg Pichler"
                                },
                                {
                                    "authorId": "1722055",
                                    "name": "C. Palamidessi"
                                },
                                {
                                    "authorId": "1743922",
                                    "name": "P. Piantanida"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 22
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Zhang et al., 2022)",
                        "snippets": [
                            "There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings.\n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18]22]."
                        ],
                        "paper": {
                            "corpus_id": 248810845,
                            "title": "Evaluating Membership Inference Through Adversarial Robustness",
                            "authors": [
                                {
                                    "authorId": "2156121564",
                                    "name": "Zhaoxi Zhang"
                                },
                                {
                                    "authorId": "2248789322",
                                    "name": "Leo Yu Zhang"
                                },
                                {
                                    "authorId": "2110300762",
                                    "name": "Xufei Zheng"
                                },
                                {
                                    "authorId": "2165378948",
                                    "name": "Bilal Hussain Abbasi"
                                },
                                {
                                    "authorId": "3115562",
                                    "name": "Shengshan Hu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer/law journal",
                            "n_citations": 16
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Yeom et al., 2017)",
                        "snippets": [
                            "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."
                        ],
                        "paper": {
                            "corpus_id": 2656445,
                            "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
                            "authors": [
                                {
                                    "authorId": "26378728",
                                    "name": "Samuel Yeom"
                                },
                                {
                                    "authorId": "3025831",
                                    "name": "Irene Giacomelli"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                },
                                {
                                    "authorId": "1680133",
                                    "name": "S. Jha"
                                }
                            ],
                            "year": 2017,
                            "venue": "IEEE Computer Security Foundations Symposium",
                            "n_citations": 1133
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."
                        ],
                        "paper": {
                            "corpus_id": 251953448,
                            "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
                            "authors": [
                                {
                                    "authorId": "2182511319",
                                    "name": "Yiyong Liu"
                                },
                                {
                                    "authorId": "2277275",
                                    "name": "Zhengyu Zhao"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 111
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Tang et al., 2021)",
                        "snippets": [
                            "Membership inference attacks are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28]29]",
                            "Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model (Leino et al., 2019)(Nasr et al., 2018), which can exploit model parameters to infer membership information."
                        ],
                        "paper": {
                            "corpus_id": 239016142,
                            "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture",
                            "authors": [
                                {
                                    "authorId": "2048002984",
                                    "name": "Xinyu Tang"
                                },
                                {
                                    "authorId": "2364685227",
                                    "name": "Saeed Mahloujifar"
                                },
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "148318826",
                                    "name": "Virat Shejwalkar"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2021,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 80
                        },
                        "score": 0.970703125
                    },
                    {
                        "id": "(Zhang et al._1, 2022)",
                        "snippets": [
                            "Membership inference attacks are divided into white-box attacks [11](Nasr et al., 2018)(Leino et al., 2019)(Hayes et al., 2017) and blackbox attacks (Yurtsever et al., 2019)(Shokri et al., 2016)(Chen et al., 2019) according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability."
                        ],
                        "paper": {
                            "corpus_id": 248870291,
                            "title": "Black-box based limited query membership inference attack",
                            "authors": [
                                {
                                    "authorId": "2153635068",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "2146383582",
                                    "name": "Huaping Zhou"
                                },
                                {
                                    "authorId": "2297143623",
                                    "name": "Pengyan Wang"
                                },
                                {
                                    "authorId": "2105622",
                                    "name": "Gaoming Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Zhou et al., 2022)",
                        "snippets": [
                            "The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings."
                        ],
                        "paper": {
                            "corpus_id": 246706163,
                            "title": "PPA: Preference Profiling Attack Against Federated Learning",
                            "authors": [
                                {
                                    "authorId": "1845880105",
                                    "name": "Chunyi Zhou"
                                },
                                {
                                    "authorId": "39922366",
                                    "name": "Yansong Gao"
                                },
                                {
                                    "authorId": "2068511826",
                                    "name": "Anmin Fu"
                                },
                                {
                                    "authorId": "2157740727",
                                    "name": "Kai Chen"
                                },
                                {
                                    "authorId": "151498397",
                                    "name": "Zhiyang Dai"
                                },
                                {
                                    "authorId": "2116763991",
                                    "name": "Zhi Zhang"
                                },
                                {
                                    "authorId": "2837434",
                                    "name": "Minhui Xue"
                                },
                                {
                                    "authorId": "2155342827",
                                    "name": "Yuqing Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 23
                        },
                        "score": 0.97412109375
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "Membership Inference Attack (MIA) was first introduced by Shokri et al. (Shokri et al., 2016). It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box (Nasr et al., 2018), [28] as well as black-box (Shokri et al., 2016), (Sablayrolles et al., 2019), (Yeom et al., 2017), (Salem et al., 2018), (Song et al., 2020) classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members (Yeom et al., 2017). Sablayrolles et al. (Sablayrolles et al., 2019) further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution."
                        ],
                        "paper": {
                            "corpus_id": 256868849,
                            "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
                            "authors": [
                                {
                                    "authorId": "47770210",
                                    "name": "Derui Zhu"
                                },
                                {
                                    "authorId": "153642281",
                                    "name": "Dingfan Chen"
                                },
                                {
                                    "authorId": "1718732",
                                    "name": "Jens Grossklags"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.98046875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparing Black-Box and White-Box Approaches",
                "tldr": "Black-box and white-box membership inference attacks differ fundamentally in their access requirements, methodologies, and effectiveness, with white-box approaches generally achieving higher attack precision when models appear well-generalized. The access-accuracy tradeoff between these approaches has important implications for privacy risk assessment and defense development. (9 sources)",
                "text": "\nMembership inference attacks can be categorized into black-box and white-box approaches, which differ significantly in their assumptions, methodologies, and effectiveness. Understanding these differences is crucial for properly assessing privacy risks and developing appropriate defenses.\n\n## Access Requirements\n\nThe fundamental distinction between these approaches lies in the adversary's access level to the target model:\n\n- **Black-box attacks** operate with limited access, typically allowing the adversary to only query the model and observe its outputs without knowledge of internal parameters or architecture <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper>. This represents a more realistic threat model in deployed machine learning services.\n\n- **White-box attacks** assume full access to the target model's internal structure, parameters, and architecture <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper>. This provides the adversary with a more comprehensive view of the model's behavior.\n\n## Attack Effectiveness\n\nResearch consistently shows differences in the effectiveness of these approaches:\n\n1. **Attack strength**: White-box attacks generally outperform black-box attacks in terms of accuracy and confidence <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>. Multiple studies confirm that white-box membership inference has stronger attack capabilities than black-box approaches <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"268819379\" paperTitle=\"(Alshantti et al., 2024)\" isShortName></Paper>.\n\n2. **Well-generalized models**: One of the most significant differences is that white-box attacks remain effective even when models appear to generalize well. Nasr et al. demonstrated that even well-generalized models that resist black-box attacks remain vulnerable to white-box methods <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper> <Paper corpusId=\"259342605\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This suggests that model memorization occurs even without overt overfitting.\n\n3. **High-precision identification**: Leino et al. showed that white-box attacks can be calibrated for high precision, making them effective at confidently identifying positive membership when black-box attacks cannot <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n4. **Performance gap**: In generative models, Chen et al. demonstrated that full white-box membership inference attacks are consistently more effective than grey-box and black-box attacks <Paper corpusId=\"268819379\" paperTitle=\"(Alshantti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221203089\" paperTitle=\"(Chen et al., 2019)\" isShortName></Paper>.\n\n## Theoretical Considerations\n\nDespite the empirical advantage of white-box attacks, some researchers have explored theoretical boundaries between these approaches:\n\n1. **Theoretical equivalence**: Sablayrolles et al. developed theoretical foundations suggesting that under certain assumptions about model parameter distribution, optimal black-box attacks can approximate the performance of white-box attacks <Paper corpusId=\"174799799\" paperTitle=\"(Sablayrolles et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256868849\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>.\n\n2. **Empirical reality**: Despite this theoretical equivalence, empirical results consistently show that white-box attacks remain more powerful in practical scenarios <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n## Practical Implications\n\nThe differences between black-box and white-box approaches have important implications for privacy protection:\n\n1. **Defense evaluation**: When evaluating privacy defenses, considering both attack types provides a more comprehensive privacy assessment. Many defenses that appear effective against black-box attacks fail against white-box attacks <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n2. **Risk assessment**: Black-box attacks represent more realistic threats in deployed machine learning services, where users typically have limited access. However, white-box attacks show the upper bound of privacy risk, particularly important for sensitive applications <Paper corpusId=\"259342605\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\n3. **Defense approaches**: Effective defenses must consider both attack types. For instance, Chen et al. found that differential privacy and strong L2 regularization are among the few defenses effective against both black-box and white-box attacks <Paper corpusId=\"259342605\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nThe evolution of both attack types has led to increasingly sophisticated approaches, with black-box attacks becoming more effective with fewer assumptions and white-box attacks exploiting increasingly subtle signals from model internals. This ongoing arms race underscores the importance of robust privacy protection mechanisms in machine learning systems that can withstand both attack vectors.",
                "citations": [
                    {
                        "id": "(Zhang et al._1, 2022)",
                        "snippets": [
                            "Membership inference attacks are divided into white-box attacks [11](Nasr et al., 2018)(Leino et al., 2019)(Hayes et al., 2017) and blackbox attacks (Yurtsever et al., 2019)(Shokri et al., 2016)(Chen et al., 2019) according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability."
                        ],
                        "paper": {
                            "corpus_id": 248870291,
                            "title": "Black-box based limited query membership inference attack",
                            "authors": [
                                {
                                    "authorId": "2153635068",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "2146383582",
                                    "name": "Huaping Zhou"
                                },
                                {
                                    "authorId": "2297143623",
                                    "name": "Pengyan Wang"
                                },
                                {
                                    "authorId": "2105622",
                                    "name": "Gaoming Yang"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Access",
                            "n_citations": 3
                        },
                        "score": 0.98193359375
                    },
                    {
                        "id": "(Grosso et al., 2022)",
                        "snippets": [
                            "The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."
                        ],
                        "paper": {
                            "corpus_id": 247595200,
                            "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
                            "authors": [
                                {
                                    "authorId": "2033372042",
                                    "name": "Ganesh Del Grosso"
                                },
                                {
                                    "authorId": "52195298",
                                    "name": "Hamid Jalalzai"
                                },
                                {
                                    "authorId": "134026950",
                                    "name": "Georg Pichler"
                                },
                                {
                                    "authorId": "1722055",
                                    "name": "C. Palamidessi"
                                },
                                {
                                    "authorId": "1743922",
                                    "name": "P. Piantanida"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 22
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Leino et al., 2019)",
                        "snippets": [
                            "Training data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25].\n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data."
                        ],
                        "paper": {
                            "corpus_id": 195699554,
                            "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference",
                            "authors": [
                                {
                                    "authorId": "35802340",
                                    "name": "Klas Leino"
                                },
                                {
                                    "authorId": "2623167",
                                    "name": "Matt Fredrikson"
                                }
                            ],
                            "year": 2019,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 272
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Alshantti et al., 2024)",
                        "snippets": [
                            "Membership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting. In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model. Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set. It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019). Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019). Federated learning can however introduce data leakages which add up to the privacy concerns. In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017; Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models. Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications. The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models. Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks."
                        ],
                        "paper": {
                            "corpus_id": 268819379,
                            "title": "Privacy Re\u2010Identification Attacks on Tabular GANs",
                            "authors": [
                                {
                                    "authorId": "2145327284",
                                    "name": "Abdallah Alshantti"
                                },
                                {
                                    "authorId": "2265981625",
                                    "name": "Adil Rasheed"
                                },
                                {
                                    "authorId": "2243101143",
                                    "name": "Frank Westad"
                                }
                            ],
                            "year": 2024,
                            "venue": "Security and Privacy",
                            "n_citations": 4
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Nasr et al., 2018)",
                        "snippets": [
                            "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                        ],
                        "paper": {
                            "corpus_id": 133091488,
                            "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Depending on the adversary capabilities, MIAs can be divided into black-box (Shokri et al., 2016), (Yeom et al., 2017), [17], (Carlini et al., 2021), (Song et al., 2020), (Choquette-Choo et al., 2020), [47], (Li et al., 2020) and white-box attacks (Leino et al., 2019), (Jayaraman et al., 2020), (Nasr et al., 2018). The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies (Jia et al., 2019), (Tang et al., 2021), (Nasr et al., 2018) (and in HAMP). Such attacks can be mounted by either shadow-training (Shokri et al., 2016), (Nasr et al., 2018), (Yeom et al., 2017) or computing statistical metrics based on the partial knowledge of the private dataset (Song et al., 2020), (Choquette-Choo et al., 2020), (Li et al., 2020). Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with (Choquette-Choo et al., 2020) or without (Li et al., 2020) partial knowledge of the membership information."
                        ],
                        "paper": {
                            "corpus_id": 259342605,
                            "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction",
                            "authors": [
                                {
                                    "authorId": "1583483502",
                                    "name": "Zitao Chen"
                                },
                                {
                                    "authorId": "1715185",
                                    "name": "K. Pattabiraman"
                                }
                            ],
                            "year": 2023,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 24
                        },
                        "score": 0.974609375
                    },
                    {
                        "id": "(Chen et al., 2019)",
                        "snippets": [
                            "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data)."
                        ],
                        "paper": {
                            "corpus_id": 221203089,
                            "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
                            "authors": [
                                {
                                    "authorId": "153642281",
                                    "name": "Dingfan Chen"
                                },
                                {
                                    "authorId": "2052212417",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 406
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sablayrolles et al., 2019)",
                        "snippets": [
                            "Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet."
                        ],
                        "paper": {
                            "corpus_id": 174799799,
                            "title": "White-box vs Black-box: Bayes Optimal Strategies for Membership Inference",
                            "authors": [
                                {
                                    "authorId": "3469062",
                                    "name": "Alexandre Sablayrolles"
                                },
                                {
                                    "authorId": "3271933",
                                    "name": "Matthijs Douze"
                                },
                                {
                                    "authorId": "2462253",
                                    "name": "C. Schmid"
                                },
                                {
                                    "authorId": "1734570",
                                    "name": "Y. Ollivier"
                                },
                                {
                                    "authorId": "1681054",
                                    "name": "H. J\u00e9gou"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 368
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "Membership Inference Attack (MIA) was first introduced by Shokri et al. (Shokri et al., 2016). It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box (Nasr et al., 2018), [28] as well as black-box (Shokri et al., 2016), (Sablayrolles et al., 2019), (Yeom et al., 2017), (Salem et al., 2018), (Song et al., 2020) classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members (Yeom et al., 2017). Sablayrolles et al. (Sablayrolles et al., 2019) further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution."
                        ],
                        "paper": {
                            "corpus_id": 256868849,
                            "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy",
                            "authors": [
                                {
                                    "authorId": "47770210",
                                    "name": "Derui Zhu"
                                },
                                {
                                    "authorId": "153642281",
                                    "name": "Dingfan Chen"
                                },
                                {
                                    "authorId": "1718732",
                                    "name": "Jens Grossklags"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.98046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Developments and Variants",
                "tldr": "Membership inference attacks have evolved significantly with innovations in label-only approaches that operate without confidence scores, multi-query strategies that leverage multiple data points, and specialized attacks against complex systems like generative models and federated learning. These developments have progressively reduced the assumptions required by attackers while improving attack performance, highlighting the persistent and growing nature of privacy risks in machine learning systems. (14 sources)",
                "text": "\nMembership inference attacks continue to evolve, with researchers developing increasingly sophisticated approaches that operate under more realistic constraints while achieving better performance. Several notable recent developments have expanded the scope and effectiveness of these attacks:\n\n## Label-Only Approaches\n\nA significant advancement in membership inference has been the development of label-only attacks that operate without access to confidence scores:\n\n1. **Decision-based attacks**: Li and Zhang introduced attacks that can determine membership status using only the predicted labels rather than confidence scores <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. These decision-based approaches include transfer attacks and boundary attacks that can sometimes outperform traditional score-based methods.\n\n2. **Perturbation-based methods**: Choquette-Choo et al. demonstrated that label-only membership inference attacks can evaluate the robustness of model predictions under perturbations to obtain membership signals <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>. These attacks break many defense mechanisms that rely on \"confidence masking\" while leaving predicted labels unchanged.\n\n3. **Effectiveness against defenses**: Label-only attacks have proven remarkably effective against various defensive strategies, with research showing that only differential privacy and strong L2 regularization consistently thwart these attacks <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\n## Multi-Query Strategies\n\nRecent research has explored more sophisticated query strategies that leverage multiple related data points:\n\n1. **Differential comparison**: Hui et al. introduced BlindMI, which uses a novel differential comparison approach to extract membership semantics without shadow models <Paper corpusId=\"230523638\" paperTitle=\"(Hui et al., 2021)\" isShortName></Paper>. The method generates non-member datasets through transformations and then measures how samples affect set distances when moved between datasets.\n\n2. **Training trajectory analysis**: Liu et al. developed TrajectoryMIA, which exploits membership signals generated throughout a model's training process instead of relying solely on the final model <Paper corpusId=\"251953448\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This approach has demonstrated significantly higher true-positive rates at low false-positive rates compared to existing methods.\n\n3. **Multiple model comparisons**: Carlini et al. created the Likelihood Ratio Attack (LiRA) that carefully combines multiple techniques to achieve substantially more powerful attacks at low false-positive rates <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>.\n\n## Attacks in Specialized Domains\n\nMembership inference has expanded beyond traditional supervised learning to specialized domains:\n\n1. **Generative models**: Researchers have demonstrated that generative models, including GANs and diffusion models, are also vulnerable to membership inference attacks <Paper corpusId=\"221203089\" paperTitle=\"(Chen et al., 2019)\" isShortName></Paper> <Paper corpusId=\"52211986\" paperTitle=\"(Hayes et al., 2017)\" isShortName></Paper>. White-box attacks on generative models have been shown to be consistently more effective than grey-box and black-box approaches.\n\n2. **Federated learning**: Nasr et al. investigated membership inference in federated learning settings, showing how adversarial participants can successfully run active membership inference attacks against other participants <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. Melis et al. further demonstrated that model updates in collaborative learning leak unintended information about participants' training data <Paper corpusId=\"53099247\" paperTitle=\"(Melis et al., 2018)\" isShortName></Paper>.\n\n3. **Large language models**: Recent work has expanded membership inference to complex models like large language models, with researchers developing attacks that rely solely on generated text outputs <Paper corpusId=\"272367776\" paperTitle=\"(Wen et al., 2024)\" isShortName></Paper>.\n\n## Active and Adversarial Approaches\n\nMore recent attacks have incorporated active techniques where the adversary takes a more proactive role:\n\n1. **Data poisoning for enhanced inference**: Tram\u00e8r et al. demonstrated that an adversary who can poison even a small fraction (<0.1%) of the training dataset can boost inference attack performance by 1-2 orders of magnitude <Paper corpusId=\"247922814\" paperTitle=\"(Tramer et al., 2022)\" isShortName></Paper>.\n\n2. **Adversarial examples for membership signals**: Grosso et al. developed an approach that measures the magnitude of perturbation needed to create adversarial examples as a signal of training membership <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper>.\n\n3. **Targeted attacks**: Long et al. introduced targeted membership inference attacks that focus on identifying specific vulnerable records rather than attacking indiscriminately, showing that even when aggregate attack precision is near the baseline, individual records can be highly vulnerable <Paper corpusId=\"226266600\" paperTitle=\"(Long et al., 2020)\" isShortName></Paper>.\n\n## Evaluation Methodologies\n\nResearchers have also improved how membership inference attacks are evaluated:\n\n1. **Privacy risk scoring**: Song et al. introduced a more fine-grained privacy analysis approach using a privacy risk score metric that measures an individual sample's likelihood of being a training member <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\n2. **Evaluation at low false-positive rates**: Carlini et al. argued that attacks should be evaluated by their true-positive rate at low false-positive rates (e.g., \u22640.1%), rather than using average-case accuracy metrics that fail to characterize whether the attack can confidently identify members <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>.\n\nThese recent developments highlight the dynamic nature of the field, with researchers continually reducing the assumptions required by attackers while improving attack performance. This ongoing evolution underscores the persistent privacy risks in machine learning systems and the need for robust defenses that can withstand increasingly sophisticated attacks.",
                "citations": [
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Shokri et al. (Shokri et al., 2016) present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. (Salem et al., 2018) later present another attack by gradually relaxing the assumptions made by Shokri et al. (Shokri et al., 2016) achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks (Hui et al., 2021)(Li et al., 2020)35,(Song et al., 2019)(Yeom et al., 2017)",
                            "Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack."
                        ],
                        "paper": {
                            "corpus_id": 237563320,
                            "title": "Membership Leakage in Label-Only Exposures",
                            "authors": [
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 246
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Choquette-Choo et al., 2020)",
                        "snippets": [
                            "Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees."
                        ],
                        "paper": {
                            "corpus_id": 220831381,
                            "title": "Label-Only Membership Inference Attacks",
                            "authors": [
                                {
                                    "authorId": "1415982317",
                                    "name": "Christopher A. Choquette-Choo"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "1967156",
                                    "name": "Nicolas Papernot"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 516
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hui et al., 2021)",
                        "snippets": [
                            "Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses."
                        ],
                        "paper": {
                            "corpus_id": 230523638,
                            "title": "Practical Blind Membership Inference Attack via Differential Comparisons",
                            "authors": [
                                {
                                    "authorId": "2000495031",
                                    "name": "Bo Hui"
                                },
                                {
                                    "authorId": "46285766",
                                    "name": "Yuchen Yang"
                                },
                                {
                                    "authorId": "2114128628",
                                    "name": "Haolin Yuan"
                                },
                                {
                                    "authorId": "1765936",
                                    "name": "P. Burlina"
                                },
                                {
                                    "authorId": "144516687",
                                    "name": "N. Gong"
                                },
                                {
                                    "authorId": "3139121",
                                    "name": "Yinzhi Cao"
                                }
                            ],
                            "year": 2021,
                            "venue": "Network and Distributed System Security Symposium",
                            "n_citations": 122
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."
                        ],
                        "paper": {
                            "corpus_id": 251953448,
                            "title": "Membership Inference Attacks by Exploiting Loss Trajectory",
                            "authors": [
                                {
                                    "authorId": "2182511319",
                                    "name": "Yiyong Liu"
                                },
                                {
                                    "authorId": "2277275",
                                    "name": "Zhengyu Zhao"
                                },
                                {
                                    "authorId": "144588806",
                                    "name": "M. Backes"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 111
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Carlini et al., 2021)",
                        "snippets": [
                            "A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \"accuracy\" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics."
                        ],
                        "paper": {
                            "corpus_id": 244920593,
                            "title": "Membership Inference Attacks From First Principles",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2059189068",
                                    "name": "Steve Chien"
                                },
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "144206374",
                                    "name": "Shuang Song"
                                },
                                {
                                    "authorId": "1763579",
                                    "name": "A. Terzis"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 708
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2019)",
                        "snippets": [
                            "Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data)."
                        ],
                        "paper": {
                            "corpus_id": 221203089,
                            "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models",
                            "authors": [
                                {
                                    "authorId": "153642281",
                                    "name": "Dingfan Chen"
                                },
                                {
                                    "authorId": "2052212417",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "2145954003",
                                    "name": "Yang Zhang"
                                },
                                {
                                    "authorId": "1739548",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 406
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hayes et al., 2017)",
                        "snippets": [
                            "Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."
                        ],
                        "paper": {
                            "corpus_id": 52211986,
                            "title": "LOGAN: Membership Inference Attacks Against Generative Models",
                            "authors": [
                                {
                                    "authorId": "9200194",
                                    "name": "Jamie Hayes"
                                },
                                {
                                    "authorId": "145557680",
                                    "name": "Luca Melis"
                                },
                                {
                                    "authorId": "1722262",
                                    "name": "G. Danezis"
                                },
                                {
                                    "authorId": "1728207",
                                    "name": "Emiliano De Cristofaro"
                                }
                            ],
                            "year": 2017,
                            "venue": "Proceedings on Privacy Enhancing Technologies",
                            "n_citations": 515
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nasr et al., 2018)",
                        "snippets": [
                            "Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."
                        ],
                        "paper": {
                            "corpus_id": 133091488,
                            "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning",
                            "authors": [
                                {
                                    "authorId": "3490923",
                                    "name": "Milad Nasr"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "1972973",
                                    "name": "Amir Houmansadr"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1452
                        },
                        "score": 0
                    },
                    {
                        "id": "(Melis et al., 2018)",
                        "snippets": [
                            "Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."
                        ],
                        "paper": {
                            "corpus_id": 53099247,
                            "title": "Exploiting Unintended Feature Leakage in Collaborative Learning",
                            "authors": [
                                {
                                    "authorId": "145557680",
                                    "name": "Luca Melis"
                                },
                                {
                                    "authorId": "3469125",
                                    "name": "Congzheng Song"
                                },
                                {
                                    "authorId": "1728207",
                                    "name": "Emiliano De Cristofaro"
                                },
                                {
                                    "authorId": "1723945",
                                    "name": "Vitaly Shmatikov"
                                }
                            ],
                            "year": 2018,
                            "venue": "IEEE Symposium on Security and Privacy",
                            "n_citations": 1482
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wen et al., 2024)",
                        "snippets": [
                            "Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances."
                        ],
                        "paper": {
                            "corpus_id": 272367776,
                            "title": "Membership Inference Attacks Against In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "2054749404",
                                    "name": "Rui Wen"
                                },
                                {
                                    "authorId": "2146247989",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2257034706",
                                    "name": "Michael Backes"
                                },
                                {
                                    "authorId": "2257291195",
                                    "name": "Yang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tramer et al., 2022)",
                        "snippets": [
                            "We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data. Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8\u00d7 more precise inference on all other users' otherwise-private data points. Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data."
                        ],
                        "paper": {
                            "corpus_id": 247922814,
                            "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets",
                            "authors": [
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "2520493",
                                    "name": "R. Shokri"
                                },
                                {
                                    "authorId": "2161243047",
                                    "name": "Ayrton San Joaquin"
                                },
                                {
                                    "authorId": "2145242672",
                                    "name": "Hoang M. Le"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "2111053680",
                                    "name": "Sanghyun Hong"
                                },
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Computer and Communications Security",
                            "n_citations": 123
                        },
                        "score": 0
                    },
                    {
                        "id": "(Grosso et al., 2022)",
                        "snippets": [
                            "The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."
                        ],
                        "paper": {
                            "corpus_id": 247595200,
                            "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage",
                            "authors": [
                                {
                                    "authorId": "2033372042",
                                    "name": "Ganesh Del Grosso"
                                },
                                {
                                    "authorId": "52195298",
                                    "name": "Hamid Jalalzai"
                                },
                                {
                                    "authorId": "134026950",
                                    "name": "Georg Pichler"
                                },
                                {
                                    "authorId": "1722055",
                                    "name": "C. Palamidessi"
                                },
                                {
                                    "authorId": "1743922",
                                    "name": "P. Piantanida"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 22
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Long et al., 2020)",
                        "snippets": [
                            "Membership Inference Attacks (MIAs) aim to determine the presence of a record in a machine learning model's training data by querying the model. Recent work has demonstrated the effectiveness of MIA on various machine learning models and corresponding defenses have been proposed. However, both attacks and defenses have focused on an adversary that indiscriminately attacks all the records without regard to the cost of false positives or negatives. In this work, we revisit membership inference attacks from the perspective of a pragmatic adversary who carefully selects targets and make predictions conservatively. We design a new evaluation methodology that allows us to evaluate the membership privacy risk at the level of individuals and not only in aggregate. We experimentally demonstrate that highly vulnerable records exist even when the aggregate attack precision is close to 50% (baseline). Specifically, on the MNIST dataset, our pragmatic adversary achieves a precision of 95.05% whereas the prior attack only achieves a precision of 51.7%."
                        ],
                        "paper": {
                            "corpus_id": 226266600,
                            "title": "A Pragmatic Approach to Membership Inferences on Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "3147214",
                                    "name": "Yunhui Long"
                                },
                                {
                                    "authorId": "2152507640",
                                    "name": "Lei Wang"
                                },
                                {
                                    "authorId": "3203018",
                                    "name": "Diyue Bu"
                                },
                                {
                                    "authorId": "3094927",
                                    "name": "Vincent Bindschaedler"
                                },
                                {
                                    "authorId": "50141047",
                                    "name": "Xiaofeng Wang"
                                },
                                {
                                    "authorId": "2112389071",
                                    "name": "Haixu Tang"
                                },
                                {
                                    "authorId": "1785347",
                                    "name": "Carl A. Gunter"
                                },
                                {
                                    "authorId": "145126969",
                                    "name": "Kai Chen"
                                }
                            ],
                            "year": 2020,
                            "venue": "European Symposium on Security and Privacy",
                            "n_citations": 83
                        },
                        "score": 0
                    },
                    {
                        "id": "(Song et al., 2020)",
                        "snippets": [
                            "In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not (Shokri et al., 2016)(Yeom et al., 2017). It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3](Pyrgelis et al., 2017). For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. (Shokri et al., 2016) conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models (Chen et al., 2019)(Hayes et al., 2017)(Hilprecht et al., 2019)(Wu et al., 2019), to differentially private models (Jayaraman et al., 2019)(Rahman et al., 2018), to decentralized settings where the models are trained across multiple users without sharing their data (Melis et al., 2018)(Nasr et al., 2018), and to white-box settings where the adversary also has the access to the target model's architecture and weights (Nasr et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 214623088,
                            "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models",
                            "authors": [
                                {
                                    "authorId": "144173853",
                                    "name": "Liwei Song"
                                },
                                {
                                    "authorId": "143615345",
                                    "name": "Prateek Mittal"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 375
                        },
                        "score": 0.97607421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.6373920000000001
    }
}