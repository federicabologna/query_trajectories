{
    "query": "are papers that multilmodal language models output text images",
    "user_id": "lib_user",
    "task_id": "a5977764-2095-4220-9911-05fab5f1f34b",
    "timestamp": "2025-06-23T22:48:27.903352",
    "n_retrieval": 256,
    "n_retrieved": 269,
    "n_candidates": 42,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.333096,
    "decomposed_query": {
        "rewritten_query": "Papers about multimodal language models that output text and images.",
        "keyword_query": "multimodal language models output text images",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009489,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Generating Images with Multimodal Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 73,
            "citation_count": 259,
            "influential_citation_count": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.17216",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ],
            "abstract": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "corpus_id": 258959284,
            "sentences": [
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
                    "score": 0.6483085288917855,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.984375
                },
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.",
                    "score": 0.5694774990136948,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 690
                        },
                        {
                            "start": 693,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1815
                        },
                        {
                            "start": 1816,
                            "end": 1869
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1180,
                            "end": 1184,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1328,
                            "end": 1332,
                            "matchedPaperCorpusId": "51876975"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98388671875
                },
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "GILL is the first multimodal language model capable of conditioning on image-and-text inputs to generate meaningful images interleaved with text. Hence, our experiments primarily focus on evaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over Stable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse. We also benchmark the performance of models in deciding whether to retrieve or generate (see appendix). GILL is capable of generating text, retrieving images, and generating images. Despite being more general than prior work [56,4,31], we find that GILL performs comparably to or better than existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).",
                    "score": 0.6097315963385495,
                    "section_title": "Experiments",
                    "char_start_offset": 16923,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 775
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 305,
                            "end": 309,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                },
                {
                    "corpus_id": "258959284",
                    "title": "Generating Images with Multimodal Language Models",
                    "text": "Multimodal Language Models Several prior works have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4,64], improving the visual encoder [4,33], finetuning on instructions [35], and training unified models on multi-task objectives [36,63,42]. CM3 [2,62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to",
                    "score": 0.6029147344937317,
                    "section_title": "Related Work",
                    "char_start_offset": 3794,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 610
                        },
                        {
                            "start": 611,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1143
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.984375,
            "relevance_judgment_input_expanded": "# Title: Generating Images with Multimodal Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov\n## Abstract\nWe propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.\n## Introduction\nAutoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.\n\n## Related Work\nMultimodal Language Models Several prior works have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4,64], improving the visual encoder [4,33], finetuning on instructions [35], and training unified models on multi-task objectives [36,63,42]. CM3 [2,62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to\n\n## Experiments\nGILL is the first multimodal language model capable of conditioning on image-and-text inputs to generate meaningful images interleaved with text. Hence, our experiments primarily focus on evaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over Stable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse. We also benchmark the performance of models in deciding whether to retrieve or generate (see appendix). GILL is capable of generating text, retrieving images, and generating images. Despite being more general than prior work [56,4,31], we find that GILL performs comparably to or better than existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).",
            "reference_string": "[258959284 | Koh et al. | 2023 | Citations: 259]"
        },
        {
            "title": "Multi-aspect Knowledge Distillation with Large Language Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2238019356",
                    "name": "Taegyeong Lee"
                },
                {
                    "authorId": "2341715815",
                    "name": "Jinsik Bang"
                },
                {
                    "authorId": "2294517321",
                    "name": "Soyeong Kwon"
                },
                {
                    "authorId": "2174161125",
                    "name": "Taehwan Kim"
                }
            ],
            "abstract": "Recent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images with considering class labels, these methods may struggle to learn various \\emph{aspects} of classes (e.g., natural positions and shape changes). Rethinking the previous approach from a novel view, we propose a multi-aspect knowledge distillation method using Multimodal Large Language Models (MLLMs). Our approach involves: 1) querying Large Language Model with multi-aspect questions relevant to the knowledge we want to transfer to the model, 2) extracting corresponding logits from MLLM, and 3) expanding the model's output dimensions to distill these multi-aspect logits. We then apply cross-entropy loss to class logits and binary cross-entropy loss to multi-aspect logits. Through our method, the model can learn not only the knowledge about visual aspects but also the abstract and complex aspects that require a deeper understanding. We primarily apply our method to image classification, and to explore the potential for extending our model, such as object detection. In all experimental results, our method improves the performance of the baselines. Additionally, we analyze the effect of multi-aspect knowledge distillation. These results demonstrate that our method can transfer knowledge about various aspects to the model and the aspect knowledge can enhance model performance in computer vision tasks.",
            "corpus_id": 275820726,
            "sentences": [
                {
                    "corpus_id": "275820726",
                    "title": "Multi-aspect Knowledge Distillation with Large Language Model",
                    "text": "Multimodal Large Language Models. Recently, Multimodal Large Language Models (MLLMs) [1,2,14,40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL [4] can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs. Motivated by this, we apply the rich knowledge of MLLMs to image classification. \n\nVisual tasks with linguistic information. Many studies [3,18,21,23,27,37,38] try to extract linguistic information from a large language model and use it to settle the visual problems. One method [18] leverages the linguistic knowledge for each visual category from LLM to generate the descriptions and use the descriptions in zero-shot image classification. Another method [37] creates a concise set of representative visual attributes from LLM by leveraging their learning-to-search method for interpretable visual recognition. While these methods focus on generating attributes for model training, our approach distills knowledge about various aspects, extending the model's output dimensions.",
                    "score": 0.5793505511517054,
                    "section_title": "Related Work",
                    "char_start_offset": 4702,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 33
                        },
                        {
                            "start": 34,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1589
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 88,
                            "end": 90,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 90,
                            "end": 93,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 581,
                            "end": 584,
                            "matchedPaperCorpusId": "266521410"
                        },
                        {
                            "start": 957,
                            "end": 960,
                            "matchedPaperCorpusId": "252111028"
                        },
                        {
                            "start": 960,
                            "end": 963,
                            "matchedPaperCorpusId": "258866192"
                        },
                        {
                            "start": 963,
                            "end": 966,
                            "matchedPaperCorpusId": "260682240"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "253735286"
                        },
                        {
                            "start": 1267,
                            "end": 1271,
                            "matchedPaperCorpusId": "260682240"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: Multi-aspect Knowledge Distillation with Large Language Model\n# Venue: arXiv.org\n# Authors: Taegyeong Lee, Jinsik Bang, Soyeong Kwon, Taehwan Kim\n## Abstract\nRecent advancements in deep learning have significantly improved performance on computer vision tasks. Previous image classification methods primarily modify model architectures or add features, and they optimize models using cross-entropy loss on class logits. Since they focus on classifying images with considering class labels, these methods may struggle to learn various \\emph{aspects} of classes (e.g., natural positions and shape changes). Rethinking the previous approach from a novel view, we propose a multi-aspect knowledge distillation method using Multimodal Large Language Models (MLLMs). Our approach involves: 1) querying Large Language Model with multi-aspect questions relevant to the knowledge we want to transfer to the model, 2) extracting corresponding logits from MLLM, and 3) expanding the model's output dimensions to distill these multi-aspect logits. We then apply cross-entropy loss to class logits and binary cross-entropy loss to multi-aspect logits. Through our method, the model can learn not only the knowledge about visual aspects but also the abstract and complex aspects that require a deeper understanding. We primarily apply our method to image classification, and to explore the potential for extending our model, such as object detection. In all experimental results, our method improves the performance of the baselines. Additionally, we analyze the effect of multi-aspect knowledge distillation. These results demonstrate that our method can transfer knowledge about various aspects to the model and the aspect knowledge can enhance model performance in computer vision tasks.\n## Related Work\nMultimodal Large Language Models. Recently, Multimodal Large Language Models (MLLMs) [1,2,14,40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL [4] can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs. Motivated by this, we apply the rich knowledge of MLLMs to image classification. \n\nVisual tasks with linguistic information. Many studies [3,18,21,23,27,37,38] try to extract linguistic information from a large language model and use it to settle the visual problems. One method [18] leverages the linguistic knowledge for each visual category from LLM to generate the descriptions and use the descriptions in zero-shot image classification. Another method [37] creates a concise set of representative visual attributes from LLM by leveraging their learning-to-search method for interpretable visual recognition. While these methods focus on generating attributes for model training, our approach distills knowledge about various aspects, extending the model's output dimensions.",
            "reference_string": "[275820726 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Creativity and Machine Learning: A Survey",
            "venue": "ACM Computing Surveys",
            "year": 2021,
            "reference_count": 344,
            "citation_count": 42,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3664595",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.02726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2067291198",
                    "name": "Giorgio Franceschelli"
                },
                {
                    "authorId": "1806767",
                    "name": "Mirco Musolesi"
                }
            ],
            "abstract": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
            "corpus_id": 233168627,
            "sentences": [
                {
                    "corpus_id": "233168627",
                    "title": "Creativity and Machine Learning: A Survey",
                    "text": "The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) [216] has an image encoder pre-trained together with a text encoder to generate a caption for an image. A Large-scale ImaGe and Noisy-text embedding (ALIGN) [132], based on similar mechanisms, can achieve remarkable performance through training based on a noisier dataset. In [272] the authors propose a frozen language model for multimodal few-shot learning: a vision encoder is trained to represent each image as a sequence of continuous embeddings, so that the frozen language model prompted with this embedding can generate the appropriate caption. \n\nIn [80] the authors present Bridging-Vision-and-Language (BriVL), which performs multimodal tasks by learning from weak semantic correlation data. Finally, there is a trend toward even more complex multimodal models. For example, Video-Audio-Text Transformer (VATT) [4] learns to extract multimodal representations from video, audio, and text; instead, Gato [225] serializes all data (e.g., text, images, games, other RL-related tasks) into a flat sequence of tokens that is then embedded and passed to a standard large-scale language model. Similarly, Gemini [93] achieves state-of-the-art performance in multimodal tasks by working on interleaved sequences of text, image, audio, and video as inputs; [94] extends it to Mixture-of-Experts setting. Finally, NExT-GPT [293] handles any combination of four modalities (text, audio, image, and video) by connecting a language model with multimodal adaptors and diffusion decoders (see Section 3.5).",
                    "score": 0.5758672481159071,
                    "section_title": "Examples of Models.",
                    "char_start_offset": 40779,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 870
                        },
                        {
                            "start": 873,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1414
                        },
                        {
                            "start": 1415,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1819
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 318,
                            "end": 323,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 475,
                            "end": 480,
                            "matchedPaperCorpusId": "231879586"
                        },
                        {
                            "start": 594,
                            "end": 599,
                            "matchedPaperCorpusId": "235658331"
                        },
                        {
                            "start": 876,
                            "end": 880,
                            "matchedPaperCorpusId": "249314857"
                        },
                        {
                            "start": 1139,
                            "end": 1142,
                            "matchedPaperCorpusId": "233346984"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                }
            ],
            "relevance_judgement": 0.97021484375,
            "relevance_judgment_input_expanded": "# Title: Creativity and Machine Learning: A Survey\n# Venue: ACM Computing Surveys\n# Authors: Giorgio Franceschelli, Mirco Musolesi\n## Abstract\nThere is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.\n## Examples of Models.\nThe first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) [216] has an image encoder pre-trained together with a text encoder to generate a caption for an image. A Large-scale ImaGe and Noisy-text embedding (ALIGN) [132], based on similar mechanisms, can achieve remarkable performance through training based on a noisier dataset. In [272] the authors propose a frozen language model for multimodal few-shot learning: a vision encoder is trained to represent each image as a sequence of continuous embeddings, so that the frozen language model prompted with this embedding can generate the appropriate caption. \n\nIn [80] the authors present Bridging-Vision-and-Language (BriVL), which performs multimodal tasks by learning from weak semantic correlation data. Finally, there is a trend toward even more complex multimodal models. For example, Video-Audio-Text Transformer (VATT) [4] learns to extract multimodal representations from video, audio, and text; instead, Gato [225] serializes all data (e.g., text, images, games, other RL-related tasks) into a flat sequence of tokens that is then embedded and passed to a standard large-scale language model. Similarly, Gemini [93] achieves state-of-the-art performance in multimodal tasks by working on interleaved sequences of text, image, audio, and video as inputs; [94] extends it to Mixture-of-Experts setting. Finally, NExT-GPT [293] handles any combination of four modalities (text, audio, image, and video) by connecting a language model with multimodal adaptors and diffusion decoders (see Section 3.5).",
            "reference_string": "[233168627 | Franceschelli et al. | 2021 | Citations: 42]"
        },
        {
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 98,
            "citation_count": 122,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.13823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                }
            ],
            "abstract": "We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "corpus_id": 258947258,
            "sentences": [
                {
                    "corpus_id": "258947258",
                    "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
                    "text": "Trained at massive scale on large text corpora, large language models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answering complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models exhibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. \n\nIn this paper, we show that we are able to efficiently leverage Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. \n\nthe capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). \n\nWe propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multimodal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multitask objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximumlikelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embeddings for its paired image.",
                    "score": 0.5933187451737371,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 582
                        },
                        {
                            "start": 585,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1228
                        },
                        {
                            "start": 1231,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 2118
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.966796875
                },
                {
                    "corpus_id": "258947258",
                    "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
                    "text": "We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
                    "score": 0.547787562610055,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9609375
                }
            ],
            "relevance_judgement": 0.966796875,
            "relevance_judgment_input_expanded": "# Title: Grounding Language Models to Images for Multimodal Inputs and Outputs\n# Venue: International Conference on Machine Learning\n# Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried\n## Abstract\nWe propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.\n## Introduction\nTrained at massive scale on large text corpora, large language models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answering complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models exhibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. \n\nIn this paper, we show that we are able to efficiently leverage Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. \n\nthe capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). \n\nWe propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multimodal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multitask objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximumlikelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embeddings for its paired image.",
            "reference_string": "[258947258 | Koh et al. | 2023 | Citations: 122]"
        },
        {
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 81,
            "citation_count": 229,
            "influential_citation_count": 32,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.12528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306603347",
                    "name": "Jinheng Xie"
                },
                {
                    "authorId": "2258958963",
                    "name": "Weijia Mao"
                },
                {
                    "authorId": "2237427303",
                    "name": "Zechen Bai"
                },
                {
                    "authorId": "2248388634",
                    "name": "David Junhao Zhang"
                },
                {
                    "authorId": "2316668014",
                    "name": "Weihao Wang"
                },
                {
                    "authorId": "2298569956",
                    "name": "Kevin Qinghong Lin"
                },
                {
                    "authorId": "2248678348",
                    "name": "Yuchao Gu"
                },
                {
                    "authorId": "2316662510",
                    "name": "Zhijie Chen"
                },
                {
                    "authorId": "2309246065",
                    "name": "Zhenheng Yang"
                },
                {
                    "authorId": "2244623976",
                    "name": "Mike Zheng Shou"
                }
            ],
            "abstract": "We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.",
            "corpus_id": 271924334,
            "sentences": [
                {
                    "corpus_id": "271924334",
                    "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                    "text": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;Tang et al., 2024;Ye et al., 2024a;Dong et al., 2024;Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner. Compared to this work, we also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion process instead of autoregressive modeling for visual generation.",
                    "score": 0.5418199205531851,
                    "section_title": "UNIFIED VISION-LANGUAGE FOUNDATION MODEL",
                    "char_start_offset": 11116,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 253
                        },
                        {
                            "start": 254,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 83,
                            "end": 101,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 118,
                            "end": 136,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 636,
                            "end": 658,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 809,
                            "end": 827,
                            "matchedPaperCorpusId": "261975252"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Show-o: One Single Transformer to Unify Multimodal Understanding and Generation\n# Venue: International Conference on Learning Representations\n# Authors: Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou\n## Abstract\nWe present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model flexibly supports a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation. Across various benchmarks, it demonstrates comparable or superior performance to existing individual models with an equivalent or larger number of parameters tailored for understanding or generation. This significantly highlights its potential as a next-generation foundation model. Code and models are released at https://github.com/showlab/Show-o.\n## UNIFIED VISION-LANGUAGE FOUNDATION MODEL\nIn recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;Tang et al., 2024;Ye et al., 2024a;Dong et al., 2024;Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner. Compared to this work, we also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion process instead of autoregressive modeling for visual generation.",
            "reference_string": "[271924334 | Xie et al. | 2024 | Citations: 229]"
        },
        {
            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 66,
            "citation_count": 69,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.17092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491798621",
                    "name": "Bohao Li"
                },
                {
                    "authorId": "51123495",
                    "name": "Yuying Ge"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2243452799",
                    "name": "Guangzhi Wang"
                },
                {
                    "authorId": "2151036422",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2268726753",
                    "name": "Ruimao Zhang"
                },
                {
                    "authorId": "2265579883",
                    "name": "Ying Shan"
                }
            ],
            "abstract": "Multimodal large language models (MLLMs), building upon the foundation of powerful large language models (LLMs), have recently demonstrated exceptional capabilities in generating not only texts but also images given interleaved multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However, existing MLLM benchmarks remain limited to assessing only models' comprehension ability of single image-text inputs, failing to keep up with the strides made in MLLMs. A comprehensive benchmark is imperative for investigating the progress and uncovering the limitations of current MLLMs. In this work, we categorize the capabilities of MLLMs into hierarchical levels from $L_0$ to $L_4$ based on the modalities they can accept and generate, and propose SEED-Bench-2, a comprehensive benchmark that evaluates the \\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2 comprises 24K multiple-choice questions with accurate human annotations, which spans 27 dimensions, including the evaluation of both text and image generation. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 23 prominent open-source MLLMs and summarize valuable observations. By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence. Dataset and evaluation code are available at \\href{https://github.com/AILab-CVC/SEED-Bench}",
            "corpus_id": 265498814,
            "sentences": [
                {
                    "corpus_id": "265498814",
                    "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
                    "text": "In recent years, Large Language Models (LLMs) [8,13,46,47,55] have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of openended tasks. Leveraging the strong generality of LLMs, Multimodal Large Language Models (MLLMs) [3,9,19,26,29,31,32,36,37,42,43,48,48,53,62,68,70] have demonstrated exceptional capabilities in comprehending multimodal data through predicting open-form texts. \n\nRecent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer. However, despite the extensive capabilities of MLLMs, existing MLLM benchmarks [4,15,38,61,63] primarily focus on evaluating single image-text comprehension, thus failing to fully demonstrate the progress and limitations of current MLLMs. The lag of benchmarks behind the rapid development of MLLMs hinders the exploration and evolution of models. \n\nIn this work, we categorize the capabilities of MLLMs into hierarchical levels ranging from L 0 to L 4 based on the modalities they can accept and generate, as depicted in Fig. 1. Building upon LLMs, the lowest-tier capability L 0 involves generating texts given text inputs, while the highest-tier capability L 4 entails producing open-form interleaved image and text output given arbitrary interleaved image-text inputs. Reaching the capability L 4 is a crucial milestone on the path towards General Artificial Intel-ligence (AGI) since a human-level AI should be able to effortlessly digest and create multimodal content. In the capability pyramid, higher levels inherently include the capabilities of lower tiers. This hierarchical categorization not only clearly illustrates the current progress of MLLMs, but also provides a well-defined roadmap for future research.",
                    "score": 0.5525716499205615,
                    "section_title": "Introduction",
                    "char_start_offset": 1364,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1133
                        },
                        {
                            "start": 1136,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1760
                        },
                        {
                            "start": 1761,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2008
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 277,
                            "end": 280,
                            "matchedPaperCorpusId": "256390509"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                }
            ],
            "relevance_judgement": 0.95654296875,
            "relevance_judgment_input_expanded": "# Title: SEED-Bench-2: Benchmarking Multimodal Large Language Models\n# Venue: arXiv.org\n# Authors: Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan\n## Abstract\nMultimodal large language models (MLLMs), building upon the foundation of powerful large language models (LLMs), have recently demonstrated exceptional capabilities in generating not only texts but also images given interleaved multimodal inputs (acting like a combination of GPT-4V and DALL-E 3). However, existing MLLM benchmarks remain limited to assessing only models' comprehension ability of single image-text inputs, failing to keep up with the strides made in MLLMs. A comprehensive benchmark is imperative for investigating the progress and uncovering the limitations of current MLLMs. In this work, we categorize the capabilities of MLLMs into hierarchical levels from $L_0$ to $L_4$ based on the modalities they can accept and generate, and propose SEED-Bench-2, a comprehensive benchmark that evaluates the \\textbf{hierarchical} capabilities of MLLMs. Specifically, SEED-Bench-2 comprises 24K multiple-choice questions with accurate human annotations, which spans 27 dimensions, including the evaluation of both text and image generation. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 23 prominent open-source MLLMs and summarize valuable observations. By revealing the limitations of existing MLLMs through extensive evaluations, we aim for SEED-Bench-2 to provide insights that will motivate future research towards the goal of General Artificial Intelligence. Dataset and evaluation code are available at \\href{https://github.com/AILab-CVC/SEED-Bench}\n## Introduction\nIn recent years, Large Language Models (LLMs) [8,13,46,47,55] have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of openended tasks. Leveraging the strong generality of LLMs, Multimodal Large Language Models (MLLMs) [3,9,19,26,29,31,32,36,37,42,43,48,48,53,62,68,70] have demonstrated exceptional capabilities in comprehending multimodal data through predicting open-form texts. \n\nRecent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer. However, despite the extensive capabilities of MLLMs, existing MLLM benchmarks [4,15,38,61,63] primarily focus on evaluating single image-text comprehension, thus failing to fully demonstrate the progress and limitations of current MLLMs. The lag of benchmarks behind the rapid development of MLLMs hinders the exploration and evolution of models. \n\nIn this work, we categorize the capabilities of MLLMs into hierarchical levels ranging from L 0 to L 4 based on the modalities they can accept and generate, as depicted in Fig. 1. Building upon LLMs, the lowest-tier capability L 0 involves generating texts given text inputs, while the highest-tier capability L 4 entails producing open-form interleaved image and text output given arbitrary interleaved image-text inputs. Reaching the capability L 4 is a crucial milestone on the path towards General Artificial Intel-ligence (AGI) since a human-level AI should be able to effortlessly digest and create multimodal content. In the capability pyramid, higher levels inherently include the capabilities of lower tiers. This hierarchical categorization not only clearly illustrates the current progress of MLLMs, but also provides a well-defined roadmap for future research.",
            "reference_string": "[265498814 | Li et al. | 2023 | Citations: 69]"
        },
        {
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 28,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15188, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254168373",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2290545053",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2333891614",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "151253861",
                    "name": "Weixin Liang"
                },
                {
                    "authorId": "2255374957",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "2256736753",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2296724476",
                    "name": "Lili Yu"
                }
            ],
            "abstract": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
            "corpus_id": 274860024,
            "sentences": [
                {
                    "corpus_id": "274860024",
                    "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                    "text": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
                    "score": 0.5102010513923,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                }
            ],
            "relevance_judgement": 0.95654296875,
            "relevance_judgment_input_expanded": "# Title: LMFusion: Adapting Pretrained Language Models for Multimodal Generation\n# Venue: arXiv.org\n# Authors: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke S. Zettlemoyer, Lili Yu\n## Abstract\nWe present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.\n",
            "reference_string": "[274860024 | Shi et al. | 2024 | Citations: 28]"
        },
        {
            "title": "Grounding Language Models to Images for Multimodal Generation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 93,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.13823",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2301.13823?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2301.13823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                }
            ],
            "abstract": "We propose an ef\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "corpus_id": 256416164,
            "sentences": [],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: Grounding Language Models to Images for Multimodal Generation\n# Venue: arXiv.org\n# Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried\n## Abstract\nWe propose an ef\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.\n",
            "reference_string": "[256416164 | Koh et al. | 2023 | Citations: 93]"
        },
        {
            "title": "Multimodal Conditionality for Natural Language Generation",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 35,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.01229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3423000",
                    "name": "Michael Sollami"
                },
                {
                    "authorId": "10818273",
                    "name": "Aashish Jain"
                }
            ],
            "abstract": "Large scale pretrained language models have demonstrated state-of-the-art performance in language understanding tasks. Their application has recently expanded into multimodality learning, leading to improved representations combining vision and language. However, progress in adapting language models towards conditional Natural Language Generation (NLG) has been limited to a single modality, generally text. We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text. We demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS can generate human quality descriptions consistent with given multimodal inputs.",
            "corpus_id": 237417089,
            "sentences": [
                {
                    "corpus_id": "237417089",
                    "title": "Multimodal Conditionality for Natural Language Generation",
                    "text": "Language representation model BERT (Devlin et al., 2018) demonstrated that transformer models trained with masked language modeling and next sentence prediction objective can lead to state-of-the-art performance for a variety of NLU tasks. VilBERT (Lu et al., 2019) extended the approach to-wards multimodality with separate transformer streams for image and text with cross-modality interaction though coattention between the two streams. Other methods (Li et al., 2020;Su et al., 2019b;Chen et al., 2020;Li et al., 2019) showed that single stream transformer models can learn the relationship between image and text. These models are pretrained on vision and language data, however Kiela et al. (2019) proposed a different approach where a pretrained unimodal (text) BERT model is fine-tuned together with a different modality (image), skipping the multimodal pretraining step. These methods are effective for understanding tasks like classification, but have not been studied for multimodal conditional generative tasks.",
                    "score": 0.5232336662290183,
                    "section_title": "Transfer Learning in Multimodal Models",
                    "char_start_offset": 3935,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1023
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 454,
                            "end": 471,
                            "matchedPaperCorpusId": "201058752"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: Multimodal Conditionality for Natural Language Generation\n# Venue: arXiv.org\n# Authors: Michael Sollami, Aashish Jain\n## Abstract\nLarge scale pretrained language models have demonstrated state-of-the-art performance in language understanding tasks. Their application has recently expanded into multimodality learning, leading to improved representations combining vision and language. However, progress in adapting language models towards conditional Natural Language Generation (NLG) has been limited to a single modality, generally text. We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text. We demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS can generate human quality descriptions consistent with given multimodal inputs.\n## Transfer Learning in Multimodal Models\nLanguage representation model BERT (Devlin et al., 2018) demonstrated that transformer models trained with masked language modeling and next sentence prediction objective can lead to state-of-the-art performance for a variety of NLU tasks. VilBERT (Lu et al., 2019) extended the approach to-wards multimodality with separate transformer streams for image and text with cross-modality interaction though coattention between the two streams. Other methods (Li et al., 2020;Su et al., 2019b;Chen et al., 2020;Li et al., 2019) showed that single stream transformer models can learn the relationship between image and text. These models are pretrained on vision and language data, however Kiela et al. (2019) proposed a different approach where a pretrained unimodal (text) BERT model is fine-tuned together with a different modality (image), skipping the multimodal pretraining step. These methods are effective for understanding tasks like classification, but have not been studied for multimodal conditional generative tasks.",
            "reference_string": "[237417089 | Sollami et al. | 2021 | Citations: 10]"
        },
        {
            "title": "Text-centric Alignment for Multi-Modality Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3328096",
                    "name": "Yun-Da Tsai"
                },
                {
                    "authorId": "2283934604",
                    "name": "Ting-Yu Yen"
                },
                {
                    "authorId": "2256994069",
                    "name": "Pei-Fu Guo"
                },
                {
                    "authorId": "2283982736",
                    "name": "Zhe-Yan Li"
                },
                {
                    "authorId": "2237949904",
                    "name": "Shou-De Lin"
                }
            ],
            "abstract": "This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.",
            "corpus_id": 267637150,
            "sentences": [
                {
                    "corpus_id": "267637150",
                    "title": "Text-centric Alignment for Multi-Modality Learning",
                    "text": "Recent advancements in large-scale foundation models have significantly enhanced content generation capabilities across various modalities. These developments span a wide range of applications, including text-to-image [35,37], text-to-video [39], audioto-image [20], text-to-speech [36], speech-to-gesture [4], speakerto-listener [30], language-to-pose [5], and even in the realms of speech and music generation [31]. Nevertheless, aligning the semantic spaces of independently trained foundation models poses a significant challenge, hindering the ability of downstream models to seamlessly switch between upstream modalities at test time. \n\nExtending beyond single-modality applications, large multimodal language models (MLLMs) have shown remarkable proficiency in both reasoning and generation tasks [48]. For instance, Flamingo [7] employs a vision encoder for feature extraction from images on top of the transformer backbone. On the other hand, Kosmos-2 [34] is comprised of a vision encoder, a linear projector, and a base transformer. These models directly concatenate image and text features, using unique tokens for differentiation. However, despite the impressive capabilities of MLLMs in handling multimodal data, acquiring large-scale datasets encompassing multiple modalities remains a significant challenge. Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input.",
                    "score": 0.5461758514615326,
                    "section_title": "RELATED WORKS 2.1 Multimodal Foundation Models",
                    "char_start_offset": 8315,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 640
                        },
                        {
                            "start": 643,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1143
                        },
                        {
                            "start": 1144,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1848
                        },
                        {
                            "start": 1849,
                            "end": 1950
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 218,
                            "end": 222,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 222,
                            "end": 225,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 261,
                            "end": 265,
                            "matchedPaperCorpusId": "61156031"
                        },
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "162169005"
                        },
                        {
                            "start": 306,
                            "end": 309,
                            "matchedPaperCorpusId": "220769219"
                        },
                        {
                            "start": 330,
                            "end": 334,
                            "matchedPaperCorpusId": "248227839"
                        },
                        {
                            "start": 353,
                            "end": 356,
                            "matchedPaperCorpusId": "195776094"
                        },
                        {
                            "start": 412,
                            "end": 416,
                            "matchedPaperCorpusId": "27706557"
                        },
                        {
                            "start": 833,
                            "end": 836,
                            "matchedPaperCorpusId": "248476411"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95166015625
                }
            ],
            "relevance_judgement": 0.95166015625,
            "relevance_judgment_input_expanded": "# Title: Text-centric Alignment for Multi-Modality Learning\n# Venue: arXiv.org\n# Authors: Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin\n## Abstract\nThis research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availability is dynamic and uncertain.\n## RELATED WORKS 2.1 Multimodal Foundation Models\nRecent advancements in large-scale foundation models have significantly enhanced content generation capabilities across various modalities. These developments span a wide range of applications, including text-to-image [35,37], text-to-video [39], audioto-image [20], text-to-speech [36], speech-to-gesture [4], speakerto-listener [30], language-to-pose [5], and even in the realms of speech and music generation [31]. Nevertheless, aligning the semantic spaces of independently trained foundation models poses a significant challenge, hindering the ability of downstream models to seamlessly switch between upstream modalities at test time. \n\nExtending beyond single-modality applications, large multimodal language models (MLLMs) have shown remarkable proficiency in both reasoning and generation tasks [48]. For instance, Flamingo [7] employs a vision encoder for feature extraction from images on top of the transformer backbone. On the other hand, Kosmos-2 [34] is comprised of a vision encoder, a linear projector, and a base transformer. These models directly concatenate image and text features, using unique tokens for differentiation. However, despite the impressive capabilities of MLLMs in handling multimodal data, acquiring large-scale datasets encompassing multiple modalities remains a significant challenge. Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input.",
            "reference_string": "[267637150 | Tsai et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 31,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.15564",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.15564, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185505465",
                    "name": "Emanuele Aiello"
                },
                {
                    "authorId": "49297123",
                    "name": "L. Yu"
                },
                {
                    "authorId": "2247227174",
                    "name": "Yixin Nie"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                }
            ],
            "abstract": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
            "corpus_id": 262898422,
            "sentences": [
                {
                    "corpus_id": "262898422",
                    "title": "Jointly Training Large Autoregressive Multimodal Models",
                    "text": "Autoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
                    "score": 0.6266407443813204,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1130
                        },
                        {
                            "start": 1133,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1723
                        },
                        {
                            "start": 1724,
                            "end": 1926
                        },
                        {
                            "start": 1929,
                            "end": 1973
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 252,
                            "end": 273,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 442,
                            "end": 462,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 697,
                            "end": 723,
                            "matchedPaperCorpusId": "235658331"
                        },
                        {
                            "start": 725,
                            "end": 746,
                            "matchedPaperCorpusId": "252199400"
                        },
                        {
                            "start": 748,
                            "end": 765,
                            "matchedPaperCorpusId": "246411402"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95068359375
                }
            ],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Jointly Training Large Autoregressive Multimodal Models\n# Venue: International Conference on Learning Representations\n# Authors: Emanuele Aiello, L. Yu, Yixin Nie, Armen Aghajanyan, Barlas O\u011fuz\n## Abstract\nIn recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.\n## INTRODUCTION\nAutoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
            "reference_string": "[262898422 | Aiello et al. | 2023 | Citations: 31]"
        },
        {
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 82,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03651, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2248946985",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2258958963",
                    "name": "Weijia Mao"
                },
                {
                    "authorId": "2344762475",
                    "name": "Mike Zheng Shou"
                }
            ],
            "abstract": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.",
            "corpus_id": 276782523,
            "sentences": [
                {
                    "corpus_id": "276782523",
                    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
                    "text": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
                    "score": 0.5439812090857853,
                    "section_title": "Unified Multimodal Generative Models",
                    "char_start_offset": 6395,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1413
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 234,
                            "end": 236,
                            "matchedPaperCorpusId": "262898422"
                        },
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 242,
                            "end": 245,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 373,
                            "end": 377,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 517,
                            "end": 521,
                            "matchedPaperCorpusId": "261975252"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles\n# Venue: arXiv.org\n# Authors: Rui Zhao, Weijia Mao, Mike Zheng Shou\n## Abstract\nAdapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle.\n## Unified Multimodal Generative Models\nUnified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
            "reference_string": "[276782523 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144484147",
                    "name": "Rongyao Fang"
                },
                {
                    "authorId": "2326302007",
                    "name": "Chengqi Duan"
                },
                {
                    "authorId": "2267252383",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2274232642",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2274118690",
                    "name": "Hao Tian"
                },
                {
                    "authorId": "2257433221",
                    "name": "Xingyu Zeng"
                },
                {
                    "authorId": "2319817758",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2292283383",
                    "name": "Jifeng Dai"
                },
                {
                    "authorId": "2268799275",
                    "name": "Hongsheng Li"
                },
                {
                    "authorId": "2286520347",
                    "name": "Xihui Liu"
                }
            ],
            "abstract": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "corpus_id": 273403986,
            "sentences": [
                {
                    "corpus_id": "273403986",
                    "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
                    "text": "Unifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
                    "score": 0.5626860543289481,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 570
                        },
                        {
                            "start": 573,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1887
                        },
                        {
                            "start": 1888,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2275
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 212,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: PUMA: Empowering Unified MLLM with Multi-granular Visual Generation\n# Venue: arXiv.org\n# Authors: Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, Xihui Liu\n## Abstract\nRecent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.\n## INTRODUCTION\nUnifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
            "reference_string": "[273403986 | Fang et al. | 2024 | Citations: 15]"
        },
        {
            "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.19900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352021201",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2352021017",
                    "name": "Zhuokai Zhao"
                },
                {
                    "authorId": "2334461697",
                    "name": "Shen Yan"
                },
                {
                    "authorId": "2352020894",
                    "name": "Lukasz Korycki"
                },
                {
                    "authorId": "2352039047",
                    "name": "Jianyu Wang"
                },
                {
                    "authorId": "2334709723",
                    "name": "Baosheng He"
                },
                {
                    "authorId": "2340394857",
                    "name": "Jiayi Liu"
                },
                {
                    "authorId": "2340399373",
                    "name": "Lizhu Zhang"
                },
                {
                    "authorId": "2323788930",
                    "name": "Xiangjun Fan"
                },
                {
                    "authorId": "2352014487",
                    "name": "Hanchao Yu"
                }
            ],
            "abstract": "The rapid advancement of large vision-language models (LVLMs) has driven significant progress in multimodal tasks, enabling models to interpret, reason, and generate outputs across both visual and textual domains. While excelling in generative tasks, existing LVLMs often face limitations in tasks requiring high-fidelity representation learning, such as generating image or text embeddings for retrieval. Recent work has proposed finetuning LVLMs for representational learning, but the fine-tuned model often loses its generative capabilities due to the representational learning training paradigm. To address this trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning framework that enhances LVLMs for both representation and generative tasks. By integrating a contrastive objective with autoregressive language modeling, our approach unifies these traditionally separate tasks, achieving state-of-the-art results in both multimodal retrieval and multimodal generative benchmarks, including object hallucination (OH) mitigation. CAFe establishes a novel framework that synergizes embedding and generative functionalities in a single model, setting a foundation for future multimodal models that excel in both retrieval precision and coherent output generation.",
            "corpus_id": 277313749,
            "sentences": [
                {
                    "corpus_id": "277313749",
                    "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
                    "text": "Recent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023d), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2024c), which integrate additional encoders into textual LLMs to process inputs from other modalities. A typical MLLM architecture consists of an LLM, a modality encoder, and a projector that connects them. For example, LLaVA (Liu et al., 2024c) leverages a pretrained LLM and a pretrained vision encoder, followed by a two-stage training process. In the first stage, the model aligns text and image inputs using image-text pairs, training only the projector between the LLM and the modality encoder. The second stage involves fine-tuning the model on a visual instruction tuning dataset for improved instruction following ability. \n\nState-of-the-art MLLMs have demonstrated excellent performance in various vision scenarios, including single-image (Dai et al., 2024;Zhu et al., 2023;Zhang et al., 2024e), multi-image (Jiang et al., 2024a;Li et al., 2023c,b), and video settings (Li et al., 2025;Lin et al., 2023;Zhang et al., 2024f). Furthermore, models like LLaVA-OneVision (Li et al., 2024a) have achieved state-of-the-art results across a broad range of tasks and in all three vision scenarios (Li et al., 2024b;Zhang et al., 2024d). Despite these success, the use of MLLMs for enhancing text and image understanding in multimodal representations remains relatively under-explored. \n\nTo address this gap, we leverage a MLLM for unified representation learning and language generation by introducing a contrastive-autoregressive fine-tuning framework.",
                    "score": 0.5726979128417279,
                    "section_title": "Multimodal Large Language Models",
                    "char_start_offset": 5630,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 772
                        },
                        {
                            "start": 773,
                            "end": 903
                        },
                        {
                            "start": 906,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1557
                        },
                        {
                            "start": 1560,
                            "end": 1726
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 196,
                            "end": 214,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 225,
                            "end": 247,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 259,
                            "end": 278,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 498,
                            "end": 516,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1021,
                            "end": 1039,
                            "matchedPaperCorpusId": "258615266"
                        },
                        {
                            "start": 1056,
                            "end": 1076,
                            "matchedPaperCorpusId": "271745829"
                        },
                        {
                            "start": 1151,
                            "end": 1168,
                            "matchedPaperCorpusId": "265466723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.943359375
                }
            ],
            "relevance_judgement": 0.943359375,
            "relevance_judgment_input_expanded": "# Title: CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning\n# Venue: arXiv.org\n# Authors: Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, Hanchao Yu\n## Abstract\nThe rapid advancement of large vision-language models (LVLMs) has driven significant progress in multimodal tasks, enabling models to interpret, reason, and generate outputs across both visual and textual domains. While excelling in generative tasks, existing LVLMs often face limitations in tasks requiring high-fidelity representation learning, such as generating image or text embeddings for retrieval. Recent work has proposed finetuning LVLMs for representational learning, but the fine-tuned model often loses its generative capabilities due to the representational learning training paradigm. To address this trade-off, we introduce CAFe, a contrastive-autoregressive fine-tuning framework that enhances LVLMs for both representation and generative tasks. By integrating a contrastive objective with autoregressive language modeling, our approach unifies these traditionally separate tasks, achieving state-of-the-art results in both multimodal retrieval and multimodal generative benchmarks, including object hallucination (OH) mitigation. CAFe establishes a novel framework that synergizes embedding and generative functionalities in a single model, setting a foundation for future multimodal models that excel in both retrieval precision and coherent output generation.\n## Multimodal Large Language Models\nRecent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023d), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2024c), which integrate additional encoders into textual LLMs to process inputs from other modalities. A typical MLLM architecture consists of an LLM, a modality encoder, and a projector that connects them. For example, LLaVA (Liu et al., 2024c) leverages a pretrained LLM and a pretrained vision encoder, followed by a two-stage training process. In the first stage, the model aligns text and image inputs using image-text pairs, training only the projector between the LLM and the modality encoder. The second stage involves fine-tuning the model on a visual instruction tuning dataset for improved instruction following ability. \n\nState-of-the-art MLLMs have demonstrated excellent performance in various vision scenarios, including single-image (Dai et al., 2024;Zhu et al., 2023;Zhang et al., 2024e), multi-image (Jiang et al., 2024a;Li et al., 2023c,b), and video settings (Li et al., 2025;Lin et al., 2023;Zhang et al., 2024f). Furthermore, models like LLaVA-OneVision (Li et al., 2024a) have achieved state-of-the-art results across a broad range of tasks and in all three vision scenarios (Li et al., 2024b;Zhang et al., 2024d). Despite these success, the use of MLLMs for enhancing text and image understanding in multimodal representations remains relatively under-explored. \n\nTo address this gap, we leverage a MLLM for unified representation learning and language generation by introducing a contrastive-autoregressive fine-tuning framework.",
            "reference_string": "[277313749 | Yu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
            "venue": "Machine Learning with Applications",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12293, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309151434",
                    "name": "Averi Bates"
                },
                {
                    "authorId": "2363651648",
                    "name": "Ryan Vavricka"
                },
                {
                    "authorId": "2350512740",
                    "name": "Shane Carleton"
                },
                {
                    "authorId": "2350505892",
                    "name": "Ruosi Shao"
                },
                {
                    "authorId": "2084643963",
                    "name": "Chongle Pan"
                }
            ],
            "abstract": null,
            "corpus_id": 277065986,
            "sentences": [
                {
                    "corpus_id": "277065986",
                    "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
                    "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
                    "score": 0.5507874225955918,
                    "section_title": "Addressing the Problem",
                    "char_start_offset": 4189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 655
                        },
                        {
                            "start": 656,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1429
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 253,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                }
            ],
            "relevance_judgement": 0.94287109375,
            "relevance_judgment_input_expanded": "# Title: Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models\n# Venue: Machine Learning with Applications\n# Authors: Averi Bates, Ryan Vavricka, Shane Carleton, Ruosi Shao, Chongle Pan\n## Abstract\nNone\n## Addressing the Problem\nLarge Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
            "reference_string": "[277065986 | Bates et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Recognition-Synergistic Scene Text Editing",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313679002",
                    "name": "Zhengyao Fang"
                },
                {
                    "authorId": "10344582",
                    "name": "Pengyuan Lyu"
                },
                {
                    "authorId": "2313920076",
                    "name": "Jingjing Wu"
                },
                {
                    "authorId": "2248958848",
                    "name": "Chengquan Zhang"
                },
                {
                    "authorId": "2349736964",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2261251059",
                    "name": "Guan-Lun Lu"
                },
                {
                    "authorId": "1678473",
                    "name": "Wenjie Pei"
                }
            ],
            "abstract": "Scene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.",
            "corpus_id": 276929408,
            "sentences": [
                {
                    "corpus_id": "276929408",
                    "title": "Recognition-Synergistic Scene Text Editing",
                    "text": "In response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
                    "score": 0.5152441020817581,
                    "section_title": "MLLM for Image Generation and Editing",
                    "char_start_offset": 6474,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 757
                        },
                        {
                            "start": 758,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1336
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 97,
                            "end": 99,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 470,
                            "end": 474,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 583,
                            "end": 586,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 966,
                            "end": 969,
                            "matchedPaperCorpusId": "253581213"
                        },
                        {
                            "start": 972,
                            "end": 975,
                            "matchedPaperCorpusId": "253581838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93994140625
                }
            ],
            "relevance_judgement": 0.93994140625,
            "relevance_judgment_input_expanded": "# Title: Recognition-Synergistic Scene Text Editing\n# Venue: arXiv.org\n# Authors: Zhengyao Fang, Pengyuan Lyu, Jingjing Wu, Chengquan Zhang, Jun Yu, Guan-Lun Lu, Wenjie Pei\n## Abstract\nScene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code is available at https://github.com/ZhengyaoFang/RS-STE.\n## MLLM for Image Generation and Editing\nIn response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
            "reference_string": "[276929408 | Fang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.06646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319817038",
                    "name": "Alessandro Serra"
                },
                {
                    "authorId": "2284691756",
                    "name": "Francesco Ortu"
                },
                {
                    "authorId": "2334479581",
                    "name": "Emanuele Panizon"
                },
                {
                    "authorId": "108193025",
                    "name": "L. Valeriani"
                },
                {
                    "authorId": "2308039508",
                    "name": "Lorenzo Basile"
                },
                {
                    "authorId": "5366502",
                    "name": "A. Ansuini"
                },
                {
                    "authorId": "104350259",
                    "name": "Diego Doimo"
                },
                {
                    "authorId": "2269294135",
                    "name": "Alberto Cazzaniga"
                }
            ],
            "abstract": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.",
            "corpus_id": 274597781,
            "sentences": [
                {
                    "corpus_id": "274597781",
                    "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
                    "text": "The rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
                    "score": 0.698192980451817,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 119,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1218
                        },
                        {
                            "start": 1221,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1515
                        },
                        {
                            "start": 1516,
                            "end": 1728
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 30,
                            "end": 33,
                            "matchedPaperCorpusId": "237091588"
                        },
                        {
                            "start": 215,
                            "end": 218,
                            "matchedPaperCorpusId": "6401679"
                        },
                        {
                            "start": 374,
                            "end": 377,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 405,
                            "end": 408,
                            "matchedPaperCorpusId": "244954250"
                        },
                        {
                            "start": 408,
                            "end": 412,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 412,
                            "end": 416,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 543,
                            "end": 547,
                            "matchedPaperCorpusId": "263605855"
                        },
                        {
                            "start": 682,
                            "end": 686,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 690,
                            "end": 694,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 874,
                            "end": 878,
                            "matchedPaperCorpusId": "261975252"
                        },
                        {
                            "start": 1302,
                            "end": 1306,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 1504,
                            "end": 1508,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 1511,
                            "end": 1514,
                            "matchedPaperCorpusId": "261696650"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.939453125,
            "relevance_judgment_input_expanded": "# Title: The Narrow Gate: Localized Image-Text Communication in Vision-Language Models\n# Venue: arXiv.org\n# Authors: Alessandro Serra, Francesco Ortu, Emanuele Panizon, L. Valeriani, Lorenzo Basile, A. Ansuini, Diego Doimo, Alberto Cazzaniga\n## Abstract\nRecent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.\n## Introduction\nThe rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
            "reference_string": "[274597781 | Serra et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114710333",
                    "name": "Samyadeep Basu"
                },
                {
                    "authorId": "2052886960",
                    "name": "Martin Grayson"
                },
                {
                    "authorId": "121927341",
                    "name": "C. Morrison"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                },
                {
                    "authorId": "2286878667",
                    "name": "Daniela Massiceti"
                }
            ],
            "abstract": "Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by the director in this photo has won a Golden Globe?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.",
            "corpus_id": 270285703,
            "sentences": [
                {
                    "corpus_id": "270285703",
                    "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
                    "text": "Multimodal Large Language Models.. We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2].Over the last year, such models have made tremendous advances in tasks like VQA and image captioning, including BLIP [15], BLIP-2 [16], Instruct-BLIP [7], LLaVA [18,19], Flamingo [3] and multi-modal Phi-2 (from the Bunny repo) [11].These MLLMs can broadly be categorized into two families based on how their visual information is integrated into the language model: (i) by embedding the vision encoder's output into each layer of the language model with a cross-attention layer (e.g., Flamingo, BLIP) or, (ii) by mapping the vision encoder's output into \"visual tokens\" in the language model's input space (i.e.alongside the text tokens) via a projection layer (e.g., LLaVA, Bunny).Both families are widely used, however, the projection layer family has recently shown stronger performance on popular benchmark [18,19,11].We, therefore, focus our study of information storage and transfer on this model family.",
                    "score": 0.6180929562840589,
                    "section_title": "Related Works",
                    "char_start_offset": 4494,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 139,
                            "end": 371
                        },
                        {
                            "start": 371,
                            "end": 750
                        },
                        {
                            "start": 750,
                            "end": 821
                        },
                        {
                            "start": 821,
                            "end": 961
                        },
                        {
                            "start": 961,
                            "end": 1049
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 269,
                            "end": 273,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 289,
                            "end": 292,
                            "matchedPaperCorpusId": "258615266"
                        },
                        {
                            "start": 304,
                            "end": 307,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 318,
                            "end": 321,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 954,
                            "end": 957,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.939453125,
            "relevance_judgment_input_expanded": "# Title: Understanding Information Storage and Transfer in Multi-modal Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Samyadeep Basu, Martin Grayson, C. Morrison, Besmira Nushi, S. Feizi, Daniela Massiceti\n## Abstract\nUnderstanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by the director in this photo has won a Golden Globe?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.\n## Related Works\nMultimodal Large Language Models.. We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2].Over the last year, such models have made tremendous advances in tasks like VQA and image captioning, including BLIP [15], BLIP-2 [16], Instruct-BLIP [7], LLaVA [18,19], Flamingo [3] and multi-modal Phi-2 (from the Bunny repo) [11].These MLLMs can broadly be categorized into two families based on how their visual information is integrated into the language model: (i) by embedding the vision encoder's output into each layer of the language model with a cross-attention layer (e.g., Flamingo, BLIP) or, (ii) by mapping the vision encoder's output into \"visual tokens\" in the language model's input space (i.e.alongside the text tokens) via a projection layer (e.g., LLaVA, Bunny).Both families are widely used, however, the projection layer family has recently shown stronger performance on popular benchmark [18,19,11].We, therefore, focus our study of information storage and transfer on this model family.",
            "reference_string": "[270285703 | Basu et al. | 2024 | Citations: 12]"
        },
        {
            "title": "When Large Language Models Meet Vector Databases: A Survey",
            "venue": "2025 Conference on Artificial Intelligence x Multimedia (AIxMM)",
            "year": 2024,
            "reference_count": 110,
            "citation_count": 38,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "15217571",
                    "name": "Zhi Jing"
                },
                {
                    "authorId": "2278407298",
                    "name": "Yongye Su"
                },
                {
                    "authorId": "2260277757",
                    "name": "Yikun Han"
                },
                {
                    "authorId": "2282533661",
                    "name": "Bo Yuan"
                },
                {
                    "authorId": "2260396712",
                    "name": "Haiyun Xu"
                },
                {
                    "authorId": "2260828894",
                    "name": "Chunjiang Liu"
                },
                {
                    "authorId": "2266796043",
                    "name": "Kehai Chen"
                },
                {
                    "authorId": "2267153486",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration\u2019s impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.",
            "corpus_id": 267412060,
            "sentences": [
                {
                    "corpus_id": "267412060",
                    "title": "When Large Language Models Meet Vector Databases: A Survey",
                    "text": "RAG has now evolved to handle a wide range of data types by lending the power of multimodal models. The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models. BLIP-2 [39] uses static image encoders with LLMs for efficient visual language pre-training, facilitating direct image-to-text transformations. Flamingo [40] presented a visual language model for text generation, showcasing remarkable adaptability and leading performance across various vision-and-language tasks. CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text. FROMAGe [42] gains robust multimodal capabilities for few-shot learning solely from image-caption pairs, unlike other models that necessitate large-scale, interwoven imagetext data from the websites. \n\nTo import speech data to RAG systems, Wav2Seq [43] allows for efficient pre-training without the need for transcriptions, using techniques like k-means clustering and bytepair encoding to generate pseudo-subwords from speech. The Large Language and Speech Model (LLaSM) [44] is an end-to-end trained, large multi-modal speech-language model equipped with cross-modal conversational skills and proficient in understanding and responding to combined speech-andlanguage directives. Videos are also made available for certain types of RAG systems. Vid2Seq [45] enhances language models with specific temporal indicators for predicting event limits and textual descriptions in a single output sequence.",
                    "score": 0.5395023044144526,
                    "section_title": "A. Multimodality of RAG",
                    "char_start_offset": 21643,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1152
                        },
                        {
                            "start": 1155,
                            "end": 1380
                        },
                        {
                            "start": 1381,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1698
                        },
                        {
                            "start": 1699,
                            "end": 1852
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 961,
                            "end": 965,
                            "matchedPaperCorpusId": "258947258"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: When Large Language Models Meet Vector Databases: A Survey\n# Venue: 2025 Conference on Artificial Intelligence x Multimedia (AIxMM)\n# Authors: Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, Min Zhang\n## Abstract\nThis survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration\u2019s impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.\n## A. Multimodality of RAG\nRAG has now evolved to handle a wide range of data types by lending the power of multimodal models. The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models. BLIP-2 [39] uses static image encoders with LLMs for efficient visual language pre-training, facilitating direct image-to-text transformations. Flamingo [40] presented a visual language model for text generation, showcasing remarkable adaptability and leading performance across various vision-and-language tasks. CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text. FROMAGe [42] gains robust multimodal capabilities for few-shot learning solely from image-caption pairs, unlike other models that necessitate large-scale, interwoven imagetext data from the websites. \n\nTo import speech data to RAG systems, Wav2Seq [43] allows for efficient pre-training without the need for transcriptions, using techniques like k-means clustering and bytepair encoding to generate pseudo-subwords from speech. The Large Language and Speech Model (LLaSM) [44] is an end-to-end trained, large multi-modal speech-language model equipped with cross-modal conversational skills and proficient in understanding and responding to combined speech-andlanguage directives. Videos are also made available for certain types of RAG systems. Vid2Seq [45] enhances language models with specific temporal indicators for predicting event limits and textual descriptions in a single output sequence.",
            "reference_string": "[267412060 | Jing et al. | 2024 | Citations: 38]"
        },
        {
            "title": "Controlling for Stereotypes in Multimodal Language Model Evaluation",
            "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
            "year": 2023,
            "reference_count": 28,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.01582",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.01582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203794234",
                    "name": "Manuj Malik"
                },
                {
                    "authorId": "145341661",
                    "name": "Richard Johansson"
                }
            ],
            "abstract": "We propose a methodology and design two benchmark sets for measuring to what extent language-and-vision language models use the visual signal in the presence or absence of stereotypes. The first benchmark is designed to test for stereotypical colors of common objects, while the second benchmark considers gender stereotypes. The key idea is to compare predictions when the image conforms to the stereotype to predictions when it does not.Our results show that there is significant variation among multimodal models: the recent Transformer-based FLAVA seems to be more sensitive to the choice of image and less affected by stereotypes than older CNN-based models such as VisualBERT and LXMERT. This effect is more discernible in this type of controlled setting than in traditional evaluations where we do not know whether the model relied on the stereotype or the visual signal.",
            "corpus_id": 256461099,
            "sentences": [
                {
                    "corpus_id": "256461099",
                    "title": "Controlling for Stereotypes in Multimodal Language Model Evaluation",
                    "text": "The Transformer (Vaswani et al., 2017) is a sequence-based model that is now the standard architecture in NLP for devising representation and generation components in neural models. Pretrained language models such as BERT (Devlin et al., 2019) based on the architecture of Transformers, have proven capable of learning powerful representations applicable to a wide range of tasks. They have yielded state-of-the-art performance in many downstream tasks.\n\nMultimodal models fusing the textual and visual modalities have been devised by researchers after looking at the huge success of pre-trained language models. In such models, multiple modalities are considered, and data for the training of the models is in multiple modalities. As our research problem revolves around the aspect of multimodality, we will focus on two modalities: a textual and a visual signal. The visual signal is in the form of images, and the natural language is the written text accompanying the images, such as captions or descriptions of the images. Examples of such visual/textual Transformers include VilBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), Vi-sualBERT (Li et al., 2020a), OSCAR (Li et al., 2020b), ImageBERT (Qi et al., 2020), FLAVA (Singh et al., 2022), and others. Most of the earlier models use features extracted from a Faster-RCNN pipeline (Ren et al., 2015), while later models use visual Transformer architectures (Dosovitskiy et al., 2021). These types of models are then trained on datasets that contain text/image pairs such as SBU Captions (Ordonez et al., 2011), MS COCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), and Visual Genome QA (Krishna et al., 2017), using various pre-training tasks. They are sometimes trained from scratch on the combined language/vision data and sometimes warm-started from a unimodal model such as BERT.\n\nFor this study, we selected three different multimodal models to run our experiments on. These image-augmented Transformer models are VisualBERT, L",
                    "score": 0.5289061000944653,
                    "section_title": "Multimodal Language Models",
                    "char_start_offset": 8508,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 16,
                            "end": 37,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 222,
                            "end": 243,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1088,
                            "end": 1105,
                            "matchedPaperCorpusId": "199453025"
                        },
                        {
                            "start": 1114,
                            "end": 1136,
                            "matchedPaperCorpusId": "201103729"
                        },
                        {
                            "start": 1150,
                            "end": 1168,
                            "matchedPaperCorpusId": "218610661"
                        },
                        {
                            "start": 1176,
                            "end": 1194,
                            "matchedPaperCorpusId": "215754208"
                        },
                        {
                            "start": 1231,
                            "end": 1251,
                            "matchedPaperCorpusId": "244954250"
                        },
                        {
                            "start": 1343,
                            "end": 1361,
                            "matchedPaperCorpusId": "10328909"
                        },
                        {
                            "start": 1419,
                            "end": 1445,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 1549,
                            "end": 1571,
                            "matchedPaperCorpusId": "14579301"
                        },
                        {
                            "start": 1581,
                            "end": 1599,
                            "matchedPaperCorpusId": "14113767"
                        },
                        {
                            "start": 1621,
                            "end": 1642,
                            "matchedPaperCorpusId": "51876975"
                        },
                        {
                            "start": 1665,
                            "end": 1687,
                            "matchedPaperCorpusId": "4492210"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: Controlling for Stereotypes in Multimodal Language Model Evaluation\n# Venue: BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\n# Authors: Manuj Malik, Richard Johansson\n## Abstract\nWe propose a methodology and design two benchmark sets for measuring to what extent language-and-vision language models use the visual signal in the presence or absence of stereotypes. The first benchmark is designed to test for stereotypical colors of common objects, while the second benchmark considers gender stereotypes. The key idea is to compare predictions when the image conforms to the stereotype to predictions when it does not.Our results show that there is significant variation among multimodal models: the recent Transformer-based FLAVA seems to be more sensitive to the choice of image and less affected by stereotypes than older CNN-based models such as VisualBERT and LXMERT. This effect is more discernible in this type of controlled setting than in traditional evaluations where we do not know whether the model relied on the stereotype or the visual signal.\n## Multimodal Language Models\nThe Transformer (Vaswani et al., 2017) is a sequence-based model that is now the standard architecture in NLP for devising representation and generation components in neural models. Pretrained language models such as BERT (Devlin et al., 2019) based on the architecture of Transformers, have proven capable of learning powerful representations applicable to a wide range of tasks. They have yielded state-of-the-art performance in many downstream tasks.\n\nMultimodal models fusing the textual and visual modalities have been devised by researchers after looking at the huge success of pre-trained language models. In such models, multiple modalities are considered, and data for the training of the models is in multiple modalities. As our research problem revolves around the aspect of multimodality, we will focus on two modalities: a textual and a visual signal. The visual signal is in the form of images, and the natural language is the written text accompanying the images, such as captions or descriptions of the images. Examples of such visual/textual Transformers include VilBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), Vi-sualBERT (Li et al., 2020a), OSCAR (Li et al., 2020b), ImageBERT (Qi et al., 2020), FLAVA (Singh et al., 2022), and others. Most of the earlier models use features extracted from a Faster-RCNN pipeline (Ren et al., 2015), while later models use visual Transformer architectures (Dosovitskiy et al., 2021). These types of models are then trained on datasets that contain text/image pairs such as SBU Captions (Ordonez et al., 2011), MS COCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), and Visual Genome QA (Krishna et al., 2017), using various pre-training tasks. They are sometimes trained from scratch on the combined language/vision data and sometimes warm-started from a unimodal model such as BERT.\n\nFor this study, we selected three different multimodal models to run our experiments on. These image-augmented Transformer models are VisualBERT, L",
            "reference_string": "[256461099 | Malik et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 91,
            "citation_count": 116,
            "influential_citation_count": 10,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277988832",
                    "name": "Chengyue Wu"
                },
                {
                    "authorId": "2326444644",
                    "name": "Xi-aokang Chen"
                },
                {
                    "authorId": "2326514370",
                    "name": "Zhiyu Wu"
                },
                {
                    "authorId": "2326805499",
                    "name": "Yiyang Ma"
                },
                {
                    "authorId": "2326998908",
                    "name": "Xingchao Liu"
                },
                {
                    "authorId": "2327000291",
                    "name": "Zizheng Pan"
                },
                {
                    "authorId": "2326440587",
                    "name": "Wen Liu"
                },
                {
                    "authorId": "2279107352",
                    "name": "Zhenda Xie"
                },
                {
                    "authorId": "2279159169",
                    "name": "Xingkai Yu"
                },
                {
                    "authorId": "2278217940",
                    "name": "C. Ruan"
                },
                {
                    "authorId": "2295886816",
                    "name": "Ping Luo"
                }
            ],
            "abstract": "In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.",
            "corpus_id": 273403725,
            "sentences": [
                {
                    "corpus_id": "273403725",
                    "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
                    "text": "Multimodal large language models (MLLMs) integrate both text and images [6,80,81]. By leveraging pretrained LLMs, MLLMs [1,2,12,51,55,82,95] demonstrate a robust ability to understand and process multimodal information. Recent advancements have explored extending MLLMs with pretrained diffusion models to facilitate image generation [27,29,36,75,76]. These methods fall under the category of tool utilization, where diffusion models are used to generate images based on the conditions output by the MLLM, while the MLLM itself does not have the ability to directly perform visual generation. Moreover, the generative ability of the entire system is often constrained by the external diffusion model, making its performance inferior to directly using the diffusion model on its own [27,75].",
                    "score": 0.5616606087193025,
                    "section_title": "Multimodal Understanding",
                    "char_start_offset": 6147,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 82
                        },
                        {
                            "start": 83,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 790
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 128,
                            "end": 131,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 134,
                            "end": 137,
                            "matchedPaperCorpusId": "258762579"
                        },
                        {
                            "start": 347,
                            "end": 350,
                            "matchedPaperCorpusId": "266374640"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9365234375
                }
            ],
            "relevance_judgement": 0.9365234375,
            "relevance_judgment_input_expanded": "# Title: Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation\n# Venue: arXiv.org\n# Authors: Chengyue Wu, Xi-aokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, C. Ruan, Ping Luo\n## Abstract\nIn this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.\n## Multimodal Understanding\nMultimodal large language models (MLLMs) integrate both text and images [6,80,81]. By leveraging pretrained LLMs, MLLMs [1,2,12,51,55,82,95] demonstrate a robust ability to understand and process multimodal information. Recent advancements have explored extending MLLMs with pretrained diffusion models to facilitate image generation [27,29,36,75,76]. These methods fall under the category of tool utilization, where diffusion models are used to generate images based on the conditions output by the MLLM, while the MLLM itself does not have the ability to directly perform visual generation. Moreover, the generative ability of the entire system is often constrained by the external diffusion model, making its performance inferior to directly using the diffusion model on its own [27,75].",
            "reference_string": "[273403725 | Wu et al. | 2024 | Citations: 116]"
        },
        {
            "title": "Few-shot Multimodal Sentiment Analysis Based on Multimodal Probabilistic Fusion Prompts",
            "venue": "ACM Multimedia",
            "year": 2022,
            "reference_count": 40,
            "citation_count": 23,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.06607",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.06607, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135971356",
                    "name": "Xiaocui Yang"
                },
                {
                    "authorId": "2087586948",
                    "name": "Shi Feng"
                },
                {
                    "authorId": "2111226672",
                    "name": "Daling Wang"
                },
                {
                    "authorId": "144656297",
                    "name": "Pengfei Hong"
                },
                {
                    "authorId": "1746416",
                    "name": "Soujanya Poria"
                }
            ],
            "abstract": "Multimodal sentiment analysis has gained significant attention due to the proliferation of multimodal content on social media. However, existing studies in this area rely heavily on large-scale supervised data, which is time-consuming and labor-intensive to collect. Thus, there is a need to address the challenge of few-shot multimodal sentiment analysis. To tackle this problem, we propose a novel method called Multimodal Probabilistic Fusion Prompts (MultiPoint) that leverages diverse cues from different modalities for multimodal sentiment detection in the few-shot scenario. Specifically, we start by introducing a Consistently Distributed Sampling approach called CDS, which ensures that the few-shot dataset has the same category distribution as the full dataset. Unlike previous approaches primarily using prompts based on the text modality, we design unified multimodal prompts to reduce discrepancies between different modalities and dynamically incorporate multimodal demonstrations into the context of each multimodal instance. To enhance the model's robustness, we introduce a probabilistic fusion method to fuse output predictions from multiple diverse prompts for each input. Our extensive experiments on six datasets demonstrate the effectiveness of our approach. First, our method outperforms strong baselines in the multimodal few-shot setting. Furthermore, under the same amount of data (1% of the full dataset), our CDS-based experimental results significantly outperform those based on previously sampled datasets constructed from the same number of instances of each class.",
            "corpus_id": 253510668,
            "sentences": [
                {
                    "corpus_id": "253510668",
                    "title": "Few-shot Multimodal Sentiment Analysis Based on Multimodal Probabilistic Fusion Prompts",
                    "text": "In the field of Natural Language Processing (NLP), prompt-based language modeling has emerged as a powerful approach for solving different few-shot tasks using pre-trained language models (PLM) [15]. Prompt-based methods treat the classification task as a masked language modeling (MLM) task, where the model is fine-tuned with a set of prompts to guide its predictions. In the beginning, promptbased approaches are introduced to handle text few-shot classification task, including LM-BFF [6], LM-SC [9], and so on. Ehsan et al. [7] propose a generative language model (GFSC) that reformulates the task as a language generation problem for text classification. However, the above-mentioned models only handle text-related tasks. Recently, there has been an increasing interest in designing models to handle few-shot multimodal tasks. Existing models for few-shot multimodal tasks, such as Frozen [22], PVLM [30], and UP-MPF [31], primarily rely on introducing image tokens to a pretrained language model for prompt-based fine-tuning. However, these approaches face the challenge of discrepancy between different modalities since image features are agnostic to language models. To this end, we propose a novel unified multimodal prompt that allows for the joint processing of both text and image modalities in a coherent manner.",
                    "score": 0.5212317757648631,
                    "section_title": "Few-shot Learning with PLM",
                    "char_start_offset": 9750,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1327
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 489,
                            "end": 492,
                            "matchedPaperCorpusId": "229923710"
                        },
                        {
                            "start": 500,
                            "end": 503,
                            "matchedPaperCorpusId": "248506039"
                        },
                        {
                            "start": 529,
                            "end": 532,
                            "matchedPaperCorpusId": "248119039"
                        },
                        {
                            "start": 907,
                            "end": 911,
                            "matchedPaperCorpusId": "251848020"
                        },
                        {
                            "start": 924,
                            "end": 928,
                            "matchedPaperCorpusId": "252782278"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93505859375
                }
            ],
            "relevance_judgement": 0.93505859375,
            "relevance_judgment_input_expanded": "# Title: Few-shot Multimodal Sentiment Analysis Based on Multimodal Probabilistic Fusion Prompts\n# Venue: ACM Multimedia\n# Authors: Xiaocui Yang, Shi Feng, Daling Wang, Pengfei Hong, Soujanya Poria\n## Abstract\nMultimodal sentiment analysis has gained significant attention due to the proliferation of multimodal content on social media. However, existing studies in this area rely heavily on large-scale supervised data, which is time-consuming and labor-intensive to collect. Thus, there is a need to address the challenge of few-shot multimodal sentiment analysis. To tackle this problem, we propose a novel method called Multimodal Probabilistic Fusion Prompts (MultiPoint) that leverages diverse cues from different modalities for multimodal sentiment detection in the few-shot scenario. Specifically, we start by introducing a Consistently Distributed Sampling approach called CDS, which ensures that the few-shot dataset has the same category distribution as the full dataset. Unlike previous approaches primarily using prompts based on the text modality, we design unified multimodal prompts to reduce discrepancies between different modalities and dynamically incorporate multimodal demonstrations into the context of each multimodal instance. To enhance the model's robustness, we introduce a probabilistic fusion method to fuse output predictions from multiple diverse prompts for each input. Our extensive experiments on six datasets demonstrate the effectiveness of our approach. First, our method outperforms strong baselines in the multimodal few-shot setting. Furthermore, under the same amount of data (1% of the full dataset), our CDS-based experimental results significantly outperform those based on previously sampled datasets constructed from the same number of instances of each class.\n## Few-shot Learning with PLM\nIn the field of Natural Language Processing (NLP), prompt-based language modeling has emerged as a powerful approach for solving different few-shot tasks using pre-trained language models (PLM) [15]. Prompt-based methods treat the classification task as a masked language modeling (MLM) task, where the model is fine-tuned with a set of prompts to guide its predictions. In the beginning, promptbased approaches are introduced to handle text few-shot classification task, including LM-BFF [6], LM-SC [9], and so on. Ehsan et al. [7] propose a generative language model (GFSC) that reformulates the task as a language generation problem for text classification. However, the above-mentioned models only handle text-related tasks. Recently, there has been an increasing interest in designing models to handle few-shot multimodal tasks. Existing models for few-shot multimodal tasks, such as Frozen [22], PVLM [30], and UP-MPF [31], primarily rely on introducing image tokens to a pretrained language model for prompt-based fine-tuning. However, these approaches face the challenge of discrepancy between different modalities since image features are agnostic to language models. To this end, we propose a novel unified multimodal prompt that allows for the joint processing of both text and image modalities in a coherent manner.",
            "reference_string": "[253510668 | Yang et al. | 2022 | Citations: 23]"
        },
        {
            "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 51,
            "citation_count": 116,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00385/1929720/tacl_a_00385.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.00529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2234342",
                    "name": "Lisa Anne Hendricks"
                },
                {
                    "authorId": "1386957852",
                    "name": "John F. J. Mellor"
                },
                {
                    "authorId": "145721402",
                    "name": "R. Schneider"
                },
                {
                    "authorId": "2285263",
                    "name": "Jean-Baptiste Alayrac"
                },
                {
                    "authorId": "3208081",
                    "name": "Aida Nematzadeh"
                }
            ],
            "abstract": "Abstract Recently, multimodal transformer models have gained popularity because their performance on downstream tasks suggests they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors that can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality-specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers.",
            "corpus_id": 231740629,
            "sentences": [
                {
                    "corpus_id": "231740629",
                    "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
                    "text": "The success of transformer-based language models on a variety of language tasks (e.g., Devlin et al., 2019) has inspired similar multimodal efforts (e.g., Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Li et al., 2020a,b). 2 The main distinction is that image-text multimodal transformers take image-text pairs as input, attend over both modalities, and are trained with additional losses. Similar to the language models, multimodal transformers are often fine-tuned on downstream tasks but multimodal ones; e.g., image retrieval (Young et al., 2014) or visual question answering (Goyal et al., 2017). \n\nWe give a brief overview of the BERT model (Devlin et al., 2019) which forms the backbone of multimodal transformers. The BERT architecture consists of a stack of transformer blocks (Vaswani et al., 2017) and has three main components. First, the input text is tokenized and three embedding functions are used to embed the token, its position in the sentence (i.e., positional encoding), and the sentence it belongs to. The final language embedding is a summation of these three vectors. The BERT model also includes a <SEP> token to separate different sentences and a <CLS> token which can be thought of as an aggregate representation of the input text. Second, the sequence of token embeddings are input into a series of transformer layers where tokens are combined through self-attention. Third, two different losses are applied to the model output: a masked language modeling loss, in which the model predicts a masked word (denoted by a <MASK> token), and a next sentence prediction loss which, given two sentences, predicts if the second sentence follows the first. \n\nMultimodal transformer models facilitate learning from multimodal data via three changes to the BERT architecture: multimodal data preprocessing (more specifically images), adding multimodal attention by changing self-attention such that it combines image and text modalities, and introducing image and multimodal loss functions.",
                    "score": 0.5308672363530664,
                    "section_title": "Multimodal Transformers",
                    "char_start_offset": 4697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 609
                        },
                        {
                            "start": 612,
                            "end": 729
                        },
                        {
                            "start": 730,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1683
                        },
                        {
                            "start": 1686,
                            "end": 2015
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 155,
                            "end": 171,
                            "matchedPaperCorpusId": "199453025"
                        },
                        {
                            "start": 189,
                            "end": 210,
                            "matchedPaperCorpusId": "201103729"
                        },
                        {
                            "start": 538,
                            "end": 558,
                            "matchedPaperCorpusId": "3104920"
                        },
                        {
                            "start": 588,
                            "end": 608,
                            "matchedPaperCorpusId": "8081284"
                        },
                        {
                            "start": 794,
                            "end": 816,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Lisa Anne Hendricks, John F. J. Mellor, R. Schneider, Jean-Baptiste Alayrac, Aida Nematzadeh\n## Abstract\nAbstract Recently, multimodal transformer models have gained popularity because their performance on downstream tasks suggests they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors that can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality-specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers.\n## Multimodal Transformers\nThe success of transformer-based language models on a variety of language tasks (e.g., Devlin et al., 2019) has inspired similar multimodal efforts (e.g., Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Li et al., 2020a,b). 2 The main distinction is that image-text multimodal transformers take image-text pairs as input, attend over both modalities, and are trained with additional losses. Similar to the language models, multimodal transformers are often fine-tuned on downstream tasks but multimodal ones; e.g., image retrieval (Young et al., 2014) or visual question answering (Goyal et al., 2017). \n\nWe give a brief overview of the BERT model (Devlin et al., 2019) which forms the backbone of multimodal transformers. The BERT architecture consists of a stack of transformer blocks (Vaswani et al., 2017) and has three main components. First, the input text is tokenized and three embedding functions are used to embed the token, its position in the sentence (i.e., positional encoding), and the sentence it belongs to. The final language embedding is a summation of these three vectors. The BERT model also includes a <SEP> token to separate different sentences and a <CLS> token which can be thought of as an aggregate representation of the input text. Second, the sequence of token embeddings are input into a series of transformer layers where tokens are combined through self-attention. Third, two different losses are applied to the model output: a masked language modeling loss, in which the model predicts a masked word (denoted by a <MASK> token), and a next sentence prediction loss which, given two sentences, predicts if the second sentence follows the first. \n\nMultimodal transformer models facilitate learning from multimodal data via three changes to the BERT architecture: multimodal data preprocessing (more specifically images), adding multimodal attention by changing self-attention such that it combines image and text modalities, and introducing image and multimodal loss functions.",
            "reference_string": "[231740629 | Hendricks et al. | 2021 | Citations: 116]"
        },
        {
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023,
            "reference_count": 130,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.05983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187404140",
                    "name": "Zhiwei Zhang"
                },
                {
                    "authorId": "2108353180",
                    "name": "Yuliang Liu"
                }
            ],
            "abstract": "The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models.",
            "corpus_id": 259164940,
            "sentences": [
                {
                    "corpus_id": "259164940",
                    "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
                    "text": "\u2022 Multimodal Input and Multimodal Output (MIMO): This task requires the model to perform multimodal reasoning and generation simultaneously (Koh et al., 2023;Jing Yu Koh, 2023;Chenfei Wu & Duan, 2023;Yang et al., 2023a). Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT and a series of pre-trained visual foundation models, allowing them \u00a7 https://github.com/zzw-zwzhang/Awesome-of-Multimodal-Dialogue-Models to accept and produce text and images during textual-visual conversations. GILL (Koh et al., 2023) proposes a mapping network that efficiently maps the output embedding space of a frozen text-only language model to that of a frozen generation model (e.g., Stable Diffusion (Rombach et al., 2022)). This mapping only requires fine-tuning a small number of parameters on image-caption pairs for tasks such as image retrieval, novel image generation, and multimodal dialogue. FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen. GPT4Tools (Yang et al., 2023a) introduces an instruction dataset and extends Visual ChatGPT (Chenfei Wu & Duan, 2023) to the image understanding task.",
                    "score": 0.5663868886717619,
                    "section_title": "Multimodal Dialogue Models",
                    "char_start_offset": 33635,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1253
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 717,
                            "end": 739,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                },
                {
                    "corpus_id": "259164940",
                    "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
                    "text": "Recently, the most important breakthrough was made by ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), which unveiled the emerging potential of the conversation between human and artificial intelligence system. ChatGPT serves as a chatbot that operates with language as both input and output, while GPT-4 is a multimodal model capable of accepting both image and text inputs and producing text outputs. \n\nA successful multimodal generative model should excel in both textual and visual reasoning, generating high-quality text and image feedback. Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT with a series of pre-trained visual foundation models, enabling text-image chat. Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for  The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. \n\n\u2022 We consider the issue of accountability in multimodal generative models by embedding pre-set rules as supervised signals in our datasets. This enables the VLMs to learn to reject human instructions in multimodal conversations. \n\n\u2022 We propose a two-stage training procedure for training the image auto-encoder and auto-regressive transformer, aiming to enable the models to learn how to reject human instructions. All of our models are trained from scratch, and their training took 350\u223c900 GPU days on our constructed datasets. \n\n\u2022 We provide extensive qualitative and quantitative results, evaluating the quality of generated images, the accuracy of answers, and the model's ability to handle uncertainty and incomplete queries. \n\nThe remainder of this paper is organized as follows. Section 2 presents detailed information about our datasets. In Section 3, we introduce our models and training process. We then present extensive experiments and detailed analysis in Section 4 to validate our proposed method. Section 5 provides an overview of related work, and Section 6 concludes the paper. Section 7 pertains to acknowledgments.",
                    "score": 0.539364963489127,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 404
                        },
                        {
                            "start": 407,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 1093
                        },
                        {
                            "start": 1096,
                            "end": 1235
                        },
                        {
                            "start": 1236,
                            "end": 1324
                        },
                        {
                            "start": 1327,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1624
                        },
                        {
                            "start": 1627,
                            "end": 1826
                        },
                        {
                            "start": 1829,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 1941
                        },
                        {
                            "start": 1942,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2190
                        },
                        {
                            "start": 2191,
                            "end": 2229
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8994140625
                }
            ],
            "relevance_judgement": 0.927734375,
            "relevance_judgment_input_expanded": "# Title: Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Zhiwei Zhang, Yuliang Liu\n## Abstract\nThe recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models.\n## Introduction\nRecently, the most important breakthrough was made by ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), which unveiled the emerging potential of the conversation between human and artificial intelligence system. ChatGPT serves as a chatbot that operates with language as both input and output, while GPT-4 is a multimodal model capable of accepting both image and text inputs and producing text outputs. \n\nA successful multimodal generative model should excel in both textual and visual reasoning, generating high-quality text and image feedback. Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT with a series of pre-trained visual foundation models, enabling text-image chat. Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for  The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. \n\n\u2022 We consider the issue of accountability in multimodal generative models by embedding pre-set rules as supervised signals in our datasets. This enables the VLMs to learn to reject human instructions in multimodal conversations. \n\n\u2022 We propose a two-stage training procedure for training the image auto-encoder and auto-regressive transformer, aiming to enable the models to learn how to reject human instructions. All of our models are trained from scratch, and their training took 350\u223c900 GPU days on our constructed datasets. \n\n\u2022 We provide extensive qualitative and quantitative results, evaluating the quality of generated images, the accuracy of answers, and the model's ability to handle uncertainty and incomplete queries. \n\nThe remainder of this paper is organized as follows. Section 2 presents detailed information about our datasets. In Section 3, we introduce our models and training process. We then present extensive experiments and detailed analysis in Section 4 to validate our proposed method. Section 5 provides an overview of related work, and Section 6 concludes the paper. Section 7 pertains to acknowledgments.\n\n## Multimodal Dialogue Models\n\u2022 Multimodal Input and Multimodal Output (MIMO): This task requires the model to perform multimodal reasoning and generation simultaneously (Koh et al., 2023;Jing Yu Koh, 2023;Chenfei Wu & Duan, 2023;Yang et al., 2023a). Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT and a series of pre-trained visual foundation models, allowing them \u00a7 https://github.com/zzw-zwzhang/Awesome-of-Multimodal-Dialogue-Models to accept and produce text and images during textual-visual conversations. GILL (Koh et al., 2023) proposes a mapping network that efficiently maps the output embedding space of a frozen text-only language model to that of a frozen generation model (e.g., Stable Diffusion (Rombach et al., 2022)). This mapping only requires fine-tuning a small number of parameters on image-caption pairs for tasks such as image retrieval, novel image generation, and multimodal dialogue. FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen. GPT4Tools (Yang et al., 2023a) introduces an instruction dataset and extends Visual ChatGPT (Chenfei Wu & Duan, 2023) to the image understanding task.",
            "reference_string": "[259164940 | Zhang et al. | 2023 | Citations: 0]"
        },
        {
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 145,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.16602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115941903",
                    "name": "Jiaxing Huang"
                },
                {
                    "authorId": "2276743977",
                    "name": "Jingyi Zhang"
                },
                {
                    "authorId": "2276609830",
                    "name": "Kai Jiang"
                },
                {
                    "authorId": "49660254",
                    "name": "Han Qiu"
                },
                {
                    "authorId": "2237947102",
                    "name": "Shijian Lu"
                }
            ],
            "abstract": "Traditional computer vision generally solves each single task independently by a dedicated model with the task instruction implicitly designed in the model architecture, arising two limitations: (1) it leads to task-specific models, which require multiple models for different tasks and restrict the potential synergies from diverse tasks; (2) it leads to a pre-defined and fixed model interface that has limited interactivity and adaptability in following user' task instructions. To address them, Visual Instruction Tuning (VIT) has been intensively studied recently, which finetunes a large vision model with language as task instructions, aiming to learn from a wide range of vision tasks described by language instructions a general-purpose multimodal model that can follow arbitrary instructions and thus solve arbitrary tasks specified by the user. This work aims to provide a systematic review of visual instruction tuning, covering (1) the background that presents computer vision task paradigms and the development of VIT; (2) the foundations of VIT that introduce commonly used network architectures, visual instruction tuning frameworks and objectives, and evaluation setups and tasks; (3) the commonly used datasets in visual instruction tuning and evaluation; (4) the review of existing VIT methods that categorizes them with a taxonomy according to both the studied vision task and the method design and highlights the major contributions, strengths, and shortcomings of them; (5) the comparison and discussion of VIT methods over various instruction-following benchmarks; (6) several challenges, open directions and possible future works in visual instruction tuning research.",
            "corpus_id": 266573642,
            "sentences": [
                {
                    "corpus_id": "266573642",
                    "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
                    "text": "integrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].",
                    "score": 0.5276241743537853,
                    "section_title": "Visual Assistant",
                    "char_start_offset": 68635,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 186
                        },
                        {
                            "start": 189,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 865
                        },
                        {
                            "start": 868,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1500
                        },
                        {
                            "start": 1503,
                            "end": 1737
                        },
                        {
                            "start": 1738,
                            "end": 1903
                        },
                        {
                            "start": 1904,
                            "end": 2111
                        },
                        {
                            "start": 2114,
                            "end": 2244
                        },
                        {
                            "start": 2245,
                            "end": 2277
                        },
                        {
                            "start": 2278,
                            "end": 2315
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey\n# Venue: arXiv.org\n# Authors: Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, Shijian Lu\n## Abstract\nTraditional computer vision generally solves each single task independently by a dedicated model with the task instruction implicitly designed in the model architecture, arising two limitations: (1) it leads to task-specific models, which require multiple models for different tasks and restrict the potential synergies from diverse tasks; (2) it leads to a pre-defined and fixed model interface that has limited interactivity and adaptability in following user' task instructions. To address them, Visual Instruction Tuning (VIT) has been intensively studied recently, which finetunes a large vision model with language as task instructions, aiming to learn from a wide range of vision tasks described by language instructions a general-purpose multimodal model that can follow arbitrary instructions and thus solve arbitrary tasks specified by the user. This work aims to provide a systematic review of visual instruction tuning, covering (1) the background that presents computer vision task paradigms and the development of VIT; (2) the foundations of VIT that introduce commonly used network architectures, visual instruction tuning frameworks and objectives, and evaluation setups and tasks; (3) the commonly used datasets in visual instruction tuning and evaluation; (4) the review of existing VIT methods that categorizes them with a taxonomy according to both the studied vision task and the method design and highlights the major contributions, strengths, and shortcomings of them; (5) the comparison and discussion of VIT methods over various instruction-following benchmarks; (6) several challenges, open directions and possible future works in visual instruction tuning research.\n## Visual Assistant\nintegrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].",
            "reference_string": "[266573642 | Huang et al. | 2023 | Citations: 23]"
        },
        {
            "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.17762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332574955",
                    "name": "Rongchang Xie"
                },
                {
                    "authorId": "2332709350",
                    "name": "Chen Du"
                },
                {
                    "authorId": "2332538297",
                    "name": "Ping Song"
                },
                {
                    "authorId": "2333419477",
                    "name": "Chang Liu"
                }
            ],
            "abstract": "We introduce MUSE-VL, a Unified Vision-Language Model through Semantic discrete Encoding for multimodal understanding and generation. Recently, the research community has begun exploring unified models for visual generation and understanding. However, existing vision tokenizers (e.g., VQGAN) only consider low-level information, which makes it difficult to align with language tokens. This results in high training complexity and necessitates a large amount of training data to achieve optimal performance. Additionally, their performance is still far from dedicated understanding models. This paper proposes Semantic Discrete Encoding (SDE), which effectively aligns the information of visual tokens and language tokens by adding semantic constraints to the visual tokenizer. This greatly reduces the amount of training data and improves the performance of the unified model. With the same LLM size, our method improved the understanding performance by 4.8% compared to the previous SOTA Emu3 and surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model also surpasses the existing unified models on visual generation benchmarks.",
            "corpus_id": 274305717,
            "sentences": [
                {
                    "corpus_id": "274305717",
                    "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
                    "text": "Unified Visual Language Models Pioneering efforts have made significant strides by enabling multimodal understanding and generation within language models. In the realm of generating visual content using VLM, many works [18,19,25,50,72,74] have integrated VLMs with diffusion models [48] to achieve high-quality visual outputs. It is important to note that VLMs inherently lack the capability to directly produce visual content, and the quality of the generated images heavily relies on the performance of the diffusion models. For example, Emu [50] uses the output of the LLM as a condition for the pretrained diffusion model and then generates images with the diffusion model. Transfusion [74] combines the language modeling loss function with diffusion to train a single transformer. \n\nOther works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks. Janus [60] uses separate encoders for understanding and generation, but it increases the complexity of the model. VILA-U [61] combines contrastive and reconstruction loss to align visual and text tokens, but it has convergence problems, requiring a specific training recipe and large-scale image-text pairs from COYO-700M [5] dataset. \n\nIn this work, we explore a semantic-aware discrete encoding method for image reconstruction and generation. Our work reconstructs Siglip's visual features, which are well aligned with text, making the training process simpler and demonstrating outstanding performance in both visual understanding and generation tasks.",
                    "score": 0.5257998490820142,
                    "section_title": "Related Work",
                    "char_start_offset": 6699,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1188
                        },
                        {
                            "start": 1189,
                            "end": 1302
                        },
                        {
                            "start": 1303,
                            "end": 1523
                        },
                        {
                            "start": 1526,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1844
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "263889455"
                        },
                        {
                            "start": 233,
                            "end": 236,
                            "matchedPaperCorpusId": "267750101"
                        },
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "271909855"
                        },
                        {
                            "start": 283,
                            "end": 287,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 691,
                            "end": 695,
                            "matchedPaperCorpusId": "271909855"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding\n# Venue: arXiv.org\n# Authors: Rongchang Xie, Chen Du, Ping Song, Chang Liu\n## Abstract\nWe introduce MUSE-VL, a Unified Vision-Language Model through Semantic discrete Encoding for multimodal understanding and generation. Recently, the research community has begun exploring unified models for visual generation and understanding. However, existing vision tokenizers (e.g., VQGAN) only consider low-level information, which makes it difficult to align with language tokens. This results in high training complexity and necessitates a large amount of training data to achieve optimal performance. Additionally, their performance is still far from dedicated understanding models. This paper proposes Semantic Discrete Encoding (SDE), which effectively aligns the information of visual tokens and language tokens by adding semantic constraints to the visual tokenizer. This greatly reduces the amount of training data and improves the performance of the unified model. With the same LLM size, our method improved the understanding performance by 4.8% compared to the previous SOTA Emu3 and surpassed the dedicated understanding model LLaVA-NeXT 34B by 3.7%. Our model also surpasses the existing unified models on visual generation benchmarks.\n## Related Work\nUnified Visual Language Models Pioneering efforts have made significant strides by enabling multimodal understanding and generation within language models. In the realm of generating visual content using VLM, many works [18,19,25,50,72,74] have integrated VLMs with diffusion models [48] to achieve high-quality visual outputs. It is important to note that VLMs inherently lack the capability to directly produce visual content, and the quality of the generated images heavily relies on the performance of the diffusion models. For example, Emu [50] uses the output of the LLM as a condition for the pretrained diffusion model and then generates images with the diffusion model. Transfusion [74] combines the language modeling loss function with diffusion to train a single transformer. \n\nOther works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks. Janus [60] uses separate encoders for understanding and generation, but it increases the complexity of the model. VILA-U [61] combines contrastive and reconstruction loss to align visual and text tokens, but it has convergence problems, requiring a specific training recipe and large-scale image-text pairs from COYO-700M [5] dataset. \n\nIn this work, we explore a semantic-aware discrete encoding method for image reconstruction and generation. Our work reconstructs Siglip's visual features, which are well aligned with text, making the training process simpler and demonstrating outstanding performance in both visual understanding and generation tasks.",
            "reference_string": "[274305717 | Xie et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 317,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02567, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359211182",
                    "name": "Xinjie Zhang"
                },
                {
                    "authorId": "2359201363",
                    "name": "Jintao Guo"
                },
                {
                    "authorId": "2337775059",
                    "name": "Shanshan Zhao"
                },
                {
                    "authorId": "2345947101",
                    "name": "Minghao Fu"
                },
                {
                    "authorId": "2275153684",
                    "name": "Lunhao Duan"
                },
                {
                    "authorId": "2339237238",
                    "name": "Guo-Hua Wang"
                },
                {
                    "authorId": "2304395071",
                    "name": "Qing-Guo Chen"
                },
                {
                    "authorId": "2304361003",
                    "name": "Zhao Xu"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
            "corpus_id": 278327487,
            "sentences": [
                {
                    "corpus_id": "278327487",
                    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
                    "text": "One major direction in unified multimodal understanding and generation models adopts autoregressive (AR) architectures, where both vision and language tokens are typically serialized and modeled sequentially. In these models, a backbone Transformer, typically adapted from large language models (LLMs) such as LLaMA family [1], [2], [133], Vicuna [58], Gemma series [134], [135], [136], and Qwen series [5], [6], [9], [10], serves as the unified modality-fusion module to autoregressively predict multimodal outputs. \n\nTo integrate visual information into the AR framework, as shown in Fig. 5, existing methods propose different strategies for image tokenization during modality encoding. These approaches can be broadly categorized into four types: pixel-based, semantic-based, learnable query-based, hybrid-based encoding methods. 1) Pixel-based Encoding. As shown in Fig. 5 (b-1), pixelbased encoding typically refers to the representation of images as continuous or discrete tokens obtained from pretrained autoencoders supervised purely by image reconstruction, such as VQGAN-like models [32], [137], [138], [139]. These encoders compress the high-dimensional pixel space into a compact latent space, where each spatial patch corresponds to an image token. In unified multimodal autoregressive models, image tokens serialized from such encoders are processed analogously to text tokens, allowing both modalities to be modeled within a single sequence. \n\nRecent works have adopted and enhanced pixel-based tokenization with various encoder designs. LWM [29] employs a VQGAN tokenizer [32] to encode images into discrete latent codes without requiring semantic supervision. It proposes a multimodal world modeling framework, wherein visual and textual tokens are serialized together for unified autoregressive modeling. By learning world dynamics purely through reconstruction-based visual tokens and textual descriptions, LWM demonstrates that largescale multimodal generation is feasible without specialized semantic tokenization.",
                    "score": 0.5162129093322342,
                    "section_title": "Auto-Regressive Models",
                    "char_start_offset": 27036,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 516
                        },
                        {
                            "start": 519,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1456
                        },
                        {
                            "start": 1459,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1676
                        },
                        {
                            "start": 1677,
                            "end": 1822
                        },
                        {
                            "start": 1823,
                            "end": 2035
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1093,
                            "end": 1097,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 1099,
                            "end": 1104,
                            "matchedPaperCorpusId": "252367709"
                        },
                        {
                            "start": 1106,
                            "end": 1111,
                            "matchedPaperCorpusId": "263671912"
                        },
                        {
                            "start": 1113,
                            "end": 1118,
                            "matchedPaperCorpusId": "247628171"
                        },
                        {
                            "start": 1588,
                            "end": 1592,
                            "matchedPaperCorpusId": "229297973"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.9208984375,
            "relevance_judgment_input_expanded": "# Title: Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities\n# Venue: arXiv.org\n# Authors: Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang\n## Abstract\nRecent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).\n## Auto-Regressive Models\nOne major direction in unified multimodal understanding and generation models adopts autoregressive (AR) architectures, where both vision and language tokens are typically serialized and modeled sequentially. In these models, a backbone Transformer, typically adapted from large language models (LLMs) such as LLaMA family [1], [2], [133], Vicuna [58], Gemma series [134], [135], [136], and Qwen series [5], [6], [9], [10], serves as the unified modality-fusion module to autoregressively predict multimodal outputs. \n\nTo integrate visual information into the AR framework, as shown in Fig. 5, existing methods propose different strategies for image tokenization during modality encoding. These approaches can be broadly categorized into four types: pixel-based, semantic-based, learnable query-based, hybrid-based encoding methods. 1) Pixel-based Encoding. As shown in Fig. 5 (b-1), pixelbased encoding typically refers to the representation of images as continuous or discrete tokens obtained from pretrained autoencoders supervised purely by image reconstruction, such as VQGAN-like models [32], [137], [138], [139]. These encoders compress the high-dimensional pixel space into a compact latent space, where each spatial patch corresponds to an image token. In unified multimodal autoregressive models, image tokens serialized from such encoders are processed analogously to text tokens, allowing both modalities to be modeled within a single sequence. \n\nRecent works have adopted and enhanced pixel-based tokenization with various encoder designs. LWM [29] employs a VQGAN tokenizer [32] to encode images into discrete latent codes without requiring semantic supervision. It proposes a multimodal world modeling framework, wherein visual and textual tokens are serialized together for unified autoregressive modeling. By learning world dynamics purely through reconstruction-based visual tokens and textual descriptions, LWM demonstrates that largescale multimodal generation is feasible without specialized semantic tokenization.",
            "reference_string": "[278327487 | Zhang et al. | 2025 | Citations: 1]"
        },
        {
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 98,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.02239",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2253444275",
                    "name": "Kaizhi Zheng"
                },
                {
                    "authorId": "2149253467",
                    "name": "Xuehai He"
                },
                {
                    "authorId": "2256599634",
                    "name": "Xin Eric Wang"
                }
            ],
            "abstract": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.",
            "corpus_id": 263608981,
            "sentences": [
                {
                    "corpus_id": "263608981",
                    "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
                    "text": "In order to endow Large Language Models with multimodal generation capabilities, we introduce a new framework that integrates pretrained multimodal Large Language Models and text-to-image generation models. Central to our approach is the introduction of \"generative vokens\", special visual tokens that effectively bridge the textual and visual domains during the training process. Additionally, we implement a two-stage training method combined with a classifier-free guidance strategy to enhance the quality and coherence of generated outputs. Fig. 2 provides an overview of our model structure. MiniGPT-5 primarily consists of two modules: the Integrated Vision-Language Encoding Module, utilizing the pretrained multimodal large language model (MiniGPT-4) for handling multimodal inputs, and the Multimodal Output Generation module, employing Stable Diffusion for generating visual outputs.",
                    "score": 0.592164319081825,
                    "section_title": "METHOD",
                    "char_start_offset": 8173,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 206
                        },
                        {
                            "start": 207,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 893
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                },
                {
                    "corpus_id": "263608981",
                    "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
                    "text": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.",
                    "score": 0.6014140482643582,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens\n# Venue: arXiv.org\n# Authors: Kaizhi Zheng, Xuehai He, Xin Eric Wang\n## Abstract\nThe effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.\n## METHOD\nIn order to endow Large Language Models with multimodal generation capabilities, we introduce a new framework that integrates pretrained multimodal Large Language Models and text-to-image generation models. Central to our approach is the introduction of \"generative vokens\", special visual tokens that effectively bridge the textual and visual domains during the training process. Additionally, we implement a two-stage training method combined with a classifier-free guidance strategy to enhance the quality and coherence of generated outputs. Fig. 2 provides an overview of our model structure. MiniGPT-5 primarily consists of two modules: the Integrated Vision-Language Encoding Module, utilizing the pretrained multimodal large language model (MiniGPT-4) for handling multimodal inputs, and the Multimodal Output Generation module, employing Stable Diffusion for generating visual outputs.",
            "reference_string": "[263608981 | Zheng et al. | 2023 | Citations: 98]"
        },
        {
            "title": "Multi-Modal Generative Embedding Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218044713",
                    "name": "Feipeng Ma"
                },
                {
                    "authorId": "1878753365",
                    "name": "Hongwei Xue"
                },
                {
                    "authorId": "2303832848",
                    "name": "Guangting Wang"
                },
                {
                    "authorId": "2141747950",
                    "name": "Yizhou Zhou"
                },
                {
                    "authorId": "2276428924",
                    "name": "Fengyun Rao"
                },
                {
                    "authorId": "2303910455",
                    "name": "Shilin Yan"
                },
                {
                    "authorId": "2240587447",
                    "name": "Yueyi Zhang"
                },
                {
                    "authorId": "2303712729",
                    "name": "Siying Wu"
                },
                {
                    "authorId": "2303652188",
                    "name": "Mike Zheng Shou"
                },
                {
                    "authorId": "2303715824",
                    "name": "Xiaoyan Sun"
                }
            ],
            "abstract": "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.",
            "corpus_id": 270094821,
            "sentences": [
                {
                    "corpus_id": "270094821",
                    "title": "Multi-Modal Generative Embedding Model",
                    "text": "The Multi-Modal Generative Embedding Model (MM-GEM) presents a unified approach to multimodal learning by integrating generative and embedding objectives within a single Large Language Model (LLM).Our experiments demonstrate that these two objectives do not significantly conflict with each other.MM-GEM achieves competitive performance across a range of tasks, including cross-modal retrieval, zero-shot classification, and image captioning.A key contribution is the PoolAggregator, enhancing the model's ability to handle fine-grained tasks.Additionally, MM-GEM's advanced text module significantly improves performance on long-form text retrieval, showcasing the benefits of leveraging a robust LLM for text processing.\n\nMM-GEM represents a significant step towards unified multi-modal models, yet there are still many subsequent potential directions: 1) We mainly focus on image captioning for generative tasks in this work, the performance impact of adding plain language data needs to be further investigated.2) MM-GEM enables LLMs generate discriminative outputs besides language tokens, this may benefit multi-modal large language model by retrieving or grounding visual information efficiently.Further investigation into these aspects will be explored in future work.",
                    "score": 0.5097448186012934,
                    "section_title": "Conclusion",
                    "char_start_offset": 25786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 197,
                            "end": 297
                        },
                        {
                            "start": 297,
                            "end": 442
                        },
                        {
                            "start": 442,
                            "end": 543
                        },
                        {
                            "start": 543,
                            "end": 722
                        },
                        {
                            "start": 724,
                            "end": 1015
                        },
                        {
                            "start": 1015,
                            "end": 1203
                        },
                        {
                            "start": 1203,
                            "end": 1276
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: Multi-Modal Generative Embedding Model\n# Venue: arXiv.org\n# Authors: Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, Xiaoyan Sun\n## Abstract\nMost multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.\n## Conclusion\nThe Multi-Modal Generative Embedding Model (MM-GEM) presents a unified approach to multimodal learning by integrating generative and embedding objectives within a single Large Language Model (LLM).Our experiments demonstrate that these two objectives do not significantly conflict with each other.MM-GEM achieves competitive performance across a range of tasks, including cross-modal retrieval, zero-shot classification, and image captioning.A key contribution is the PoolAggregator, enhancing the model's ability to handle fine-grained tasks.Additionally, MM-GEM's advanced text module significantly improves performance on long-form text retrieval, showcasing the benefits of leveraging a robust LLM for text processing.\n\nMM-GEM represents a significant step towards unified multi-modal models, yet there are still many subsequent potential directions: 1) We mainly focus on image captioning for generative tasks in this work, the performance impact of adding plain language data needs to be further investigated.2) MM-GEM enables LLMs generate discriminative outputs besides language tokens, this may benefit multi-modal large language model by retrieving or grounding visual information efficiently.Further investigation into these aspects will be explored in future work.",
            "reference_string": "[270094821 | Ma et al. | 2024 | Citations: 4]"
        },
        {
            "title": "ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.02368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333522984",
                    "name": "Leixin Zhang"
                },
                {
                    "authorId": "2264186499",
                    "name": "Steffen Eger"
                },
                {
                    "authorId": "2333620039",
                    "name": "Yinjie Cheng"
                },
                {
                    "authorId": "2333424820",
                    "name": "Weihe Zhai"
                },
                {
                    "authorId": "2138207755",
                    "name": "Jonas Belouadi"
                },
                {
                    "authorId": "66122857",
                    "name": "Christoph Leiter"
                },
                {
                    "authorId": "2029669151",
                    "name": "Simone Paolo Ponzetto"
                },
                {
                    "authorId": "2333424310",
                    "name": "Fahimeh Moafian"
                },
                {
                    "authorId": "2333625440",
                    "name": "Zhixue Zhao"
                }
            ],
            "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for accelerating scientific progress--remains underexplored. In this work, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal capabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as their combinations, focusing on the relationships between scientific objects (e.g., squares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E, and StableDiffusion, using two modes of output generation: code-based outputs (Python, TikZ) and direct raster image generation. Additionally, we examine four different input languages: English, German, Farsi, and Chinese. Our evaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific accuracy), reveals that while GPT-4o produces outputs of decent quality for simpler prompts involving individual dimensions such as spatial, numeric, or attribute understanding in isolation, all models face challenges in this task, especially for more complex prompts.",
            "corpus_id": 274445941,
            "sentences": [],
            "relevance_judgement": 0.91796875,
            "relevance_judgment_input_expanded": "# Title: ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?\n# Venue: arXiv.org\n# Authors: Leixin Zhang, Steffen Eger, Yinjie Cheng, Weihe Zhai, Jonas Belouadi, Christoph Leiter, Simone Paolo Ponzetto, Fahimeh Moafian, Zhixue Zhao\n## Abstract\nMultimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for accelerating scientific progress--remains underexplored. In this work, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal capabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as their combinations, focusing on the relationships between scientific objects (e.g., squares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E, and StableDiffusion, using two modes of output generation: code-based outputs (Python, TikZ) and direct raster image generation. Additionally, we examine four different input languages: English, German, Farsi, and Chinese. Our evaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific accuracy), reveals that while GPT-4o produces outputs of decent quality for simpler prompts involving individual dimensions such as spatial, numeric, or attribute understanding in isolation, all models face challenges in this task, especially for more complex prompts.\n",
            "reference_string": "[274445941 | Zhang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 6,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116785555",
                    "name": "Zhimin Li"
                },
                {
                    "authorId": "2253579462",
                    "name": "Haichao Miao"
                },
                {
                    "authorId": "2251997858",
                    "name": "Valerio Pascucci"
                },
                {
                    "authorId": "2289268771",
                    "name": "Shusen Liu"
                }
            ],
            "abstract": "The recent introduction of multimodal large language models (MLLMs) combine the inherent power of large language models (LLMs) with the renewed capabilities to reason about the multimodal context. The potential usage scenarios for MLLMs significantly outpace their text-only counterparts. Many recent works in visualization have demonstrated MLLMs' capability to understand and interpret visualization results and explain the content of the visualization to users in natural language. In the machine learning community, the general vision capabilities of MLLMs have been evaluated and tested through various visual understanding benchmarks. However, the ability of MLLMs to accomplish specific visualization tasks based on visual perception has not been properly explored and evaluated, particularly, from a visualization-centric perspective. In this work, we aim to fill the gap by utilizing the concept of visualization literacy to evaluate MLLMs. We assess MLLMs' performance over two popular visualization literacy evaluation datasets (VLAT and mini-VLAT). Under the framework of visualization literacy, we develop a general setup to compare different multimodal large language models (e.g., GPT4-o, Claude 3 Opus, Gemini 1.5 Pro) as well as against existing human baselines. Our study demonstrates MLLMs' competitive performance in visualization literacy, where they outperform humans in certain tasks such as identifying correlations, clusters, and hierarchical structures.",
            "corpus_id": 271218251,
            "sentences": [
                {
                    "corpus_id": "271218251",
                    "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
                    "text": "Recently, models that accept multiple modalities as input are gaining increasing popularity.From the early work, e.g., CLIP [26], that jointly embedded image and text to models designed for input in one modality and output in another, such as text-to-image [28,27] and image-to-text [37] models.In the context of this work, we focus on multi-modal large language models (LLM) [36] that can understand both image and text inputs for text generation.LLMs are often referred to as foundation models due to their extensive capabilities and scale.They often possess a wide range of knowledge, allowing them to understand implicit context and common sense.\n\nSince humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35].Since the initial introduction of GPT4-V, many state-of-the-art MLLMs have been introduced such as the recent update to GPT4-o from OpenAI, multimodal support for Claude 3 [3] from Anthropic, and Gemini family from Google.On the open weight model side, many attempts have been made to develop similar capabilities, such as LLAVA [18], and CogVLM [30].",
                    "score": 0.6327649990521756,
                    "section_title": "Multimodal Vision Language Models",
                    "char_start_offset": 8099,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 92,
                            "end": 295
                        },
                        {
                            "start": 295,
                            "end": 448
                        },
                        {
                            "start": 448,
                            "end": 542
                        },
                        {
                            "start": 542,
                            "end": 650
                        },
                        {
                            "start": 652,
                            "end": 885
                        },
                        {
                            "start": 885,
                            "end": 1107
                        },
                        {
                            "start": 1107,
                            "end": 1236
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 124,
                            "end": 128,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 257,
                            "end": 261,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1214,
                            "end": 1218,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: Visualization Literacy of Multimodal Large Language Models: A Comparative Study\n# Venue: arXiv.org\n# Authors: Zhimin Li, Haichao Miao, Valerio Pascucci, Shusen Liu\n## Abstract\nThe recent introduction of multimodal large language models (MLLMs) combine the inherent power of large language models (LLMs) with the renewed capabilities to reason about the multimodal context. The potential usage scenarios for MLLMs significantly outpace their text-only counterparts. Many recent works in visualization have demonstrated MLLMs' capability to understand and interpret visualization results and explain the content of the visualization to users in natural language. In the machine learning community, the general vision capabilities of MLLMs have been evaluated and tested through various visual understanding benchmarks. However, the ability of MLLMs to accomplish specific visualization tasks based on visual perception has not been properly explored and evaluated, particularly, from a visualization-centric perspective. In this work, we aim to fill the gap by utilizing the concept of visualization literacy to evaluate MLLMs. We assess MLLMs' performance over two popular visualization literacy evaluation datasets (VLAT and mini-VLAT). Under the framework of visualization literacy, we develop a general setup to compare different multimodal large language models (e.g., GPT4-o, Claude 3 Opus, Gemini 1.5 Pro) as well as against existing human baselines. Our study demonstrates MLLMs' competitive performance in visualization literacy, where they outperform humans in certain tasks such as identifying correlations, clusters, and hierarchical structures.\n## Multimodal Vision Language Models\nRecently, models that accept multiple modalities as input are gaining increasing popularity.From the early work, e.g., CLIP [26], that jointly embedded image and text to models designed for input in one modality and output in another, such as text-to-image [28,27] and image-to-text [37] models.In the context of this work, we focus on multi-modal large language models (LLM) [36] that can understand both image and text inputs for text generation.LLMs are often referred to as foundation models due to their extensive capabilities and scale.They often possess a wide range of knowledge, allowing them to understand implicit context and common sense.\n\nSince humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35].Since the initial introduction of GPT4-V, many state-of-the-art MLLMs have been introduced such as the recent update to GPT4-o from OpenAI, multimodal support for Claude 3 [3] from Anthropic, and Gemini family from Google.On the open weight model side, many attempts have been made to develop similar capabilities, such as LLAVA [18], and CogVLM [30].",
            "reference_string": "[271218251 | Li et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 157,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.09259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2263703053",
                    "name": "Xuannan Liu"
                },
                {
                    "authorId": "2212020693",
                    "name": "Xing Cui"
                },
                {
                    "authorId": "2253583840",
                    "name": "Peipei Li"
                },
                {
                    "authorId": "2265220595",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2268583100",
                    "name": "Huaibo Huang"
                },
                {
                    "authorId": "2306060428",
                    "name": "Shuhan Xia"
                },
                {
                    "authorId": "2330734159",
                    "name": "Miaoxuan Zhang"
                },
                {
                    "authorId": "2305465534",
                    "name": "Yueying Zou"
                },
                {
                    "authorId": "2261958972",
                    "name": "Ran He"
                }
            ],
            "abstract": "The rapid evolution of multimodal foundation models has led to significant advancements in cross-modal understanding and generation across diverse modalities, including text, images, audio, and video. However, these models remain susceptible to jailbreak attacks, which can bypass built-in safety mechanisms and induce the production of potentially harmful content. Consequently, understanding the methods of jailbreak attacks and existing defense mechanisms is essential to ensure the safe deployment of multimodal generative models in real-world scenarios, particularly in security-sensitive applications. To provide comprehensive insight into this topic, this survey reviews jailbreak and defense in multimodal generative models. First, given the generalized lifecycle of multimodal jailbreak, we systematically explore attacks and corresponding defense strategies across four levels: input, encoder, generator, and output. Based on this analysis, we present a detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models. Additionally, we cover a wide range of input-output configurations, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight current research challenges and propose potential directions for future research. The open-source repository corresponding to this work can be found at https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.",
            "corpus_id": 274023126,
            "sentences": [
                {
                    "corpus_id": "274023126",
                    "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
                    "text": "Current multimodal generative models can be broadly classified into three distinct categories. The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output [3]- [5], [27]- [30]. The second category encompasses Any-to-Vision (Any Modality to Vision) Models, which encode inputs across different modalities as conditional information and leverage diffusion models to generate visual outputs [10], [31]- [38]. Thirdly, Any-to-Any (Any Modality to Any Modality) Models perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio [2], [39]- [41]. We summarize the combinations of modalities regarding both inputs and outputs of all categories in TABLE I. Additionally, we provide a comprehensive analysis of their underlying architectures as follows: \n\n\u2022 Any-to-Text Models. Typical models in this category consist of three primary components: an encoder, a pre-trained LLM, and a modality interface that connects them. The modality encoder functions akin to human sensory organs, transforming raw visual or audio data into compact representations. A common approach is to use pre-trained encoders that are already aligned with language data, as seen in CLIP models [43], which facilitate alignment with LLMs. The LLM, often chosen from established pre-trained models like LLaMA [44] and Vicuna [44], serves as the central reasoning unit. These models benefit from extensive pre-training on web corpora, allowing for rich knowledge representation and reasoning capabilities. To bridge the gap between modalities and language, a modality interface is introduced. This interface can either be a learnable projector that directly aligns the encoded modality features with the LLM's input requirements or an expert model that translates non-textual data into language. Overall, Any-to-Text Models utilize a multi-module architecture to effectively integrate multimodal inputs and generate coherent textual outputs. \n\n\u2022 Any-to-Vision Models.",
                    "score": 0.7205171166146709,
                    "section_title": "A. Multimodal Generative Models",
                    "char_start_offset": 8699,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 936
                        },
                        {
                            "start": 939,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1524
                        },
                        {
                            "start": 1525,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2096
                        },
                        {
                            "start": 2099,
                            "end": 2122
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 311,
                            "end": 314,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 316,
                            "end": 319,
                            "matchedPaperCorpusId": "267412182"
                        },
                        {
                            "start": 321,
                            "end": 325,
                            "matchedPaperCorpusId": "258291930"
                        },
                        {
                            "start": 544,
                            "end": 548,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 721,
                            "end": 725,
                            "matchedPaperCorpusId": "261696650"
                        },
                        {
                            "start": 1352,
                            "end": 1356,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.91357421875,
            "relevance_judgment_input_expanded": "# Title: Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey\n# Venue: arXiv.org\n# Authors: Xuannan Liu, Xing Cui, Peipei Li, Zekun Li, Huaibo Huang, Shuhan Xia, Miaoxuan Zhang, Yueying Zou, Ran He\n## Abstract\nThe rapid evolution of multimodal foundation models has led to significant advancements in cross-modal understanding and generation across diverse modalities, including text, images, audio, and video. However, these models remain susceptible to jailbreak attacks, which can bypass built-in safety mechanisms and induce the production of potentially harmful content. Consequently, understanding the methods of jailbreak attacks and existing defense mechanisms is essential to ensure the safe deployment of multimodal generative models in real-world scenarios, particularly in security-sensitive applications. To provide comprehensive insight into this topic, this survey reviews jailbreak and defense in multimodal generative models. First, given the generalized lifecycle of multimodal jailbreak, we systematically explore attacks and corresponding defense strategies across four levels: input, encoder, generator, and output. Based on this analysis, we present a detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models. Additionally, we cover a wide range of input-output configurations, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight current research challenges and propose potential directions for future research. The open-source repository corresponding to this work can be found at https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.\n## A. Multimodal Generative Models\nCurrent multimodal generative models can be broadly classified into three distinct categories. The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output [3]- [5], [27]- [30]. The second category encompasses Any-to-Vision (Any Modality to Vision) Models, which encode inputs across different modalities as conditional information and leverage diffusion models to generate visual outputs [10], [31]- [38]. Thirdly, Any-to-Any (Any Modality to Any Modality) Models perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio [2], [39]- [41]. We summarize the combinations of modalities regarding both inputs and outputs of all categories in TABLE I. Additionally, we provide a comprehensive analysis of their underlying architectures as follows: \n\n\u2022 Any-to-Text Models. Typical models in this category consist of three primary components: an encoder, a pre-trained LLM, and a modality interface that connects them. The modality encoder functions akin to human sensory organs, transforming raw visual or audio data into compact representations. A common approach is to use pre-trained encoders that are already aligned with language data, as seen in CLIP models [43], which facilitate alignment with LLMs. The LLM, often chosen from established pre-trained models like LLaMA [44] and Vicuna [44], serves as the central reasoning unit. These models benefit from extensive pre-training on web corpora, allowing for rich knowledge representation and reasoning capabilities. To bridge the gap between modalities and language, a modality interface is introduced. This interface can either be a learnable projector that directly aligns the encoded modality features with the LLM's input requirements or an expert model that translates non-textual data into language. Overall, Any-to-Text Models utilize a multi-module architecture to effectively integrate multimodal inputs and generate coherent textual outputs. \n\n\u2022 Any-to-Vision Models.",
            "reference_string": "[274023126 | Liu et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Multimodal Neurons in Pretrained Text-Only Transformers",
            "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 32,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.01544",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.01544, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "5478802",
                    "name": "Sarah Schwettmann"
                },
                {
                    "authorId": "2226764679",
                    "name": "Neil Chowdhury"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ],
            "abstract": "Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying \"multimodal neurons\" that convert visual representations into corresponding text, and decoding the concepts they inject into the model\u2019s residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning. Project page: mmns.csail.mit.edu",
            "corpus_id": 260438635,
            "sentences": [
                {
                    "corpus_id": "260438635",
                    "title": "Multimodal Neurons in Pretrained Text-Only Transformers",
                    "text": "Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying \"multimodal neurons\" that convert visual representations into corresponding text, and decoding the concepts they inject into the model\u2019s residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning. Project page: mmns.csail.mit.edu",
                    "score": 0.530520355495257,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: Multimodal Neurons in Pretrained Text-Only Transformers\n# Venue: 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)\n# Authors: Sarah Schwettmann, Neil Chowdhury, A. Torralba\n## Abstract\nLanguage models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying \"multimodal neurons\" that convert visual representations into corresponding text, and decoding the concepts they inject into the model\u2019s residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning. Project page: mmns.csail.mit.edu\n",
            "reference_string": "[260438635 | Schwettmann et al. | 2023 | Citations: 32]"
        },
        {
            "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 96,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279954226",
                    "name": "Yutong Xia"
                },
                {
                    "authorId": "2283843071",
                    "name": "Ao Qu"
                },
                {
                    "authorId": "97527544",
                    "name": "Yunhan Zheng"
                },
                {
                    "authorId": "2243088943",
                    "name": "Yihong Tang"
                },
                {
                    "authorId": "1580217088",
                    "name": "Dingyi Zhuang"
                },
                {
                    "authorId": "2261965063",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2334035234",
                    "name": "Cathy Wu"
                },
                {
                    "authorId": "2249532794",
                    "name": "Roger Zimmermann"
                },
                {
                    "authorId": "2283883570",
                    "name": "Jinhua Zhao"
                }
            ],
            "abstract": "Urban causal research is essential for understanding the complex, dynamic processes that shape cities and for informing evidence-based policies. However, current practices are often constrained by inefficient and biased hypothesis formulation, challenges in integrating multimodal data, and fragile experimental methodologies. Imagine a system that automatically estimates the causal impact of congestion pricing on commute times by income group or measures how new green spaces affect asthma rates across neighborhoods using satellite imagery and health reports, and then generates comprehensive, policy-ready outputs, including causal estimates, subgroup analyses, and actionable recommendations. In this Perspective, we propose UrbanCIA, an LLM-driven conceptual framework composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy insights. We begin by examining the current landscape of urban causal research through a structured taxonomy of research topics, data sources, and methodological approaches, revealing systemic limitations across the workflow. Next, we introduce the design principles and technological roadmap for the four modules in the proposed framework. We also propose evaluation criteria to assess the rigor and transparency of these AI-augmented processes. Finally, we reflect on the broader implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces LLM-driven tools as catalysts for more scalable, reproducible, and inclusive urban research.",
            "corpus_id": 277857505,
            "sentences": [
                {
                    "corpus_id": "277857505",
                    "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
                    "text": "Large Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023;Team et al., 2023;Guo et al., 2025) are trained on large-scale text corpora to predict token sequences in an autoregressive manner. Given a sequence of tokens x = (x 1 , x 2 , . . . , x t ), an LLM models the conditional probability of the next token as: \n\nwhere h t denotes the hidden state at time step t, and W is the learned weight matrix mapping hidden states to token logits. The model generates text by sampling iteratively from this distribution. Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023a;Yan et al., 2024), audio (Chu et al., 2023), graph (Tang et al., 2024), and structured charts (Han et al., 2023), alongside text. These non-text inputs are embedded into a shared representation space as e V , while the textual prompt is embedded as e T . The MLLM then generates output tokens y = (y 1 , y 2 , . . . , y L ) according to: \n\nwhere y <t denotes previously generated tokens. By integrating multimodal contexts, MLLMs significantly expand the capabilities of LLMs-enabling tasks such as image captioning, visual question answering, and multimodal reasoning that are beyond the scope of text-only models.",
                    "score": 0.5264903022528368,
                    "section_title": "B.2 LLMs & MLLMs",
                    "char_start_offset": 57083,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 252
                        },
                        {
                            "start": 253,
                            "end": 325
                        },
                        {
                            "start": 328,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 998
                        },
                        {
                            "start": 1001,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1276
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 642,
                            "end": 661,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 661,
                            "end": 678,
                            "matchedPaperCorpusId": "264590333"
                        },
                        {
                            "start": 712,
                            "end": 731,
                            "matchedPaperCorpusId": "264405943"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9033203125
                }
            ],
            "relevance_judgement": 0.9033203125,
            "relevance_judgment_input_expanded": "# Title: Reimagining Urban Science: Scaling Causal Inference with Large Language Models\n# Venue: arXiv.org\n# Authors: Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Cathy Wu, Roger Zimmermann, Jinhua Zhao\n## Abstract\nUrban causal research is essential for understanding the complex, dynamic processes that shape cities and for informing evidence-based policies. However, current practices are often constrained by inefficient and biased hypothesis formulation, challenges in integrating multimodal data, and fragile experimental methodologies. Imagine a system that automatically estimates the causal impact of congestion pricing on commute times by income group or measures how new green spaces affect asthma rates across neighborhoods using satellite imagery and health reports, and then generates comprehensive, policy-ready outputs, including causal estimates, subgroup analyses, and actionable recommendations. In this Perspective, we propose UrbanCIA, an LLM-driven conceptual framework composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy insights. We begin by examining the current landscape of urban causal research through a structured taxonomy of research topics, data sources, and methodological approaches, revealing systemic limitations across the workflow. Next, we introduce the design principles and technological roadmap for the four modules in the proposed framework. We also propose evaluation criteria to assess the rigor and transparency of these AI-augmented processes. Finally, we reflect on the broader implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces LLM-driven tools as catalysts for more scalable, reproducible, and inclusive urban research.\n## B.2 LLMs & MLLMs\nLarge Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023;Team et al., 2023;Guo et al., 2025) are trained on large-scale text corpora to predict token sequences in an autoregressive manner. Given a sequence of tokens x = (x 1 , x 2 , . . . , x t ), an LLM models the conditional probability of the next token as: \n\nwhere h t denotes the hidden state at time step t, and W is the learned weight matrix mapping hidden states to token logits. The model generates text by sampling iteratively from this distribution. Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023a;Yan et al., 2024), audio (Chu et al., 2023), graph (Tang et al., 2024), and structured charts (Han et al., 2023), alongside text. These non-text inputs are embedded into a shared representation space as e V , while the textual prompt is embedded as e T . The MLLM then generates output tokens y = (y 1 , y 2 , . . . , y L ) according to: \n\nwhere y <t denotes previously generated tokens. By integrating multimodal contexts, MLLMs significantly expand the capabilities of LLMs-enabling tasks such as image captioning, visual question answering, and multimodal reasoning that are beyond the scope of text-only models.",
            "reference_string": "[277857505 | Xia et al. | 2025 | Citations: 2]"
        },
        {
            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
            "venue": "Remote Sensing",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 44,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2072-4292/16/9/1477/pdf?version=1713858844",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs16091477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs16091477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1795469",
                    "name": "Y. Bazi"
                },
                {
                    "authorId": "25206458",
                    "name": "Laila Bashmal"
                },
                {
                    "authorId": "2267672536",
                    "name": "Mohamad Mahmoud Al Rahhal"
                },
                {
                    "authorId": "2212706249",
                    "name": "Riccardo Ricci"
                },
                {
                    "authorId": "1774633",
                    "name": "F. Melgani"
                }
            ],
            "abstract": "In this paper, we delve into the innovative application of large language models (LLMs) and their extension, large vision-language models (LVLMs), in the field of remote sensing (RS) image analysis. We particularly emphasize their multi-tasking potential with a focus on image captioning and visual question answering (VQA). In particular, we introduce an improved version of the Large Language and Vision Assistant Model (LLaVA), specifically adapted for RS imagery through a low-rank adaptation approach. To evaluate the model performance, we create the RS-instructions dataset, a comprehensive benchmark dataset that integrates four diverse single-task datasets related to captioning and VQA. The experimental results confirm the model\u2019s effectiveness, marking a step forward toward the development of efficient multi-task models for RS image analysis.",
            "corpus_id": 269354784,
            "sentences": [
                {
                    "corpus_id": "269354784",
                    "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
                    "text": "Vision-language models represent a category of models that integrate computer vision and NLP techniques to achieve a comprehensive understanding of visual and textual data.The field of research in this area has witnessed significant advancements, encompassing a wide range of techniques and architectures that aim to fuse vision and language, thereby enhancing performance and capabilities across various tasks.\n\nOne approach adopted by these models involves learning generic multi-modal representations from a large corpus of image-text pairs.Models such as CLIP [46] and ALIGN [47] employ dual encoders trained with a contrastive objective on extensive imagetext datasets, enabling them to model the interaction between modalities.However, these models do not natively support multi-modal generation tasks (e.g., image captioning and VQA).Other models, like BLIB [44] and CoCa [45], combine contrastive and generative pre-training to support both cross-modal alignment and multi-modal generation tasks.\n\nAnother paradigm of vision-language models are the generalized models that are trained to jointly perform multiple tasks without task-specific fine-tuning.For instance, UniT [48] unifies different tasks in a single model by designing a decoder with per-task query embedding and task-specific output heads.OFA [49] formulates different visionlanguage tasks as sequence-to-sequence tasks and trains a transformer model on them without task-specific layers.GIT [50] is a multi-tasking transformer-based model that conditions the decoder on vision inputs and text to generate the output text.Flamingo [51] extends sequence-to-sequence models to support interleaved image, video and text inputs for generating the output text.\n\nBuilding upon the success of GPT models, GPT-4 [14] has exhibited impressive capabilities in engaging in multi-modal dialogues with humans.The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input.Multimodal-GPT [52] utilizes gated cross-attention layers to facilitate seamless interactions between images and text.",
                    "score": 0.5533780875555541,
                    "section_title": "Vision-Language Models in General Computer Vision",
                    "char_start_offset": 13494,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 172,
                            "end": 411
                        },
                        {
                            "start": 413,
                            "end": 544
                        },
                        {
                            "start": 544,
                            "end": 733
                        },
                        {
                            "start": 733,
                            "end": 841
                        },
                        {
                            "start": 841,
                            "end": 1004
                        },
                        {
                            "start": 1006,
                            "end": 1161
                        },
                        {
                            "start": 1161,
                            "end": 1311
                        },
                        {
                            "start": 1311,
                            "end": 1460
                        },
                        {
                            "start": 1460,
                            "end": 1594
                        },
                        {
                            "start": 1594,
                            "end": 1727
                        },
                        {
                            "start": 1729,
                            "end": 1868
                        },
                        {
                            "start": 1868,
                            "end": 2005
                        },
                        {
                            "start": 2005,
                            "end": 2154
                        },
                        {
                            "start": 2154,
                            "end": 2272
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 564,
                            "end": 568,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 865,
                            "end": 869,
                            "matchedPaperCorpusId": "221279805"
                        },
                        {
                            "start": 879,
                            "end": 883,
                            "matchedPaperCorpusId": "226602850"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90234375
                }
            ],
            "relevance_judgement": 0.90234375,
            "relevance_judgment_input_expanded": "# Title: RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery\n# Venue: Remote Sensing\n# Authors: Y. Bazi, Laila Bashmal, Mohamad Mahmoud Al Rahhal, Riccardo Ricci, F. Melgani\n## Abstract\nIn this paper, we delve into the innovative application of large language models (LLMs) and their extension, large vision-language models (LVLMs), in the field of remote sensing (RS) image analysis. We particularly emphasize their multi-tasking potential with a focus on image captioning and visual question answering (VQA). In particular, we introduce an improved version of the Large Language and Vision Assistant Model (LLaVA), specifically adapted for RS imagery through a low-rank adaptation approach. To evaluate the model performance, we create the RS-instructions dataset, a comprehensive benchmark dataset that integrates four diverse single-task datasets related to captioning and VQA. The experimental results confirm the model\u2019s effectiveness, marking a step forward toward the development of efficient multi-task models for RS image analysis.\n## Vision-Language Models in General Computer Vision\nVision-language models represent a category of models that integrate computer vision and NLP techniques to achieve a comprehensive understanding of visual and textual data.The field of research in this area has witnessed significant advancements, encompassing a wide range of techniques and architectures that aim to fuse vision and language, thereby enhancing performance and capabilities across various tasks.\n\nOne approach adopted by these models involves learning generic multi-modal representations from a large corpus of image-text pairs.Models such as CLIP [46] and ALIGN [47] employ dual encoders trained with a contrastive objective on extensive imagetext datasets, enabling them to model the interaction between modalities.However, these models do not natively support multi-modal generation tasks (e.g., image captioning and VQA).Other models, like BLIB [44] and CoCa [45], combine contrastive and generative pre-training to support both cross-modal alignment and multi-modal generation tasks.\n\nAnother paradigm of vision-language models are the generalized models that are trained to jointly perform multiple tasks without task-specific fine-tuning.For instance, UniT [48] unifies different tasks in a single model by designing a decoder with per-task query embedding and task-specific output heads.OFA [49] formulates different visionlanguage tasks as sequence-to-sequence tasks and trains a transformer model on them without task-specific layers.GIT [50] is a multi-tasking transformer-based model that conditions the decoder on vision inputs and text to generate the output text.Flamingo [51] extends sequence-to-sequence models to support interleaved image, video and text inputs for generating the output text.\n\nBuilding upon the success of GPT models, GPT-4 [14] has exhibited impressive capabilities in engaging in multi-modal dialogues with humans.The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input.Multimodal-GPT [52] utilizes gated cross-attention layers to facilitate seamless interactions between images and text.",
            "reference_string": "[269354784 | Bazi et al. | 2024 | Citations: 44]"
        },
        {
            "title": "ChatCam: Empowering Camera Control through Conversational AI",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 87,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144483892",
                    "name": "Xinhang Liu"
                },
                {
                    "authorId": "2237983377",
                    "name": "Yu-Wing Tai"
                },
                {
                    "authorId": "2238101157",
                    "name": "Chi-Keung Tang"
                }
            ],
            "abstract": "Cinematographers adeptly capture the essence of the world, crafting compelling visual narratives through intricate camera movements. Witnessing the strides made by large language models in perceiving and interacting with the 3D world, this study explores their capability to control cameras with human language guidance. We introduce ChatCam, a system that navigates camera movements through conversations with users, mimicking a professional cinematographer's workflow. To achieve this, we propose CineGPT, a GPT-based autoregressive model for text-conditioned camera trajectory generation. We also develop an Anchor Determinator to ensure precise camera trajectory placement. ChatCam understands user requests and employs our proposed tools to generate trajectories, which can be used to render high-quality video footage on radiance field representations. Our experiments, including comparisons to state-of-the-art approaches and user studies, demonstrate our approach's ability to interpret and execute complex instructions for camera operation, showing promising applications in real-world production settings.",
            "corpus_id": 272910588,
            "sentences": [
                {
                    "corpus_id": "272910588",
                    "title": "ChatCam: Empowering Camera Control through Conversational AI",
                    "text": "Multimodal Language Models. Large-scale language models (LLMs) [9,19,17,1,70] enabled by extensive datasets and model size, have demonstrated surprising emerging abilities. The emergence of multimodal models [42,41,32] is captivating as they can process text alongside other modalities such as images [24], audio [26], and videos [79]. Some unified models can perceive inputs and generate outputs in various combinations of text, images, videos, and audio [51,68,78,87]. LLMs hold the potential to act as agents [76,81,62], allowing them to be driven by goals, reason about their objectives, devise plans, utilize tools, and interact with and gather feedback from the environment. Our proposed method involves multiple modalities including language, images, 3D fields, and camera trajectories, and utilizes LLMs as agents to assist users in operating cameras. \n\nRadiance Field Representations. Utilizing continuous 3D fields modeled by MLPs and volumetric rendering, Neural Radiance Fields (NeRFs) [52] achieved breakthrough for novel view synthesis. \n\nFrom outside the window, enter the bedroom and explore its layout following an S-shaped path.",
                    "score": 0.5292777015680312,
                    "section_title": "Related Work",
                    "char_start_offset": 3795,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 27
                        },
                        {
                            "start": 28,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 859
                        },
                        {
                            "start": 862,
                            "end": 893
                        },
                        {
                            "start": 894,
                            "end": 1050
                        },
                        {
                            "start": 1053,
                            "end": 1146
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 63,
                            "end": 66,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 66,
                            "end": 69,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 69,
                            "end": 72,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 208,
                            "end": 212,
                            "matchedPaperCorpusId": "246411402"
                        },
                        {
                            "start": 212,
                            "end": 215,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 215,
                            "end": 218,
                            "matchedPaperCorpusId": "257219775"
                        },
                        {
                            "start": 301,
                            "end": 305,
                            "matchedPaperCorpusId": "258564264"
                        },
                        {
                            "start": 313,
                            "end": 317,
                            "matchedPaperCorpusId": "235624127"
                        },
                        {
                            "start": 330,
                            "end": 334,
                            "matchedPaperCorpusId": "238215257"
                        },
                        {
                            "start": 456,
                            "end": 460,
                            "matchedPaperCorpusId": "249848272"
                        },
                        {
                            "start": 460,
                            "end": 463,
                            "matchedPaperCorpusId": "258822817"
                        },
                        {
                            "start": 466,
                            "end": 469,
                            "matchedPaperCorpusId": "265506847"
                        },
                        {
                            "start": 512,
                            "end": 516,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 519,
                            "end": 522,
                            "matchedPaperCorpusId": "256697342"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90234375
                }
            ],
            "relevance_judgement": 0.90234375,
            "relevance_judgment_input_expanded": "# Title: ChatCam: Empowering Camera Control through Conversational AI\n# Venue: Neural Information Processing Systems\n# Authors: Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang\n## Abstract\nCinematographers adeptly capture the essence of the world, crafting compelling visual narratives through intricate camera movements. Witnessing the strides made by large language models in perceiving and interacting with the 3D world, this study explores their capability to control cameras with human language guidance. We introduce ChatCam, a system that navigates camera movements through conversations with users, mimicking a professional cinematographer's workflow. To achieve this, we propose CineGPT, a GPT-based autoregressive model for text-conditioned camera trajectory generation. We also develop an Anchor Determinator to ensure precise camera trajectory placement. ChatCam understands user requests and employs our proposed tools to generate trajectories, which can be used to render high-quality video footage on radiance field representations. Our experiments, including comparisons to state-of-the-art approaches and user studies, demonstrate our approach's ability to interpret and execute complex instructions for camera operation, showing promising applications in real-world production settings.\n## Related Work\nMultimodal Language Models. Large-scale language models (LLMs) [9,19,17,1,70] enabled by extensive datasets and model size, have demonstrated surprising emerging abilities. The emergence of multimodal models [42,41,32] is captivating as they can process text alongside other modalities such as images [24], audio [26], and videos [79]. Some unified models can perceive inputs and generate outputs in various combinations of text, images, videos, and audio [51,68,78,87]. LLMs hold the potential to act as agents [76,81,62], allowing them to be driven by goals, reason about their objectives, devise plans, utilize tools, and interact with and gather feedback from the environment. Our proposed method involves multiple modalities including language, images, 3D fields, and camera trajectories, and utilizes LLMs as agents to assist users in operating cameras. \n\nRadiance Field Representations. Utilizing continuous 3D fields modeled by MLPs and volumetric rendering, Neural Radiance Fields (NeRFs) [52] achieved breakthrough for novel view synthesis. \n\nFrom outside the window, enter the bedroom and explore its layout following an S-shaped path.",
            "reference_string": "[272910588 | Liu et al. | 2024 | Citations: 3]"
        },
        {
            "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 68,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312271163",
                    "name": "Tan-Hanh Pham"
                },
                {
                    "authorId": "2337102747",
                    "name": "Hoang-Nam Le"
                },
                {
                    "authorId": "2337398686",
                    "name": "Phu-Vinh Nguyen"
                },
                {
                    "authorId": "2336865725",
                    "name": "Chris Ngo"
                },
                {
                    "authorId": "2308040088",
                    "name": "Truong-Son Hy"
                }
            ],
            "abstract": "Visual Language Models have demonstrated remarkable capabilities across tasks, including visual question answering and image captioning. However, most models rely on text-based instructions, limiting their effectiveness in human-machine interactions. Moreover, the quality of language models depends on reasoning and prompting techniques, such as COT, which remain underexplored when using speech instructions. To address these challenges, we propose SilVar, a novel end-to-end multimodal model that uses speech instructions for reasoning in visual question answering. In addition, we investigate reasoning techniques with levels including conversational, simple, and complex speech instruction. SilVar is built upon CLIP, Whisper, and LLaMA 3.1-8B, enabling intuitive interactions by allowing users to provide verbal or text instructions. To this end, we introduce a dataset designed to challenge models with speech-based reasoning tasks for object localization. This dataset enhances the model ability to process and explain visual scenes from spoken input, moving beyond object recognition to reasoning-based interactions. The experiments show that SilVar achieves SOTA performance on the MMMU and ScienceQA benchmarks despite the challenge of speech-based instructions. We believe SilVar will inspire next-generation multimodal reasoning models, toward expert artificial general intelligence. Our code and dataset are available here.",
            "corpus_id": 274981694,
            "sentences": [
                {
                    "corpus_id": "274981694",
                    "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
                    "text": "Visual Language Models (VLMs) have gained significant attention due to their capacity to bridge the gap between visual and textual modalities, facilitating more intuitive interactions between humans and machines. These models are valuable in tasks like visual question answering (VQA), which may involve yes/no answers, multiple-choice questions, or even generating image descriptions. With advances in deep learning, VLMs can now effectively handle visual scenes and provide meaningful textual outputs that explain or describe those scenes in natural language. \n\nRecent advancements in VLMs, such as CLIP [43], have enabled Large Language Models (LLMs) to process images and text simultaneously [3,6,45]. For example, Flamingo is a VLM that can handle both modalities and excels at tasks like image captioning and VQA [3]. Similarly, BLIP-2 connects a visual encoder to LLMs using a querying transformer, creating a more efficient multimodal model [29]. There are several VLM models, such as LLava [33], LocVLM [46], and LISA [27], that further improve model predictions through reasoning and prompting techniques. \n\nDespite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions. However, there is still a lack of exploration in the open-source community on building such speech interaction models based on foundation models. \n\nTo enable speech interaction with LLMs, speech instruction models such as Qwen2-Audio [14], SALMONN [50], and Llama-Omni [19] have been developed to process speech instead of text-based instructions. Although these models enable speech instruction, they are not capable of understanding both images and text simultaneously. Inspired by VLMs and ARS models, we propose SilVar, a multimodal model that can understand both images and audio or images and text, at the same time.",
                    "score": 0.523569737443122,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 561
                        },
                        {
                            "start": 564,
                            "end": 705
                        },
                        {
                            "start": 706,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1115
                        },
                        {
                            "start": 1118,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1796
                        },
                        {
                            "start": 1799,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2122
                        },
                        {
                            "start": 2123,
                            "end": 2273
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 606,
                            "end": 610,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 696,
                            "end": 699,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 701,
                            "end": 704,
                            "matchedPaperCorpusId": "259991075"
                        },
                        {
                            "start": 819,
                            "end": 822,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 949,
                            "end": 953,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 999,
                            "end": 1003,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1012,
                            "end": 1016,
                            "matchedPaperCorpusId": "269043025"
                        },
                        {
                            "start": 1027,
                            "end": 1031,
                            "matchedPaperCorpusId": "260351258"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9013671875
                }
            ],
            "relevance_judgement": 0.9013671875,
            "relevance_judgment_input_expanded": "# Title: SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization\n# Venue: arXiv.org\n# Authors: Tan-Hanh Pham, Hoang-Nam Le, Phu-Vinh Nguyen, Chris Ngo, Truong-Son Hy\n## Abstract\nVisual Language Models have demonstrated remarkable capabilities across tasks, including visual question answering and image captioning. However, most models rely on text-based instructions, limiting their effectiveness in human-machine interactions. Moreover, the quality of language models depends on reasoning and prompting techniques, such as COT, which remain underexplored when using speech instructions. To address these challenges, we propose SilVar, a novel end-to-end multimodal model that uses speech instructions for reasoning in visual question answering. In addition, we investigate reasoning techniques with levels including conversational, simple, and complex speech instruction. SilVar is built upon CLIP, Whisper, and LLaMA 3.1-8B, enabling intuitive interactions by allowing users to provide verbal or text instructions. To this end, we introduce a dataset designed to challenge models with speech-based reasoning tasks for object localization. This dataset enhances the model ability to process and explain visual scenes from spoken input, moving beyond object recognition to reasoning-based interactions. The experiments show that SilVar achieves SOTA performance on the MMMU and ScienceQA benchmarks despite the challenge of speech-based instructions. We believe SilVar will inspire next-generation multimodal reasoning models, toward expert artificial general intelligence. Our code and dataset are available here.\n## Introduction\nVisual Language Models (VLMs) have gained significant attention due to their capacity to bridge the gap between visual and textual modalities, facilitating more intuitive interactions between humans and machines. These models are valuable in tasks like visual question answering (VQA), which may involve yes/no answers, multiple-choice questions, or even generating image descriptions. With advances in deep learning, VLMs can now effectively handle visual scenes and provide meaningful textual outputs that explain or describe those scenes in natural language. \n\nRecent advancements in VLMs, such as CLIP [43], have enabled Large Language Models (LLMs) to process images and text simultaneously [3,6,45]. For example, Flamingo is a VLM that can handle both modalities and excels at tasks like image captioning and VQA [3]. Similarly, BLIP-2 connects a visual encoder to LLMs using a querying transformer, creating a more efficient multimodal model [29]. There are several VLM models, such as LLava [33], LocVLM [46], and LISA [27], that further improve model predictions through reasoning and prompting techniques. \n\nDespite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions. However, there is still a lack of exploration in the open-source community on building such speech interaction models based on foundation models. \n\nTo enable speech interaction with LLMs, speech instruction models such as Qwen2-Audio [14], SALMONN [50], and Llama-Omni [19] have been developed to process speech instead of text-based instructions. Although these models enable speech instruction, they are not capable of understanding both images and text simultaneously. Inspired by VLMs and ARS models, we propose SilVar, a multimodal model that can understand both images and audio or images and text, at the same time.",
            "reference_string": "[274981694 | Pham et al. | 2024 | Citations: 1]"
        },
        {
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 119,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2252413754",
                    "name": "Z. Wang"
                },
                {
                    "authorId": "2323111394",
                    "name": "King Zhu"
                },
                {
                    "authorId": "2280285922",
                    "name": "Chunpu Xu"
                },
                {
                    "authorId": "2284803168",
                    "name": "Wangchunshu Zhou"
                },
                {
                    "authorId": "2294523552",
                    "name": "Jiaheng Liu"
                },
                {
                    "authorId": "2277149577",
                    "name": "Yibo Zhang"
                },
                {
                    "authorId": "2310651349",
                    "name": "Jiashuo Wang"
                },
                {
                    "authorId": "2322957423",
                    "name": "Ning Shi"
                },
                {
                    "authorId": "2323142735",
                    "name": "Siyu Li"
                },
                {
                    "authorId": "2129449392",
                    "name": "Yizhi Li"
                },
                {
                    "authorId": "2249762724",
                    "name": "Haoran Que"
                },
                {
                    "authorId": "2322607797",
                    "name": "Zhaoxiang Zhang"
                },
                {
                    "authorId": "2279778502",
                    "name": "Yuanxing Zhang"
                },
                {
                    "authorId": "2307558672",
                    "name": "Ge Zhang"
                },
                {
                    "authorId": "2249845515",
                    "name": "Ke Xu"
                },
                {
                    "authorId": "2253966125",
                    "name": "Jie Fu"
                },
                {
                    "authorId": "2239245627",
                    "name": "Wenhao Huang"
                }
            ],
            "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
            "corpus_id": 272911245,
            "sentences": [
                {
                    "corpus_id": "272911245",
                    "title": "MIO: A Foundation Model on Multimodal Tokens",
                    "text": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
                    "score": 0.5421668678778351,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89697265625
                },
                {
                    "corpus_id": "272911245",
                    "title": "MIO: A Foundation Model on Multimodal Tokens",
                    "text": "For example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images. GPT-4 (OpenAI et al., 2023) can utilize either an image generator like DALL-E 3 (Betker et al., 2024) or a text-to-speech (TTS) tool like Whisper (Radford et al., 2022) to support multimodal generation.1 However, these methods are not end-to-end, relying on the text modality as an interface. \n\nRecently, the release of GPT-4o has demonstrated the capabilities of any-to-any and end-to-end foundation models. complex multimodal instruction-following, reasoning, planning, and other generalist capabilities. Furthermore, as the continuous scaling up of LLMs in the community depletes high-quality language tokens, GPT-4o verifies a new source of data for LLM training: multimodal tokens. This approach suggests that the next generation AGI could derive more knowledge from multimodal tokens when language tokens are exhausted. However, GPT-4o is closed source and focuses primarily on end-toend support for speech I/O, image I/O, 3D generation, and video understanding. Its recent open-source \"alternatives\", such as VITA (Fu et al., 2024), still lack the ability to generate data of all supported modalities, particularly for the generation of multimodal interleaved sequences. \n\nTo address the aforementioned issues, we introduce MIO (Multimodal Input and Output, or Multimodal Interleaved Output), the first open-source any-to-any foundation model that unifies multimodal understanding and generation across four modalities-text, image, speech (with voice), and video, while enabling the generation of multimodal interleaved sequences. Specifically, MIO is built on discrete multimodal tokens that capture both semantic representations through contrastive loss and low-level features via reconstruction loss (Ge et al., 2023a;Zhang et al., 2023b) from raw multimodal data.",
                    "score": 0.5214686270984334,
                    "section_title": "Models",
                    "char_start_offset": 4895,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 425
                        },
                        {
                            "start": 428,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1310
                        },
                        {
                            "start": 1313,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1907
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 279,
                            "end": 301,
                            "matchedPaperCorpusId": "252923993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89501953125
                },
                {
                    "corpus_id": "272911245",
                    "title": "MIO: A Foundation Model on Multimodal Tokens",
                    "text": "Specifically, MIO is built on discrete multimodal tokens that capture both semantic representations through contrastive loss and low-level features via reconstruction loss (Ge et al., 2023a;Zhang et al., 2023b) from raw multimodal data. Due to the consistent data format shared with textual corpora, the model can treat non-textual modalities as \"foreign languages\", allowing it to be trained with the next-token-prediction. Note that since the representation of an image remains the same whether it is used as an input or an output, our model flexibly supports multimodal interleaved sequence generation, where an image functions simultaneously for both understanding and generation. Moreover, we employ three-stage pre-training with an additional SFT stage to effectively train the model for modality scaling. \n\nOur experimental results show that MIO, trained on a mixture of four modalities, demonstrates competitive performance compared to its dual-modal counterparts and previous any-to-any multimodal language model baselines. Additionally, MIO is the first model to demonstrate interleaved video-text generation, chain-of-visual-thought reasoning, and other emergent abilities relying on any-to-any and multimodal interleaved output features (c.f., \u00a73.5).",
                    "score": 0.5933665732593227,
                    "section_title": "Models",
                    "char_start_offset": 6566,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 811
                        },
                        {
                            "start": 814,
                            "end": 1032
                        },
                        {
                            "start": 1033,
                            "end": 1262
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.89697265625,
            "relevance_judgment_input_expanded": "# Title: MIO: A Foundation Model on Multimodal Tokens\n# Venue: arXiv.org\n# Authors: Z. Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang\n## Abstract\nIn this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.\n## Models\nFor example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images. GPT-4 (OpenAI et al., 2023) can utilize either an image generator like DALL-E 3 (Betker et al., 2024) or a text-to-speech (TTS) tool like Whisper (Radford et al., 2022) to support multimodal generation.1 However, these methods are not end-to-end, relying on the text modality as an interface. \n\nRecently, the release of GPT-4o has demonstrated the capabilities of any-to-any and end-to-end foundation models. complex multimodal instruction-following, reasoning, planning, and other generalist capabilities. Furthermore, as the continuous scaling up of LLMs in the community depletes high-quality language tokens, GPT-4o verifies a new source of data for LLM training: multimodal tokens. This approach suggests that the next generation AGI could derive more knowledge from multimodal tokens when language tokens are exhausted. However, GPT-4o is closed source and focuses primarily on end-toend support for speech I/O, image I/O, 3D generation, and video understanding. Its recent open-source \"alternatives\", such as VITA (Fu et al., 2024), still lack the ability to generate data of all supported modalities, particularly for the generation of multimodal interleaved sequences. \n\nTo address the aforementioned issues, we introduce MIO (Multimodal Input and Output, or Multimodal Interleaved Output), the first open-source any-to-any foundation model that unifies multimodal understanding and generation across four modalities-text, image, speech (with voice), and video, while enabling the generation of multimodal interleaved sequences. Specifically, MIO is built on discrete multimodal tokens that capture both semantic representations through contrastive loss and low-level features via reconstruction loss (Ge et al., 2023a;Zhang et al., 2023b) from raw multimodal data.\n...\nSpecifically, MIO is built on discrete multimodal tokens that capture both semantic representations through contrastive loss and low-level features via reconstruction loss (Ge et al., 2023a;Zhang et al., 2023b) from raw multimodal data. Due to the consistent data format shared with textual corpora, the model can treat non-textual modalities as \"foreign languages\", allowing it to be trained with the next-token-prediction. Note that since the representation of an image remains the same whether it is used as an input or an output, our model flexibly supports multimodal interleaved sequence generation, where an image functions simultaneously for both understanding and generation. Moreover, we employ three-stage pre-training with an additional SFT stage to effectively train the model for modality scaling. \n\nOur experimental results show that MIO, trained on a mixture of four modalities, demonstrates competitive performance compared to its dual-modal counterparts and previous any-to-any multimodal language model baselines. Additionally, MIO is the first model to demonstrate interleaved video-text generation, chain-of-visual-thought reasoning, and other emergent abilities relying on any-to-any and multimodal interleaved output features (c.f., \u00a73.5).",
            "reference_string": "[272911245 | Wang et al. | 2024 | Citations: 12]"
        },
        {
            "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 34,
            "influential_citation_count": 15,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12580, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259012212",
                    "name": "Ting Jiang"
                },
                {
                    "authorId": "2260342581",
                    "name": "Minghui Song"
                },
                {
                    "authorId": "2260851231",
                    "name": "Zihan Zhang"
                },
                {
                    "authorId": "2146285313",
                    "name": "Haizhen Huang"
                },
                {
                    "authorId": "2066621592",
                    "name": "Weiwei Deng"
                },
                {
                    "authorId": "2247156451",
                    "name": "Feng Sun"
                },
                {
                    "authorId": "2256972722",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2145351849",
                    "name": "Deqing Wang"
                },
                {
                    "authorId": "2162961864",
                    "name": "Fuzhen Zhuang"
                }
            ],
            "abstract": "Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored. In this work, we introduce a new framework, E5-V, designed to adapt MLLMs for achieving universal multimodal embeddings. Our findings highlight the significant potential of MLLMs in representing multimodal inputs compared to previous approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the modality gap between different types of inputs, demonstrating strong performance in multimodal embeddings even without fine-tuning. We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs. This method demonstrates significant improvements over traditional multimodal training on image-text pairs, while reducing training costs by approximately 95%. Additionally, this approach eliminates the need for costly multimodal training data collection. Extensive experiments across four types of tasks demonstrate the effectiveness of E5-V. As a universal multimodal model, E5-V not only achieves but often surpasses state-of-the-art performance in each task, despite being trained on a single modality.",
            "corpus_id": 271245054,
            "sentences": [
                {
                    "corpus_id": "271245054",
                    "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
                    "text": "By designing prompts to project multimodal inputs into the same embedding spaces, we show that MLLMs can represent multimodal inputs correctly even without fine-tuning.\u2022 We introduce a new framework, E5-V, to adapt MLLMs for achieving universal multimodal embeddings.With single modality training on text pairs, E5-v even achieve better multimodal embeddings than image-text pairs.\u2022 Extensive experiments on text-image retrieval and composed image retrieval tasks demonstrate the effectiveness of E5-V in representing multimodal information.E5-V successfully transfers single modality representation capabilities to multimodal embeddings by following task-specific prompts that were not included in the training data.To achieve universal multimodal embeddings, there are several works, such as and UNIIR [WCC + 23], fine-tune CLIP on interlevaed dataset with fusion modal to fuse the visual and language information.There are also some works like VISTA [ZLX + 24] or UniVL-DR [LXL + 22] feed text encoder with CLIP outputs to input visual information.However, it can harm the original text-image retrieval performance of CLIP, and is hard to make text encoder understand the visual information with only contrastive learning, which show poor zero-shot performance on composed image retrieval tasks.\n\nTo achieve universal multimodal embeddings, several works, such as UNIIR [WCC + 23], finetune CLIP with a fusion model to integrate visual and language information.Other works, like VISTA [ZLX + 24] or UniVL-DR [LXL + 22], feed the text embedding models with CLIP outputs to incorporate visual information.However, this approach can harm the original text-image retrieval performance of CLIP and makes it difficult for the text embedding models to understand visual information using only contrastive learning.As a result, these methods show poor zero-shot performance on composed image retrieval tasks.Moreover, these methods require large interleaved training data to achieve universal multimodal embeddings.",
                    "score": 0.5599114956736954,
                    "section_title": "Introduction",
                    "char_start_offset": 3783,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 168,
                            "end": 267
                        },
                        {
                            "start": 267,
                            "end": 381
                        },
                        {
                            "start": 381,
                            "end": 541
                        },
                        {
                            "start": 541,
                            "end": 717
                        },
                        {
                            "start": 717,
                            "end": 916
                        },
                        {
                            "start": 916,
                            "end": 1051
                        },
                        {
                            "start": 1051,
                            "end": 1298
                        },
                        {
                            "start": 1300,
                            "end": 1464
                        },
                        {
                            "start": 1464,
                            "end": 1606
                        },
                        {
                            "start": 1606,
                            "end": 1810
                        },
                        {
                            "start": 1810,
                            "end": 1903
                        },
                        {
                            "start": 1903,
                            "end": 2010
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: E5-V: Universal Embeddings with Multimodal Large Language Models\n# Venue: arXiv.org\n# Authors: Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang\n## Abstract\nMultimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored. In this work, we introduce a new framework, E5-V, designed to adapt MLLMs for achieving universal multimodal embeddings. Our findings highlight the significant potential of MLLMs in representing multimodal inputs compared to previous approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the modality gap between different types of inputs, demonstrating strong performance in multimodal embeddings even without fine-tuning. We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs. This method demonstrates significant improvements over traditional multimodal training on image-text pairs, while reducing training costs by approximately 95%. Additionally, this approach eliminates the need for costly multimodal training data collection. Extensive experiments across four types of tasks demonstrate the effectiveness of E5-V. As a universal multimodal model, E5-V not only achieves but often surpasses state-of-the-art performance in each task, despite being trained on a single modality.\n## Introduction\nBy designing prompts to project multimodal inputs into the same embedding spaces, we show that MLLMs can represent multimodal inputs correctly even without fine-tuning.\u2022 We introduce a new framework, E5-V, to adapt MLLMs for achieving universal multimodal embeddings.With single modality training on text pairs, E5-v even achieve better multimodal embeddings than image-text pairs.\u2022 Extensive experiments on text-image retrieval and composed image retrieval tasks demonstrate the effectiveness of E5-V in representing multimodal information.E5-V successfully transfers single modality representation capabilities to multimodal embeddings by following task-specific prompts that were not included in the training data.To achieve universal multimodal embeddings, there are several works, such as and UNIIR [WCC + 23], fine-tune CLIP on interlevaed dataset with fusion modal to fuse the visual and language information.There are also some works like VISTA [ZLX + 24] or UniVL-DR [LXL + 22] feed text encoder with CLIP outputs to input visual information.However, it can harm the original text-image retrieval performance of CLIP, and is hard to make text encoder understand the visual information with only contrastive learning, which show poor zero-shot performance on composed image retrieval tasks.\n\nTo achieve universal multimodal embeddings, several works, such as UNIIR [WCC + 23], finetune CLIP with a fusion model to integrate visual and language information.Other works, like VISTA [ZLX + 24] or UniVL-DR [LXL + 22], feed the text embedding models with CLIP outputs to incorporate visual information.However, this approach can harm the original text-image retrieval performance of CLIP and makes it difficult for the text embedding models to understand visual information using only contrastive learning.As a result, these methods show poor zero-shot performance on composed image retrieval tasks.Moreover, these methods require large interleaved training data to achieve universal multimodal embeddings.",
            "reference_string": "[271245054 | Jiang et al. | 2024 | Citations: 34]"
        },
        {
            "title": "Visual Programming for Text-to-Image Generation and Evaluation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 68,
            "citation_count": 51,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.15328",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2706729",
                    "name": "Jaemin Cho"
                },
                {
                    "authorId": "2008198436",
                    "name": "Abhaysinh Zala"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows that VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope that our work encourages future progress on interpretable/explainable generation and evaluation for T2I models.",
            "corpus_id": 258865230,
            "sentences": [
                {
                    "corpus_id": "258865230",
                    "title": "Visual Programming for Text-to-Image Generation and Evaluation",
                    "text": "Text-to-image generation models. In the T2I generation task, models generate images from text. Early deep learning models used the Generative Adversarial Networks (GAN) [38] framework for this task [39; 40; 41; 31]. More recently, multimodal language models [42; 22] and diffusion models [43; 44; 21; 45] have gained popularity. Recent advances in multimodal language models such as Parti [46] and MUSE [47], and diffusion models like Stable Diffusion [21], UnCLIP [23], and Imagen [24], have demonstrated a high level of photorealism in zero-shot image generation. \n\nBridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions. \n\nLanguage models with visual modules. Although large language models (LLMs) have shown a broad range of commonsense knowledge, most of them are trained only on text corpus and cannot understand image inputs to tackle vision-and-language (VL) tasks. Thus, recent works explore tackling VL tasks by solving sub-tasks with external visual modules and combining their outputs to obtain the final response [9; 10; 11; 12; 13; 14; 15]. The visual sub-tasks include describing images as text, finding image regions relevant to the text, editing images with text guidance, and obtaining answers from a VQA model. However, existing work focuses on converting visual inputs into text format so that LLMs can understand them. Our work is the first work using visual programming for interpretable and explainable T2I generation and evaluation. \n\nEvaluation of text-to-image generation models.",
                    "score": 0.5191247090590297,
                    "section_title": "Related Works",
                    "char_start_offset": 6589,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 565
                        },
                        {
                            "start": 568,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1164
                        },
                        {
                            "start": 1167,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1414
                        },
                        {
                            "start": 1415,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1880
                        },
                        {
                            "start": 1881,
                            "end": 1997
                        },
                        {
                            "start": 2000,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 389,
                            "end": 393,
                            "matchedPaperCorpusId": "249926846"
                        },
                        {
                            "start": 452,
                            "end": 456,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8916015625
                }
            ],
            "relevance_judgement": 0.8916015625,
            "relevance_judgment_input_expanded": "# Title: Visual Programming for Text-to-Image Generation and Evaluation\n# Venue: arXiv.org\n# Authors: Jaemin Cho, Abhaysinh Zala, Mohit Bansal\n## Abstract\nAs large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows that VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope that our work encourages future progress on interpretable/explainable generation and evaluation for T2I models.\n## Related Works\nText-to-image generation models. In the T2I generation task, models generate images from text. Early deep learning models used the Generative Adversarial Networks (GAN) [38] framework for this task [39; 40; 41; 31]. More recently, multimodal language models [42; 22] and diffusion models [43; 44; 21; 45] have gained popularity. Recent advances in multimodal language models such as Parti [46] and MUSE [47], and diffusion models like Stable Diffusion [21], UnCLIP [23], and Imagen [24], have demonstrated a high level of photorealism in zero-shot image generation. \n\nBridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions. \n\nLanguage models with visual modules. Although large language models (LLMs) have shown a broad range of commonsense knowledge, most of them are trained only on text corpus and cannot understand image inputs to tackle vision-and-language (VL) tasks. Thus, recent works explore tackling VL tasks by solving sub-tasks with external visual modules and combining their outputs to obtain the final response [9; 10; 11; 12; 13; 14; 15]. The visual sub-tasks include describing images as text, finding image regions relevant to the text, editing images with text guidance, and obtaining answers from a VQA model. However, existing work focuses on converting visual inputs into text format so that LLMs can understand them. Our work is the first work using visual programming for interpretable and explainable T2I generation and evaluation. \n\nEvaluation of text-to-image generation models.",
            "reference_string": "[258865230 | Cho et al. | 2023 | Citations: 51]"
        },
        {
            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306162276",
                    "name": "Yi Fang"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2346106510",
                    "name": "Jiacheng Shen"
                },
                {
                    "authorId": "2307734628",
                    "name": "Sirui Ding"
                },
                {
                    "authorId": "2282981308",
                    "name": "Qiaoyu Tan"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ],
            "abstract": "The rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework. However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG). It is underexplored how MLLMs can incorporate the relational information (\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts and images) on such graphs for multimodal comprehension and generation. In this paper, we propose GraphGPT-o, which supports omni-multimodal understanding and creation on MMAGs. We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs. Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs. Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.",
            "corpus_id": 276422421,
            "sentences": [
                {
                    "corpus_id": "276422421",
                    "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
                    "text": "Multimodal Large Language Models (MLLMs) [6,20,22,31] have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. However, in real-world * Equal contribution. scenarios, there exists a pervasive graph-structured relationships between texts and images. Such graph-structured relationship can be described as \"Multimodal Attributed Graphs\" (MMAGs) [17,27,40], where nodes are associated with image and text information. For example, the artwork graph [25] is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references. While MLLMs have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGs. \n\nIn this context, we formulate the problem of multimodal content generation on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, generating an image and a text for a product node linked to others through copurchase edges in an e-commerce MMAG is equivalent to recommending [5,24] potential future products to users. Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork [7,14] that reflects the subtle styles of various artists and genres. \n\nHowever, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges: (1) Graph Size Explosion: Although MMAGs provide sub-arXiv:2502.11925v1 [cs.AI] 17 Feb 2025 stantial context for image and text generation, inputting the entire local subgraph structure to a model is impractical due to the exponential increase in size with additional hops, leading to excessively long context sequences.",
                    "score": 0.5341211112557316,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 853
                        },
                        {
                            "start": 854,
                            "end": 1039
                        },
                        {
                            "start": 1042,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1797
                        },
                        {
                            "start": 1800,
                            "end": 1976
                        },
                        {
                            "start": 1977,
                            "end": 2225
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 44,
                            "end": 47,
                            "matchedPaperCorpusId": "256390509"
                        },
                        {
                            "start": 47,
                            "end": 50,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 50,
                            "end": 53,
                            "matchedPaperCorpusId": "266374640"
                        },
                        {
                            "start": 533,
                            "end": 537,
                            "matchedPaperCorpusId": "195848773"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs\n# Venue: arXiv.org\n# Authors: Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, Jiawei Han\n## Abstract\nThe rapid development of Multimodal Large Language Models (MLLMs) has enabled the integration of multiple modalities, including texts and images, within the large language model (LLM) framework. However, texts and images are usually interconnected, forming a multimodal attributed graph (MMAG). It is underexplored how MLLMs can incorporate the relational information (\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts and images) on such graphs for multimodal comprehension and generation. In this paper, we propose GraphGPT-o, which supports omni-multimodal understanding and creation on MMAGs. We first comprehensively study linearization variants to transform semantic and structural information as input for MLLMs. Then, we propose a hierarchical aligner that enables deep graph encoding, bridging the gap between MMAGs and MLLMs. Finally, we explore the inference choices, adapting MLLM to interleaved text and image generation in graph scenarios. Extensive experiments on three datasets from different domains demonstrate the effectiveness of our proposed method. Datasets and codes will be open-sourced upon acceptance.\n## Introduction\nMultimodal Large Language Models (MLLMs) [6,20,22,31] have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. However, in real-world * Equal contribution. scenarios, there exists a pervasive graph-structured relationships between texts and images. Such graph-structured relationship can be described as \"Multimodal Attributed Graphs\" (MMAGs) [17,27,40], where nodes are associated with image and text information. For example, the artwork graph [25] is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references. While MLLMs have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGs. \n\nIn this context, we formulate the problem of multimodal content generation on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, generating an image and a text for a product node linked to others through copurchase edges in an e-commerce MMAG is equivalent to recommending [5,24] potential future products to users. Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork [7,14] that reflects the subtle styles of various artists and genres. \n\nHowever, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges: (1) Graph Size Explosion: Although MMAGs provide sub-arXiv:2502.11925v1 [cs.AI] 17 Feb 2025 stantial context for image and text generation, inputting the entire local subgraph structure to a model is impractical due to the exponential increase in size with additional hops, leading to excessively long context sequences.",
            "reference_string": "[276422421 | Fang et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 52,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240946204",
                    "name": "Liang Chen"
                },
                {
                    "authorId": "2247821453",
                    "name": "Shuai Bai"
                },
                {
                    "authorId": "2347531789",
                    "name": "Wenhao Chai"
                },
                {
                    "authorId": "2324707004",
                    "name": "Weichu Xie"
                },
                {
                    "authorId": "2112675144",
                    "name": "Haozhe Zhao"
                },
                {
                    "authorId": "2347531570",
                    "name": "Leon Vinci"
                },
                {
                    "authorId": "2333476820",
                    "name": "Junyang Lin"
                },
                {
                    "authorId": "2261083637",
                    "name": "Baobao Chang"
                }
            ],
            "abstract": "The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.",
            "corpus_id": 276647135,
            "sentences": [
                {
                    "corpus_id": "276647135",
                    "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
                    "text": "Recent years have witnessed remarkable advancements in text-to-image generation, primarily driven by powerful diffusion models [3,13,19,37]. While these models excel at generating images that align with simple text prompts, they struggle to handle more complex instructions that interweave graphical and textual elements. Although condition augmentation methods like IP-Adapter [52] and Con-trolNet [54] enhance text-to-image models with additional low-level control signals such as canny edges, depth maps, or reference images, they lack the flexibility to process complex and high-level text-image interleaved instructions, for example, merging visual elements from multiple images using natural language descriptions. This inability restricts more creative image generation processes where users might want to precisely orchestrate visual compositions by combining and manipulating elements from multiple sources with simple text instructions. \n\nMeanwhile, Large Multimodal Models (LMMs) [1,6,26,45] have shown remarkable progress in understanding visual content and natural language instructions, enabling various tasks such as image captioning, visual question answering, and visual grounding. This advancement raises an intriguing question: Can we take advantage of the advanced visual language understanding capabilities of LMMs to improve diffusion-based image generation models, enabling more flexible text-image interleaved control? Several recent works have explored integrating LMMs with diffusion models to enhance image generation control. As shown in Figure 2, Emu-1 and 2 [41,42] incorporate a specialized regression head on the hidden output states of LMM tokens following multimodal input processing. Seed-Tokenizer [15] expands the LMM vocabulary with discrete vision tokens, which serve as condition for the diffusion model during image generation. BLIP-Diffusion [23] employs a multimodal query-transformer encoder to extract subject representations, which are then combined with text prompts to guide the generation process.",
                    "score": 0.564364834597312,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 946
                        },
                        {
                            "start": 949,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 133,
                            "end": 136,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 1592,
                            "end": 1595,
                            "matchedPaperCorpusId": "266374640"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                }
            ],
            "relevance_judgement": 0.88720703125,
            "relevance_judgment_input_expanded": "# Title: Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think\n# Venue: arXiv.org\n# Authors: Liang Chen, Shuai Bai, Wenhao Chai, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang\n## Abstract\nThe field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.\n## Introduction\nRecent years have witnessed remarkable advancements in text-to-image generation, primarily driven by powerful diffusion models [3,13,19,37]. While these models excel at generating images that align with simple text prompts, they struggle to handle more complex instructions that interweave graphical and textual elements. Although condition augmentation methods like IP-Adapter [52] and Con-trolNet [54] enhance text-to-image models with additional low-level control signals such as canny edges, depth maps, or reference images, they lack the flexibility to process complex and high-level text-image interleaved instructions, for example, merging visual elements from multiple images using natural language descriptions. This inability restricts more creative image generation processes where users might want to precisely orchestrate visual compositions by combining and manipulating elements from multiple sources with simple text instructions. \n\nMeanwhile, Large Multimodal Models (LMMs) [1,6,26,45] have shown remarkable progress in understanding visual content and natural language instructions, enabling various tasks such as image captioning, visual question answering, and visual grounding. This advancement raises an intriguing question: Can we take advantage of the advanced visual language understanding capabilities of LMMs to improve diffusion-based image generation models, enabling more flexible text-image interleaved control? Several recent works have explored integrating LMMs with diffusion models to enhance image generation control. As shown in Figure 2, Emu-1 and 2 [41,42] incorporate a specialized regression head on the hidden output states of LMM tokens following multimodal input processing. Seed-Tokenizer [15] expands the LMM vocabulary with discrete vision tokens, which serve as condition for the diffusion model during image generation. BLIP-Diffusion [23] employs a multimodal query-transformer encoder to extract subject representations, which are then combined with text prompts to guide the generation process.",
            "reference_string": "[276647135 | Chen et al. | 2025 | Citations: 8]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "274023126",
            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "text": "Current multimodal generative models can be broadly classified into three distinct categories. The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output [3]- [5], [27]- [30]. The second category encompasses Any-to-Vision (Any Modality to Vision) Models, which encode inputs across different modalities as conditional information and leverage diffusion models to generate visual outputs [10], [31]- [38]. Thirdly, Any-to-Any (Any Modality to Any Modality) Models perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio [2], [39]- [41]. We summarize the combinations of modalities regarding both inputs and outputs of all categories in TABLE I. Additionally, we provide a comprehensive analysis of their underlying architectures as follows: \n\n\u2022 Any-to-Text Models. Typical models in this category consist of three primary components: an encoder, a pre-trained LLM, and a modality interface that connects them. The modality encoder functions akin to human sensory organs, transforming raw visual or audio data into compact representations. A common approach is to use pre-trained encoders that are already aligned with language data, as seen in CLIP models [43], which facilitate alignment with LLMs. The LLM, often chosen from established pre-trained models like LLaMA [44] and Vicuna [44], serves as the central reasoning unit. These models benefit from extensive pre-training on web corpora, allowing for rich knowledge representation and reasoning capabilities. To bridge the gap between modalities and language, a modality interface is introduced. This interface can either be a learnable projector that directly aligns the encoded modality features with the LLM's input requirements or an expert model that translates non-textual data into language. Overall, Any-to-Text Models utilize a multi-module architecture to effectively integrate multimodal inputs and generate coherent textual outputs. \n\n\u2022 Any-to-Vision Models.",
            "score": 0.7205171166146709,
            "section_title": "A. Multimodal Generative Models",
            "char_start_offset": 8699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "267412182"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 721,
                    "end": 725,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "The rise of foundation models [1] trained on vast amounts of text has transformed natural language processing (NLP), \n\nshowing that a single large language model (LLM) [2] can handle many different linguistic tasks [3][4][5]. The rich set of features encoded in LLM embeddings has been then used as an effective prior knowledge both for text-conditional image generation [6][7][8] and image understanding [9][10][11][12][13]. Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings [16] have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture [17][18][19][20]. This unification allows a richer understanding of the visual world, as generative tasks often require insight into the fundamental concepts and relationships within the data [21]. For example, a model that generates images from text descriptions must grasp the semantic content of those images to ensure they faithfully reflect the details and intent of the text [22,23]. As a result, research has rapidly progressed to integrate multiple modalities into a unified framework with increasingly deeper multimodal fusion. \n\nEarly approaches used cross-attention modules between modality-specific encoders [17]. Further advancements highlighted the importance of using a pre-trained LLM backbone combined with lightweight projection layers, often fine-tuning the LLM to refine the multimodal representations [19,24,25]. More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks.",
            "score": 0.698192980451817,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 33,
                    "matchedPaperCorpusId": "237091588"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "6401679"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 405,
                    "end": 408,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 682,
                    "end": 686,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 690,
                    "end": 694,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 1302,
                    "end": 1306,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 1504,
                    "end": 1508,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "261696650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "270514555",
            "title": "Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output",
            "text": "Among the above models, only the HAN model could output both texts and images.Other models could only generate text-only summaries.To have multimodal outputs, we further design different algorithms to select the images from the source image set.Given a multimodal input, there is a reference summary image set (with N images).We edit this reference set with the following methods:",
            "score": 0.6576863375643913,
            "section_title": "RQ2: Meta-Evaluation",
            "char_start_offset": 10915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "271309993",
            "title": "Braille-to-Speech Generator: Audio Generation Based on Joint Fine-Tuning of CLIP and Fastspeech2",
            "text": "The purpose of image-to-text conversion is to obtain corresponding text representations from input images (Zhu et al., 2023(Zhu et al., , 2024b)).Image-text models generally consist of three modules: a visual encoder to capture fine-grained image features, a text encoder to perceive contextual semantic connections, and a sequence decoder.The decoder outputs corresponding text fields based on hidden features.(Bao et al., 2022b)proposed the VL-BEiT model based on a bidirectional multimodal Transformer for visual-language pre-training.By predicting masks for input image-text pairs, it captures local connections between the two modalities.(Bao et al., 2022a) designed a hybrid modality expert Transformer structure that can be used as both a dual encoder for image-text retrieval and a fused encoder to simulate deep interaction between imagetext pairs.This architecture uses separate pooling layers and shared self-attention mechanisms for specific modalities.(Yu et al., 2022) proposed the CoCa (Contrastive Captioner) model, which decomposes the decoding layer into two parts: the first half omits cross-modal attention to encode unimodal text representations, while the remaining decoding layer realizes multimodal image-text representations.The model is trained using contrastive loss between jointly embedded image-text pairs and subtitle loss of the multimodal decoder.(Li et al., 2022) designed the BLIP (Bootstrapping Language-Image Pre-training) model for visual language understanding, optimizing it from three aspects: image-text contrastive loss, language modeling loss, and image-text matching loss, to achieve more powerful image-text understanding.(Li et al., 2023b) proposed FLIP (Fast Language-Image Pretraining) based on the CLIP model, which mainly reduces training time and improves accuracy by randomly masking and removing parts of images.",
            "score": 0.656086497904943,
            "section_title": "Related Work",
            "char_start_offset": 4397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 965
                },
                {
                    "start": 965,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1668
                },
                {
                    "start": 1668,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 123,
                    "matchedPaperCorpusId": "259370754"
                },
                {
                    "start": 123,
                    "end": 145,
                    "matchedPaperCorpusId": "267540150"
                },
                {
                    "start": 643,
                    "end": 661,
                    "matchedPaperCorpusId": "241035439"
                },
                {
                    "start": 1380,
                    "end": 1397,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.798828125
        },
        {
            "corpus_id": "277626963",
            "title": "Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly",
            "text": "Vision language models are a type of multi-modal model that can learn simultaneously from images and text [18]. There are different categories of such models. The first is multi-modal vision language models that take an image (I) and some query (Q) as input and produce a text response, for example, answering some question about a chart [24]. These models have three components: a vision transformer (e.g., CLIPs' ViT [44]), and adapter, and a separate text transformer (e.g., Vicuna [15]). In Figure 2 we show how these components work together and we describe this process in more detail in \u00a73.1.1. Another category of models, which we do not explore, are text-to-image generation models, for example, DALL-E2 [45] or Imagen [46]. These models rely on diffusion models to synthesize images and often incorporate CLIP-based image and text alignment mechanisms to ensure a coherent output [57]. However, these models are not suitable for our investigations, because they do not produce text outputs, despite being trained on text. Their primary function is image generation rather than image understanding or reasoning.",
            "score": 0.6521627188465499,
            "section_title": "VLM classes and Architectures",
            "char_start_offset": 14409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1120
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "247026006"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "248571821"
                },
                {
                    "start": 419,
                    "end": 423,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 728,
                    "end": 732,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "252070859"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65673828125
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "score": 0.6483085288917855,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
            "score": 0.6398129053477415,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "Unified multimodal models aim to build a single architecture capable of both understanding and generating data across multiple modalities. These models are designed to process diverse forms of input (e.g., text, image, video, audio) and produce outputs in one or more modalities in a unified manner. A typical unified multimodal framework can be abstracted into three core components: modalityspecific encoders that project different input modalities into a representation space; a modality-fusion backbone that integrates information from multiple modalities and enables cross-modal reasoning; and modality-specific decoders that generate output in the desired modality (e.g., text generation or image synthesis). \n\nIn this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation. \n\nIn the following sections, we will delve deeper into each category: Section 3.1 explores diffusion-based models, discussing their unique advantages in terms of generating high-quality images and text from noisy representations. Section 3.2 focuses on autoregressive-based models, detailing how different encoding methods impact their performance in vision-language tasks. Section 3.3 covers fused AR + diffusion models, examining how the combination of these two paradigms can enhance multimodal generation capabilities.",
            "score": 0.6342693033040866,
            "section_title": "UNIFIED MULTIMODAL MODELS FOR UNDER-STANDING AND GENERATION",
            "char_start_offset": 20720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "259164940",
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "text": "In this section, we first review some representative datasets and methods on vision-and-language tasks whose inputs and outputs are always unimodal. Next, we introduce the prior multimodal dialogue models. Finally, we briefly summarize the recent controllable text-to-image generation methods and the main differences between them and our task.",
            "score": 0.6341949981684393,
            "section_title": "Related works",
            "char_start_offset": 25480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 344
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34521484375
        },
        {
            "corpus_id": "276902413",
            "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
            "text": "Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate\"algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.",
            "score": 0.6329114197106758,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "271218251",
            "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
            "text": "Recently, models that accept multiple modalities as input are gaining increasing popularity.From the early work, e.g., CLIP [26], that jointly embedded image and text to models designed for input in one modality and output in another, such as text-to-image [28,27] and image-to-text [37] models.In the context of this work, we focus on multi-modal large language models (LLM) [36] that can understand both image and text inputs for text generation.LLMs are often referred to as foundation models due to their extensive capabilities and scale.They often possess a wide range of knowledge, allowing them to understand implicit context and common sense.\n\nSince humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35].Since the initial introduction of GPT4-V, many state-of-the-art MLLMs have been introduced such as the recent update to GPT4-o from OpenAI, multimodal support for Claude 3 [3] from Anthropic, and Gemini family from Google.On the open weight model side, many attempts have been made to develop similar capabilities, such as LLAVA [18], and CogVLM [30].",
            "score": 0.6327649990521756,
            "section_title": "Multimodal Vision Language Models",
            "char_start_offset": 8099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 92,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 650
                },
                {
                    "start": 652,
                    "end": 885
                },
                {
                    "start": 885,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1236
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1214,
                    "end": 1218,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "273233771",
            "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models",
            "text": "Multimodal large language models (MLLMs) represent a significant advancement in artificial intelligence by integrating visual encoders with pre-trained large language models. This integration allows these models to jointly process and understand diverse modalities, such as images and text, enabling more comprehensive and contextually aware analyses. \n\nFor a given image I, the visual encoder f visual transforms it into a series of token representations X I : \n\nHere, X I denotes a set of visual tokens that capture essential visual features and semantic information from the image. This process allows image content to be encoded in a format compatible with the language model, facilitating the effective integration of visual and textual data. \n\nTo ensure that the visual tokens are compatible with the language model's processing framework, a visual projection layer is employed. This layer converts visual tokens X I into language embedding tokens y 1 , y 2 , . . . , y n . The transformation is performed such that these language embedding tokens share the same dimensional space d as the word embeddings used by the language model: \n\nwhere f projection denotes the projection function that aligns the dimensionality of the visual tokens with that of the language embeddings. This alignment ensures that the visual information encoded in y 1 , y 2 , . . . , y n can be processed seamlessly alongside the textual information by the language model. Consequently, the MLMM can effectively integrate and leverage both visual and textual representations, improving its ability to generate accurate and contextually relevant outputs.",
            "score": 0.6301712109250629,
            "section_title": "Preliminaries",
            "char_start_offset": 10284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "267782812",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "text": "Given an image I and a text fragment T , the task is to predict the next token y. The visual language model utilizes a projector P to map the encoded representation of the image Fig. 2. Timeline of MultiModal Large Language Models (inspired by [25]) into the word embedding sequence. This process can be formally described as follows: \n\nHere, x T denotes the encoded representation of text, x I represents the encoded image, \u03b8 T emb is the text embedding function, \u03d5 I enc is the image encoder function, P is the projector that maps the image encoding to the word embedding space, \u2225 signifies concatenation, and \u03b8 dec is the decoder function that generates the prediction for the next token y.",
            "score": 0.6289281363479654,
            "section_title": "A. Vision-Language Models task",
            "char_start_offset": 8584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 693
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47998046875
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "Autoregressive text-to-image models, as exemplified by works such as Yu et al. (2023;2022), have made remarkable strides in generating highly detailed images, paralleling the achievements of Diffusion Models Nichol et al. (2022); Ramesh et al. (2022); Rombach et al. (2022). These models bear architectural resemblance to Large Language Models (LLMs), yet their training regimen is tailored for paired image-text data. LLMs on the other hand (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023) are limited to text-based output, thus lacking multimodal generative capabilities despite their proficiency in textual tasks. The subfield of Multimodal Large Models has emerged in recent years Tsimpoukelli et al. (2021); Alayrac et al. (2022); Li et al. (2022a) in the quest to bring together the disparate strengths of vision and language models. Despite important advances in this direction, these models still predominantly generate one modality, thereby constraining their expressiveness. This study aspires to break this limitation by developing a multimodal model capable of generating integrated text and image outputs. \n\nTo achieve this objective, we conduct a comprehensive empirical investigation into the fusion of two specialized autoregressive, decoder-only, large transformer models, each designed for unique tasks (one for text-to-image and a text only model). We introduce a set of methods under the umbrella of the Joint Autoregressive Mixture (JAM) framework. In building this framework, we take advantage of the inherent architectural compatibility of autoregressive text-to-image models with LLMs, allowing us to do deep model fusion and joint training in ways which would otherwise not be possible. Our modular and data-efficient solution allows for deep, rapid and effective integration of continually evolving large models, using less than 1% of the original pretraining data for both parent models. \n\nOur contributions to this study are twofold.",
            "score": 0.6266407443813204,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 273,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 442,
                    "end": 462,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 697,
                    "end": 723,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 725,
                    "end": 746,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 748,
                    "end": 765,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "271843410",
            "title": "Survey: Transformer-based Models in Data Modality Conversion",
            "text": "The right-hand boxes for each modality illustrate conversion processes and notable applications. \n\n\u2022 Related Surveys: Numerous surveys have explored TB models across text processing [3], computer vision [75], and speech processing [91] domains. Each of these surveys typically reviews papers focusing on a single Manuscript submitted to ACM Fig. 2. The overall structure of the paper is as follows: Related surveys will be introduced in the second section. The basic transformer model (Vanilla) will be detailed in the third section. The last three sections will cover methods for text, vision, and speech processing, respectively. \n\nmodality, processing the input to generate the desired output based on specific applications. There are also surveys on data fusion that aim to integrate data from different modalities. These papers generally review various types of fusion models and the types of inputs, such as text, vision, and speech. For instance, the survey by Davis et al. [35] on multimodal learning with transformers examines the synergistic use of multiple modalities, demonstrating substantial improvements in tasks requiring comprehensive understanding from diverse data sources. In conclusion, there is no existing survey paper that comprehensively reviews the literature on data conversion across different modalities (text, vision, and speech). \n\n\u2022 Paper Contribution: In this paper, we present a comprehensive review of TB models used for data modality conversion. We focus on three primary modalities: text, vision, and speech. The input can be in any of these modalities for each transformer model, and the output can be in a different or the same modality. For instance, given a text input, the output could be a translated text (machine translation), an image (story visualization), or speech. Similarly, the outputs can be converted to any of the other three modalities for vision and speech inputs. \n\nWe have systematically reviewed all the relevant literature on modality conversion using transformer-based models ( Fig. 1). \n\n\u2022 Scope: Our review is restricted to papers published from 2017 to 2024, given that transformers, introduced by Vaswani et al. in 2017 [157], are a relatively new technology. Focusing on this period allows us to include the most recent and pertinent advancements in transformers related to modality representation and conversion.",
            "score": 0.6262658199893022,
            "section_title": "Introduction",
            "char_start_offset": 2429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "236986986"
                },
                {
                    "start": 231,
                    "end": 235,
                    "matchedPaperCorpusId": "202572959"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "249642175"
                },
                {
                    "start": 2186,
                    "end": 2191,
                    "matchedPaperCorpusId": "232320340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41943359375
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "A unified model capable of both understanding and generating multimodal content holds immense potential: it could generate images based on complex instructions, reason about visual data, and visualize multimodal analyses through generated outputs. The unveiling of GPT-4o's enhanced capabilities [27] in March 2025 has further highlighted this potential, sparking widespread interest in unification. \n\nHowever, designing such a unified framework presents significant challenges. It requires integrating the strengths of autoregressive models for reasoning and text generation with the robustness of diffusion-based models for highquality image synthesis. Key questions remain unresolved, state of unified multimodal models (as illustrated in Fig. 1), thereby benefiting future research endeavors, we present this survey. We begin by introducing the foundational concepts and recent advancements in both multimodal understanding and image generation, covering both autoregressive and diffusion-based paradigms. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. Within the autoregressive and hybrid categories, we further classify models based on their image tokenization strategies, reflecting the diversity of approaches in this area. \n\nBeyond architecture, we assemble datasets and benchmarks tailored for training and evaluating unified multimodal models. These resources span multimodal understanding, text-to-image generation, image editing, and other relevant tasks, providing a foundation for future exploration. Finally, we discuss the key challenges facing this nascent field, including efficient tokenization strategy, data construction, model evaluation, etc. Tackling these challenges will be crucial for advancing the capabilities and scalability of unified multimodal models. \n\nIn the community, there exist excellent surveys on large language models [40], [41], multimodal understanding [42], [43], [44], and image generation [45], [46], while our work focuses specifically on the integration of understanding and generation tasks. Readers are encouraged to consult these complementary surveys for a broader perspective on related topics. We aim to inspire further research in this rapidly evolving field and provide a valuable reference for the community.",
            "score": 0.6260756627986225,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2314
                },
                {
                    "start": 2315,
                    "end": 2432
                }
            ],
            "ref_mentions": [
                {
                    "start": 2032,
                    "end": 2036,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 2063,
                    "end": 2067,
                    "matchedPaperCorpusId": "271750640"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "265308931"
                },
                {
                    "start": 2075,
                    "end": 2079,
                    "matchedPaperCorpusId": "259243718"
                },
                {
                    "start": 2102,
                    "end": 2106,
                    "matchedPaperCorpusId": "252199918"
                },
                {
                    "start": 2108,
                    "end": 2112,
                    "matchedPaperCorpusId": "265821110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "274130618",
            "title": "SoK: Unifying Cybersecurity and Cybersafety of Multimodal Foundation Models with an Information Theory Approach",
            "text": "In this section, we first explore how multimodal learning has evolved from unimodal learning and then introduce several typical MFMs. From unimodal to multimodal learning. Multimodal learning extends beyond traditional unimodal learning by integrating data from various modalities like text, images, and audio. In unimodal learning (Figure 2(a)), the model maps input features to output labels within a discrete feature space, focusing on patterns within one data type. It extracts features, converts them into vectors, and then predicts labels based on these vectors. In contrast, multimodal learning (Figure 2(b)) involves mapping and aligning continuous feature spaces across different modalities, such as pairing images with text. Instead of directly predicting labels, multimodal models create unified representations by linking feature spaces from different data types. This allows for more complex tasks as the model can work with more diverse information. Existing MFMs. Within the image/text multimodal space, there are four typical classes of multimodal foundation models (MFMs): feature alignment models, text-to-image generation models, audio-to-text generation models, and models that take both text and image inputs, as summarized in Table    that uses a dual-encoder framework to process images and text separately. It trains on large image-text pairs using contrastive learning [17] to create a shared representation space, excelling in tasks like zero-shot classification and image retrieval. \u2022 Text-to-image generation models (T2I). DALL\u2022E [18] and Stable Diffusion [3] are prominent models in this category, generating images by embedding text prompts into models. These models typically leverage feature alignment techniques, such as CLIP, to convert text into feature vectors that serve as conditional guidance for image generation, enabling the model to synthesize visuals corresponding to the input text. \u2022 Text generation from text and/or image inputs (I/T2T).",
            "score": 0.6187427980709085,
            "section_title": "Multimodal Foundation Models",
            "char_start_offset": 6865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1558,
                    "end": 1562,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1584,
                    "end": 1587,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "270285703",
            "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
            "text": "Multimodal Large Language Models.. We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2].Over the last year, such models have made tremendous advances in tasks like VQA and image captioning, including BLIP [15], BLIP-2 [16], Instruct-BLIP [7], LLaVA [18,19], Flamingo [3] and multi-modal Phi-2 (from the Bunny repo) [11].These MLLMs can broadly be categorized into two families based on how their visual information is integrated into the language model: (i) by embedding the vision encoder's output into each layer of the language model with a cross-attention layer (e.g., Flamingo, BLIP) or, (ii) by mapping the vision encoder's output into \"visual tokens\" in the language model's input space (i.e.alongside the text tokens) via a projection layer (e.g., LLaVA, Bunny).Both families are widely used, however, the projection layer family has recently shown stronger performance on popular benchmark [18,19,11].We, therefore, focus our study of information storage and transfer on this model family.",
            "score": 0.6180929562840589,
            "section_title": "Related Works",
            "char_start_offset": 4494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 304,
                    "end": 307,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "270619856",
            "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
            "text": "6 and Eq. 7.\n\nto the input text and uses them as contextual input to the language model [Tan and Bansal, 2020, Lu et al., 2022, Wang et al., 2023].Similarly, Tang et al. [2021] employs a knowledge distillation approach to fuse visual knowledge.Other works [Zhang et al., 2023, Guo et al., 2023, Li et al., 2023b] distill visual knowledge from multimodal embedding methods such as CLIP [Radford et al., 2021] into text-only language models.Another set of works utilizes pre-trained text-to-image generative models.In the context of diffusion-based text-to-image models, Z-LaVi [Yang et al., 2022] leverage generated visuals that match possible label predictions of a given text-only language model.Our method, instead, considers visuals that match the input text.LiVE [Tang et al., 2023] introduces a vision-text plug-and-play vision-text fusion layer, which is inserted within transformed blocks of pre-trained language models (LMs).iNLG [Zhu et al., 2023] uses generated images as additional visual supervision to guide the language model in text generation, where the visual input is provided as an additional input to the LM in the form of a visual prefix.Unlike LiVE and iNGL, which integrate visual knowledge as input to the LM or as an integrated layer, we, instead, use the output of an unmodified pre-trained LLM together with an encoding of a generated image, using a late-fusion layer.This enables our model to focus on the input text, while also using visual information if needed.Second, instead of using a specialized attention-like mechanism or a mapping network, our work aggregates scores simply by averaging predictions made using different generated images, obtained from variations of the input text.This enables our method to use a diverse set of predictions obtained using diverse visual \"experts\" and gauge its final prediction towards the more confident predictions.\n\nMultiple Generations Agreement.",
            "score": 0.6114527458629276,
            "section_title": "Related Work",
            "char_start_offset": 4435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 14,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 762
                },
                {
                    "start": 762,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 1922
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 176,
                    "matchedPaperCorpusId": "235743020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77734375
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "Notably, not all multimodal models are VLMs; for instance, text-to-image models like Midjourney and DALL-E [70] lack a language generation component, underscoring the diverse landscape of multimodal AI landscape.The general architecture of a VLM consists of an image and text encoder to generate the embeddings which are then fused in an image-text fusion layer and this fused vector is passed through an LLM to generate the final visually aware generated text.The working of a VLM is shown in Figure -2 In this survey paper, we have categorized VLMs based on their \u2022 Text Generation with Multimodal Input: Within this classification, we explore models that excel in generating textual content while leveraging multimodal inputs, thereby incorporating diverse forms of information.\n\n\u2022 Multimodal Output with Multimodal Input: This category delves into models that exhibit proficiency in generating multimodal outputs by processing multimodal inputs.This involves the synthesis of diverse modalities, such as visual and textual elements, to produce comprehensive and coherent results.We have shown this broad classification in the figure-1. 1omparative Analysis We have conducted an extensive analysis of several Vision-and-Language Models (VLMs) across ten widely recognized benchmark datasets, spanning tasks such as Visual Question Answering (VQA) and image captioning.The results of this analysis are presented in Table 1.Additionally, we have evaluated the perception and cognition capabilities of these VLMs using the Multimodal Model Evaluation (MME) benchmark, with findings summarized in Table 2. Furthermore, a comparative examination of various VLMs on video question-answering datasets is detailed in Table 3.\n\n2.1.Vision-Language Understanding CLIP [69]: CLIP, introduced by OpenAI, is a neural network proficient in grasping visual concepts through natural language guidance.It seamlessly identifies visual categories on diverse benchmarks, mirroring the \"zero-shot\" capabilities seen in GPTpowered models.By scaling a basic contrastive pre-training task, it achieved competitive zero-shot performance on diverse image classification datasets.CLIP gives a much more robust performance than the fine-tuned deep learning vision models in general for classification tasks.",
            "score": 0.6100927134746671,
            "section_title": "Introduction",
            "char_start_offset": 2209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1371
                },
                {
                    "start": 1371,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1720
                },
                {
                    "start": 1722,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1888
                },
                {
                    "start": 1888,
                    "end": 2156
                },
                {
                    "start": 2156,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "GILL is the first multimodal language model capable of conditioning on image-and-text inputs to generate meaningful images interleaved with text. Hence, our experiments primarily focus on evaluating its ability to produce novel images (Sec. 4.1). Our results show that GILL improves over Stable Diffusion [49] on tasks that require processing long-form text such as dialogue and discourse. We also benchmark the performance of models in deciding whether to retrieve or generate (see appendix). GILL is capable of generating text, retrieving images, and generating images. Despite being more general than prior work [56,4,31], we find that GILL performs comparably to or better than existing multimodal LMs on contextual image retrieval and text generation tasks (see Sec. 5).",
            "score": 0.6097315963385495,
            "section_title": "Experiments",
            "char_start_offset": 16923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 775
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "264145994",
            "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
            "text": "We propose VLIS, a novel framework to alleviate the language modeling burden of visual-language models (VLMs). VLIS combines the linguistic understanding capability of the text-only language models with the visual conditioning strength of the VLMs by importance sampling. To isolate the VLMs' visual conditioning power, VLIS uses pointwise mutual information to suppress their text-only marginal distribution. Our framework enhances the base VLM in commonsense reasoning (WHOOPS (Bitton-Guetta et al., 2023), OK-VQA (Marino et al., 2019), and ScienceQA (Lu et al., 2022a)) and complicated text generation (Concadia (Kreiss et al., 2022), Image Paragraph Captioning (Krause et al., 2017), and ROCStories (Mostafazadeh et al., 2016)) problems. In the future, VLIS can be extended to incorporate other modalities for which the paired multimodal data is even scarcer. We hope that VLIS sparks an interest in better utilization of off-the-shelf multimodal pretrained models.",
            "score": 0.6062505425491436,
            "section_title": "Conclusion",
            "char_start_offset": 23299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 969
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 537,
                    "matchedPaperCorpusId": "173991173"
                },
                {
                    "start": 553,
                    "end": 571,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 615,
                    "end": 636,
                    "matchedPaperCorpusId": "253224427"
                },
                {
                    "start": 665,
                    "end": 686,
                    "matchedPaperCorpusId": "14464447"
                },
                {
                    "start": 703,
                    "end": 730,
                    "matchedPaperCorpusId": "1726501"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "Multimodal Language Models Several prior works have developed multimodal language models which process image and text inputs to generate text outputs. Frozen [56] showed that it is possible to finetune a visual encoder to map images into the hidden space of a text-only LLM, and that this exhibits compelling few-shot, captioning, and question answering abilities. Other methods improve upon this approach by introducing adapters [19], scaling up model and data sizes [4,64], improving the visual encoder [4,33], finetuning on instructions [35], and training unified models on multi-task objectives [36,63,42]. CM3 [2,62] trained multimodal LMs on HTML webpages consisting of interleaved images and text. Many state-of-the-art models also require significant computational resources to train. For example, Flamingo [4] is trained on 1535 TPUs for 15 days, while RA-CM3 [62] use 256 GPUs for 5 days. In contrast, our efficient adaptation method is trained on 2 GPUs for 2 days. The most similar work to our approach is FROMAGe [31], which trains a multimodal language model capable of processing arbitrarily interleaved image and text inputs to",
            "score": 0.6029147344937317,
            "section_title": "Related Work",
            "char_start_offset": 3794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.",
            "score": 0.6014140482643582,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "267406379",
            "title": "Can MLLMs Perform Text-to-Image In-Context Learning?",
            "text": "Consequently, we focus on models capable of processing both text and multiple images. We consider two types of MLLMs: (i) proficient in generating both text and images, including Emu (Sun et al., 2023b), GILL (Koh et al., 2023), and SEED-LLaMA (Ge et al., 2023b), and (ii) those limited to text generation, including GPT-4V (OpenAI, 2023), LLaVA (Liu et al., 2023b), and Qwen-VL (Bai et al., 2023b). For text-only MLLMs, we evaluate their capacity to infer visual outputs by prompt-ing them to describe the anticipated image. Conversely, for MLLMs capable of image generation, we not only elicit image outputs but also ask for descriptive text, ensuring an apple-to-apple comparison with text-only models. \n\nImage-to-Text ICL in MLLMs. Most existing work on M-ICL focuses on the image-to-text generation, i.e., I2T-ICL, which involves mapping from high-dimensional input (i.e., images) to low-dimensional output (i.e., text). In particular, Tsimpoukelli et al. (2021) were the first to extend ICL from the text domain to the multimodal domain, focusing on image-to-text generation such as visual question-answering (see Figure 1(c)). Alayrac et al. (2022) introduced Flamingo, an MLLM that achieves state-of-the-art performance in a variety of image and video understanding tasks using I2T-ICL with 32 demonstrations, implying the efficacy of I2T-ICL in performance enhancement in their model. In contrast, Monajatipoor et al. (2023) explores whether the in-context capabilities of LLMs can be seamlessly extended to I2T-ICL by incorporating a visual encoder. Chen et al. (2023b) conducted a systematic study on the importance of visual and textual information in I2T-ICL.",
            "score": 0.5983558477647969,
            "section_title": "Related Works",
            "char_start_offset": 13033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1672
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 365,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 941,
                    "end": 967,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1134,
                    "end": 1155,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1407,
                    "end": 1433,
                    "matchedPaperCorpusId": "259064260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "273502334",
            "title": "Generative AI Agents in Autonomous Machines: A Safety Perspective",
            "text": "As transformer models proved highly effective in handling sequential data, their capabilities soon extended beyond text to multimodal applications, integrating different data types such as images, text, and speech. A key milestone in the shift towards multimodal models was the introduction of the Vision Transformer (ViT) [49], which applied transformers to images by splitting them into patches treated as tokens, much like words in language models. Building on this, CLIP (Contrastive Language-Image Pretraining) [133] aligned vision and language by training on image-text pairs, associating visual data with textual descriptions. This alignment became foundational for Vision-Language Models (VLMs) [26], which integrate image and text inputs to generate text outputs, driving applications such as image captioning and visual question answering [6,27,97,190]. Multimodal capabilities have since expanded beyond vision and language, incorporating other modalities such as audio [44].",
            "score": 0.5978444064167728,
            "section_title": "Multi-Modal Language Models",
            "char_start_offset": 7351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 986
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 521,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 849,
                    "end": 852,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 855,
                    "end": 858,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "258823141"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7568359375
        },
        {
            "corpus_id": "272911245",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "text": "Specifically, MIO is built on discrete multimodal tokens that capture both semantic representations through contrastive loss and low-level features via reconstruction loss (Ge et al., 2023a;Zhang et al., 2023b) from raw multimodal data. Due to the consistent data format shared with textual corpora, the model can treat non-textual modalities as \"foreign languages\", allowing it to be trained with the next-token-prediction. Note that since the representation of an image remains the same whether it is used as an input or an output, our model flexibly supports multimodal interleaved sequence generation, where an image functions simultaneously for both understanding and generation. Moreover, we employ three-stage pre-training with an additional SFT stage to effectively train the model for modality scaling. \n\nOur experimental results show that MIO, trained on a mixture of four modalities, demonstrates competitive performance compared to its dual-modal counterparts and previous any-to-any multimodal language model baselines. Additionally, MIO is the first model to demonstrate interleaved video-text generation, chain-of-visual-thought reasoning, and other emergent abilities relying on any-to-any and multimodal interleaved output features (c.f., \u00a73.5).",
            "score": 0.5933665732593227,
            "section_title": "Models",
            "char_start_offset": 6566,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "258947258",
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "text": "Trained at massive scale on large text corpora, large language models (LLMs) are able to demonstrate compelling abilities such as generating human-like dialogue and answering complex questions. While undeniably impressive, most state-of-the-art LLMs are trained on text-only data scraped from the Internet. They are not exposed to rich visual cues, and are often unable to learn concepts grounded in the real world. Consequently, most existing language models exhibit limitations on tasks that involve visual reasoning and grounding, and they are also incapable of producing images. \n\nIn this paper, we show that we are able to efficiently leverage Figure 1. Our method grounds a language model to the visual domain, enabling it to process arbitrarily interleaved image-text inputs and generate coherent text outputs interleaved with relevant retrieved images. Speech bubbles in green are model generated, while grey bubbles are input prompts. \n\nthe capabilities of a frozen LLM for multimodal (image and text) input and output. Our approach equips text-only models with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations (Fig. 1 and Fig. 3). \n\nWe propose a method to bootstrap a frozen language model for processing and outputting arbitrarily interleaved multimodal data. We start from a frozen pretrained LLM, and a frozen pretrained visual encoder, and train with a multitask objective for (1) image captioning (learning to process interleaved multimodal inputs) and (2) image-text retrieval (learning to produce interleaved multimodal outputs). For captioning, we extract visual embeddings from the visual encoder, and learn a linear mapping through the maximumlikelihood objective to map embeddings into the input space of the language model. For image-text retrieval, we train the language model to learn a new [RET] token which represents an image, and learn a linear mapping through contrastive learning (Oord et al., 2018) to map the [RET] embeddings for a caption to be close to the visual embeddings for its paired image.",
            "score": 0.5933187451737371,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2118
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "In order to endow Large Language Models with multimodal generation capabilities, we introduce a new framework that integrates pretrained multimodal Large Language Models and text-to-image generation models. Central to our approach is the introduction of \"generative vokens\", special visual tokens that effectively bridge the textual and visual domains during the training process. Additionally, we implement a two-stage training method combined with a classifier-free guidance strategy to enhance the quality and coherence of generated outputs. Fig. 2 provides an overview of our model structure. MiniGPT-5 primarily consists of two modules: the Integrated Vision-Language Encoding Module, utilizing the pretrained multimodal large language model (MiniGPT-4) for handling multimodal inputs, and the Multimodal Output Generation module, employing Stable Diffusion for generating visual outputs.",
            "score": 0.592164319081825,
            "section_title": "METHOD",
            "char_start_offset": 8173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 893
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "263137930",
            "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
            "text": "We discuss the current limitations of our work as follows. \n\nFirst, the proposed causal multimodal language modeling approach still encounters challenges in establishing a robust grounding with the input modality. Specifically, we observe that during the generation, the model occasionally prioritizes focusing more on the generated text rather than the input image. This leads to the generation of output that incorporates biases acquired from the underlying language model (LLM), which can incur inaccuracies when compared against the image context. We expect that additional architectural adjustments or unfreezing LLM parameters are necessary to address this limitation effectively (albeit the much higher computational costs it might entail). Second, while we greatly increase the size of the pretraining dataset, the understanding of visual concepts and entities remains constrained by the quantity of paired image-text data included in the training process. In the domain of text-only language models, it is commonly observed that approaches incorporating external knowledge retrieval significantly enhance the model's ability to overcome its knowledge limitations. These approaches offer a potential means to alleviate the limitations mentioned earlier. \n\nLastly, in the scope of our work, the multimodal adaptation of an LLM is bounded by four modalities: image, video, audio, and IMU signals. While we believe that the proposed approach has the potential to encompass any other modality, provided there exists a paired dataset, its effectiveness for such modalities still needs to be substantiated.",
            "score": 0.592006152701403,
            "section_title": "Limitations",
            "char_start_offset": 30555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 61,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "250303261",
            "title": "Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective",
            "text": "Vision-Language (VL) tasks consist in jointly processing a picture and a text related to the picture. VL tasks, such as visual question answering, cross modal retrieval or generation, are notoriously difficult because of the necessity for models to build sensible multimodal representations that can relate fine-grained elements of the text and the picture. Following the success of pre-trained transformers for language modeling such as BERT (Devlin et al. 2018), the community has proposed various transformer-based models, such as Vil-BERT (Lu et al. 2019), LXMERT (Tan and Bansal 2019), VLBERT (Su et al. 2019), UNITER (Chen et al. 2020), OS-CAR (Li et al. 2020b), VinVL (Zhang et al. 2021), ViLT (Kim, Son, and Kim 2021) or ERNIE-VIL (Yu et al. 2021), that combine representations from both the text and image modalities to reach state-of-the-art results in several multimodal tasks. Similar models have been developed in the field of video-language pre-training, such as ClipBERT (Lei et al. 2021) and HERO (Li et al. 2020a). \n\nWhile the results are impressive, it is important to understand how multimodal information is encoded in the representations learned by those models, and how affected they are by various bias and properties of their training data. A few studies have been conducted to better understand those models and their representations. (Cao et al. 2020) have probed attention heads at various layers of the models, showing that textual modality is more important than visual modality for model decisions. This prevalence of language over vision in multimodal models is not specific to transformer-based representation models, as noticed by (Goyal et al. 2017a). (Li, Gan, and Liu 2020) have looked into the robustness of the representations to manipulations of the input, compared to more traditional models. (Hendricks and Nematzadeh 2021) relied on probing tasks to study verb understanding in pre-trained transformer-based models and determined that models learn less multimodal concepts associated to verbs than to subjects and objects.",
            "score": 0.5912335526240265,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 463,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 623,
                    "end": 641,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 650,
                    "end": 667,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 675,
                    "end": 694,
                    "matchedPaperCorpusId": "235692795"
                },
                {
                    "start": 701,
                    "end": 725,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 739,
                    "end": 755,
                    "matchedPaperCorpusId": "220265934"
                },
                {
                    "start": 1360,
                    "end": 1377,
                    "matchedPaperCorpusId": "218665405"
                },
                {
                    "start": 1664,
                    "end": 1684,
                    "matchedPaperCorpusId": "8081284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "270870796",
            "title": "From Efficient Multimodal Models to World Models: A Survey",
            "text": "Fig. 6.General architecture of a multimodal large model.This figure illustrates how different encoders (Text, Vision, Audio) process their respective inputs and align them before projecting into a shared latent space managed by a Large Language Model (LLM).The model then performs diffusion processes to generate or refine outputs, integrating information across multiple modalities to enhance understanding and generation capabilities.complementary visual knowledge.Efficient visual encoding models use techniques like token processing to manage highresolution images without excessive computational burden.High-resolution images are input into lightweight visual encoders, resized, and segmented to generate initial visual tokens.These tokens are compressed by visual token compression modules to reduce computational and storage overhead.Compressed tokens are projected into the language model's feature space by efficient visual-language projectors, aligned with text tokens.A small language model combines and processes these aligned visual features and text tokens, generating language responses.LLaVA-UHD [67] introduces an image modular strategy, dividing images into smaller fragments for efficient encoding, reducing computational load while maintaining perceptual ability.Advances in visual encoders also include MAE (Masked Autoencoders) [68], a self-supervised learning method that learns image representations by masking and reconstructing parts of input images.\n\nText encoders are another crucial component of multimodal models, used to process and understand textual data.Transformers are a common text encoding architecture, with selfattention mechanisms efficiently capturing long-range dependencies in text.BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model based on Transformers, generating high-quality text representations through bidirectional training on large-scale corpora, widely applied in various natural language processing tasks.\n\nIn audio encoding, AudioCLIP [69] is an effective choice, generating audio representations by combining audio and text information.AudioCLIP uses an architecture similar to CLIP, aligning audio, text, and image features in the same feature space through contrastive learning.This method enhances audio data representation and improves multimodal model performance in audio-text and audio-image tasks.",
            "score": 0.5908212360853017,
            "section_title": "Audio Diffusion",
            "char_start_offset": 46635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 7,
                    "end": 56
                },
                {
                    "start": 56,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1476
                },
                {
                    "start": 1478,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1995
                },
                {
                    "start": 1997,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2397
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 2026,
                    "end": 2030,
                    "matchedPaperCorpusId": "235624127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "259164940",
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "text": "The task of vision-to-language involves using visual samples as input and generating text as output, as seen in tasks like Visual Dialog. Conversely, language-to-vision tasks operate in the opposite direction. In multimodal dialogue methods, the model must reason about multimodal or unimodal inputs and produce multimodal or unimodal outputs. These methods can be categorized into the following two types \u00a7 : \n\n\u2022 Multimodal Input and Unimodal Output (MIUO): This conversational task is similar to visual dialog, where the input typically consists of visual data and text-based prompts (i.e., multimodal input). \n\nThe visual language model performs visual reasoning on the image and provides an answer (i.e., unimodal output) (Alayrac et al., 2022;Brooks et al., 2023;Gong et al., 2023;Bo Li, 2023;Li et al., 2023a;OpenAI, 2023b;Su et al., 2023;Yang et al., 2023b;Zhao et al., 2023;Zhu et al., 2023;Liu et al., 2023;Mu et al., 2023;Wang et al., 2023;Zhang et al., 2023). GPT-4 (OpenAI, 2023b) and MultiModal-GPT (Gong et al., 2023) can handle prompts consisting of interleaved visual inputs and text-based queries, generating text outputs. VideoChat (Li et al., 2023a) introduces a videocentric multimodal dialogue dataset, enabling trained models to understand and generate detailed conversations about videos. \n\n\u2022 Multimodal Input and Multimodal Output (MIMO): This task requires the model to perform multimodal reasoning and generation simultaneously (Koh et al., 2023;Jing Yu Koh, 2023;Chenfei Wu & Duan, 2023;Yang et al., 2023a).",
            "score": 0.5901582172149532,
            "section_title": "Multimodal Dialogue Models",
            "char_start_offset": 32321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1534
                }
            ],
            "ref_mentions": [
                {
                    "start": 726,
                    "end": 748,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 748,
                    "end": 768,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "257632157",
            "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
            "text": "Multimodal learning refers to learning a unified representation of data from different modalities. It aims at extracting complementary information to facilitate compositional tasks (Baltru\u0161aitis et al., 2018;Gao et al., 2020). In this survey, we include all modalities whose formats are different from natural language, including image, code, structured knowledge (e.g. tables, knowledge graphs), audio, and video. \n\nMultimodal generative models have a wide range of applications, such as text-image generation, creative writing generation, and multilingual translation. For instance, the image recognition task can benefit from analyzing images and videos in conjunction with textual descriptions (Ju et al., 2022;Alayrac et al., 2022a;Jia et al., 2021;Radford et al., 2021b). Conversely, incorporating visual information also aids language understanding and generation (Zhou et al., 2020;Lei et al., 2021;?). Moreover, they have the potential to significantly improve machine learning systems across various domains by enabling models to learn from and integrate multiple sources of information (Tsai et al., 2019;Acosta et al., 2022;Nagrani et al., 2021). Additionally, there has been growing interest in developing generative models that can output multiple modalities of data (Ramesh et al., 2021b;Crowson et al., 2022;Lin and Byrne, 2022a;Chen et al., 2022a). However, there remain challenges for multimodal generative models, such as gaining access to a large amount of multimodal data and designing a network that produces semantically meaningful outputs.",
            "score": 0.5886587149900949,
            "section_title": "Multimodal Learning",
            "char_start_offset": 3883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1563
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 225,
                    "matchedPaperCorpusId": "212748233"
                },
                {
                    "start": 698,
                    "end": 715,
                    "matchedPaperCorpusId": "244954623"
                },
                {
                    "start": 715,
                    "end": 737,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 737,
                    "end": 754,
                    "matchedPaperCorpusId": "235490277"
                },
                {
                    "start": 754,
                    "end": 776,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 871,
                    "end": 890,
                    "matchedPaperCorpusId": "202734445"
                },
                {
                    "start": 1097,
                    "end": 1116,
                    "matchedPaperCorpusId": "173990158"
                },
                {
                    "start": 1116,
                    "end": 1136,
                    "matchedPaperCorpusId": "252309887"
                },
                {
                    "start": 1136,
                    "end": 1157,
                    "matchedPaperCorpusId": "235694621"
                },
                {
                    "start": 1281,
                    "end": 1303,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1324,
                    "end": 1345,
                    "matchedPaperCorpusId": "252780775"
                },
                {
                    "start": 1345,
                    "end": 1364,
                    "matchedPaperCorpusId": "253098783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "237490408",
            "title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation",
            "text": "We propose a unified framework for multimodal summarization that is able to produce extractive and abstractive text output, and jointly select image output for the final pictorial summary. Grounding on BART, our method uses an encoder-decoder architecture consisting of three training objectives, i.e., extractive & abstractive text summarization, and image selection. In particular, extractive and image selection objectives cooperatively supervise the encoder. To improve the image selection, we distill the knowledge from existing pre-trained vision-language models. We also propose a visual guided decoder that separately attends to the textual and visual modalities while performing abstractive text generation. Overall, our best model achieves a new state-of-the-art result on the MSMO dataset. We believe such a unified framework with multi-functional use can serve as a stepping stone to further improve multimodal summarization, as well as baselines against which future methods are tested.",
            "score": 0.5868590769639748,
            "section_title": "Conclusion",
            "char_start_offset": 27384,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "273962846",
            "title": "Autoregressive Models in Vision: A Survey",
            "text": "To further enhance the efficacy of visual in-context learning, CoTVL (Ge et al., 2023) successfully applies visual chain-of-thought prompt tuning for vision-language modeling, and performs better in tasks that require more reasoning abilities. These models highlight the potential for autoregressive frameworks to serve as foundational architectures in multi-modal applications, promoting a more holistic approach to understanding and generating content across modalities. \n\nThe previously mentioned multimodal tokenization involves encoding images into tokens, allowing Large Language Models (LLMs) to process both visual and language signals in a unified space. Visionto-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies. \n\nIn summary, the evolution of visual autoregressive models in the realm of multi-modality underscores their growing importance and versatility. From early integrations to sophisticated native architectures, these models continue to push the boundaries of what's possible in generating and understanding multimodal content. Future research directions may focus on improving model efficiency, scalability, and the ability to handle an even broader array of modalities.",
            "score": 0.5852128017750142,
            "section_title": "The Framework for Unifying Multimodal Understanding and Generation",
            "char_start_offset": 88771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1624
                }
            ],
            "ref_mentions": [
                {
                    "start": 698,
                    "end": 717,
                    "matchedPaperCorpusId": "268363788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "Large language models have been demonstrated to be capable of performing a diverse range of tasks out of the box [86,12,81,25,45] by training on large datasets with simple objectives [30,84,86,106]. \n\nIn vision, however, many scaling efforts have instead focused on training specialized models on a single task and modality, such as predicting masked RGB pixels [20,31,4,48,118,34], discrete tokens [7,132], or other (deep) features [117,6,82,114,36,70] from RGB inputs. Training models instead on multiple tasks [16,33,63,42,92,11] and modalities [85,122,76,59,133,1,3,57,43], or both [54,103,114,5,44] usually requires modality-specific modeling choices, making it difficult to extend these methods. \n\nWhile some recent works aim to consolidate various modalities and tasks by representing them as images [77,8,115], these approaches have limitations when dealing with modalities that cannot be readily converted into images, such as text or neural network feature maps. Instead, 4M adopts the approach of Pix2Seq [21,22] and Unified-IO [74] which addresses these issues by unifying the representation space on which models are trained through tokenization [110,35,30]. However, unlike methods like Unified-IO which operate on a single RGB image tokenizer, 4M's ability to work with multiple modality-specific tokenizers enables scaling to visual modalities beyond those that can be represented as images, such as neural network feature maps. 4M also builds upon the multimodal masking approach of MultiMAE [5] and extends it beyond image-like modalities. \n\nBoth token-based generative models [88,123,17,18,65] and diffusion models [89,79,95,97] have been mostly limited text-to-image generation.",
            "score": 0.5809030862885552,
            "section_title": "Related Work",
            "char_start_offset": 29223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1698
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 371,
                    "end": 374,
                    "matchedPaperCorpusId": "243985980"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "244346275"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "235436185"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "244117494"
                },
                {
                    "start": 433,
                    "end": 438,
                    "matchedPaperCorpusId": "245218767"
                },
                {
                    "start": 438,
                    "end": 440,
                    "matchedPaperCorpusId": "246652264"
                },
                {
                    "start": 513,
                    "end": 517,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "102496818"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "8070108"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "237291521"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "248834301"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 552,
                    "end": 556,
                    "matchedPaperCorpusId": "248512473"
                },
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "235694621"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "236635379"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "244799261"
                },
                {
                    "start": 568,
                    "end": 570,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "237204499"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 598,
                    "end": 600,
                    "matchedPaperCorpusId": "247939918"
                },
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "249712367"
                },
                {
                    "start": 811,
                    "end": 813,
                    "matchedPaperCorpusId": "251979350"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "254246343"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "237593074"
                },
                {
                    "start": 1020,
                    "end": 1023,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 1159,
                    "end": 1164,
                    "matchedPaperCorpusId": "20282961"
                },
                {
                    "start": 1164,
                    "end": 1167,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1167,
                    "end": 1170,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1509,
                    "end": 1512,
                    "matchedPaperCorpusId": "247939918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "237259899",
            "title": "CIGLI: Conditional Image Generation from Language & Image",
            "text": "Multimodal reasoning over visual and language is a long-standing research problem with the ultimate goal of building a system to connect and understand information across two vastly different inputs. Current research in this area primarily focuses on classification tasks, where the model jointly understands the text and image information to produce a classification label. State-of-the-art multimodal models separately model the caption and the images before jointly processing the combined representation to perform a TRUE/FALSE evaluation [16,13,2].\n\nOne such discriminative multimodal reasoning task is Natural Language Visual Reasoning (NLVR) [14] which defines a binary classification task to test model's reasoning capabilities across modalities. Given a sentence description and three synthetic images, the task is to predict whether the sentence correctly described the images or not. In NLVR2 [15], realistic natural images are added though the task re-* Equal Contribution mains the same.\n\nWhile advances in modeling have led to improved results on these tasks, current models are largely based around BERT-style autoencoding architectures which are not wellsuited for generation. As opposed to classification tasks which only require models to output a label, image generation from natural language captions presents further challenges in the multimodal regime as it requires a much richer output than a single discriminative label [1,4,10]. In this work, we propose a novel generative task: given a description describing two images, and one of the images, can a model learn to generate the second image that is semantically correct? For example, given a text description \"There are two dogs in total\", and a first image with one dog in it, the model should be able to generate the second image with exactly one dog. While recent work has explored conditional generation of images from natural language [9], to our knowledge there has been no prior work constraining generation based on both language and visual inputs. Our task provides the model with the caption and the first image in order to generate the second image, requiring the examination of both the language and visual prompts and understand the semantic relationships between the input text and image. This task requires the model to both understand the natural language caption and identify the missing visual components from the first image to generate the second image in the pair. Thus,",
            "score": 0.5803579437401347,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 550,
                    "end": 552,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 649,
                    "end": 653,
                    "matchedPaperCorpusId": "19435386"
                },
                {
                    "start": 1445,
                    "end": 1448,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 1448,
                    "end": 1450,
                    "matchedPaperCorpusId": "1930231"
                },
                {
                    "start": 1450,
                    "end": 1453,
                    "matchedPaperCorpusId": "1515901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "258833460",
            "title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge",
            "text": "In this paper, PGIM enables the integration of multimodal tasks into large language model by converting images into image captions. While PGIM achieves impressive results, we consider this Text-Text paradigm as a transitional phase in the development of MNER, rather than the ultimate solution. \n\nBecause image captions are inherently limited in their ability to fully capture all the details of an image. This issue may potentially be further resolved in conjunction with the advancement of multimodal capabilities in language and vision models (e.g., GPT-4).",
            "score": 0.580350798250858,
            "section_title": "Limitations",
            "char_start_offset": 23290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 560
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "275820726",
            "title": "Multi-aspect Knowledge Distillation with Large Language Model",
            "text": "Multimodal Large Language Models. Recently, Multimodal Large Language Models (MLLMs) [1,2,14,40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL [4] can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs. Motivated by this, we apply the rich knowledge of MLLMs to image classification. \n\nVisual tasks with linguistic information. Many studies [3,18,21,23,27,37,38] try to extract linguistic information from a large language model and use it to settle the visual problems. One method [18] leverages the linguistic knowledge for each visual category from LLM to generate the descriptions and use the descriptions in zero-shot image classification. Another method [37] creates a concise set of representative visual attributes from LLM by leveraging their learning-to-search method for interpretable visual recognition. While these methods focus on generating attributes for model training, our approach distills knowledge about various aspects, extending the model's output dimensions.",
            "score": 0.5793505511517054,
            "section_title": "Related Work",
            "char_start_offset": 4702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1589
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 90,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 581,
                    "end": 584,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "252111028"
                },
                {
                    "start": 960,
                    "end": 963,
                    "matchedPaperCorpusId": "258866192"
                },
                {
                    "start": 963,
                    "end": 966,
                    "matchedPaperCorpusId": "260682240"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "253735286"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "260682240"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "264145994",
            "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
            "text": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.",
            "score": 0.5788127885256944,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "252780868",
            "title": "Distill The Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation",
            "text": "In this part, we introduce the frame, symbol definitions and task goal of multimodal feature generation in advance. \n\nThe frame is composed of a multimodal feature generator F , a visual teacher model T and a multimodal student model S. The detailed architecture of each module is shown in Table 7 of the appendix. The model parameters of S are denoted as \u03b8 s . When the global text feature t is fed into S, the hidden representation produced by the l-th layer is denoted as \u03d5 S l t, \u03b8 s l . The F outputs a multimodal feature m, and the S produces an inverse feature I s after the S-conv1 layer. The real image and the inverse feature are {I s , I r } \u2208 R m * n * 3 . Given a feature I as input, the hidden representation produced by the l-th layer of T is denoted as \u03d5 T l (I). Our goal is to generate multimodal features from the source text to break the image-must restriction in testing. The visual perception of this multimodal feature is extracted from the visual distillation of the teacher-student model, while the textual semantic of that is derived from the text translation of the input text.",
            "score": 0.5777519295575804,
            "section_title": "Preliminaries",
            "char_start_offset": 7705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1104
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254638671875
        },
        {
            "corpus_id": "265033305",
            "title": "Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models",
            "text": "MDETR [19] model consists of an image and text encoders, and a multimodal transformer (see Figure 2). Given an image encoder f i and text encoder f t parameterized by \u03b8 i and \u03b8 t , we denote the output representations z i \u2208 R N \u00d7D and z t \u2208 R M \u00d7D as \n\nwhere x i and x t represent input image and text. N and M denote the number of image tokens and text tokens, respectively, D is the feature dimension. The image encoder is parameterized by a CNN (e.g., ResNet [17]). The output image features are flattened as sequential image tokens, which are added with a sequence of position embeddings to preserve the spatial information. The text encoder learns text representations through a pre-trained transformer language model RoBERTa [29]. The image and text features are projected into a shared embedding space with a modality dependent linear projection. The modality-specific representations z i and z t are then concatenated and passed to a transformer encoder f e , parameterized by \u03b8 e as \n\nwhere z e is the output of transformer encoder. The output representation z e as well as object queries z q \u2208 R L\u00d7D are fed into a transformer decoder f d , where L is the number of object queries. We denote the output object embeddings \n\nwhere \u03b8 d is the transformer decoder parameters.",
            "score": 0.5777159346399734,
            "section_title": "Preliminary From MDETR",
            "char_start_offset": 6894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 250
                },
                {
                    "start": 253,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "272689156",
            "title": "Keypoint-Integrated Instruction-Following Data Generation for Enhanced Human Pose and Action Understanding in Multimodal Models",
            "text": "Our model follows a multimodal approach, integrating both image data and text prompts to facilitate a comprehensive understanding of visual and textual content. As shown in Figure 1, the architecture consists of several key components: \n\nInput Layer: The model accepts two types of inputs: image data and text prompts. Image data, such as photographs containing human actions and poses, are processed by the visual encoder. Text prompts, comprising natural language instructions or questions, are directly fed into the language model. This bifurcation allows the model to handle both visual and linguistic modalities simultaneously, thereby offering richer contextual understanding. \n\nVisual Encoder: The image data is processed through a visual encoder that extracts pertinent visual features. In our model, we utilize the pre-trained CLIP visual encoder [29], which efficiently captures detailed image representations that are crucial for understanding human-centric actions. The visual encoder's output is then projected into a space compatible with the LLaVA core's language processing abilities . \n\nLanguage Model (LLaVA Core): Text prompts bypass the visual encoder and are directly input into the LLaVA core, a language model built upon the Vicuna architecture [30], known for its strong instruction-following capabilities . The language model processes these prompts and the encoded visual features, aligning the semantic representations of both visual and textual data.",
            "score": 0.5771783458896327,
            "section_title": "Overall Architecture",
            "char_start_offset": 18015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1478
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "233168627",
            "title": "Creativity and Machine Learning: A Survey",
            "text": "The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) [216] has an image encoder pre-trained together with a text encoder to generate a caption for an image. A Large-scale ImaGe and Noisy-text embedding (ALIGN) [132], based on similar mechanisms, can achieve remarkable performance through training based on a noisier dataset. In [272] the authors propose a frozen language model for multimodal few-shot learning: a vision encoder is trained to represent each image as a sequence of continuous embeddings, so that the frozen language model prompted with this embedding can generate the appropriate caption. \n\nIn [80] the authors present Bridging-Vision-and-Language (BriVL), which performs multimodal tasks by learning from weak semantic correlation data. Finally, there is a trend toward even more complex multimodal models. For example, Video-Audio-Text Transformer (VATT) [4] learns to extract multimodal representations from video, audio, and text; instead, Gato [225] serializes all data (e.g., text, images, games, other RL-related tasks) into a flat sequence of tokens that is then embedded and passed to a standard large-scale language model. Similarly, Gemini [93] achieves state-of-the-art performance in multimodal tasks by working on interleaved sequences of text, image, audio, and video as inputs; [94] extends it to Mixture-of-Experts setting. Finally, NExT-GPT [293] handles any combination of four modalities (text, audio, image, and video) by connecting a language model with multimodal adaptors and diffusion decoders (see Section 3.5).",
            "score": 0.5758672481159071,
            "section_title": "Examples of Models.",
            "char_start_offset": 40779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1819
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 323,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 475,
                    "end": 480,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 594,
                    "end": 599,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "249314857"
                },
                {
                    "start": 1139,
                    "end": 1142,
                    "matchedPaperCorpusId": "233346984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "269791266",
            "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
            "text": "Vision-Language Models are a family of models which aim to capture and exploit the relationship between text and images/video, and are capable of performing interactive tasks between the two modalities.The majority of VLMs have transformer-based architectures.By leveraging the attention module, visual and textual contents are conditioned on each other, which enables mutual interaction.In the following paragraphs, we briefly cover the applications of VLMs to discriminative and generative tasks.\n\nDiscriminative tasks involve predicting a certain characteristic about the data.VLMs, such as CLIP [174] and ALIGN [175], have shown exceptional performance in zero-shot transferability to unseen data in image classification.Both models comprise two modules: a visual encoder and a text encoder.Given an image and its category, CLIP and ALIGN are trained by maximizing the similarity between the image embedding and the textual embedding of the sentence \"a photo of a {image category}\".The zero-shot transferability is achieved by replacing \"{image category}\" with possible candidates during inference and searching for sentences matching best with the image.These two works have inspired numerous follow-ups such as [176,177,178,179], which further boost image classification accuracy.The models can also distill learned knowledge for other tasks, including object detection [180,181,182,183], image segmentation [184,185,186,187,188], document understanding [189,190] and video recognition [191].\n\nGenerative tasks utilize VLMs to generate text or images from input data.By leveraging large-scale training data, a single VLM can usually perform several image-to-text generation tasks, such as image captioning and visual question answering (VQA).Notable examples include SimVLM [192], BLIP [193], and OFA [194], etc.More powerful VLMs such as BLIP-2 [195], Flamingo [196], and LLaVA [197], are able to handle multi-turn conversations and reasoning based on the input image.Following the introduction of the diffusion model, text-to-image generation has also become a focus of the research community [198,199].",
            "score": 0.5731300539398955,
            "section_title": "2D Vision-Language models",
            "char_start_offset": 17148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 498
                },
                {
                    "start": 500,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1286
                },
                {
                    "start": 1286,
                    "end": 1498
                },
                {
                    "start": 1500,
                    "end": 1573
                },
                {
                    "start": 1573,
                    "end": 1748
                },
                {
                    "start": 1748,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2111
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 604,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 615,
                    "end": 620,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1217,
                    "end": 1222,
                    "matchedPaperCorpusId": "248006101"
                },
                {
                    "start": 1222,
                    "end": 1226,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "248721818"
                },
                {
                    "start": 1389,
                    "end": 1393,
                    "matchedPaperCorpusId": "245218534"
                },
                {
                    "start": 1414,
                    "end": 1419,
                    "matchedPaperCorpusId": "247794227"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "matchedPaperCorpusId": "252780581"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "245144732"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "245836975"
                },
                {
                    "start": 1431,
                    "end": 1435,
                    "matchedPaperCorpusId": "250895808"
                },
                {
                    "start": 1460,
                    "end": 1465,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "209515395"
                },
                {
                    "start": 1492,
                    "end": 1497,
                    "matchedPaperCorpusId": "251320177"
                },
                {
                    "start": 1780,
                    "end": 1785,
                    "matchedPaperCorpusId": "237291550"
                },
                {
                    "start": 1792,
                    "end": 1797,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1807,
                    "end": 1812,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 1868,
                    "end": 1873,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1885,
                    "end": 1890,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "270559550",
            "title": "CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation",
            "text": "Interleaved Image-Text Web Document Datasets. Training on interleaved image-text web documents has demonstrated superior performance over image-description pairs, as evidenced by studies such as Flamingo [25] and KOSMOS-1 [26]. These findings underscore the significant advantages of utilizing the richer and more meaningful correlations inherent in interleaved image-text documents [14]. However, the training data in these two studies is not made publicly available. \n\nThe scarcity of interleaved image-text data has been addressed by the introduction of two datasets: MMC4 [2] and OBELICS [14]. MMC4 builds upon the text-only C4 corpus [27] by integrating images into text passages using CLIP features. In contrast, OBELICS emphasizes comprehensive filtering strategies and preserves the original structure of web pages. These datasets present a diverse and extensive interleaved image-text corpus for multimodal language models. Despite their depth and breadth, they exhibit notable deficiencies in document quality, such as weaker completeness and image-text coherence (cf., Table 1). Additionally, as depicted in Figure 2, their documents lack comprehensiveness in the image modality, with most documents containing only one or two images. To address this, we propose CoMM that focuses explicitly on coherence-rich scenarios with more image illustrations. CoMM greatly enhances the baseline's [28] performance (in Table 2), highlighting the critical role of a high-quality coherent interleaved image-text training corpus. \n\nModeling of Interleaved Image-Text Data. The impressive performance of language modeling by LLM motivates researchers to delve deeper into the comprehension and generation capabilities of multimodal data. Flamingo [25] introduces image tokens into the language modeling process, facilitating interleaved image-text input and enabling few-shot transfer to tasks (e.g., VQA). Emu [29] leverages Stable Diffusion [30] as the image decoder, enabling the generation of image or text from interleaved image-text input. DreamLLM [8] pioneers the generation of free-form interleaved image-text outputs by modeling text and image within a unified multimodal space.",
            "score": 0.5728293330904706,
            "section_title": "Related Work",
            "char_start_offset": 9427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "277313749",
            "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
            "text": "Recent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023d), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2024c), which integrate additional encoders into textual LLMs to process inputs from other modalities. A typical MLLM architecture consists of an LLM, a modality encoder, and a projector that connects them. For example, LLaVA (Liu et al., 2024c) leverages a pretrained LLM and a pretrained vision encoder, followed by a two-stage training process. In the first stage, the model aligns text and image inputs using image-text pairs, training only the projector between the LLM and the modality encoder. The second stage involves fine-tuning the model on a visual instruction tuning dataset for improved instruction following ability. \n\nState-of-the-art MLLMs have demonstrated excellent performance in various vision scenarios, including single-image (Dai et al., 2024;Zhu et al., 2023;Zhang et al., 2024e), multi-image (Jiang et al., 2024a;Li et al., 2023c,b), and video settings (Li et al., 2025;Lin et al., 2023;Zhang et al., 2024f). Furthermore, models like LLaVA-OneVision (Li et al., 2024a) have achieved state-of-the-art results across a broad range of tasks and in all three vision scenarios (Li et al., 2024b;Zhang et al., 2024d). Despite these success, the use of MLLMs for enhancing text and image understanding in multimodal representations remains relatively under-explored. \n\nTo address this gap, we leverage a MLLM for unified representation learning and language generation by introducing a contrastive-autoregressive fine-tuning framework.",
            "score": 0.5726979128417279,
            "section_title": "Multimodal Large Language Models",
            "char_start_offset": 5630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1726
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 214,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 225,
                    "end": 247,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 259,
                    "end": 278,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1021,
                    "end": 1039,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 1056,
                    "end": 1076,
                    "matchedPaperCorpusId": "271745829"
                },
                {
                    "start": 1151,
                    "end": 1168,
                    "matchedPaperCorpusId": "265466723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "271601172",
            "title": "SynesLM: A Unified Approach for Audio-visual Speech Recognition and Translation via Language Model and Synthetic Data",
            "text": "Multimodal Language Models (MLMs). Recently, numerous MLMs have been proposed for various modalities. Advances in Visual Language Models (VLMs), such as [6,[13][14][15][16][17][18], have significantly improved the integration of visual information into pre-trained language models. These models use a pre-trained vision encoder for visual feature extraction, excelling in tasks like image captioning and visual question answering. Similarly, recent studies [8,[19][20][21][22][23][24][25] have begun exploring unified models for various speech and text tasks, employing self-supervised learning (SSL) feature extractors as audio encoders alongside pre-trained language models to enhance language comprehension. Some recent research [10,11,[26][27][28] has attempted to incorporate both visual and audio modalities into language models. However, these models can only process one specific modality with text and do not explore the interaction between audio and visual modalities. The most related work to ours is Video-SALMONN [29], but it lacks multilingual capabilities, limiting its ability to perform translation tasks. \n\nAV-ASR Methods. Several methods have been designed for AV-ASR tasks. AV-HuBERT [3] uses video recordings to learn robust speech representations through masked multimodal cluster prediction, focusing on lip motion. AVATAR [4] employs a multimodal encoder with a transformer decoder for natural language speech recognition output. AVFormer [5] explores injecting vision into frozen speech models for zero-shot AVSR, showing the potential for lightweight domain adaptation. Additionally, prompting-whisper [30] designs a cascade model that injects visual prompts for AV-ASR. However, these methods are limited to AV-ASR tasks or utilize cascade structures, while we design an end-to-end approach for multiple speech-visual tasks.",
            "score": 0.572536267684965,
            "section_title": "Related Work",
            "char_start_offset": 3867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "261881790"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "252111134"
                },
                {
                    "start": 480,
                    "end": 484,
                    "matchedPaperCorpusId": "259501685"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "265709786"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "259075356"
                },
                {
                    "start": 1026,
                    "end": 1030,
                    "matchedPaperCorpusId": "270703250"
                },
                {
                    "start": 1204,
                    "end": 1207,
                    "matchedPaperCorpusId": "245769552"
                },
                {
                    "start": 1463,
                    "end": 1466,
                    "matchedPaperCorpusId": "257804808"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86962890625
        },
        {
            "corpus_id": "276580677",
            "title": "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models",
            "text": "In this section, we introduce three main categories of multimodal models covered by our survey, including (i) Contrastive (i.e., Non-Generative) Vision-Language Models, Generative Vision-Language Models, and Text-to-image Diffusion Models. We choose these three families as they encompass the majority of the state-of-the-art architectures used by the community currently.",
            "score": 0.5700261573357426,
            "section_title": "Details on Model Architectures",
            "char_start_offset": 6539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2890625
        },
        {
            "corpus_id": "260775783",
            "title": "Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception",
            "text": "Multimodal Pretrained Models. Multimodal pretrained models (also referred to as foundation models [7] or base models [24]) are capable of processing, understanding, and generating data across multiple formats, such as images, text, and audio. These models are pretrained on large training datasets, and they have demonstrated strong empirical performance across a variety of tasks, such as question-answering and next-word prediction, even without further task-specific fine-tuning [2]. \n\nThe Generative Pretrained Transformer (GPT) series of models [2,23] consists of the most well-known multimodal pretrained models that can generate natural language or other data formats. In addition to GPT, pretrained models such as PaLM [5], BLOOM [28], Codex [4], and Megatron [31] also have the capability of generating outputs in natural language or other formats. Language generation is the core capability of these models, which we will use in the rest of the paper. Hence we denote this category of multimodal pretrained models as GLMs. \n\nVision-language models such as CLIP [24], Yolo [25], and the Segment Anything Model [14] are another type of multimodal pretrained model. CLIP takes an image and a set of texts as inputs, and measures the image-text consistency. Yolo, R-CNN [26] and Segment Anything Model are object detection models, which take an image and a set of words that describe objects, and classify whether the objects appear in the image. These models are capable of processing and understanding texts and images but are not capable of content generation. Finite State Automaton. A finite state automaton (FSA) is a tuple A = \u27e8\u03a3, \u0393, ,  0 , , \u27e9 where \u03a3 is the input alphabet (the set of input symbols), \u0393 is the output alphabet (the set of output symbols),  0 \u2208  is the initial state,  :  \u00d7 \u03a3 \u00d7  \u2192 {0, 1} is the transition function, and  :  \u00d7 \u03a3 \u00d7  \u2192 \u0393 is the output function. \n\nWe use  to denote the set of atomic propositions, which we use to define the input alphabet, \u03a3 2  .",
            "score": 0.5699396863942277,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 5319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 482,
                    "end": 485,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 550,
                    "end": 553,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "206594738"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "10328909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "258959284",
            "title": "Generating Images with Multimodal Language Models",
            "text": "Autoregressive language models (LMs) and large language models (LLMs) trained on text corpora have shown impressive abilities to efficiently adapt to other modalities. Prior work showcased the effectiveness of grounding text-only LMs to images for vision-and-language tasks [56,4,29,33,31,35], to embodied settings for robotics [3,18], offline reinforcement learning [48], and more. These methods typically keep most of the LLM weights frozen. This allows them to leverage the capabilities that the LLM learns during large scale text-only pretraining, such as the ability to learn from in-context examples [9], more effectively process longer context, and condition on inputs more strongly. \n\nIn this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1). Our findings show that it is possible to efficiently map the output embedding space of a frozen text-only LLM to that of a frozen generation model (in this work, Stable Diffusion [49]) despite both models using entirely different text encoders. We achieve this by finetuning a small number of parameters on image-caption pairs [52], in contrast to other methods which require interleaved image-text data [4,2]. Our approach is computationally efficient and does not require running the image generation model at training time. To achieve strong image generation performance, we propose efficient architectural changes to learn the LLM-to-generation mapping effectively with the GILLMapper module. GILLMapper is a lightweight Transformer [57] conditioned on special FROMAGe Retrieval only multimodal language model. Image outputs limited to the candidate retrieval set.",
            "score": 0.5694774990136948,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 1180,
                    "end": 1184,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1328,
                    "end": 1332,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98388671875
        },
        {
            "corpus_id": "263137930",
            "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
            "text": "Large Language Models (LLMs), known for their substantial size and complexity, have significantly enhanced the capacity of machines to understand and articulate human language. The progress in LLMs has also led to notable advancements in the vision-language domain [1,2,3,4], bridging the gap between image encoders and LLMs to combine their reasoning capabilities. Prior multimodal LLM research has concentrated on models that combine text and one other modality [3,5], such as text and image models, or has centered on proprietary language models that are not open sourced [2,4]. To tackle the previously mentioned challenges, we introduce Any-Modality Augmented Language Model (AnyMAL) -a collection of multi-modal encoders trained to transform data from various modalities, including images, videos, audio, and IMU motion sensor data, into the text embedding space of an LLM. To achieve this, we extend the work by [1] to (1) more capable instruction-tuned LLMs (i.e. LLaMA-2-70B-chat [6]), (2) larger pre-trained modality encoders, and (3) advanced projection layers to handle variable input lengths. The model output examples are shown in Figure 1, and an illustration of the overall methodology is shown in Figure 2. \n\nThe key contributions of the work are as follows: \n\n\u2022 We present an efficient and scalable solution for building Multimodal LLMs. We provide projection layers pre-trained on large datasets with diverse modalities (e.g. 200M images, 2.2M audio, 500K IMU time-series, 28M videos) all aligned to the same LLM (LLaMA-2-70B-chat), thus enabling interleaved multimodal in-context prompting. \n\n\u2022 We further fine-tune the model with the multimodal instruction set across three modalities (image, video, and audio) covering diverse unconstrained tasks beyond simple QA domains. The dataset features high-quality manually collected instruction data, which we thus also use as a benchmark for complex multimodal reasoning tasks.",
            "score": 0.5684377763974684,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 268,
                    "end": 270,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 575,
                    "end": 578,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 919,
                    "end": 922,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84912109375
        },
        {
            "corpus_id": "259164940",
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "text": "\u2022 Multimodal Input and Multimodal Output (MIMO): This task requires the model to perform multimodal reasoning and generation simultaneously (Koh et al., 2023;Jing Yu Koh, 2023;Chenfei Wu & Duan, 2023;Yang et al., 2023a). Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT and a series of pre-trained visual foundation models, allowing them \u00a7 https://github.com/zzw-zwzhang/Awesome-of-Multimodal-Dialogue-Models to accept and produce text and images during textual-visual conversations. GILL (Koh et al., 2023) proposes a mapping network that efficiently maps the output embedding space of a frozen text-only language model to that of a frozen generation model (e.g., Stable Diffusion (Rombach et al., 2022)). This mapping only requires fine-tuning a small number of parameters on image-caption pairs for tasks such as image retrieval, novel image generation, and multimodal dialogue. FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen. GPT4Tools (Yang et al., 2023a) introduces an instruction dataset and extends Visual ChatGPT (Chenfei Wu & Duan, 2023) to the image understanding task.",
            "score": 0.5663868886717619,
            "section_title": "Multimodal Dialogue Models",
            "char_start_offset": 33635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1253
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 739,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "268711537",
            "title": "Structure-Aware Multimodal Sequential Learning for Visual Dialog",
            "text": "The representations for the contextualized embedding tokens will be trained to ground the visual information into the language semantic space constructed from the history and utterance. \n\nIn the 3, using a matrix to encode the structural information brings another advantage when it comes to multimodal data. If the context consists of images along with a text sequence, the correlation of the image-text pair can be lost when each modality is encoded with the modality-specific pretrained models. By encoding the relationships between the image-text pair in the matrix, this can be prevented and the model can match the true pair. \n\nOutputs of cross-modulation blocks are contextualized embedding vectors. These vectors are trained and can be viewed as a contextualized representation of retrieved information. By using these, the language model can generate the answer by viewing the proper context at each turn.",
            "score": 0.5652103925384421,
            "section_title": "Retrieval-augmented Structural Alignment",
            "char_start_offset": 10846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 914
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81298828125
        },
        {
            "corpus_id": "270764892",
            "title": "Revision Matters: Generative Design Guided by Revision Edits",
            "text": "For models discussed in Sec.3.2, we formulate our input example as a sequence of tokens, including both text and visual ones.The input state representations (i.e., design code C and image I) are interleaved by a natural language instruction.We use such a language instruction to hint the pretrained model about image and text co-reference.During supervised fine-tuning, the input sequence in Tab. 2 is frozen, and the training only supervises on the output text tokens.\n\nTo unify multimodal features into the same sequence, we use an image tokenizer to generate visual encodings, which are then concatenated with text token embeddings.For both image and text tokenization, we use the builtin tokenizers in public Gemini-1.5backbone.",
            "score": 0.5651832679771418,
            "section_title": "Multimodal Example Construction",
            "char_start_offset": 13217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 28,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 469
                },
                {
                    "start": 471,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "276647135",
            "title": "Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think",
            "text": "Recent years have witnessed remarkable advancements in text-to-image generation, primarily driven by powerful diffusion models [3,13,19,37]. While these models excel at generating images that align with simple text prompts, they struggle to handle more complex instructions that interweave graphical and textual elements. Although condition augmentation methods like IP-Adapter [52] and Con-trolNet [54] enhance text-to-image models with additional low-level control signals such as canny edges, depth maps, or reference images, they lack the flexibility to process complex and high-level text-image interleaved instructions, for example, merging visual elements from multiple images using natural language descriptions. This inability restricts more creative image generation processes where users might want to precisely orchestrate visual compositions by combining and manipulating elements from multiple sources with simple text instructions. \n\nMeanwhile, Large Multimodal Models (LMMs) [1,6,26,45] have shown remarkable progress in understanding visual content and natural language instructions, enabling various tasks such as image captioning, visual question answering, and visual grounding. This advancement raises an intriguing question: Can we take advantage of the advanced visual language understanding capabilities of LMMs to improve diffusion-based image generation models, enabling more flexible text-image interleaved control? Several recent works have explored integrating LMMs with diffusion models to enhance image generation control. As shown in Figure 2, Emu-1 and 2 [41,42] incorporate a specialized regression head on the hidden output states of LMM tokens following multimodal input processing. Seed-Tokenizer [15] expands the LMM vocabulary with discrete vision tokens, which serve as condition for the diffusion model during image generation. BLIP-Diffusion [23] employs a multimodal query-transformer encoder to extract subject representations, which are then combined with text prompts to guide the generation process.",
            "score": 0.564364834597312,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "266374640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "Recent advances in multimodal training have significantly improved the integration of image understanding and generation within a unified model. This study investigates how vision-language models (VLMs) handle image-understanding tasks, specifically focusing on how visual information is processed and transferred to the textual domain. We compare VLMs that generate both images and text with those that output only text, highlighting key differences in information flow. We find that in models with multimodal outputs, image and text embeddings are more separated within the residual stream. Additionally, models vary in how information is exchanged from visual to textual tokens. VLMs that only output text exhibit a distributed communication pattern, where information is exchanged through multiple image tokens. In contrast, models trained for image and text generation tend to rely on a single token that acts as a narrow gate for visual information. We demonstrate that ablating this single token significantly deteriorates performance on image understanding tasks. Furthermore, modifying this token enables effective steering of the image semantics, showing that targeted, local interventions can reliably control the model's global behavior.",
            "score": 0.5633452330841138,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Unifying multimodal understanding and generation capabilities within a single model is a critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b;Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing a unified framework that excels at both comprehending and generating multimodal content remains a significant challenge in the field of artificial intelligence. \n\nRecent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging from detailed visual analysis to creative image synthesis. \n\nHowever, existing MLLM-based methods (Sun et al., 2023;2024b) face a common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from a visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents a significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks.",
            "score": 0.5626860543289481,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "265659463",
            "title": "Towards More Unified In-Context Visual Understanding",
            "text": "Multimodal understanding and generation represent an emerging frontier in artificial intelligence that seeks to interpret and synthesize information across various forms of data, such as text, images, sounds, and even more modalities. Inspired by the success of ChatGPT as well as GPT-4 [32,33], recent works primarily concentrate on aligning visual features with the pre-trained LLMS for multimodal comprehension tasks [18,24,26,27,44,45,53,57]. While pre-trained LLMs have empowered systems to follow human instructions for vision-language interactions, their application has been confined to generating textual outputs. \n\nExpanding the horizons of multimodal capabilities, a burgeoning spectrum of studies [15,21,40,42,51,54] are pioneering innovations in both understanding and generative capacities across modalities. IMAGEBIND [15] utilizes the image-paired data to connect five different modalities with a single joint embedding space, demonstrating impressive zero-shot capabilities across these modalities. Oth-erwise, CoDi [42] introduces a composable generation strategy by bridging alignment in the diffusion process, facilitating the synchronized generation of any combination of output modalities, including language, image, video, or audio. Furthermore, NExT-GPT [51] integrates an LLM with multimodal adaptors and diverse diffusion decoders, enabling it to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio with understanding and reasoning. \n\nHowever, these models are not designed for in-context learning, without the benefit of the multiple prompts. \n\nMixture of Experts models. Mixture of Experts (MoEs), which have demonstrated remarkable success in both computer vision [28,35,46] and natural language processing [11,14,22,36,59] with the context of conditional computation. Conditional computation aims to increase the number of model parameters without significantly increasing computational cost by selectively activating relevant parts of the model based on input-dependent factors [6,9]. [36] first provides compelling evidence for the efficacy of MoEs by incorporating MoE layers into LSTM models.",
            "score": 0.5626422907631559,
            "section_title": "Related Works",
            "char_start_offset": 6388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 1743,
                    "end": 1746,
                    "matchedPaperCorpusId": "235417196"
                },
                {
                    "start": 1746,
                    "end": 1749,
                    "matchedPaperCorpusId": "119184034"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1786,
                    "end": 1789,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 2055,
                    "end": 2058,
                    "matchedPaperCorpusId": "6083596"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "More recently, research has shown success in training multimodal-output vision-language models (VLMs) from scratch [20,26], achieving performance close to VLMs that only output text on visual understanding tasks. \n\nWhile recent studies have examined the internal mechanisms of unimodal-output VLMs [27][28][29][30], little to none is known about how different modalities interact in the hidden representations of multimodal-output models or whether Chameleon and Emu3 (left-panel), early-fusion and multimodal-output models, encode visual and textual tokens in separated regions, and knowledge transfer from image to text happens through a narrow gate, the end-of-image token [EOI]. On the contrary, in models like Llava (center-panel) designed for text generation, the image-text communication is distributed across many internal image tokens that directly communicate with the textual ones, and late-layer visual tokens tend to align more text tokens in the embedding space. The right panel shows that when a narrow gate is present, ablating the attention given by textual tokens to [EOI]has a stronger impact on performance than ablating the whole image-text attention. Results refer to image captioning on MS-COCO and are expressed as a fraction of performance without any ablation (Sec. 4.3 and Tab. 1). \n\ntheir information processing resembles that of text-only VLMs. In this work, we focus on some of the latest earlyfusion multimodal-output VLMs, Chameleon [20] and Emu3 [26], and compare them with a range of unimodal-output VLMs, such as Llava [11], on how they transfer information from the visual domain to the textual domain in various image understanding tasks. \n\nIn Chameleon and Emu3, we observe that image and text representations remain well separated in different clusters from input to output, while in models like Llava they tend to mix in late layers (Sec. 4.1). Additionally, in Llava the information flow from image to text is distributed across multiple tokens.",
            "score": 0.5616871103804317,
            "section_title": "Introduction",
            "char_start_offset": 1531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1675
                },
                {
                    "start": 1678,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 306,
                    "matchedPaperCorpusId": "270285703"
                },
                {
                    "start": 310,
                    "end": 314,
                    "matchedPaperCorpusId": "257766294"
                },
                {
                    "start": 1554,
                    "end": 1558,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "273403725",
            "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "text": "Multimodal large language models (MLLMs) integrate both text and images [6,80,81]. By leveraging pretrained LLMs, MLLMs [1,2,12,51,55,82,95] demonstrate a robust ability to understand and process multimodal information. Recent advancements have explored extending MLLMs with pretrained diffusion models to facilitate image generation [27,29,36,75,76]. These methods fall under the category of tool utilization, where diffusion models are used to generate images based on the conditions output by the MLLM, while the MLLM itself does not have the ability to directly perform visual generation. Moreover, the generative ability of the entire system is often constrained by the external diffusion model, making its performance inferior to directly using the diffusion model on its own [27,75].",
            "score": 0.5616606087193025,
            "section_title": "Multimodal Understanding",
            "char_start_offset": 6147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 790
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 131,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 134,
                    "end": 137,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "266374640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9365234375
        },
        {
            "corpus_id": "255394080",
            "title": "Progressive Text-to-Image Generation",
            "text": "The task of text-to-image generation aims to create natural and consistent images from the input text and has received extensive research interest. Recently, latent autoregressive (AR) generation frameworks have achieved great success in advancing the start-of-the-arts, by learning knowledge and patterns from a large-scale multimodal corpus [35,42,64]. Generally, they treat the task as a form of language modeling and use Transformer-like [59] architectures to learn the relationship between language inputs and visual outputs. A key component of these approaches is the conversion of each image into a sequence of discrete units through the use of a VQ-VAE [57] based image tokenizer, e.g., VQ-GAN [16,63], RQ-VAE [32] and ViT VQ-GAN [63]. Visual tokenization essentially unifies the view of text and images so that both can be treated simply as sequences of discrete tokens and is adaptable to sequenceto-sequence models. To that end, DALL-E [47], CogView [12], RQ-Transformer [32], and Parti [65] employ autoregressive models to learn text-to-image task from a large collection of potentially noisy text-image pairs [5,17,27]. In particular, [62] further expand on this AR over AR modeling approach to support arbitrarily-sized image generation. \n\nAnother research line for text-to-image generation involves diffusion-based methods, such as GLIDE [38], DALL-E 2 [46], stable diffusion [49], RQ-Transformer [32], and Imagen [51]. These models pursue to directly generate images or latent image features with diffusion process [11,22] and produce high-quality images with great aesthetic appeal. Even so, discrete sequence modeling for text-to-image generation remains appealing given extensive prior work on large language models [4] and advances in discretizing other modalities, such as video and audio, as cross-language tokens [3].",
            "score": 0.5611963321929531,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 982,
                    "end": 986,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 1122,
                    "end": 1125,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 1128,
                    "end": 1131,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1391,
                    "end": 1395,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1535,
                    "end": 1538,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "258947258",
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "text": "To integrate vision and language, we learn translation parameters to map between the image and text embedding spaces. This extends a LLM for multimodal inputs and outputs. \n\nMapping image-to-text. We learn a linear mapping W c \u2208 R m\u00d7kd which maps visual embeddings v \u03d5 (y) from the visual model for image y as v \u03d5 (y) T W c \u2208 R k\u00d7d (after reshaping kd to k \u00d7 d). This represents a sequence of k vectors of the same hidden dimensionality d as the text embeddings that the LLM produces for input tokens. \n\nMapping text-to-image. Our approach aims to retrieve images using the outputs of an autoregressive language model. A challenge with this is that autoregressive causal attention over text is strictly less expressive than the bidirectional attention typically used in previous models (Radford et al., 2021;Jia et al., 2021). In order to bootstrap strong retrieval abilities on our autoregressive LLM, we propose adding a special [RET] token to the model vocabulary and learning its embeddings (keeping all other token embeddings frozen). During training, we append [RET] to the end of input captions. This allows the model to perform an extra step of attention over all tokens in the text to produce a stronger text representation for the caption. We found that this significantly improves image-text retrieval performance (see Sec. 5.1 for analysis). This also allows our model to learn to generate [RET] at inference time (Fig. 1), seamlessly interleaving image retrieval within generated text. \n\nFinally, to map the model's output representations to visual space, we train a linear mapping W t \u2208 R p\u00d7q . This maps the hidden representation of [RET] from the last hidden layer of the LLM, h \u03b8 (x i ) \u2208 R p , into a vector space for retrieval, where q is a dimension smaller than p. Similarly, we train another linear mapping W i \u2208 R m\u00d7q to map the visual embeddings v \u03d5 (y i ) into the same retrieval space.",
            "score": 0.5606248348673254,
            "section_title": "Translating Between Image-and-Text",
            "char_start_offset": 8896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 786,
                    "end": 808,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 808,
                    "end": 825,
                    "matchedPaperCorpusId": "231879586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "276581041",
            "title": "PosterSum: A Multimodal Benchmark for Scientific Poster Summarization",
            "text": "Multimodal Large Language Models. After the emergence of LLMs, recent work (Liu et al., 2023;Wang et al., 2024b;Alayrac et al., 2022) investigated their use in processing multimodal inputs, giving rise to Multimodal Large Language Models (MLLMs). The core idea in this line of research is to align visual and textual features by using shared representations. This framework typically involves using a pre-trained visual encoder to extract visual features, a projection layer to map visual represen-tations into corresponding text representations, and a pre-trained LLM to generate textual responses, allowing the model to condition the output on visual and textual inputs. MLLM architectures such as LLaVA (Liu et al., 2023) and MiniCPM (Yao et al., 2024) demonstrated impressive zero-shot generalization across diverse visual and language tasks. However, most existing MLLMs focus on general domain tasks and relatively simple visual inputs; the challenge of understanding complex and information-dense visual documents like scientific posters remains under-explored. \n\nSummarization in Scientific Domains. Scientific summarization consists of generating concise summaries for scientific content (Yasunaga et al., 2019;Cachola et al., 2020;Ju et al., 2021;Sotudeh and Goharian, 2022). Several scientific summarization benchmarks have been proposed, designed to process modalities such as videos (Lev et al., 2019;Chen et al., 2024), slide decks (Tanaka et al., 2023), surveys (Liu et al., 2024d), and research papers (Takeshita et al., 2024;Liu et al., 2024a). However, scientific poster summarization remains unexplored despite the widespread use of posters in academic communication.",
            "score": 0.5603335564863356,
            "section_title": "Related Work",
            "char_start_offset": 3730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 93,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 93,
                    "end": 112,
                    "matchedPaperCorpusId": "265034288"
                },
                {
                    "start": 112,
                    "end": 133,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 706,
                    "end": 724,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 737,
                    "end": 755,
                    "matchedPaperCorpusId": "269004627"
                },
                {
                    "start": 1197,
                    "end": 1220,
                    "matchedPaperCorpusId": "58053521"
                },
                {
                    "start": 1220,
                    "end": 1241,
                    "matchedPaperCorpusId": "216867622"
                },
                {
                    "start": 1241,
                    "end": 1257,
                    "matchedPaperCorpusId": "238259960"
                },
                {
                    "start": 1257,
                    "end": 1284,
                    "matchedPaperCorpusId": "249282538"
                },
                {
                    "start": 1396,
                    "end": 1414,
                    "matchedPaperCorpusId": "174798022"
                },
                {
                    "start": 1414,
                    "end": 1432,
                    "matchedPaperCorpusId": "268554286"
                },
                {
                    "start": 1446,
                    "end": 1467,
                    "matchedPaperCorpusId": "255749397"
                },
                {
                    "start": 1477,
                    "end": 1496,
                    "matchedPaperCorpusId": "271931329"
                },
                {
                    "start": 1518,
                    "end": 1542,
                    "matchedPaperCorpusId": "268296824"
                },
                {
                    "start": 1542,
                    "end": 1560,
                    "matchedPaperCorpusId": "268691679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "261043320",
            "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
            "text": "A popular format for vision and language models is (image, text) \u2192 text, i.e., models take as input an image and some text, and produce text as output, e.g., BLIP-2 [22].The flexible format directly supports tasks like image classification and visual question answering (VQA). \n\nHowever, assuming a single image as input is limiting: autoregressive vision-language models enable new capabilities by instead mapping an arbitrarily interleaved sequence of images and  . More details regarding selecting demonstrations can be found in Section 3.4. We compare OpenFlamingo-3B and -4B models to Flamingo-3B, and OpenFlamingo-9B to Flamingo-9B. Error bars are standard deviations over settings. \"OF-3B (I)\" refers to OpenFlamingo-3B (Instruct), the 3B model trained with a language-instruction-tuned backbone. \n\ntext to textual outputs. This interface provides important flexibility: the input sequence can include demonstrations for a new task, enabling fewshot, in-context learning [3] or multi-round multimodal chatbot interactions. Evaluations suggest that autoregressive vision-language models can be performant foundation models [5]: models like Flamingo [3], CM3 [1], Kosmos-1 [12], PALM-E [8], and multimodal GPT-4 [28] generalize well across diverse vision-language tasks. spacesuit. \n\nAn apple with the word \"iPod\" written on it.",
            "score": 0.5600669118389622,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1333
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 978,
                    "end": 981,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1155,
                    "end": 1158,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2451171875
        },
        {
            "corpus_id": "271245054",
            "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
            "text": "By designing prompts to project multimodal inputs into the same embedding spaces, we show that MLLMs can represent multimodal inputs correctly even without fine-tuning.\u2022 We introduce a new framework, E5-V, to adapt MLLMs for achieving universal multimodal embeddings.With single modality training on text pairs, E5-v even achieve better multimodal embeddings than image-text pairs.\u2022 Extensive experiments on text-image retrieval and composed image retrieval tasks demonstrate the effectiveness of E5-V in representing multimodal information.E5-V successfully transfers single modality representation capabilities to multimodal embeddings by following task-specific prompts that were not included in the training data.To achieve universal multimodal embeddings, there are several works, such as and UNIIR [WCC + 23], fine-tune CLIP on interlevaed dataset with fusion modal to fuse the visual and language information.There are also some works like VISTA [ZLX + 24] or UniVL-DR [LXL + 22] feed text encoder with CLIP outputs to input visual information.However, it can harm the original text-image retrieval performance of CLIP, and is hard to make text encoder understand the visual information with only contrastive learning, which show poor zero-shot performance on composed image retrieval tasks.\n\nTo achieve universal multimodal embeddings, several works, such as UNIIR [WCC + 23], finetune CLIP with a fusion model to integrate visual and language information.Other works, like VISTA [ZLX + 24] or UniVL-DR [LXL + 22], feed the text embedding models with CLIP outputs to incorporate visual information.However, this approach can harm the original text-image retrieval performance of CLIP and makes it difficult for the text embedding models to understand visual information using only contrastive learning.As a result, these methods show poor zero-shot performance on composed image retrieval tasks.Moreover, these methods require large interleaved training data to achieve universal multimodal embeddings.",
            "score": 0.5599114956736954,
            "section_title": "Introduction",
            "char_start_offset": 3783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1298
                },
                {
                    "start": 1300,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1903
                },
                {
                    "start": 1903,
                    "end": 2010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "271097376",
            "title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
            "text": "In the rapidly evolving domain of large language models (LLMs) [53,8,10] and multimodal large language models (MLLMs) [23,60,30,40,6,29,58,26,46,57,15,14,55,11,61,47,24], recent research, such as StoryGPTV [44], explores using MLLMs for story generation by converting visual features into token embeddings.StoryGPTV requires character and object masks for training its LDM.In contrast, our model, with the visual tokenization and de-tokenization, have a stronger visual comprehension ability, making it trains directly from interleaved text and image data without the need for masks.MM-interleaved [52] design an MMFS module to process interleaved text-image data and achieve multimodal story generation through image features and LLM output.While our approach directly generates images through LLM output tokens.We take MM-interleaved as an important baseline and compare with it.\n\nVisual Story Dataset In the landscape of datasets for visual storytelling, various collections have been developed.The VIST [20] dataset is noteworthy for its use of realistic images, though it struggles with maintaining character consistency across stories.The Pororo [25] and Flintstones [18]  datasets, while popular for animation-based story datasets, are hindered by their low resolution and the simplicity of their accompanying texts.Another significant dataset is StorySalon [27], which offers high-resolution images and is large in scale, but it lacks global consistency across a series of images.To address these gaps, we introduce StoryStream, a globally consistent, large-scale, high-resolution animated style dataset with engaging, narrative-rich text for complex storytelling, overcoming the limitations of existing datasets.The analysis is shown in Table 1.\n\n3 Method",
            "score": 0.5597656501652131,
            "section_title": "MLLM for Multimodal Story Generation",
            "char_start_offset": 5916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 881
                },
                {
                    "start": 883,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1323
                },
                {
                    "start": 1323,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1754
                },
                {
                    "start": 1756,
                    "end": 1764
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 69,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "2574224"
                },
                {
                    "start": 1152,
                    "end": 1156,
                    "matchedPaperCorpusId": "54457433"
                },
                {
                    "start": 1173,
                    "end": 1177,
                    "matchedPaperCorpusId": "4752763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "267028646",
            "title": "OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality",
            "text": "Multimodal Large Language Models (MLLMs): Multimodal large language models can accept images and text as input, reason about them, and generate a text response. We used two such models, GPT4-V [16] and LLaVa-1.5 [17], to create a list of nouns by prompting them with the image and the following instructions:",
            "score": 0.5597300276218705,
            "section_title": "Stage 1. Image Understanding",
            "char_start_offset": 10685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "277066741",
            "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization",
            "text": "Multimodal Large Language Models (MLLMs) [1,7,13,16,19,34,40,46,47] have shown remarkable advancements across a wide range of vision-language understanding tasks, demonstrating their capabilities in comprehending and analyzing visual contents across various application domains. Early research on MLLMs primally focuses on text generation based on text prompts and input multiple modalities such as images [18,19], videos [8,31]. Recent advancements further enhance the capabilities of MLLMs from various aspects. For example, recent models [23,39] incorporate multimodal inputs and outputs such as video, audio, and point cloud inputs beyond text and images. In addition, some efforts attempt to adapt MLLMs for domain-specific tasks, such as medical image understanding [15,17,50] and document analysis [20,44]. In this work, we focus on enhancing the reasoning ability of MLLMs in tackling complex reasoning tasks and introduce R1-VL, a series of MLLMs that have superior reasoning capability.",
            "score": 0.5590433815270823,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 6160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 996
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "271947166"
                },
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "257913547"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "274023126",
            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
            "text": "These output modalities also correspond to specific multimodal model architectures, specifically any-totext by LLM-backbone models and any-to-image by diffusionbackbone models. While these surveys have made valuable contributions, they lack a unified framework that spans a broad range of modalities (i.e., text, images, audio, and video) within different generative systems, as shown in Fig. 1.",
            "score": 0.5585385263552101,
            "section_title": "Any-to-Text Models Any-to-Vision Models Any-to-Any Models",
            "char_start_offset": 3538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "269187998",
            "title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
            "text": "The rapid advancement of artificial intelligence has revolutionized the way we interact with technology, giving rise to the development of sophisticated AI agents capable of performing complex tasks and making decisions based on various forms of input, including natural language [53,44,51,15,20] and visual information [9,23].These agents have the potential to automate a wide range of processes, from simple tasks like image recognition and language translation to more complex endeavors such as medical diagnosis [18,34] and autonomous navigation [13,16].At the heart of these AI agents are multimodal language models, which enable them to understand and generate human-like responses by processing and integrating multiple modalities of data, such as text, images, and even audio or video.\n\nMultimodal language models represent a significant leap forward from traditional language models, which primarily focus on processing and generating text.By incorporating visual information, these models can better understand the context and semantics of the input data, leading to more accurate and relevant outputs.For example, a multimodal language model trained on a dataset of images and their corresponding captions would be able to generate more descriptive and contextually appropriate captions for new images compared to a text-only model.The ability to process and integrate multiple modalities of data is crucial for the development of multimodal AI agents, as it allows them to perform tasks that require an understanding of both language and visual information, such as visual question answering, image-guided navigation, and multimodal sentiment analysis.\n\nOne of the key challenges in developing multimodal language models is effectively encoding visual information into a format that can be processed by the model.This is typically achieved through the use of convolutional neural networks (CNNs) [26] or transformer-based architectures [50], such as the Vision Transformer (ViT) [3].CNNs have been widely used in computer vision tasks for their ability to extract hierarchical features from images, allowing the model to learn increasingly complex representations of the input data.On the other hand, transformer-based architectures like ViT have recently gained popularity due to their ability to capture long-range dependencies and model global context, which is particularly useful for understanding the relationships between objects in an image.",
            "score": 0.5578299346023454,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 793
                },
                {
                    "start": 795,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1664
                },
                {
                    "start": 1666,
                    "end": 1825
                },
                {
                    "start": 1825,
                    "end": 1995
                },
                {
                    "start": 1995,
                    "end": 2194
                },
                {
                    "start": 2194,
                    "end": 2461
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "260499677"
                },
                {
                    "start": 293,
                    "end": 296,
                    "matchedPaperCorpusId": "251872344"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "3617644"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "259303296"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "258841310"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "265308931"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "259924488"
                },
                {
                    "start": 1908,
                    "end": 1912,
                    "matchedPaperCorpusId": "214803074"
                },
                {
                    "start": 1991,
                    "end": 1994,
                    "matchedPaperCorpusId": "232417054"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78271484375
        },
        {
            "corpus_id": "262054213",
            "title": "Language as the Medium: Multimodal Video Classification through text only",
            "text": "Mini-GPT4 [19] leverages the Q-Former to provide a soft prompt to a Llamabased model. In contrast to these techniques, we demonstrate that using only text as the medium can convey multimodal information to downstream LLMs. This has several key advantages. Firstly, this approach ensures a straightforward \"plug and play\" interface for chaining models without extra adaptation. This is particularly relevant with the rise of API-based language models that prohibit modifications. Secondly, inter-model communication becomes transparent and interpretable in natural language. Crucially, this method simplifies tasks like multimodal video classification into two phases: a \"perception\" phase using unimodal or multimodal models as surrogates for various senses, followed by a \"reasoning\" phase where a foundation model consolidates diverse inputs to create a comprehensive video narrative. \n\nMore recent methods such as LENS [6] or Video Chat-   Visual Captioning \"a boy is holding two orange balls\" Audio Tags \"Basketball bounce\" \"a boy is holding two orange balls\" \"a boy is holding two orange balls\" \n\nFigure 1. Our method combines a \"perception\" module, which uses visual and auditory models to get multimodal textual descriptors as sensory proxies for \"sight\" and \"hearing\", and a \"reasoning\" module that processes these textual inputs to form a coherent narrative and identify the likeliest content in the video, completed by justifications. \n\nCaptioner [8] explore similar textual interactions between models. While LENS only explores the ability of LLMs to reason over visual question answering tasks given visual clues about images, Video ChatCaptioner proposes chaining together BLIP-2 and ChatGPT in order to have conversations about images. Our method goes beyond just question answering tasks, demonstrating that both visual and auditory clues can be used by LLMs for video classification. In summary, our contributions are: 1) We introduce a new multimodal classification approach consisting of two phases: a \"perception\" phase where models act as sensory proxies and a \"reasoning\" phase that consolidates multimodal textual inputs into a coherent narrative. 2) We demonstrate the efficacy of text as the primary medium of interpreting multimodal data.",
            "score": 0.5570439809326605,
            "section_title": "Introduction",
            "char_start_offset": 2107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "266210376",
            "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",
            "text": "VL-GPT aims to process the vision and language understanding and generation within a single transformer model in a unified way, similar to GPT handles language tasks. It is capable of perceiving the interleaved multi-modal data and generating content across various modalities. By employing unified modeling, our VL-GPT can conduct autoregressive pre-training on web-scale multimodal corpora, thereby holding the potential to serve as a powerful foundation model in the multimodal research community. \n\nArchitecture As depicted at the bottom of Fig. 1, our VL-GPT comprises five components: a large vision-language transformer model M, an image tokenizer E v , a text tokenizer E t , an image detokenizer D v and a text detokenizer D t . In comparison to a language model, VL-GPT incorporates additional image tokenizer and image detokenizer elements. \n\nGiven any interleaved image-text data, the image tokenizer and the text tokenizer initially encode them into a multimodal sequence. More specifically, the image tokenizer E v converts each image into N continuous visual embeddings x v . Additionally, two special tokens [IMG] and [/IMG] are appended at the beginning and end of the visual embeddings, respectively. The visual embeddings are then combined with the discrete text tokens encoded by the text tokenizer E t to form a interleaved multimodal sequence v = (v 1 , v 2 , . . . , v n ), where v i can be either a discrete text token or a continuous visual embedding. The multimodal sequence v is then fed into the large VL model M for unified auto-regressive modeling. \n\nThe output embedding M(v i ) can be flexibly transformed into a text embedding through a language modeling head for the predefined vocabulary or into a visual embedding with a separate regression head. During training, the selection of the transformed head depends on whether the target for the current embedding is a text token or a visual embedding.",
            "score": 0.556635108441198,
            "section_title": "VL-GPT",
            "char_start_offset": 13294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1578
                },
                {
                    "start": 1581,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1932
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "270702315",
            "title": "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks",
            "text": "Most recently, the advent of multimodal models has significantly expanded the frontiers of Artificial Intelligence research and application, showcasing remarkable versatility and efficiency across many tasks that were once the exclusive domain of single-modality large language models (LLMs).Unlike their predecessors, which primarily processed and generated text-based information, multimodal models are adept at understanding, interpreting, and generating outputs that span across multiple forms of data, including but not limited to text, images, audio, and video.This capability not only enhances the models' comprehension and interaction with the world but also aligns more closely with human cognitive processes, which are inherently multimodal.Integrating multiple data types allows these models to perform a broader spectrum of tasks with enhanced accuracy, creativity, and contextual understanding, thus opening new avenues for AI applications in more nuanced and complex scenarios.\n\nMultimodal models have found application in diverse tasks, illustrating their flexibility and the breadth of their utility.Some of the most notable applications include visual question answering (VQA), where the model responds to text-based questions about visual content; image captioning, which involves generating descriptive text for images; and audio-visual speech recognition, where the model uses both visual and auditory signals to improve speech recognition accuracy.Additionally, these models have been successfully employed in sentiment analysis by analyzing text and vocal tones, in object detection and recognition by integrating spatial data with visual inputs, and in machine translation, where visual context enhances the accuracy of text translations.\n\nAnother application of multimodal models is the generation of step-by-step instructions for complex tasks.This involves not just understanding a text-based query but also analyzing accompanying visual data to produce a coherent, easily understandable sequence of instructions.A multimodal model needs to have a sufficient temporal understanding of events and actions taking place within video inputs.Models should also be able to capture a detailed understanding of the relationships among different actions presented.Additionally, the reasoning ability to work through the relationships between different tools, ingredients, or materials is necessary for a meaningful understanding of inputted tasks.This capability represents a significant leap forward in models' potential to assist with practical, everyday tasks, offering personalized guidance that considers the unique aspects of each scenario.",
            "score": 0.5561089243481123,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 991
                },
                {
                    "start": 993,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1761
                },
                {
                    "start": 1763,
                    "end": 1869
                },
                {
                    "start": 1869,
                    "end": 2039
                },
                {
                    "start": 2039,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2281
                },
                {
                    "start": 2281,
                    "end": 2464
                },
                {
                    "start": 2464,
                    "end": 2663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "269157508",
            "title": "Autoregressive Pre-Training on Pixels and Texts",
            "text": "The integration of visual and textual information represents a promising direction in the advancement of language models. In this paper, we explore the dual modality of language\u2014both visual and textual\u2014within an autoregressive framework, pre-trained on both document images and texts. Our method employs a multimodal training strategy, utilizing visual data through next patch prediction with a regression head and/or textual data through next token prediction with a classification head. We focus on understanding the interaction between these two modalities and their combined impact on model performance. Our extensive evaluation across a wide range of benchmarks shows that incorporating both visual and textual data significantly improves the performance of pixel-based language models. Remarkably, we find that a unidirectional pixel-based model trained solely on visual data can achieve comparable results to state-of-the-art bidirectional models on several language understanding tasks. This work uncovers the untapped potential of integrating visual and textual modalities for more effective language modeling. We release our code, data, and model checkpoints at https://github.com/ernie-research/pixelgpt.",
            "score": 0.5558029114732412,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Multimodal models has begun to adopt an end-to-end training method, which means that the model can learn the characteristics of images and text simultaneously within a single framework, rather than processing them separately.This approach enables the model to better capture the complex relationship between visual and textual information.The high degree of integration and collaboration of this model relies on two key components: text encoder and visual encoder.\n\nThe main responsibility of the text encoder module is to understand and process natural language text.By applying deep learning and natural language processing techniques, especially Transformer architecture-based models such as BERT and GPT, these encoders are able to deeply analyze the semantics and structure of text.They are not only able to identify keywords and phrases in text, but also understand more complex language structures and meanings, allowing the model to understand language information at a deeper level.\n\nMeanwhile, the visual encoder module focuses on extracting and understanding visual information from images or videos [195,196,197].These encoders are usually based on convolutional neural networks (CNN) or Transformerbased vision models that have emerged in recent years, such as Vision Transformer (ViT).Their function is to capture elements such as objects, textures, colors, etc. in the image and convert this visual information into a form that allows the model to be further processed and analyzed.\n\nImage-text multimodal models combine information from image and text modalities.For example, CLIP [38], a visual encoder models image information, while a text encoder models text information.The feature vectors of images and text can be pre-calculated and stored, and the modal interaction is handled through the cosine similarity of the image and text feature vectors.Since CLIP was proposed, it has become a classic in the field of multimodal learning and is widely used as the basic model of today's multimodal models.CLIP uses 400 million pairs of (image, text) data for training through self-supervision.It maps images and text into a shared vector space, allowing the model to understand the semantic relationship between images and text.This approach represents a new way to learn visual models under natural language supervision.",
            "score": 0.555754919317823,
            "section_title": "Recent Image-text Multimodal Models",
            "char_start_offset": 14620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 464
                },
                {
                    "start": 466,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 991
                },
                {
                    "start": 993,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1497
                },
                {
                    "start": 1499,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1869
                },
                {
                    "start": 1869,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2109
                },
                {
                    "start": 2109,
                    "end": 2244
                },
                {
                    "start": 2244,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 1111,
                    "end": 1116,
                    "matchedPaperCorpusId": "86426900"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "230435805"
                },
                {
                    "start": 1120,
                    "end": 1124,
                    "matchedPaperCorpusId": "243985875"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "277065932",
            "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
            "text": "The release of models such as GPT-4V [181], Gemini 2.0 [7], and Claude3 [194] has demonstrated remarkable capabilities in multimodal understanding, sparking significant interest in MLLMs within the research community. Initial investigations into MLLMs focused on developing robust language models capable of interpreting multimodal content and generating textual responses. In the domain of image-text understanding, notable progress has been achieved with Visual Large Language Models (VLLMs) such as BLIP2 [195], OpenFlamingo [196], MiniGPT-4 [197], and LLaVA [13]. \n\nConcurrently, advancements in video-text understanding have emerged, with significant contributions from VideoChat [198] and Video-ChatGPT [17]. Audio and speech comprehension have also garnered attention, exemplified by models like Qwen-Audio [199,16] and LLaSM [200]. A noteworthy development is VideoLLaMA [18], which leverages Qformer [195] to enable both audio and video understanding. In simple terms, mainstream MLLMs typically follow a consistent model architecture by processing multimodal embeddings or tokens into the decoder structure and generating contextually relevant outputs in an autoregressive manner, as shown in the left of Figure 4. \n\nParallel to these works about multimodal understanding, research also explored multimodal content generation. In image generation, models such as Kosmos-2 [201], GILL [202], Emu [203], and MiniGPT-5 [204] have achieved breakthroughs. Audio generation has seen advancements with SpeechGPT [205,206] and AudioPaLM [207], while video generation research, including CogVideo [208], VideoPoet [209], Video-Lavit [210], and StreamingT2V [211], has laid the groundwork for multimodal content creation. The recent introduction of GPT-4o [212], capable of both understanding and generating images and audio, has shifted attention toward \"any-to-any\" paradigm models.",
            "score": 0.5555787126669048,
            "section_title": "Multimodal LLMs",
            "char_start_offset": 12087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 513,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 909,
                    "end": 914,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1394,
                    "end": 1399,
                    "matchedPaperCorpusId": "258959284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "277467664",
            "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
            "text": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
            "score": 0.5543381736861889,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "269354784",
            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
            "text": "Vision-language models represent a category of models that integrate computer vision and NLP techniques to achieve a comprehensive understanding of visual and textual data.The field of research in this area has witnessed significant advancements, encompassing a wide range of techniques and architectures that aim to fuse vision and language, thereby enhancing performance and capabilities across various tasks.\n\nOne approach adopted by these models involves learning generic multi-modal representations from a large corpus of image-text pairs.Models such as CLIP [46] and ALIGN [47] employ dual encoders trained with a contrastive objective on extensive imagetext datasets, enabling them to model the interaction between modalities.However, these models do not natively support multi-modal generation tasks (e.g., image captioning and VQA).Other models, like BLIB [44] and CoCa [45], combine contrastive and generative pre-training to support both cross-modal alignment and multi-modal generation tasks.\n\nAnother paradigm of vision-language models are the generalized models that are trained to jointly perform multiple tasks without task-specific fine-tuning.For instance, UniT [48] unifies different tasks in a single model by designing a decoder with per-task query embedding and task-specific output heads.OFA [49] formulates different visionlanguage tasks as sequence-to-sequence tasks and trains a transformer model on them without task-specific layers.GIT [50] is a multi-tasking transformer-based model that conditions the decoder on vision inputs and text to generate the output text.Flamingo [51] extends sequence-to-sequence models to support interleaved image, video and text inputs for generating the output text.\n\nBuilding upon the success of GPT models, GPT-4 [14] has exhibited impressive capabilities in engaging in multi-modal dialogues with humans.The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input.Multimodal-GPT [52] utilizes gated cross-attention layers to facilitate seamless interactions between images and text.",
            "score": 0.5533780875555541,
            "section_title": "Vision-Language Models in General Computer Vision",
            "char_start_offset": 13494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 411
                },
                {
                    "start": 413,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 1004
                },
                {
                    "start": 1006,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1311
                },
                {
                    "start": 1311,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1727
                },
                {
                    "start": 1729,
                    "end": 1868
                },
                {
                    "start": 1868,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2154
                },
                {
                    "start": 2154,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "221279805"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "226602850"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "271213320",
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "text": "Multimodal learning in the intersection of visual and linguistic data has seen significant advancements in recent years. This area of research focuses on developing models that can understand and generate content by integrating information from both visual inputs, such as images or videos, and textual descriptions. These models leverage the complementary nature of visual and textual data to perform a wide range of tasks, including but not limited to image captioning, visual question answering, and cross-modal information retrieval. \n\nOne of the key innovations in this field has been the development of models that are trained on vast datasets of image-text pairs. [12,7,8] These models employ contrastive learning techniques to align the representations of images and their corresponding textual descriptions in a shared embedding space. By doing so, they learn to associate specific visual features with relevant linguistic concepts, enabling them to understand complex queries and content with a high degree of accuracy. Moreover, the use of transformer-based [13] architectures has further enhanced the capabilities of these models. Transformers provide a flexible and powerful framework for modeling sequential data, and their adoption in multimodal learning has allowed for the effective integration of visual and textual inputs. This has led to significant improvements in tasks requiring nuanced understanding and generation of content, such as generating descriptive captions for images that accurately reflect their content and context or answering questions based on visual information. \n\nThe emergence of Large Language Models (LLMs) has also marked a significant milestone in the field of artificial intelligence, particularly in natural language processing. Models like GPT [1], BERT [4], and others have demonstrated exceptional abilities in generating and understanding text, significantly advancing the capabilities of AI in understanding human languages. These models have been instrumental in a variety of applications, including text generation, translation, and semantic analysis, showcasing their versatility and powerful computational abilities. However, their primary limitation lies in their text-only nature, confining their applicability to tasks that do not require understanding or generating content in other modalities such as images, speech, and videos. \n\nTo bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum.",
            "score": 0.5528985147593463,
            "section_title": "Related Work",
            "char_start_offset": 2249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2391
                },
                {
                    "start": 2394,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 679,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "265498814",
            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
            "text": "In recent years, Large Language Models (LLMs) [8,13,46,47,55] have exhibited remarkable capabilities to understand, reason, and generate texts across a variety of openended tasks. Leveraging the strong generality of LLMs, Multimodal Large Language Models (MLLMs) [3,9,19,26,29,31,32,36,37,42,43,48,48,53,62,68,70] have demonstrated exceptional capabilities in comprehending multimodal data through predicting open-form texts. \n\nRecent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer. However, despite the extensive capabilities of MLLMs, existing MLLM benchmarks [4,15,38,61,63] primarily focus on evaluating single image-text comprehension, thus failing to fully demonstrate the progress and limitations of current MLLMs. The lag of benchmarks behind the rapid development of MLLMs hinders the exploration and evolution of models. \n\nIn this work, we categorize the capabilities of MLLMs into hierarchical levels ranging from L 0 to L 4 based on the modalities they can accept and generate, as depicted in Fig. 1. Building upon LLMs, the lowest-tier capability L 0 involves generating texts given text inputs, while the highest-tier capability L 4 entails producing open-form interleaved image and text output given arbitrary interleaved image-text inputs. Reaching the capability L 4 is a crucial milestone on the path towards General Artificial Intel-ligence (AGI) since a human-level AI should be able to effortlessly digest and create multimodal content. In the capability pyramid, higher levels inherently include the capabilities of lower tiers. This hierarchical categorization not only clearly illustrates the current progress of MLLMs, but also provides a well-defined roadmap for future research.",
            "score": 0.5525716499205615,
            "section_title": "Introduction",
            "char_start_offset": 1364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "277314021",
            "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
            "text": "As illustrated in Fig. 2, CoLLM consists of several essential components: (1) a vision encoder f (\u2022) for extracting image features; (2) modules for synthesizing reference image embeddings and modification text; (3) an image adapter g(\u2022) that maps visual features into the language model's semantic space; (4) a LLM \u03a6(\u2022) that processes multimodal queries and (5) a projection layer proj(\u2022) (omitted from the figure for simplicity) that maps the LLM output to a suitable representation for retrieval. It is important to note that Fig. 2 (a) and Fig. 2 (b) illustrate architectures designed for two distinct input formats. The former is tailored for image-caption pairs, while the latter is optimized for CIR triplets.",
            "score": 0.5525533328790923,
            "section_title": "Model Architecture",
            "char_start_offset": 10399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 715
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "254044324",
            "title": "Unified Discrete Diffusion for Simultaneous Vision-Language Generation",
            "text": "The recently developed discrete diffusion models perform extraordinarily well in the text-to-image task, showing significant promise for handling the multi-modality signals. In this work, we harness these traits and present a unified multimodal generation model that can conduct both the\"modality translation\"and\"multi-modality generation\"tasks using a single model, performing text-based, image-based, and even vision-language simultaneous generation. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified transition matrix. Moreover, we design a mutual attention module with fused embedding layer and a unified objective function to emphasise the inter-modal linkages, which are vital for multi-modality generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.",
            "score": 0.5515750622413648,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "208513058",
            "title": "Multimodal machine translation through visuals and speech",
            "text": "Multimodal machine translation involves drawing information from more than one modality, based on the assumption that the additional modalities will contain useful alternative views of the input data. The most prominent tasks in this area are spoken language translation, image-guided translation, and video-guided translation, which exploit audio and visual modalities, respectively. These tasks are distinguished from their monolingual counterparts of speech recognition, image captioning, and video captioning by the requirement of models to generate outputs in a different language. This survey reviews the major data resources for these tasks, the evaluation campaigns concentrated around them, the state of the art in end-to-end and pipeline approaches, and also the challenges in performance evaluation. The paper concludes with a discussion of directions for future research in these areas: the need for more expansive and challenging datasets, for targeted evaluations of model performance, and for multimodality in both the input and output space.",
            "score": 0.5514209263152506,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70947265625
        },
        {
            "corpus_id": "277065986",
            "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating human-like text. Advanced transformer-based architectures such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and T5 (Raffel et al., 2020) capture contextual relationships within text, enabling tasks like summarization, question answering, and code generation. However, LLMs process only textual data, limiting their utility in domains requiring multimodal understanding, such as image or video analysis. Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation.",
            "score": 0.5507874225955918,
            "section_title": "Addressing the Problem",
            "char_start_offset": 4189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 253,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "270391200",
            "title": "A Concept-Based Explainability Framework for Large Multimodal Models",
            "text": "Model architecture. We consider a general model architecture for a large multimodal model f , that consists of: a visual encoder f V , a trainable connector C, and an LLM f LM consisting of N L layers. We assume f is pretrained for captioning task with an underlying dataset S = {(X i , y i )} N i=1 consisting of images X i \u2208 X and their associated caption y i \u2282 Y. X and Y denote the space of images and set of text tokens respectively. Note that caption y i can be viewed as a subset of all tokens. The input to the language model f LM is denoted by the sequence of tokens h 1 , h 2 , ..., h p and the output as \u0177. The internal representation of any token at some layer l and position p inside f LM is denoted as h p (l) , with h p (0) = h p . Note that h p (l) is same as the residual stream representation in LLM transformers [13] at position p and layer l. For the multimodal model, the input sequence of tokens for f LM consists of the concatenation of: (1) N V visual tokens provided by the visual encoder f V operating on an image X, followed by the connector C, and (2) linearly embedded textual tokens previously predicted by f LM . For p > N V , this can be expressed as: \n\nwhere h 1 , . . . , h N V = C(f V (X)), and h p = Emb(\u0177 p\u22121 ) for p > N V , where Emb denotes the token embedding function. To start the prediction, h N V +1 is defined as the beginning of sentence token. The output token \u0177p is obtained by normalizing h p (N L ) , followed by an unembedding layer that applies a matrix W U followed by a softmax. The predicted caption \u0177 consists of the predicted tokens \u0177 = {\u0177 p } p>N V until the end of sentence token. \n\nTraining The model is trained with next token prediction objective, to generate text conditioned on images in an auto-regressive fashion.",
            "score": 0.5507862162716397,
            "section_title": "Background for Large Multimodal Models (LMMs)",
            "char_start_offset": 8362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1779
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "246634906",
            "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
            "text": "In NLP, recent studies unify diverse tasks covering natural language understanding and generation to text-to-text transfer [30] or language modeling [3]. Following this idea, [56] and [57] demonstrate text-generation-based multimodal pretrained models. [7] and [58] propose a simple framework that can process information from multiple modalities with a uniform byte-sequence representation. [59] and [60] unify tasks of different modalities by designing various task-specific layers. \n\n[61] explores to employ a retrieval-based unified paradigm. However, these multimodal pretrained models suffer from performance degradation in downstream tasks, e.g., VQA, image captioning, etc., and they have no image generation capability.",
            "score": 0.5505403221386298,
            "section_title": "Related Work",
            "char_start_offset": 6761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 728
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "231802355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "273374871",
            "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
            "text": "Existing caption rewriting works (e.g., VeCLIP (Lai et al., 2024)) rely on a multimodal captioning model to generate captions that are short sentences containing visual concepts. Image captions can be very descriptive but often only cover the most salient object of the scene, they are coarse-grained in structure (whole sentence or paragraph), and are hard to modify. Our key distinction is that VTM produces a comprehensive list of metadata information that describes the visual concepts in an image as completely as possible. \n\nLanguage Model. Large language models (LLMs) have exhibited strong instruction-following capabilities. The goal of an LLM in CtrlSynth is to take an input textual instruction on how to generate a synthetic text that meets the requirements specified in the instruction. CtrlSynth employs the reasoning and composition capability of LLMs to recombine the visual image tags in the task instruction and compose new synthetic texts. The instruction for an LLM consists of three parts (Figure 3): (i) task template that specifies the details of the text synthesis task, (ii) task content that contains the actual visual tags (phrases) and an optional caption paired with the image, and (iii) task constraint that describes the style and formatting of the output text. Users can also apply custom policies over the instructions to guide the text synthesis process. \n\nText-to-Image Model. Text-to-image models generate novel and diverse image samples based on different input text prompts. CtrlSynth applies an image controller to account for the user-specified control policies and accordingly, updates the input text instructions from the previous step (i.e., language model). These updated instructions are then fed to text-to-image models for generating the image as an output. In our experiments, we use StableDiffusion models for text-to-image generation. \n\nText and Image Controllers. The controller in CtrlSynth is a function that takes an input text and transforms it into a specific text instruction for the LLM or text-to-image model. \n\nThe text controller accepts the visual tags of an image and a user-defined policy along with an optional original text as input and produces instructions to control the generation of synthetic text.",
            "score": 0.5498736699613821,
            "section_title": "KEY COMPONENTS",
            "char_start_offset": 11182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2068
                },
                {
                    "start": 2071,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "271328975",
            "title": "Videogamebunny: Towards Vision Assistants for Video Games",
            "text": "Large multimodal models (LMMs) enhance large language models (LLMs) by incorporating additional modalities such as images or audio, enabling them to process multimodal inputs and generate textual outputs.The role of the language model is to comprehend user instructions and produce responses based on the additional modality inputs provided.Standard approaches to create LMMs involve combining pre-trained models with different modalities via projection layers.These layers can be implemented using simple mechanisms such as multilayer perceptrons (MLP) [24,27] or transformer layers [25].Alternatively, a resampler module like Perceiver [3,20,21] or Qformer [10,60] selectively chooses features to reduce the number of visual tokens based on the context and instruction, enhancing efficiency and maintaining performance.\n\nIn this study, we focus on LMMs that accept input images and text to produce responses, particularly using the LLaVA-style architecture [29], which is one of the most popular methods [14,26,28,33].This architecture employs an MLP layer to integrate vision tokens with a language model.",
            "score": 0.5486682427282237,
            "section_title": "Large multimodal models",
            "char_start_offset": 2083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 341
                },
                {
                    "start": 341,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 821
                },
                {
                    "start": 823,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1108
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 558,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 641,
                    "end": 644,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "268984025",
            "title": "FigurA11y: AI Assistance for Writing Scientific Alt Text",
            "text": "Advances in multimodal language models, which incorporate vision and language, point toward expansive future capabilities for automated alt text generation.Our approach relied on metadata extracted from figures and papers to provide contextual grounding for language models, since today's large language models substantially outpace widely available multimodal models in terms of their generation capabilities and can better incorporate large amounts of metadata representing knowledge about figures.However, the ability to process complex figures directly could reduce dependence on potentially error-prone metadata extraction pipelines while incorporating the right kinds of contextual knowledge for support.\n\nWhile this could enable purely automated description systems, risks accompany such approaches.Recent work has shown how current state-of-the-art multimodal models can make errors when processing complex figures such as scientific figures [17,18].Without human validation, model errors or biases could more easily propagate.Maintaining author discretion may prove wise, even as automated methods become more capable.Furthermore, descriptive tasks require not just visual recognition, but reasoning, inference, and judgment.The wisdom accumulated in authors and fields, who can respond to changing contexts, might allow tailoring descriptions for clarity and relevance.Thus, while future multimodal models may better parse figures, the role of human guidance and customization is unlikely to dissolve.Specialized metadata extraction models could also enhance such models' zero-shot capabilities.Visual control could, however, be useful; automatically decomposing complex compound figures into components for iterative description is a promising approach we did not explore.",
            "score": 0.5485675888335991,
            "section_title": "Rise of Multimodal Models",
            "char_start_offset": 63843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 500
                },
                {
                    "start": 500,
                    "end": 710
                },
                {
                    "start": 712,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1127
                },
                {
                    "start": 1127,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1511
                },
                {
                    "start": 1511,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1783
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "272987039",
            "title": "Surveying the MLLM Landscape: A Meta-Review of Current Surveys",
            "text": "One of the most prominent trends in the surveyed literature is the enhanced integration of multiple modalities-such as text, images, and audio-within MLLMs. Surveys like \"The (R)Evolution of Multimodal Large Language Models\", etc. [93,94,95,96] emphasize the shift from unimodal to multimodal systems as a transformative leap, enabling models to more closely mimic human perception. This trend is echoed across multiple papers, highlighting the field's focus on achieving a more holistic understanding of information by combining different types of data inputs.",
            "score": 0.5478041091751567,
            "section_title": "Increased Integration of Multimodality",
            "char_start_offset": 46244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 561
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "258947258",
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "text": "We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "score": 0.547787562610055,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9609375
        },
        {
            "corpus_id": "226262333",
            "title": "MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French",
            "text": "The related work to the content of this paper is split in two parts. We first discuss the related datasets, alongside comparisons with CMU-MOSEAS. Afterwards, we discuss the machine learning literature for modeling multimodal language.",
            "score": 0.5473260482125347,
            "section_title": "Background",
            "char_start_offset": 2423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423095703125
        },
        {
            "corpus_id": "273962846",
            "title": "Autoregressive Models in Vision: A Survey",
            "text": "V2T (Zhu et al., 2024a) translates images into discrete tokens from an LLM's vocabulary, aligning visual and textual data for tasks such as image denoising. MARS (He et al., 2024) features the Semantic Vision-Language Integration Expert (SemVIE) module, deeply integrating textual and visual tokens, illustrating the flexibility of token-wise generation in producing detailed images from textual descriptions. Lumina-mGPT (Liu et al., 2024a) applies Flexible Progressive Supervised Finetuning (FP-SFT) to Chameleon (Team, 2024) with high-quality image-text pairs to fully unlock the potential of the model for high-aesthetic image synthesis while preserving its general multimodal capabilities. \n\nExpansion to Novel Tasks. IconShop (Wu et al., 2023b) further extends token-wise generation to vector graphics, enabling scalable vector icon creation from text prompts. Make-a-story (Rahman et al., 2023) focuses on generating visually coherent stories from text, employing a scale-wise generation process to maintain consistency across multiple image frames within complex narratives. SEED-Story (Yang et al., 2024), a multimodal storytelling model, generates images and text in parallel, ensuring that visual details correspond seamlessly with the narrative across different resolution levels. \n\nIts hierarchical attention mechanism preserves coherence between text and images at all scales, from coarse descriptions to intricate image details.",
            "score": 0.5472019760530769,
            "section_title": "3.1.2.a Token-wise Generation",
            "char_start_offset": 49602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1443
                }
            ],
            "ref_mentions": [
                {
                    "start": 4,
                    "end": 23,
                    "matchedPaperCorpusId": "268363788"
                },
                {
                    "start": 732,
                    "end": 750,
                    "matchedPaperCorpusId": "258352680"
                },
                {
                    "start": 880,
                    "end": 901,
                    "matchedPaperCorpusId": "254017562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "265506863",
            "title": "M$^{2}$Chat: Empowering VLM for Multimodal LLM Interleaved Text-Image Generation",
            "text": "VLM Encoder. In anticipation of the multimodal language model's strong text-image encoding capabilities, we incorporate LLaMA-AdapterV2 as our foundational pretrained VLM. As illustrated in Fig. 2, each context in the sequence {y} N i=1 is encoded into text embeddings e text \u2208 R length\u00d74096 using a text encoder. Simultaneously, the corresponding set of images {x} N i=1 undergo encoding by a visual encoder, yielding visual features f img \u2208 R length\u00d7768 , leveraging a CLIP based ViT+MLP framework [30]. \n\nText-Image Token Generation. The VLM outputs a sequence of hidden tokens t out \u2208 R length\u00d74096 , mirroring the dimension of the input text embeddings. We partition this output into three segments: answer tokens t ans \u2208 R lengthans\u00d74096 , caption tokens t cap , and image tokens t img \u2208 R lengthimg\u00d74096 , with the intricate token structure discussed in the instruction formatting section. The answer tokens t cap \u2208 R lengthcap\u00d74096 are transformed into humanreadable text by LLaMA text decoder, while the image generation tokens t {cap,img} \u2208 R (lengthcap+lengthimg)\u00d74096 provide the foundational features for synthesizing images.",
            "score": 0.5470260038094197,
            "section_title": "Framework Architecture",
            "char_start_offset": 9491,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1138
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8544921875
        },
        {
            "corpus_id": "252367988",
            "title": "How to Adapt Pre-trained Vision-and-Language Models to a Text-only Input?",
            "text": "Having models learn language from text alone has been criticised based on several aspects, from fundamental arguments about how language works (Bender and Koller, 2020) to findings on lack of certain information in text (Gordon and Van Durme, 2013;Paik et al., 2021). To train language models on more sources than text is therefore a proposed direction for creating language models with better language understanding (Bisk et al., 2020). These models would then become multimodal, with the capability to process both text and information from other modalities. \n\nThe multimodal models of interest in this work are vision-and-language (VL) models that have been trained on images and their corresponding captions or visual questions (Lu et al., 2019;Tan and Bansal, 2019;Su et al., 2020;Li et al., 2019;Chen et al., 2020). These models are performant on several image-text tasks such as image captioning and VQA, while there also is an increased interest for evaluating how their natural language understanding is influenced by their multimodal training (Iki and Aizawa, 2021;Yun et al., 2021). \n\nIt is however tricky to investigate the pure natural language understanding of the aforementioned VL models, since their language processing is conditioned on visual features. For certain investigations, we may simply wish to evaluate the models on textonly domains, while these models have not been developed for this purpose. If we do not attend to the issue of accurately adapting VL models to text-only domains we risk evaluating them out-of-distribution and fail to accurately measure their natural language understanding capabilities. \n\nDifferent methods for adapting VL models to a text-only input have already been tried and we have some results on the natural language understanding capabilities of these models (Iki and Aizawa, 2021;Yun et al., 2021). However, no systematic search for the best way to adapt VL models to a textonly input has been performed and it is unclear how well the VL models work with the previously proposed adaptations. If we wish to continue the search for better natural language understanding in multimodal models, we should ensure that we evaluate them in the best way possible.",
            "score": 0.5469135490895882,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 168,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 220,
                    "end": 248,
                    "matchedPaperCorpusId": "16567195"
                },
                {
                    "start": 248,
                    "end": 266,
                    "matchedPaperCorpusId": "239009473"
                },
                {
                    "start": 417,
                    "end": 436,
                    "matchedPaperCorpusId": "216035815"
                },
                {
                    "start": 732,
                    "end": 749,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 749,
                    "end": 770,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 770,
                    "end": 786,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1053,
                    "end": 1075,
                    "matchedPaperCorpusId": "237604959"
                },
                {
                    "start": 1817,
                    "end": 1839,
                    "matchedPaperCorpusId": "237604959"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55322265625
        },
        {
            "corpus_id": "267637150",
            "title": "Text-centric Alignment for Multi-Modality Learning",
            "text": "Recent advancements in large-scale foundation models have significantly enhanced content generation capabilities across various modalities. These developments span a wide range of applications, including text-to-image [35,37], text-to-video [39], audioto-image [20], text-to-speech [36], speech-to-gesture [4], speakerto-listener [30], language-to-pose [5], and even in the realms of speech and music generation [31]. Nevertheless, aligning the semantic spaces of independently trained foundation models poses a significant challenge, hindering the ability of downstream models to seamlessly switch between upstream modalities at test time. \n\nExtending beyond single-modality applications, large multimodal language models (MLLMs) have shown remarkable proficiency in both reasoning and generation tasks [48]. For instance, Flamingo [7] employs a vision encoder for feature extraction from images on top of the transformer backbone. On the other hand, Kosmos-2 [34] is comprised of a vision encoder, a linear projector, and a base transformer. These models directly concatenate image and text features, using unique tokens for differentiation. However, despite the impressive capabilities of MLLMs in handling multimodal data, acquiring large-scale datasets encompassing multiple modalities remains a significant challenge. Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input.",
            "score": 0.5461758514615326,
            "section_title": "RELATED WORKS 2.1 Multimodal Foundation Models",
            "char_start_offset": 8315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "61156031"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "162169005"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "220769219"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "248227839"
                },
                {
                    "start": 353,
                    "end": 356,
                    "matchedPaperCorpusId": "195776094"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "27706557"
                },
                {
                    "start": 833,
                    "end": 836,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Image-text multimodal models [108,169,170,171,172,173,174,175]are a product of artificial intelligence that can process and understand various types of information, such as text and images, simultaneously.These models aim to integrate multiple sensory inputs or data types into a unified framework to more comprehensively understand and process information.Image-text multimodal models are at the forefront of technological innovation, capable of processing diverse forms of input and, in certain instances, producing varied output modalities.Training these multimodal models usually requires a large amount of annotated data, as they need to understand and process multiple types of data simultaneously.Collecting and annotating such extensive multimodal datasets can be both expensive and time-consuming.\n\nIn the initial stage, the primary research emphasis lies in learning the joint feature space between images and their corresponding descriptions.These methods project image features and sentence features into a common space, which can be used for image search or ranking image captions.Image-text multimodal models initially use various methods to learn projections, including N-grams methods [176], Corpus-Guided methods [177], and kernel canonical correlation analysis (KCCA) methods [16].\n\nThe early stages of the development of image-text multimodal models focus on exploring the bidirectional mapping between images and text descriptions.Research initially attempts to use CNN and RNN.Because the CNN architecture benefits from the convolution kernel mechanism, it is particularly suitable for identifying and classifying local patterns in images.CNN is usually used to extract and process image features; RNN is usually used to process text sequence data due to its temporal processing mechanism.When generating or building a visual representation of the scene as the title is read dynamically, the representation automatically learns to remember long-term visual concepts, generate novel captions given images, and reconstruct visual features given image descriptions [17,178,18,19].\n\nAlthough the capabilities of early models are limited, they lay the foundation for subsequent multi-modal research.But RNN has weaknesses in re-memorizing concepts after several iterations.Without dedicated gating units, RNN language models often have difficulty learning long-distance relationships.",
            "score": 0.5460320416012927,
            "section_title": "Initial Stage and Early Stage",
            "char_start_offset": 7121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 806
                },
                {
                    "start": 808,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1094
                },
                {
                    "start": 1094,
                    "end": 1299
                },
                {
                    "start": 1301,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 2098
                },
                {
                    "start": 2100,
                    "end": 2215
                },
                {
                    "start": 2215,
                    "end": 2289
                },
                {
                    "start": 2289,
                    "end": 2400
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "169032532"
                },
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "212748233"
                },
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "247038603"
                },
                {
                    "start": 50,
                    "end": 54,
                    "matchedPaperCorpusId": "249642175"
                },
                {
                    "start": 54,
                    "end": 58,
                    "matchedPaperCorpusId": "265308931"
                },
                {
                    "start": 1201,
                    "end": 1206,
                    "matchedPaperCorpusId": "10702193"
                },
                {
                    "start": 1230,
                    "end": 1235,
                    "matchedPaperCorpusId": "1539668"
                },
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "928608"
                },
                {
                    "start": 2083,
                    "end": 2087,
                    "matchedPaperCorpusId": "6785090"
                },
                {
                    "start": 2094,
                    "end": 2097,
                    "matchedPaperCorpusId": "8517067"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "265498786",
            "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
            "text": "LMMs are multimodal models that directly reason over both vision and language modalities. They are typically given inputs of one image I and an associated task prompt in text form P in (e.g., questions, caption generation, etc.). Each modality is then encoded into a shared embedding space that a language model f \u03b8 (\u2022) (parameterized by \u03b8) can reason over. More concretely, the image is encoded using a trainable vision encoder v \u03d5 (\u2022) (parameterized by \u03d5), while the task prompt is tokenized and then encoded using a fixed language embedding l. Given an input image I and input task prompt P in , the language model (typically an LLM) then outputs a text response R. \n\nThe exact LMM sub-modules of the LLM, vision encoding architecture, and pretraining method for parameters \u03b8, \u03d5 differ between models but the overarching method described above remains the same. \n\nWe propose CCoT, a zero-shot chain-of-thought prompting method that leverages scene graph generation to improve an LMM's compositional visual understanding and multimodal reasoning. Notably, this method does not require any finetuning as it is purely prompting-based. Furthermore, no annotated SGs are required as the method is zero-shot. Ultimately, our method is centered around a scene-graph generation prompt S in that can be integrated into P in such that the LMM can output a scene graph S g as an intermediate multimodal reasoning step to output better responses to the task prompts, such as questions, classification, or caption generation.",
            "score": 0.5456098926827088,
            "section_title": "Preliminaries",
            "char_start_offset": 10766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1515
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "269983697",
            "title": "From Text to Pixel: Advancing Long-Context Understanding in MLLMs",
            "text": "We aim to leverage sequential data processing to fine-tune models on a combination of textual and visual inputs, enabling them to generate coherent and contextually relevant responses based on both text and image data. In the domain of multimodal large language models, the autoregressive training objective is a pivotal technique, which can be formulated as follows: \n\nwhere x i represents tokens with length L, X O denotes the target output given the features of multimodal queries Q, and \u03b8 denotes the model parameters. This loss function encourages the model to predict the next token in the sequence, given the previous visual and textual tokens. \n\n4 Implementation Details",
            "score": 0.5453002883950522,
            "section_title": "Supervised Fine-tuning Strategy",
            "char_start_offset": 11037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 678
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,2,15,19,59,62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder [45] are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM [15] extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities. Our work leverages the advancements made by these foundational models and explores how to adapt the foundational model to specific domains.",
            "score": 0.5439812090857853,
            "section_title": "Unified Multimodal Generative Models",
            "char_start_offset": 6395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1413
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 236,
                    "matchedPaperCorpusId": "262898422"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "261975252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "256461099",
            "title": "Controlling for Stereotypes in Multimodal Language Model Evaluation",
            "text": "The center of gravity of NLP research has shifted to the development of language models (LMs) for representation and generation of text, and most recent high-impact research contributions describe new LMs. For some tasks, a model needs to take into account not only a text but also some non-textual information, and a wide range of multimodal LMs have been developed that allow the representation of a text jointly with some external modality. Most of this work focuses on visual tasks where NLP models need to be integrated with computer vision models; examples of tasks in this area include visual question answering and caption generation. A range of combined language-and-vision LMs have been developed using different approaches for integrating representations of text and of images or videos.\n\nBut can we be sure that a multimodal model actually uses the provided visual information instead Figure 1: An example of a controlled test of a masked language model for a color stereotype. We compute the output from the MLM head when providing an image of an object with a stereotypical color (a yellow banana) and compare it to the output when the object has an unusual color (green). If the MLM is strongly affected by a stereotype bias, the predictions change little. of just relying on statistical tendencies in the text corpus? With the development of multimodal LMs, some recent work has investigated what information is stored in the representations of the multiple modalities and how the multiple representations interact. For instance,  carried out a set of controlled tests to tease apart the effects of the textual and visual modalities.\n\nIt has been widely noted that representations of language are affected by several kinds of stereotypes, which we loosely define as any type of phenomenon that has a highly skewed prior probability distribution. In these cases, the skewed distribution may cause a model to simply go with the default choice and ignore contextual information that would suggest an unusual analysis. Most of the discussion in the field has been about stereotypes relating to various demographic attributes (Bolukbasi et al., 2016), but in this work, we use the term \"stereotype\" in the more general sense mentioned above. This issue is likely to affect multimodal LMs as well, although we are aware of no previous work that investigates this phenomenon systematically; for instance, if some object is often",
            "score": 0.5436355223839099,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2137,
                    "end": 2161,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73046875
        },
        {
            "corpus_id": "276117091",
            "title": "Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development",
            "text": "To demonstrate the proposed suite's usability and effectiveness, aiding understanding and boosting confidence in its applicability, in this section, we conduct extensive experiments in three practical use cases. They vary in models and data OPs (Sec. 4.2 and Sec 4.3), data samples and compute cost (Sec. 4.4), leading to insights and recipes useful for larger-scale scenarios (Sec. 4.5). \n\nSpecifically, for image-to-text (I2T) generation, we will show that the optimal recipe derived from small pools achieves superior model performance and higher data efficiency on the MGM model when applied to larger datasets. Multimodal models' efficacy is closely tied to the fidelity of their output modalities, which can be explicitly reflected in filtering of input training data. \n\nFor text-to-video task, all top-3 OPs are video-only OPs. \n\nFor image-to-text and image-text pre-training tasks, two of their top-3 OPs are text-only, and image-text OPs respectively. This trend of influential OPs holds beyond top-3 ranks (detailed in Appendix E.2), suggesting that more attention and resources should be allocated to data processing related to the output modalities of the studied models. \n\nObservation 2 (Diversity vs. Quality) \n\nIn contrast, data diversity is more crucial for image-totext model, while data quality is key for text-to-video and image-text pre-training models. \n\nWe find that some top OPs share similar functionalities, while their best-performing pools exist in different statistical ranges. For example, P i,high in image-to-text, and P i,low in text-to-video and image-text pre-training. A deeper analysis of OP ranks in terms of NSFW scores and language scores reveals that the studied image-to-text model prefer more diverse data compared to the text-to-video and image-text pre-training models. Intuitively, high-scoring images or videos in NSFW (Not Safe For Work) content are generally rare and occupy the long tail of the data distribution. Consequently, pools with high NSFW scores tend to be more diverse.",
            "score": 0.5430858697455302,
            "section_title": "Use-Cases and Experiment Overview",
            "char_start_offset": 12708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07781982421875
        },
        {
            "corpus_id": "18034318",
            "title": "Zero-resource machine translation by multimodal encoder\u2013decoder network with multimedia pivot",
            "text": "Our goal is to build a translation model from a source language s to a target language t by utilizing the side information (images) as the pivot. Below, we call a pair of a text description d and its counterpart image i a \"document.\" For training the system, suppose that we have N s monolingual documents in the source language, T s = {d s k , i s k } N s k=1 . Similarly, we also have N t documents in the target language, T t = {d t k , i t k } N t k=1 . Importantly, T s and T t do not overlap; they do not share the same images at all. While d s and d t obviously appear in different spaces, i s and i t share a common visual space and can be handled by the same encoder. We let E s (d s ), E t (d t ), and E v (i) denote non-linear encoders (i.e., feature extractors) for source language descriptions, target language descriptions, and images, respectively. \n\nOur model can be divided into roughly two important components. The first component is the multimodal representation learning, in which the parameters of the encoders, E s (d s ), E t (d t ), and E v (i), are optimized so that they are mapped into the same semantic space, which we call \"the multimodal space.\" If such a good multimodal space is obtained, instances of all modalities should have roughly the same vector representation as long as they are tied together with similar semantic concepts. The second component is to build a target language decoder, D t , on top of the multimodal space so that the final translation can be realized by D t (E s (d s )). It should be emphasized that we only need texts for input during the testing phase, similar to standard machine translation. \n\nFigures 1 illustrates our approach. There are several options in the model topology and training strategies that are thoroughly compared in the experiments. We describe the details in the following sections.",
            "score": 0.5427335426326592,
            "section_title": "Overview",
            "char_start_offset": 9921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "272911245",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "text": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.",
            "score": 0.5421668678778351,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "257766245",
            "title": "Equivariant Similarity for Vision-Language Foundation Models",
            "text": "Powered by the remarkable capabilities of the large language model (LLM), the community has witnessed an emergent interest in developing Multimodal Large Language Model (MLLM) [81,40,18] very recently. Instead of accepting the pure text as the input, MLLM additionally sees the image and provides the response, which can be regarded as another line of VLMs. Here we conduct a pilot study of the performance of MLLM on our EQBEN. We adopt LLaVa-7B [40] as our base model with Vicuna as the LLM backend. Given two matched image-text pairs {I 1 , T 1 } and {I 2 , T 2 }, we concatenate I 1 and I 2 horizontally as the single input image. We build the question prompt with the template: \"There are two images (left and right). Now you have two captions: caption 1: {T 1 }; caption 2: {T 2 }. Please indicate which caption corresponds to the left image and which caption corresponds to the right Caption Order EQ-AG EQ-Y. EQ-G. EQ-K. EQ-SD T 1 , T 2 95.00 90.00 90.00 88.33 90.00 T 2 , T 1 0.00 0.00 0.00 1.66 0.00 Table 7: Group accuracy (%) of LLaVa on different FT data corpus with different caption input order. Y., G., K. are the short for YouCook2, GEBC and Kubric. \n\none. The answer should follow the format: \"#index for the left image; #index for the right image\". For example, \"1;2\" represents that caption 1 corresponds to image left.\" Since it is hard to reformat the MLLM free-form textual output to the label space, we randomly collect 20 samples from each subset of EQBEN and manually compare the MLLM output and the ground-truth label. The results are shown in Table 7. Interestingly, by comparing two rows, we can find that the performance of MLLM is quite sensitive to the order of the input caption T 1 and T 2 (\u223c 90% v.s \u223c 0%).",
            "score": 0.541926939520783,
            "section_title": "Pilot Study of MLLM on EQBEN",
            "char_start_offset": 30116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1741
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4736328125
        },
        {
            "corpus_id": "271924334",
            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
            "text": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;Tang et al., 2024;Ye et al., 2024a;Dong et al., 2024;Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2024) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner. Compared to this work, we also adopt discrete tokens to represent all modalities. In contrast, we utilize a discrete diffusion process instead of autoregressive modeling for visual generation.",
            "score": 0.5418199205531851,
            "section_title": "UNIFIED VISION-LANGUAGE FOUNDATION MODEL",
            "char_start_offset": 11116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 101,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 118,
                    "end": 136,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 636,
                    "end": 658,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 809,
                    "end": 827,
                    "matchedPaperCorpusId": "261975252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "265281544",
            "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
            "text": "Our goal is to map images and videos into a shared feature space to enable the large language model to learn from a unified visual representation. We assume that the same information can be conveyed through multiple media. For example, a running dog can be expressed through language, a image or a video simultaneously. Therefore, we can compress information from different modalities into a common feature space, allowing the model to extract information from a dense feature space, facilitating modality interactions and complementarity. Hence, we chose the modality encoders from Lan-guageBind (Zhu et al., 2023a), which align images and videos with the textual feature space.",
            "score": 0.5418199205531851,
            "section_title": "United Visual Representation",
            "char_start_offset": 9704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 679
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5419921875
        },
        {
            "corpus_id": "259847295",
            "title": "T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation",
            "text": "Multimodal Large Language Models enable users to instruct LLMs to analyze user-provided image inputs. By integrating the visual modality, MLLMs enhance the capability of language-only systems, providing them with new interfaces to address a variety of tasks. However, the multimodal abilities of MLLMs regarding the compositional text-to-image generation remain unclear. Within this context, we analyze the abilities of three types of MLLMs, namely MiniGPT-4, ShareGPT4V [15] and GPT-4V [16], focusing on their performance in compositional problems. \n\nEvaluation methods of MLLMs. By aligning a pretrained visual encoder with a frozen large language model, multimodal large language models such as MiniGPT-4 [77] have demonstrated great abilities in vision-language cross-modal understanding. But the current MiniGPT-4 model exhibit limitations such as inaccurate understanding of images and hallucination issues. ShareGPT4V [15] is a large-scale image-text dataset featuring 1.2 million detailed captions characterized by richness and diversity. GPT-4V, a state-of-the-art MLLM, is developed on the foundation of the state-of-the-art LLM, GPT-4 [81], and trained extensively on a large-scale dataset containing multimodal information [81]. Employing MLLM as an evaluation metric, we submit generated images to the model and assess their alignment with the provided text prompt. This evaluation involves soliciting predictions for the image-text alignment score. \n\nPrompt template for MLLM evaluation. We leverage MLLMs as an evaluation metric by feeding the generated images to the model and asking two questions with Chain-of-Thought [78]: \"describe the image\" and \"predict the imagetext alignment score\". We detail the prompts used for MLLM evaluation metric. For each sub-category, we ask two questions in sequence: \"describe the image\" and \"predict the image-text alignment score\". Specifically, Table III shows the prompts for evaluating attribute binding (color, shape, texture).",
            "score": 0.5418199205531851,
            "section_title": "D. MLLM-based Evaluation Metric",
            "char_start_offset": 29754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1986
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "230437551",
            "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation",
            "text": "\"A picture is worth a thousand words\" is a common adage. Indeed, a lot of human knowledge is conveyed by a combination of language and images. Scientific papers or patents usually feature both text and figures, and so do countless other data sources, e.g. math quizzes for high school students, or news reports in magazines. The combination of language and visuals is commonplace, so it is only natural to ask to what extent this combination may help machines to understand meaning.\n\nRecently, Visual-Language (VL) tasks, including Visual Question Answering (VQA) (Antol et al., 2015), Visual Commonsense Reasoning (VCR) (Zellers et al., 2019), etc, have drawn increasing attention from the community. These VL tasks require the pretrained models to simultaneously process multimodal inputs for jointly * The first three authors contribute equally to this work. comprehending visual and textual information. Inspired by successful pretrained language models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), numerous multimodal imagetext pretraining and representation learning models (Tan and Bansal, 2019;Lu et al., 2019;Chen et al., 2020; have been proposed. These multimodal pretrained models use BERT as backbone and are denoising autoencoders, which are trained to predict the alignment of image-text segments and to predict the semantics of masked words and image regions.\n\nTo further bridge the gap between visual and textual clues in multimodal data, a model should not only comprehend cross-modal representations, but also acquire generation capabilities to complete generation tasks, for example, Image Captioning (You et al., 2016). However, applying directly a model pretrained on VL understanding tasks to generation tasks is infeasible, as these models are merely BERT-based encoders, and are thus not suitable for generation tasks. To ease this problem, researchers propose various multimodal models capable of generation tasks (Zhou et al., 2020;. These models achieve state-of-the-art performance in various downstream multimodal generation tasks, including Image Captioning (You et al., 2016), Visual Question Answering (Antol et al.,",
            "score": 0.541146898598784,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 584,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 621,
                    "end": 643,
                    "matchedPaperCorpusId": "53734356"
                },
                {
                    "start": 968,
                    "end": 989,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1101,
                    "end": 1123,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1123,
                    "end": 1139,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1139,
                    "end": 1157,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1641,
                    "end": 1659,
                    "matchedPaperCorpusId": "3120635"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77099609375
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "Generating visual contents from text and describing them through natural language have been extensively studied as core multimodal tasks. Advanced generative models [3,5,6,8,12,14,17,20,40,41,47,49,50,54,[82][83][84], such as DALL\u2022E [46,48], Stable Diffusion [51], demonstrate remarkable generation capabilities, producing high-quality and diverse contents from textual prompts. Meanwhile, image captioning models [26,28,29,66,67,69], such as mPLUG [34], and BLIP [36], push the boundaries of visual understanding, generating accurate and context-aware descriptions. Additionally, recent advancements in multimodal large language models [35], such as LLaVA [39], MiniGPT-4 [86], and InstructBLIP [11], have significantly improved the ability to understand and reason about visual content. \n\nBesides the powerful foundational generative models, adapting or customizing them attracts increasing interest, which enables more personalized and specific outputs based on user preferences [7,9,18,21,32,33,42,75]. Approaches like DreamBooth [52] enable user-specific customization by fine-tuning generative models with personal data, allowing the generation of content tailored to individual needs or preferences.",
            "score": 0.5410859434677915,
            "section_title": "Multimodal Generation and Understanding",
            "char_start_offset": 5148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 1206
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 174,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "258960192"
                },
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "257279850"
                },
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "263909602"
                },
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "52947736"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "251564151"
                },
                {
                    "start": 421,
                    "end": 424,
                    "matchedPaperCorpusId": "244527510"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "246634906"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 985,
                    "end": 987,
                    "matchedPaperCorpusId": "259951373"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "257985512"
                },
                {
                    "start": 996,
                    "end": 999,
                    "matchedPaperCorpusId": "254408780"
                },
                {
                    "start": 999,
                    "end": 1002,
                    "matchedPaperCorpusId": "253581838"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "244345805"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "270559550",
            "title": "CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation",
            "text": "In this paper, we introduced CoMM, a high-quality coherent interleaved image-text dataset designed to overcome the limitations of existing multimodal large language models (MLLMs) in generating coherent and contextually integrated content. By sourcing diverse instructional content and visual storytelling, and implementing a rigorous filtering strategy, CoMM ensures high-quality alignment of text and images. Extensive few-shot experiments demonstrate that CoMM significantly enhances the in-context learning capabilities of MLLMs across various downstream tasks, surpassing existing datasets in text completeness, image-text alignment, and entity consistency, thereby setting a new benchmark in the domain of multimodal datasets. \n\nAdditionally, we proposed four novel tasks to evaluate the interleaved generation abilities of MLLMs, supported by a comprehensive evaluation framework with detailed and robust metrics. Our contributions underscore the importance of high-quality interleaved image-text data in advancing MLLM capabilities. By setting new standards for dataset quality and evaluation, CoMM paves the way for next-generation AI models that excel in multimodal understanding and in-context learning.",
            "score": 0.5407292017635936,
            "section_title": "Conclusion",
            "char_start_offset": 27013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1214
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "The development of large-scale vision-and-language models is significantly impacting a wide range of fields like automated dialogue systems and digital content creation. With the surge in research and development in this domain, the current state-of-the-art Large Language Models (LLMs) (OpenAI, 2023;Chiang et al., 2023;Ouyang et al., 2022) and vision-and-language models such as (Wu et al., 2023a;Li et al., 2023c;Tsimpoukelli et al., 2021;Alayrac et al., 2022) fall short in generating coherent multimodal outputs. This limitation becomes particularly evident in tasks that demand an integrated handling of vision and language, essential for the next generation Large Language Models (LLMs). \n\nOur work, as illustrated in Fig. 1, seeks to address these shortcomings by enhancing the integration of text and image generation in LLMs. The challenges in developing a multimodal LLM capable of interleaved vision and language generation are manifold. First, LLMs typically lack mechanisms to directly produce images, prompting us to introduce \"generative vokens\" that bridge the gap between textual and visual feature spaces. Second, the constraint of data scarcity, especially in visionand-language tasks (Sharma et al., 2018) lacking extensive detailed descriptions of images (Huang et al., 2016), is countered by our unique description-free training approach. Third, maintaining both image-text and image-image consistency poses a significant challenge, which we address through dual-loss strategies. Finally, as we push forward the boundaries with LLMs, the large memory requirements urge us to devise more efficient end-to-end strategies and create an efficient training pipeline accessible for the community, especially in downstream tasks. \n\nSpecifically, to overcome these challenges, we present MiniGPT-5, a novel approach for interleaved vision-and-language generation. By combing the Stable Diffusion with LLMs through special visual tokens (Tan & Bansal, 2020) -\"generative vokens\", we develop a new approach for multimodal generation. Our two-stage training methodology emphasizes a description-free foundational phase, enabling effective model training even with limited caption-grounded images.",
            "score": 0.5407004171042371,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 341,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 416,
                    "end": 442,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 442,
                    "end": 463,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1205,
                    "end": 1226,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81787109375
        },
        {
            "corpus_id": "237490408",
            "title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation",
            "text": "multimodal input and output. Therefore, we are motivated to integrate textual and visual modalities into the BART model and extend its architecture to further improve the multimodal summarization task.\n\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence  Next, to leverage the visual modality and improve the quality of model-selected image, Zhu et al. (2020) proposes to incorporate image references into the training process, and jointly consider summary generation and image selection as training targets. Due to the lack of image references, Zhu et al. (2020) proposes a ROUGE-ranking method using the similarity of image caption and text reference to rank images, which hence builds the pseudo image reference for training. It assumes the image caption has already contained condensed textual information of visual modalities, which semantically matches with the corresponding image. However, this assumption largely depends on the quality of the image caption instead of the image content itself. To sum up, the effectiveness of this ROUGE-ranking method relies on the presence of high-quality image captions. Unfortunately, the image captions are not often qualified or even not present in a multitude of irregular multimedia data.\n\nThus, we propose to distill knowledge from a pretrained vision-language teacher model (e.g., CLIP) to help our model on learning the relevance rank of images in the given textual context without any requirement on the existence and quality of image captions. Specially, given the recent process in contrastive vision-language pre-training (Radford et al. 2021), we are motivated to use the cosine similarity of text references and images to represent text-image content relevance, and use this relevance to build pseudo image references. As far as we know, distilling knowledge from visionlanguage models to mentor multimodal summarization remains unexplored.\n\nConsidering the above mentioned abstractive and image selection objectives, we design UniMS: a Unified framework for Multimodal Summarization grounding on BART, which integrates inputs of both textual and visual modalities along with multiple multimodal objective functions in a unified multitask framework. We modify the BART architecture separately on its encoder and decoder. In the encoder, we distill the knowledge from a pretrained vision-language teacher model to",
            "score": 0.5404524386573775,
            "section_title": "Introduction",
            "char_start_offset": 1924,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 368,
                    "matchedPaperCorpusId": "214273679"
                },
                {
                    "start": 555,
                    "end": 572,
                    "matchedPaperCorpusId": "214273679"
                },
                {
                    "start": 1588,
                    "end": 1609,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.422607421875
        },
        {
            "corpus_id": "235363427",
            "title": "FL-MSRE: A Few-Shot Learning based Approach to Multimodal Social Relation Extraction",
            "text": "Multimodal methods extract relevant information from different modalities and combine them collaboratively. In terms of visual question answering (Das et al. 2018), image captioning (Chen et al. 2015) and visual reasoning (Zellers et al. 2019), state-of-the-art methods have demonstrated that the performance of these tasks can be improved by multimodal information. Pre-trained models are considered to contain abundant syntax and semantic information to understand plain texts, benefiting the process of text information. The first pre-trained model for visual-linguistic is VideoBERT (Sun et al. 2019), where images cropped from videos are encoded into feature vectors and assigned into different clusters which can be treated as visual tokens. ViLBERT (Lu et al. 2019) extends BERT (Devlin et al. 2019) to a multimodal two-stream model, which processes both visual and textual inputs in separate streams through a co-attention Transformer layer. LXMERT (Tan and Bansal 2019) is also a multimodal model built upon the Transformer model. It consists of an object relationship encoder, a language encoder and a cross-modality encoder. In addition, VL-BERT (Su et al. 2020) is a single-stream network which extends the Transformer model to cope with visual inputs.",
            "score": 0.5397812977352965,
            "section_title": "Multimodal Learning",
            "char_start_offset": 6765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1264
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 163,
                    "matchedPaperCorpusId": "35985986"
                },
                {
                    "start": 222,
                    "end": 243,
                    "matchedPaperCorpusId": "53734356"
                },
                {
                    "start": 587,
                    "end": 604,
                    "matchedPaperCorpusId": "102483628"
                },
                {
                    "start": 756,
                    "end": 772,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 786,
                    "end": 805,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 957,
                    "end": 978,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1157,
                    "end": 1172,
                    "matchedPaperCorpusId": "201317624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "261076491",
            "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages",
            "text": "Multimodal learning can be formulated as modeling the relationship between images, denoted as x, and text, denoted as y, in a target language l t . In this context, the image-to-text generation, which can be roughly summarized as generating description for input images, is to learn the conditional distribution p \u03b8 (y lt |x) parameterized by \u03b8; the text-to-image generation, which is to synthesize relevant images given input text prompts, is to learn p \u03d5 (x|y lt ) parameterized by \u03d5. \n\nIn the vanilla setting, these conditional distributions are typically trained using image-text pairs D t = {(x i , y lt i )} N i=1 in the target language l t (Radford et al., 2021;Yang et al., 2022). However, high-quality image-text pairs are extremely scarce in most languages. To mitigate the dependency on native image-text pairs, we introduce the pivot language l p , which contains abundant multimodal pairs \n\n, where M \u226b N . Imitating the human learning mechanism that can naturally align visual concepts with various learned languages, MPM aims to transfer visual concepts learned in the pivot language to the target language. \n\nMPM divides the multimodal learning process in target language l t into two consecutive stages: multilingual alignment and multimodal alignment. For the multilingual alignment, MPM aims to establish the cross-lingual alignment for l t and l p . This is achieved by directly leveraging a pretrained multilingual LLM, denoted as f \u03c3 , which can provide close hidden representations for text pair y lt and y lp with similar semantics, i.e., f \u03c3 (y lt ) \u2248 f \u03c3 (y lp ). For the multimodal alignment, MPM utilize the sufficient multimodal resource D p in the pivot language and optimize the image-to-text objective p \u03b8 (y lp |x) and text-to-image objective p \u03d5 (x|y lp ). In the following sections, we introduce the training process of multimodal alignment stage.",
            "score": 0.5396631045335353,
            "section_title": "PROBLEM FORMULATION AND OVERVIEW",
            "char_start_offset": 8262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 669,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "267412060",
            "title": "When Large Language Models Meet Vector Databases: A Survey",
            "text": "RAG has now evolved to handle a wide range of data types by lending the power of multimodal models. The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models. BLIP-2 [39] uses static image encoders with LLMs for efficient visual language pre-training, facilitating direct image-to-text transformations. Flamingo [40] presented a visual language model for text generation, showcasing remarkable adaptability and leading performance across various vision-and-language tasks. CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text. FROMAGe [42] gains robust multimodal capabilities for few-shot learning solely from image-caption pairs, unlike other models that necessitate large-scale, interwoven imagetext data from the websites. \n\nTo import speech data to RAG systems, Wav2Seq [43] allows for efficient pre-training without the need for transcriptions, using techniques like k-means clustering and bytepair encoding to generate pseudo-subwords from speech. The Large Language and Speech Model (LLaSM) [44] is an end-to-end trained, large multi-modal speech-language model equipped with cross-modal conversational skills and proficient in understanding and responding to combined speech-andlanguage directives. Videos are also made available for certain types of RAG systems. Vid2Seq [45] enhances language models with specific temporal indicators for predicting event limits and textual descriptions in a single output sequence.",
            "score": 0.5395023044144526,
            "section_title": "A. Multimodality of RAG",
            "char_start_offset": 21643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "258947258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "259164940",
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "text": "Recently, the most important breakthrough was made by ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), which unveiled the emerging potential of the conversation between human and artificial intelligence system. ChatGPT serves as a chatbot that operates with language as both input and output, while GPT-4 is a multimodal model capable of accepting both image and text inputs and producing text outputs. \n\nA successful multimodal generative model should excel in both textual and visual reasoning, generating high-quality text and image feedback. Visual ChatGPT (Chenfei Wu & Duan, 2023) is a pioneering work that combines ChatGPT with a series of pre-trained visual foundation models, enabling text-image chat. Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for  The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. \n\n\u2022 We consider the issue of accountability in multimodal generative models by embedding pre-set rules as supervised signals in our datasets. This enables the VLMs to learn to reject human instructions in multimodal conversations. \n\n\u2022 We propose a two-stage training procedure for training the image auto-encoder and auto-regressive transformer, aiming to enable the models to learn how to reject human instructions. All of our models are trained from scratch, and their training took 350\u223c900 GPU days on our constructed datasets. \n\n\u2022 We provide extensive qualitative and quantitative results, evaluating the quality of generated images, the accuracy of answers, and the model's ability to handle uncertainty and incomplete queries. \n\nThe remainder of this paper is organized as follows. Section 2 presents detailed information about our datasets. In Section 3, we introduce our models and training process. We then present extensive experiments and detailed analysis in Section 4 to validate our proposed method. Section 5 provides an overview of related work, and Section 6 concludes the paper. Section 7 pertains to acknowledgments.",
            "score": 0.539364963489127,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2229
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "259190355",
            "title": "Your Prompt is My Command: On Assessing the Human-Centred Generality of Multimodal Models",
            "text": "A new paradigm of AI has emerged at the intersection of generative models and large language models. The resulting AI systems are able to perform a wide variety of tasks by being 'prompted', in which flexible inputs are 'continued' by equally flexible outputs. We introduce the term massive multimodal models (M \u22c6 s) to emphasise both the connection with massive language models and their multimodal capabilities-both inputs and outputs contain snippets of, e.g., text, images, or audio, and the output modes may differ from the input modes 1 . New variants of these models are being released at a rapid pace, and we show an illustrative selection with different input/output modalities in Table 1. \n\nTable 1: Illustrative selection of current massive multimodal models (M \u22c6 s).",
            "score": 0.5389491448461957,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 778
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "1172944",
            "title": "A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities",
            "text": "Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views. In this work, we improve a two-dimensional multimodal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways. (1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model. (2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images. The clusters are directly interpretable and improve on our evaluation tasks. (3) We provide two novel ways to extend the bimodal models to support three or more modalities. We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontextual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.",
            "score": 0.5387934128176176,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "259287020",
            "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
            "text": "Recent systems demonstrate the effectiveness of training large multimodal models such as Flamingo on naturally occurring multimodal documents (Alayrac et al., 2022;Aghajanyan et al., 2022;Huang et al., 2023). A multimodal document is a succession of text paragraphs interleaved by images, such as web pages that contain images. Models trained on these web documents outperform vision and language models trained solely on image-text pairs on various benchmarks (Alayrac et al., 2022). They can also generate long and coherent text about a set of multiple images.\n\nWhile these results are compelling, they have not been replicable. The datasets used in these works are not publicly available, and relatively little information is known about their creation process and composition. This state motivates the creation of large-scale collections of high-quality multimodal web documents to support the creation of the next generation of models.\n\nWe take inspiration from existing large open image-text datasets such as LAION (Schuhmann et al., 2022) and COYO (Byeon et al., 2022), comprised of hundreds of millions of image-text The match between Tottenham Spurs vs Chelsea will kick off from 16:30 at Tottenham Hotspur Stadium, London.\n\nThe derby had been played 54 times and the Blues have dominated the Spurs. Out of 54 matches played, Chelsea has won 28 times and Spurs had only won 7 times. The remaining 19 matches had ended in draw.\n\nHowever, in recent 5 meetings, Spurs had won 3 times where Chelsea had won the other two times. \u2026",
            "score": 0.5385084360731496,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 164,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 461,
                    "end": 483,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "270765959",
            "title": "FigCLIP: A Generative Multimodal Model with Bidirectional Cross-attention for Understanding Figurative Language via Visual Entailment",
            "text": "The Multimodal Figurative Language (MFL) Shared Task can be treated as a classification and generation problem.Given an <image, claim> pair, a MFL model is required to align image-claim representations, learn a binary classification function F c to predict entailment or contradiction labels by following Eq. 1, and learn a generation function F g to generate explanations by following Eq. 2.\n\n(1) V , where d denotes the dimension of 768.This claim-guided multimodal vector achieves an effective interaction of observing details in images based on text.Similarly, the image vector v image serves as Q, and the claim vector v claim serves as K and V .They are fed into the fusion module and then a imageguided multimodal vector v image-gui multi is calculated by the same cross-attention calculation process.This image-guided multimodal vector achieves an effective interaction of observing details in text based on images.These two mentioned-above steps share parameters, enhancing the alignment of figurative image-text representations.After that, the v claim-gui multi and the v image-gui multi are concatenated and input to a binary linear-layer classifier to predict a label of entailment or contradiction.\n\nThe original representation space of CLIP is inconsistent with that of GPT-2.GPT-2 relies on a 50257-dimensional vocabulary to generate text, while the CLIP multimodal space is 768dimensional.For generating explanation, the Fig-CLIP model needs to match the low-dimensional multimodal representations to 50257 dimensions in a mapping space.Inspired by ClipCap (Mokady et al., 2021), we stack multiple linear layers of different dimensions as a projector.This projector is composed of three sets of linear layers of (768*2\u21922048), (2048\u21924096), (4096\u219250257).In order to further compress the size of parameters to reduce training costs, the parameters of this (4096\u219250257) linear layer are frozen and treated as a fixed matrix.This is the reason why FigCLIP is more lightweight than ClipCap, despite their similar model architectures.",
            "score": 0.5378931569570118,
            "section_title": "Task formulation",
            "char_start_offset": 6890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1211
                },
                {
                    "start": 1213,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1405
                },
                {
                    "start": 1405,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1768
                },
                {
                    "start": 1768,
                    "end": 1936
                },
                {
                    "start": 1936,
                    "end": 2043
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486572265625
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "To assess the quality of multimodal generation, we test both our model and the baselines on the VIST validation set by human evaluation. Given a preceding multimodal sequence, models are tasked with producing the subsequent scenario for each task. We select a random sample of 5,000 sequences, with each requiring evaluation by two workers. These evaluators are tasked with determining the superior multimodal output based on three criteria: Language Continuity, Image Quality, and Multimodal Coherence. This assessment is facilitated using Amazon Mechanical Turk (Crowston, 2012), with a representative example (Fig. 4) provided in the Appendix. As depicted in Table 3, our model, MiniGPT-5, is found to generate more fitting text narrations in around 55% of instances, deliver superior image quality in around 53% of cases, and produce more coherent multimodal outputs in around 56% of the scenarios. This data distinctly showcases its enhanced multimodal generation capabilities compared to the two-stage baseline, which must generate intermediate image captions first.",
            "score": 0.5378275029338605,
            "section_title": "Multimodal Generation with Multimodal Input",
            "char_start_offset": 21472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "258170346",
            "title": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence",
            "text": "Developing artificial intelligence models that are capable of performing multimodal reasoning and understanding on complex data is a promising idea. Humans naturally perform multimodal reasoning in daily life [108] for example, when a person is thinking about the concept of 'dog', they will not only think about the English word and its meaning but also a visual image and a sound associated with it. In the context of geospatial tasks, multimodal data are ubiquitous. \n\nIn general, data from different modalities provide different 'views' that complement each other and provide more information to facilitate a holistic understanding of the data. \n\nRecently, much progress has been made in building large-scale multimodal foundation models for joint reasoning from various domains, in particular, vision and language. CLIP [54,112] is one of the first widely-adopted vision-language joint training frameworks. It uses self-supervised contrastive learning to learn a joint embedding of visual and text features. BLIP [82] improves over CLIP by training on synthetically-generated captions from internet-collected images. \n\nIt is designed to handle both visual-language understanding and generation tasks. BEiT-3 [138] is a general-purpose multimodal foundation model that achieves state-of-the-art performance on both vision and vision-language tasks. It combines features from multi-modality expert networks. Florence [151] is a vision-language foundation model that learns universal visual-language representations for objects, scenes, images, videos, as well as captions. Similarly, KOSMOS-1 [49] learns from web-scale multimodal data including text and image pairs. It can transfer knowledge from one modality to another. Flamingo [6] is a family of visual language models that can be adapted to novel tasks using only a few annotated examples, i.e., few-shot learning. It encodes images or videos as inputs along with textual tokens to jointly reason about vision tasks. The newest version of the GPT model, the GPT-4 [105], also claims to perform multimodal analysis including text, audio, images, and videos.",
            "score": 0.5370835461641392,
            "section_title": "Multimodal Foundation Model",
            "char_start_offset": 8588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 214,
                    "matchedPaperCorpusId": "199449027"
                },
                {
                    "start": 829,
                    "end": 833,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "246411402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "267412619",
            "title": "LitLLM: A Toolkit for Scientific Literature Review",
            "text": "Generate the output using 5 sentences. Cite [1] on line 2. Cite [2], [3] on line 3. Cite [4] on line 5. The field of multimodal research has seen significant advancements in the comprehension and creation of both images and text, with models like Flamingo and DALL-E leading the way. In a similar vein, the Contrastive Captioner (CoCa) model presented in [1] pretrains an image-text encoder-decoder foundation model with contrastive loss and captioning loss, achieving state-of-the-art performance on a broad range of downstream tasks. Other works have also focused on improving the fine-grained image-text interaction, with [2] proposing a jointly masked multimodal modeling method and [3] developing a dynamic multimodal interaction framework for image-text retrieval. These models, while effective, often rely on expert experience and empirical feedback, which may limit their flexibility. The introduction of the Wikipedia-based Image Text (WIT) Dataset in [4] provides a large, multilingual dataset for pretraining multimodal models, offering a more diverse set of concepts and real-world entities for model training. This diversity and scale could potentially enhance the performance of models like Flamingo and DALL-E, further advancing the field of multimodal research. \n\nTable 1: We show an example generated related work for a randomly chosen recent paper (Li et al., 2023) with LLM summarized query and retrieved papers. We show the generated related work from our system using both zero-shot and plan-based generation, producing a more succinct and readily usable literature review. Note: The number of citations is retrieved by Semantic Scholar at the date of submission of this work.",
            "score": 0.5369070120835538,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 23125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 1366,
                    "end": 1383,
                    "matchedPaperCorpusId": "255096283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54345703125
        },
        {
            "corpus_id": "269362481",
            "title": "Cantor: Inspiring Multimodal Chain-of-Thought of MLLM",
            "text": "Recent researches indicate that the development of Multimodal Large Language Models (MLLMs) [6,10,11,33,37,39,47,48] is the result of combining the advanced reasoning capabilities of Large Language Models (LLMs) with the capabilities of Vision-Language models (VLMs).These models have achieved significant performance improvements in multimodal tasks by integrating visual and linguistic information.In particular, significant progress [13,23,36]has been made in connecting visual and text representations with contrastive visual and language models, but they encounter limitations when dealing with downstream tasks that require generating components or performing more refined reasoning on visual and language.To overcome these limitations, MLLM extends the reasoning and generation capabilities of LLM to the visual domain by directly inferring embedded visual features [1,2,7,9,24,54].In addition, MLLMs further improve performance through finetuning visual instructions [28].These advances not only demonstrate the ability of MLLM to handle complex multimodal information but also provide new possibilities for achieving General Artificial Intelligence (AGI) with rich multimodal information.By integrating the text reasoning ability of LLM with the image understanding ability of visual language models, MLLM can achieve deep understanding and expression in multiple modalities, processing complex tasks such as image captioning and visual question answering.Open-source MLLMs such as LLaVA [28] demonstrate these capabilities, while closed-source models such as GPT4-V [34] and Gemini [40] have taken a greater step in capturing scene context, reasoning, and creativity.Although for specific tasks these closed-source models may not be directly com-petent or fine-tuning.However, prompt learning can to some extent overcome these limitations.This paper is dedicated to exploring the technique of CoT [43] to enhance the ability of MLLMs to capture the complete context of complex visual scenes, thereby further strengthening their reasoning capabilities.",
            "score": 0.5365591972502125,
            "section_title": "Multimodal Large Language Models",
            "char_start_offset": 7542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 95,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 104,
                    "end": 107,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "249192199"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 873,
                    "end": 876,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 878,
                    "end": 880,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "In recent years, the field of natural language processing (NLP) has seen a shift toward training large language models (LLMs) that are inherently capable of performing a wide range of tasks without requiring extensive task-specific adaptations [12,25]. While these models have demonstrated remarkable success in NLP, there remains a need to develop similarly versatile and scalable models for vision. A crucial aspect of scalability and versatility in vision is the ability to handle multiple (input) modalities and (output) tasks, as vision models must deal with a diverse range of sensory inputs, such as images, 3D, and text and solve a wide range of tasks. \u2020 Unlike NLP, where language modeling on raw text has led to multitask capabilities [84,86], training on only RGB images with a single objective has not exhibited the same behavior for vision. Therefore, it is deemed important to incorporate multiple modalities and tasks in training. It has been indeed suggested by psychophysical studies that multimodality is one key driver behind the development of biological intelligence [104]. This, coupled with the model's ability to perform in-painting, enables powerful image editing capabilities. This generalist model transfers well to a broad range of downstream tasks or to novel modalities, and can be easily fine-tuned into more specialized variants of itself. \n\nTo create a model that exhibits the desirable properties of foundation models in vision, it is important to consider three key aspects in terms of scalability: data, architecture, and training objective. For data, scalability means being able to benefit from more training samples toward improving performance. \n\nIn terms of architecture, scalability implies increased performance with growing model size and remaining stable when trainingd at large sizes. Lastly, a scalable training objective should efficiently handle a growing number of modalities without incurring excessive computational costs. In our approach, we target scalability across these three aspects while maintaining compatibility with multiple modalities. \n\nWe address these challenges by proposing a method consisting of training a single unified Transformer encoder-decoder using a multimodal masked modeling objective. We name this approach 4M (short for \"Massively Multimodal Masked Modeling\") \u2021 to emphasize its ability to scale to many diverse modalities.",
            "score": 0.5364292234448982,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2098
                },
                {
                    "start": 2101,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2404
                }
            ],
            "ref_mentions": [
                {
                    "start": 749,
                    "end": 752,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1088,
                    "end": 1093,
                    "matchedPaperCorpusId": "7107473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "274234079",
            "title": "One Diffusion to Generate Them All",
            "text": "However, they are limited to utilization of multiple image conditions. These models usually requires to design complicated adapters for different conditions. [35,36,55,71] propose unified models for language and images. Concurrently, [59] propose finetuning multimodal large language model with diffusion objective on diverse tasks like text-to-image, editing, and subject-driven generation etc. In contrast, our model distinguishes itself by leveraging bidirectional capabilities of diffusion models and addressing a wide range of diverse tasks.",
            "score": 0.5363789541679107,
            "section_title": "Related work",
            "char_start_offset": 7199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 546
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "249848272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "274306239",
            "title": "Cross-modal Information Flow in Multimodal Large Language Models",
            "text": "The focus of this paper is on auto-regressive multimodal large language models, which consist of an image encoder and a decoder-only language model, as shown in Figure 2. The image encoder transforms images into representations that the language model can take as input, while the language model integrates these visual cues with any provided text, generating responses one word at a time. Often, these components are initialized from a pre-trained image encoder (e.g. CLIP-ViT-L-336px [35] ) and a large language model (e.g. Llama 2 [40]) respectively. Since the interaction between modalities only occurs in the decoder-only transformer, our analysis centers around it and we refer to it as MLLM for brevity unless otherwise specified.",
            "score": 0.5363789541679107,
            "section_title": "Tracing information flow in MLLMs",
            "char_start_offset": 7440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 737
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "265466737",
            "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
            "text": "Before the era of large language models, there were relatively few efforts which try to fuse different modalities seamlessly in the conversational flow. According to different modalities of input and output in dialogue systems, these works can be divided into two categories11: 1) systems with diverse modalities in the input but not in the output [244,245,246,247,248]; 2) systems with multiple modalities in the output [249,250,251,252] 12. \n\nPredominantly, a significant portion of research endeavors align with the first category, where input comprises text and image modalities, resulting in textual responses pertaining to the image content. A noteworthy milestone in this progression was achieved by Alamri et al. [245] when they pioneered the integration of audio information into the input phase, marking a notable enhancement in the multimodal dialogue system's capabilities. It is essential to highlight that the majority of efforts falling under the second category primarily involve the generation of output in alternative modalities, such as images, employing a discriminative approach by selecting one from existing sources rather than generating one [240,243]. Furthermore, some works explore multilingual situations in the context of multimodal dialogue [253,254].",
            "score": 0.5349837489107436,
            "section_title": "Before the era of LLMs",
            "char_start_offset": 56876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 353,
                    "matchedPaperCorpusId": "1820614"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "59316836"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "216914409"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "237091474"
                },
                {
                    "start": 421,
                    "end": 426,
                    "matchedPaperCorpusId": "19100351"
                },
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "236459946"
                },
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "248780460"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "253447251"
                },
                {
                    "start": 721,
                    "end": 726,
                    "matchedPaperCorpusId": "59316836"
                },
                {
                    "start": 1166,
                    "end": 1171,
                    "matchedPaperCorpusId": "2551793"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "53038840"
                },
                {
                    "start": 1271,
                    "end": 1276,
                    "matchedPaperCorpusId": "11640905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.828125
        },
        {
            "corpus_id": "276422421",
            "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
            "text": "Multimodal Large Language Models (MLLMs) [6,20,22,31] have made significant progress in recent years, allowing the comprehension and generation of diverse data modalities including text and images. However, in real-world * Equal contribution. scenarios, there exists a pervasive graph-structured relationships between texts and images. Such graph-structured relationship can be described as \"Multimodal Attributed Graphs\" (MMAGs) [17,27,40], where nodes are associated with image and text information. For example, the artwork graph [25] is composed of nodes that include images (pictures) and text (titles), with edges representing shared genres and authorship. This structure uniquely represents each artwork in relation to thousands of others within the graph, providing a context that extends beyond simple language descriptions or image references. While MLLMs have demonstrated outstanding comprehension and generation capability for text and image data, it is questionable how they could utilize the structural information on MMAGs. \n\nIn this context, we formulate the problem of multimodal content generation on MMAGs which tasks MLLMs with producing both a textual description and an accompanying image for a new node based on the graph connectivity and node attributes. This task focuses on generating text-image pairs for a node from MMAGs, reflecting a wide range of practical applications. For example, generating an image and a text for a product node linked to others through copurchase edges in an e-commerce MMAG is equivalent to recommending [5,24] potential future products to users. Likewise, creating an image and a title for a virtual artwork node in the art MMAG is comparable to creating virtual artwork [7,14] that reflects the subtle styles of various artists and genres. \n\nHowever, directly adopting MLLMs on MMAGs for multimodal content generation presents several challenges: (1) Graph Size Explosion: Although MMAGs provide sub-arXiv:2502.11925v1 [cs.AI] 17 Feb 2025 stantial context for image and text generation, inputting the entire local subgraph structure to a model is impractical due to the exponential increase in size with additional hops, leading to excessively long context sequences.",
            "score": 0.5341211112557316,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 50,
                    "end": 53,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "195848773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "273963138",
            "title": "A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks",
            "text": "This survey and application guide to multimodal large language models(MLLMs) explores the rapidly developing field of MLLMs, examining their architectures, applications, and impact on AI and Generative Models. Starting with foundational concepts, we delve into how MLLMs integrate various data types, including text, images, video and audio, to enable complex AI systems for cross-modal understanding and generation. It covers essential topics such as training methods, architectural components, and practical applications in various fields, from visual storytelling to enhanced accessibility. Through detailed case studies and technical analysis, the text examines prominent MLLM implementations while addressing key challenges in scalability, robustness, and cross-modal learning. Concluding with a discussion of ethical considerations, responsible AI development, and future directions, this authoritative resource provides both theoretical frameworks and practical insights. It offers a balanced perspective on the opportunities and challenges in the development and deployment of MLLMs, and is highly valuable for researchers, practitioners, and students interested in the intersection of natural language processing and computer vision.",
            "score": 0.5336167526487947,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "246867293",
            "title": "XFBoost: Improving Text Generation with Controllable Decoders",
            "text": "Multimodal models, visolinguistic transformers in particular, have demonstrated state-of-the-art performance in vision-and-language tasks such as captioning (Chen et al., 2015), visual question answering (Antol et al., 2015), and visual reasoning (Suhr et al., 2018). A natural extension to the captioning task is conditional Natural Language Generation (NLG). In this setting, multimodal NLG models are prefixed on image and associated textual data and trained to produce natural language descriptions conditioned on both image and text input. The description generation task differs from the captioning task in that text produced requires not only  (3) At the decoding stage, sampling probability of the topk tokens are adjusted by its similarity with extracted attributes. Our system guides generations towards those that satisfy the extracted attributes and given constraints. \n\nthe accuracy of describing images coherently, but also has to satisfy complex lexical constraints. Large-scale, transformer-based neural language models-such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), and GPT (Radford et al., 2019)-are widely used in state-of-the-art multimodal models. They are first pretrained on vast corpora of internet scraped text and books, then finetuned on supervised data together with a taskrelated head. These language models generate text that is statistically representative of the corpora on which they were trained. Consequently, it is often possible for language models to generate lowquality descriptions, i.e. containing contradictory or irrelevant items, for out-of-distribution images not similar to those seen during training. \n\nOverview Inspired by Jain (2021), our paper focuses on improving the quality of generated product descriptions-describing images accurately with common-sense and human-preferred language that conform to conditional inputs (such as brand, title, or category). In this paper, we propose a simple but efficient controllable language generation framework (Figure 1) called Extract-Finetune-Boost (XFBoost) which addresses this issue of output incoherence in multimodal NLG models.",
            "score": 0.5334053494413177,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 1049,
                    "end": 1070,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 1108,
                    "end": 1130,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88671875
        },
        {
            "corpus_id": "264451774",
            "title": "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models",
            "text": "We provide results on additional text-conditioned image generation models, covering various model architectures, pretraining data and input languages. We consider the SOTA text-to-image diffusion models DeepFloyd-IF [69] and Kandinsky [57], as well as the SOTA text-conditioned GAN model GALIP [72]. To evaluate these models, we use the methodology from our main paper. This includes prompts and evaluation metrics, calculated by embedding generated images with CLIP. \n\nWe use the following model checkpoints from Hugging Face Model Hub to evaluate DeepFloyd-IF and Kandinsky: DeepFloyd/IF-I-M-v1.0, kandinsky-community/kandinsky-2-1-prior, kandinsky-community/kandinsky-2-1. For GALIP, we use the official code implementation with the checkpoint trained on the CC12M dataset. 4: Quantitative results for additional multimodal models. We report results for additional SOTA text-to-image models with various architectures and pretraining data sources. We also report the results for the vision-and-language models considered in our work for comparison, as well as the random baseline indicated by (random). \n\nWe use the following inference settings: We run DeepFloyd-IF and Kandinsky in half-precision. \n\nFor DeepFloyd-IF, we only run the first stage of inference which outputs low-resolution (64 \u00d7 64) images. For Kandinsky, we use both prior and image-to-text pipelines with guidance scale set to 1.0 and 768 \u00d7 768 output. All other inference settings use the defaults from the model checkpoint configuration files and the DiffusionPipeline class from the Hugging Face diffusers library. \n\nFor GALIP, we use the default settings provided in the inference notebook in its official repository. For all of these models, we generate a single image for each pseudoword; for \"kiki\" and \"bouba\" generations to calculate \u2206P kb , we generate 10 images for each for each. \n\nIn Table 4, we show quantitative results for these models, which all show significant sound symbolic effects.",
            "score": 0.533278234624706,
            "section_title": "C.1 Additional Multimodal Models",
            "char_start_offset": 39237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "256389964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "270765591",
            "title": "A Textual Modal Supplement Framework for Understanding Multi-Modal Figurative Language",
            "text": "In recent years, advances in language modeling notably improved model comprehension of metaphorical language.broader study of metaphorical language.Their findings indicated that although pre-trained language models could achieve commendable performance after fine-tuning, their performance on a limited number of samples still fell significantly short of human capabilities.\n\nIn addition, with the development of multimedia, there had been an increased focus on generative understanding of multimodal metaphorical language.Hessel et al. investigated visual language models and language-only models for understanding multimodal metaphorical language and found that both types of models had difficulties in all three tasks.Desai et al. introduced an architecture based on a multimodal Transformer, which included a cross-modal attention mechanism focusing on the distinctive features between images and captions.This model obtained relatively high consistency scores in human evaluations.Yosef et al. utilized the state-of-the-art vision and language model CLIP (Radford et al., 2021) to perform on a multi-modal metaphorical language comprehension task and found that it performed relatively poorly.The experimental results showed that the best model was only 22% accurate in the detection task, much lower than the 97% accuracy achieved by humans.This discrepancy was mainly due to the poor performance of model in understanding the connection between metaphorical language and images, with a tendency to prefer partially literal images over metaphorical ones.\n\nThese studies have primarily improved performance through methods such as model finetuning and knowledge enhancement.However, they still face challenges in understanding multimodal metaphorical language.To enhance the capability of visual language models to comprehend metaphorical language, we design prompts to clarify task requirements and employ modal supplement methods to boost the integration of multimodal data, aiming to narrow the gap between models and humans in multimodal metaphor comprehension.",
            "score": 0.5325532327103594,
            "section_title": "Related work",
            "char_start_offset": 2397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 374
                },
                {
                    "start": 376,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1560
                },
                {
                    "start": 1562,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 1060,
                    "end": 1082,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "278636596",
            "title": "A Survey on Large Language Models in Multimodal Recommender Systems",
            "text": "Image Summarisation for LLM Input. To align image data with text-only LLMs, these approaches convert visual inputs into textual summaries or descriptions using MLLMs or vision-language models. This enables compatibility with generative models without architectural changes. However, this conversion may omit finegrained visual signals, and its effectiveness depends on the summarisation model's quality. \n\nHLLM [136] addresses the challenge of sequential multimodal recommendation by summarising image sequences into textual descriptions using prompt-based generation, which are then combined with textual item features for user preference modelling. The approach improves scalability by avoiding direct image sequence input into MLLMs and supports fine-tuned prediction via supervised learning. Molar [77] adopts a more integrated strategy, converting item-level multimodal features-including images, structured attributes, and text-into unified embeddings using prompt-based alignment. Structured attributes are textualised, and contrastive loss with temporal modelling enhances alignment across modalities. \n\nThese works illustrate how summarisation enables LLM compatibility with image content, laying the groundwork for multimodal fusion via textual representations. The use of MLLMs, their cost and infrequency can be seen as disadvantages of these methods.",
            "score": 0.5314494837444783,
            "section_title": "Image Summarisation for LLM Input.",
            "char_start_offset": 47641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1363
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81201171875
        },
        {
            "corpus_id": "15415700",
            "title": "Towards Multimodal Content Representation",
            "text": "Multimodal interfaces, combining the use of speech, graphics, gestures, and facial expressions in input and output, promise to provide new possibilities to deal with information in more effective and efficient ways, supporting for instance: \n\n-the understanding of possibly imprecise, partial or ambiguous multimodal input; -the generation of coordinated, cohesive, and coherent multimodal presentations; -the management of multimodal interaction (e.g., task completion, adapting the interface, error prevention) by representing and exploiting models of the user, the domain, the task, the interactive context, and the media (e.g. text, audio, video). \n\nAn intelligent multimodal interface requires a number of functionalities concerning media input processing and output rendering, deeper analysis and synthesis drawing at least upon underlying models of media and modalities (language, gesture, facial expression of user or animated agent), fusion and coordination of multimodal input and output at a semantic level, interpretation of multimodal input within the current state of the interaction and the context, and reasoning about and planning of multimodal messages. This implies an architecture with many components and interfaces; a reference architecture of an intelligent multimodal dialogue system was established at the workshop `Coordination and Fusion in Multimodal Interaction' in Dagstuhl, Germany, November 2001 (see Bunt, Kipp, Maybury and Wahlster, forthcoming, and http://www.dfki.de/~wahlster/Dagstuhl_Multi_Modality). The communication between many of the components in a multimodal interactive system rely upon an enabling syntax, semantics and pragmatics. A multimodal meaning representation plays central stage in such a system, supporting both interpretation and generation. Such a representation should support any kind of multimodal input and output, and should, in order to be useful in a field which is still developing, be sufficiently open to support a range of theories and approaches to multimodal communication. \n\nThe present document is intended to support the discussion on multimodal content representation, its possible objectives and basic constraints, and how the definition of a generic representation framework for multimodal content representation may be approached.",
            "score": 0.5311330127658389,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2045
                },
                {
                    "start": 2048,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23193359375
        },
        {
            "corpus_id": "276408999",
            "title": "Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines",
            "text": "Previous work MuRAR (Zhu et al., 2024) generates and integrates text and images independently. We refer to this approach as the separate strategy, which neglects interactions between multi-modal data during text generation. To address this limitation, we propose two joint modeling strategies that explicitly capture the relationships between text and images: the single-stage strategy and the multistage strategy, as illustrated in Step 4 of Figure 2. \n\nSingle-stage requires the model to directly generate a multi-modal output by placing selected images into their corresponding placeholders, using all multi-modal content provided within a single prompt. \n\nMulti-stage addresses the common limitation of foundation models, which often struggle to process a large number of images simultaneously. It involves three stages: (1) Text Generation: generating a plain text answer based on the multi-modal input; (2) Image Interleaving: dividing the text into segments and prompting LLMs or MLLMs to identify which segments require image insertions for improved readability; (3) Text Refinement: refining each segment by incorporating information from the selected images. \n\nBoth LLMs and MLLMs are able to handle M 2 RAG task with these two strategies. Specifically, LLMs and MLLMs get the prompt P as input to handle M 2 RAG task, which is constructed using a structured template T : \n\nwhere G, Q, K In-Doc denotes the task guidelines, user query and knowledge base. In the prompt P , each image is represented using Markdown format: ![IMAGE_CONTENT](PSEUDO_URL): \n\n(1) For MLLMs, IMAGE_CONTENT refers to encoded embeddings of the image, with the entire placeholder positioned where the image originally appears in the context (Dubey et al., 2024;Wang et al., 2024a), ensuring coherence between the image and the surrounding text; \n\n(2) For LLMs, we convert the image into a detailed textual description as IMAGE_CONTENT, enabling the LLMs to comprehend the semantic information of the image. \n\nPSEUDO_URL serves as the identifier for each input image. In the output of the single-stage approach, images are also represented using the aforementioned Markdown format, where PSEUDO_URL indicates the index of the selected image.",
            "score": 0.5310019991518748,
            "section_title": "Generation Strategy for M 2 RAG",
            "char_start_offset": 16367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2224
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "265466034",
            "title": "Large Language Models Meet Computer Vision: A Brief Survey",
            "text": "A visual language model is an AI-driven mechanism tailored to interpret and derive insights from visual content like images and videos. By training on vast collections of visual content, these models become adept at identifying and categorizing various visual components, from objects to individuals to entire scenes. VLMs have evolved significantly over the years, transitioning from the use of hand-crafted image descriptors and pre-trained word vectors to the adoption of advanced transformer architectures for both image and text encoding. Central to these models are three core components: an image encoder, a text encoder, and a fusion mechanism to integrate information from both domains as shown in Fig 3 . The contemporary approach emphasizes strategic pre-training objectives to enhance transfer performance across various tasks. Among the notable pre-training strategies are: \n\n\u2022 Contrastive Learning: Aligns images and texts in a joint feature space. \n\n\u2022 PrefixLM: Uses images as a prefix to a language model, enabling joint learning of image and text embeddings. \n\n\u2022 Multi-modal Fusing with Cross Attention: Integrates visual data into language model layers. \n\n\u2022 MLM/ITM: Focuses on aligning specific image parts with corresponding text using masked-language modeling and image-text matching. \n\n\u2022 No Training approach: Leverages standalone vision and language models through iterative optimization.",
            "score": 0.5310019991518748,
            "section_title": "Vision-Language Models (VLMs)",
            "char_start_offset": 8252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "268987507",
            "title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance",
            "text": "We analyze two classes of multimodal models: Image-Text and Text-to-Image.For both, we detail the pretraining and testing datasets, along with their associated evaluation parameters.",
            "score": 0.5310019991518748,
            "section_title": "Experimental Setup",
            "char_start_offset": 10300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 74,
                    "end": 182
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "231740629",
            "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
            "text": "The success of transformer-based language models on a variety of language tasks (e.g., Devlin et al., 2019) has inspired similar multimodal efforts (e.g., Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Li et al., 2020a,b). 2 The main distinction is that image-text multimodal transformers take image-text pairs as input, attend over both modalities, and are trained with additional losses. Similar to the language models, multimodal transformers are often fine-tuned on downstream tasks but multimodal ones; e.g., image retrieval (Young et al., 2014) or visual question answering (Goyal et al., 2017). \n\nWe give a brief overview of the BERT model (Devlin et al., 2019) which forms the backbone of multimodal transformers. The BERT architecture consists of a stack of transformer blocks (Vaswani et al., 2017) and has three main components. First, the input text is tokenized and three embedding functions are used to embed the token, its position in the sentence (i.e., positional encoding), and the sentence it belongs to. The final language embedding is a summation of these three vectors. The BERT model also includes a <SEP> token to separate different sentences and a <CLS> token which can be thought of as an aggregate representation of the input text. Second, the sequence of token embeddings are input into a series of transformer layers where tokens are combined through self-attention. Third, two different losses are applied to the model output: a masked language modeling loss, in which the model predicts a masked word (denoted by a <MASK> token), and a next sentence prediction loss which, given two sentences, predicts if the second sentence follows the first. \n\nMultimodal transformer models facilitate learning from multimodal data via three changes to the BERT architecture: multimodal data preprocessing (more specifically images), adding multimodal attention by changing self-attention such that it combines image and text modalities, and introducing image and multimodal loss functions.",
            "score": 0.5308672363530664,
            "section_title": "Multimodal Transformers",
            "char_start_offset": 4697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 171,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 189,
                    "end": 210,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 538,
                    "end": 558,
                    "matchedPaperCorpusId": "3104920"
                },
                {
                    "start": 588,
                    "end": 608,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 794,
                    "end": 816,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "252668479",
            "title": "Linearly Mapping from Image to Text Space",
            "text": "We connect image representations as inputs to an LM with an extremely simple transformation: a linear projection. We interpret the success of generating text relevant to an image with this approach as indicative of an underexplored representational similarity between language and vision representations. Depending on the linguistic guidance in the pretraining task used to pretrain the image encoder, we see varying performance. Using a vision-only encoder (BEIT) leads to generations from the LM that are often incorrect but close under measures of perceptual relatedness. Unless finetuned with image classification BEIT has no inductive bias in pretraining to distinguish concepts that we might normally distinguish with language, especially when they are perceptually very similar. The fact that only image encoders trained with linguistic supervision can do this suggests interesting future work on the role of language in category formation. Despite strong performance with a linear transformation, the representation spaces of these models seem to contain differences that cannot be approximated in the language space. Multimodal models, ideally, will learn richer representations by taking advantage of these differences. It is useful to think about how current multimodal pretraining objectives succeed and/or fail at doing this. LiMBeR can serve as a strong baseline for future multimodal models, as it provides a point of comparison for a minimal mapping between vision and language. \n\nWe see LiMBeR as a useful tool for understanding how representations trained from different modalities can be similar or different. For concepts that are represented similarly, can we take advantage of this fact to reduce the amount of data required for learning good text representations? Where they are different, can multimodal models learn richer representations by incorporating information from both? For example, vision data may help with reporting bias in text corpora (Paik et al., 2021). Answering these questions can help us understand the limits of text-only pretraining, as well as how to better ground LMs to non-linguistic data.",
            "score": 0.5305920441175516,
            "section_title": "DISCUSSION & FUTURE WORK",
            "char_start_offset": 25209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1974,
                    "end": 1993,
                    "matchedPaperCorpusId": "239009473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "260438635",
            "title": "Multimodal Neurons in Pretrained Text-Only Transformers",
            "text": "Language models demonstrate remarkable capacity to generalize representations learned in one modality to downstream tasks in other modalities. Can we trace this ability to individual neurons? We study the case where a frozen text transformer is augmented with vision using a self-supervised visual encoder and a single linear projection learned on an image-to-text task. Outputs of the projection layer are not immediately decodable into language describing image content; instead, we find that translation between modalities occurs deeper within the transformer. We introduce a procedure for identifying \"multimodal neurons\" that convert visual representations into corresponding text, and decoding the concepts they inject into the model\u2019s residual stream. In a series of experiments, we show that multimodal neurons operate on specific visual concepts across inputs, and have a systematic causal effect on image captioning. Project page: mmns.csail.mit.edu",
            "score": 0.530520355495257,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "273798011",
            "title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models",
            "text": "We adopt the manifold multimodal model architecture [55,52,11,6,34], formulated as follows: \n\nVisual Representation. We first process x img subject to a visual representation backbone V \u03c9 that outputs a sequence of features p img \u2208 R L\u00d7hvision where p img = V \u03c9 (x img ). As an example, p img might be the patch features output by a Vision Transformer. \n\nVision-Language Projector. Next, we map p img to a sequence of embeddings e img \u2208 R L\u00d7htext via a learned projector F \u03c8 , where e img = F \u03c8 (p img ). \n\nLanguage Model. Finally, we concatenate the sequence e img with the text prompt embeddings e prompt = embed(u prompt ), passing the result to the language model. Generally, we have the interleaved image-text input x input by concatting all the e prompt and e img . The language model generates output text u gen = LM \u03b8 (x input ). \n\nRetrieval Projector. For discriminative tasks, we select the token d i from MLLM's hidden state and map it to r i via a learned projector F \u03c6 . \n\nIn Implementation, we utilize CLIP ViT-L/14 [73] as the visual encoder, and Vicuna 1.5 [14] as the language model.",
            "score": 0.5304204623107431,
            "section_title": "D Method Details D.1 Architecture Details",
            "char_start_offset": 29184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 94,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1100
                }
            ],
            "ref_mentions": [
                {
                    "start": 1030,
                    "end": 1034,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "264405778",
            "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
            "text": "Figure 4 illustrates the overall architecture of our proposed model. Steve-Eye, functioning as a generative model, connects an image-oriented tokenizer f v with the pre-trained LLM backbone \u0398. \n\nWe adopt the image tokenizer, e.g., VQ-GAN (Esser et al., 2021), to encode the raw images I into token embeddings V = {v 1 , v 2 , ..., v n } \u2208 R n\u00d7d , where n denotes the number of visual tokens and d is the dimensionality of each token. We further utilize a lightweight projection module f l with a trainable projection matrix W . This module maps the visual tokens to the same space with text embeddings, yielding V = {v 1 , v2 , ..., vn } \u2208 R n\u00d7 d: \n\nTo effectively process visual-language inputs and generate corresponding outputs, our model integrates the visual codebook C v into the pre-existing language vocabulary C l . This integration leads to the formation of a unified multimodal codebook, denoted as \n\nAdditionally, in order to mark the starting and ending points of visual elements in I/O sequences, we introduce two special tokens, namely <vis> and </vis>. The LLM backbone \u0398 of our Steve-Eye is built upon a decoder-only architecture with casual transformers. Our model employs an auto-regressive prediction mechanism, generating responses based on the provided multimodal input tokens. The resulting response is a mixed sequence of visual and textual tokens, represented as Y = {y 1 , y 2 , ..., y m }. For each embedding y i , we pass it through a linear layer f p followed by a softmax operation, mapping it into a probability distribution of the multimodal vocabulary. The final prediction for the i-th token z i is determined by selecting the token from the multimodal codebook with the highest score: \n\n(2)",
            "score": 0.5302127611118428,
            "section_title": "MODEL ARCHITECTURE",
            "char_start_offset": 15142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1725
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 258,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4140625
        },
        {
            "corpus_id": "270391793",
            "title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks",
            "text": "Conventional MLLMs. With the advancement of large language models (LLMs) [145,146,21,215,171,37,172,13,170,9,103,7,59,43,22], multimodal large language models (MLLMs) have also gained significant momentum recently. Notable commercial models include GPT-4V [2], Gemini series [169,150], Claude-3 [10], and Qwen-VL-Max [14], known for their outstanding performance. \n\nEarly open-source MLLMs like InstructBLIP [42], LLaVA [107] and MiniGPT-4 [223] fine-tune on instruction-following datasets. InternVL [34,33] series models align a large-scale vision encoder with LLMs and perform comparably to commercial models. Efficient MLLMs [100,228,38] have also studied. However, these models only can output text, restricting their applications. \n\nExtension of MLLMs' Text Output. To extend MLLMs to downstream tasks, models like Kosmo-2 [140], Shikra [27], VisionLLM [182], Ferret [201,212], and All-Seeing V2 [180] achieve this using specially-designed tokens or encoding coordinates as text tokens. Despite these advancements, using LLMs solely as visual decoders falls short of resolving the fine-grained visual context needed for precise detection and segmentation. The other line of works focus on broadening the modality scope. AnyGPT [210] builds a multimodal text-centric dataset for any-to-any multimodal generation (text, image, speech, music) with sequence modeling. Chameleon [168] uses fully token-based representations for both texts and images, capable of understanding and generating interleaved imagetext sequences. CM3leon [5,205] are autoregressive models for text-to-image and image-to-text tasks. All these works could unify image understanding and generation in one network. Our model can support more vision and vision-language tasks. \n\nMLLMs w/ Downstream Tools.",
            "score": 0.5298946942092443,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 5416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 420,
                    "end": 425,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 440,
                    "end": 445,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 858,
                    "end": 863,
                    "matchedPaperCorpusId": "258762579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "272910588",
            "title": "ChatCam: Empowering Camera Control through Conversational AI",
            "text": "Multimodal Language Models. Large-scale language models (LLMs) [9,19,17,1,70] enabled by extensive datasets and model size, have demonstrated surprising emerging abilities. The emergence of multimodal models [42,41,32] is captivating as they can process text alongside other modalities such as images [24], audio [26], and videos [79]. Some unified models can perceive inputs and generate outputs in various combinations of text, images, videos, and audio [51,68,78,87]. LLMs hold the potential to act as agents [76,81,62], allowing them to be driven by goals, reason about their objectives, devise plans, utilize tools, and interact with and gather feedback from the environment. Our proposed method involves multiple modalities including language, images, 3D fields, and camera trajectories, and utilizes LLMs as agents to assist users in operating cameras. \n\nRadiance Field Representations. Utilizing continuous 3D fields modeled by MLPs and volumetric rendering, Neural Radiance Fields (NeRFs) [52] achieved breakthrough for novel view synthesis. \n\nFrom outside the window, enter the bedroom and explore its layout following an S-shaped path.",
            "score": 0.5292777015680312,
            "section_title": "Related Work",
            "char_start_offset": 3795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1146
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 69,
                    "end": 72,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 208,
                    "end": 212,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "257219775"
                },
                {
                    "start": 301,
                    "end": 305,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "235624127"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "238215257"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "265506847"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "256697342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "256461099",
            "title": "Controlling for Stereotypes in Multimodal Language Model Evaluation",
            "text": "The Transformer (Vaswani et al., 2017) is a sequence-based model that is now the standard architecture in NLP for devising representation and generation components in neural models. Pretrained language models such as BERT (Devlin et al., 2019) based on the architecture of Transformers, have proven capable of learning powerful representations applicable to a wide range of tasks. They have yielded state-of-the-art performance in many downstream tasks.\n\nMultimodal models fusing the textual and visual modalities have been devised by researchers after looking at the huge success of pre-trained language models. In such models, multiple modalities are considered, and data for the training of the models is in multiple modalities. As our research problem revolves around the aspect of multimodality, we will focus on two modalities: a textual and a visual signal. The visual signal is in the form of images, and the natural language is the written text accompanying the images, such as captions or descriptions of the images. Examples of such visual/textual Transformers include VilBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), Vi-sualBERT (Li et al., 2020a), OSCAR (Li et al., 2020b), ImageBERT (Qi et al., 2020), FLAVA (Singh et al., 2022), and others. Most of the earlier models use features extracted from a Faster-RCNN pipeline (Ren et al., 2015), while later models use visual Transformer architectures (Dosovitskiy et al., 2021). These types of models are then trained on datasets that contain text/image pairs such as SBU Captions (Ordonez et al., 2011), MS COCO (Lin et al., 2014), Conceptual Captions (Sharma et al., 2018), and Visual Genome QA (Krishna et al., 2017), using various pre-training tasks. They are sometimes trained from scratch on the combined language/vision data and sometimes warm-started from a unimodal model such as BERT.\n\nFor this study, we selected three different multimodal models to run our experiments on. These image-augmented Transformer models are VisualBERT, L",
            "score": 0.5289061000944653,
            "section_title": "Multimodal Language Models",
            "char_start_offset": 8508,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 37,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 222,
                    "end": 243,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1088,
                    "end": 1105,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1114,
                    "end": 1136,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 1150,
                    "end": 1168,
                    "matchedPaperCorpusId": "218610661"
                },
                {
                    "start": 1176,
                    "end": 1194,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 1231,
                    "end": 1251,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1343,
                    "end": 1361,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 1419,
                    "end": 1445,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1549,
                    "end": 1571,
                    "matchedPaperCorpusId": "14579301"
                },
                {
                    "start": 1581,
                    "end": 1599,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 1621,
                    "end": 1642,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 1665,
                    "end": 1687,
                    "matchedPaperCorpusId": "4492210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "score": 0.5288352967397612,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386474609375
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm -from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.",
            "score": 0.5288070906149773,
            "section_title": "ABSTRACT",
            "char_start_offset": 11,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382080078125
        },
        {
            "corpus_id": "1729289",
            "title": "Self-Guiding Multimodal LSTM\u2014When We Do Not Have a Perfect Training Dataset for Image Captioning",
            "text": "Later, this content information is utilized to drive a language generation system, e.g., a recurrent neural network (RNN), to output the image descriptions (You et al., 2016;Gan et al., 2017). Some frameworks model image and text jointly into a multimodal space where later the joint representation space is used to perform cross-modal retrieval based on a query image. Karpathy and Li (2015) presented an alignment model which uses a structured object to align the two modalities (i.e., CNN over image regions and bidirectional RNN over sentences) through a multimodal embedding. An encoder-decoder framework is presented by Kiros et al. (2015) utilizing a joint multimodal space in which the LSTM is a big success. Another represented work in this category is the m-RNN model (Mao et al., 2015a) in which a multimodal component is introduced to explicitly connect the language model and the vision model by a one-layer representation. \n\nWith image captioning being a thriving topic, it is driven by the technical trials and improvements in both computer vision and NLP, and also importantly, the availability of relevant datasets. Other than the traditional image captioning task, efforts have been made to special captioning tasks. Mao et al. (2015b) modified m-RNN to address the task of learning novel visual concepts. Hendricks et al. ( 2016) incorporated unpaired image data with labeling and unpaired text data to address the concept limitations in the image-sentence paired dataset. Similarly, Venugopalan et al. (2017) proposed the Novel Object Captioner (NOC) to describe object categories that are not present in the existing image-sentence datasets. 'Referring expression' was explored (Kazemzadeh et al., 2014;Mao et al., 2016) to generate unambiguous descriptions. Park and Kim (2015) presented a coherence recurrent convolutional network (CRCN) to describe an image stream in a storytelling manner utilizing blog data.",
            "score": 0.5280744204073204,
            "section_title": "Image Captioning",
            "char_start_offset": 9234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "276421723",
            "title": "Natural Language Generation from Visual Sequences: Challenges and Future Directions",
            "text": "The Vision-to-Language Bridge Some V2L model architectures utilize an intermediate module that bridges the input and output modalities for effectively conditioning the text generation on the extracted visual features. Different models operationalize this module with different degrees of complexity. Earlier approaches for several multiimage-to-text tasks condition the text generation process by directly fusing vision encoder outputs with the language decoder input (Kim et al., 2018). Some architectures employ cross-attention mechanisms to focus on the relevant parts of the visual features at various temporal positions during decoding (Yao et al., 2015). However, approaches that adopt pre-trained models-e.g., CLIP-ViT-L (Radford et al., 2021) as the visual model-tend to employ learnable intermediate layers for aligning and converting outputs of the vision encoder into a format that the language decoder can understand. \n\nIn some of the proposed models, this intermediate module is a single linear layer that transforms the visual features into a common shared space, which can be used by the language decoder (Ko et al., 2023;Liu et al., 2023). In other models, advanced transformer-based projectors such as a Q-Former (Li et al., 2023c) are used for their ability to leverage cross-modal interactions effectively (Han et al., 2024). In essence, Q-Former uses dynamic query vectors that are pre-trained to attend to both visual and textual representations, enhancing its ability to generalize and perform well (relative to a single linear layer) across different tasks. Besides these popular methods for adapting multimodal information, some approaches make use of graph neural networks for capturing relationships between objects in the images at different temporal positions and words in the corresponding sentences of the text (Zhang and Peng, 2019). While there is no definitive way to design this intermediate module, recent work has compared the two approaches, i.e., using cross-attention between modalities or using a multimodal projector for transforming vision encoder features into the language space, and found that the latter leads to a stable/improved per-formance of models (Lauren\u00e7on et al., 2024b).",
            "score": 0.5280282735253508,
            "section_title": "4.2",
            "char_start_offset": 19973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 659,
                    "matchedPaperCorpusId": "623318"
                },
                {
                    "start": 728,
                    "end": 750,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1120,
                    "end": 1137,
                    "matchedPaperCorpusId": "264438945"
                },
                {
                    "start": 1230,
                    "end": 1248,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1325,
                    "end": 1343,
                    "matchedPaperCorpusId": "269293975"
                },
                {
                    "start": 1841,
                    "end": 1863,
                    "matchedPaperCorpusId": "184487531"
                },
                {
                    "start": 2200,
                    "end": 2225,
                    "matchedPaperCorpusId": "269587869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "266573642",
            "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey",
            "text": "integrating multiple modalities. Overall, the architecture enables the integration of diverse modalities into a large language model for multimodal understanding and response generation. \n\nPandaGPT is a model that integrates multimodal encoders from ImageBind and language models from Vicuna to perform instruction-following tasks across six modalities: image/video, text, audio, depth, thermal, and IMU. It demonstrates the ability to connect information from different modalities and compose their semantics naturally, enabling tasks such as image description generation, story writing inspired by videos, and answering questions about audios. PandaGPT's training on aligned image-text pairs allows it to display emergent cross-modal capabilities for data other than image and text, paving the way for holistic understanding of inputs across different modalities. \n\nLAMM introduces the Language-Assisted multimodal (LAMM) dataset, framework, and benchmark, aiming to facilitate the and evaluation of multimodal large language models (MLLMs). The main contributions include the comprehensive dataset and benchmark covering a wide range of vision tasks for 2D and 3D vision, a detailed methodology for constructing multimodal instruction tuning datasets, and a primary MLLM training framework optimized for modality extension. Additionally, the paper provides baseline models, extensive experimental observations, and analysis to accelerate future research in the field of multimodal language models. \n\nLLaVAR As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework. \n\nOtterHD introduces OtterHD-8B model, which addresses the limitations of fixed-resolution inputs in Large Multimodal Models (LMMs). It leverages the Fuyu-8B ar-Fig. 28: Illustration of SEED-LLaMA [137].",
            "score": 0.5276241743537853,
            "section_title": "Visual Assistant",
            "char_start_offset": 68635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2277
                },
                {
                    "start": 2278,
                    "end": 2315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "237290028",
            "title": "Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training",
            "text": "To learn the cross-lingual correspondence between source and target language sentences and enhance the cross-modal fusion between images and texts for better translation, we pre-train our model with three pre-training tasks, described in this section. Task #1: Multimodal Translation Language Modeling (MTLM) The cross-lingual alignment is important to machine translation. Inspired by the multilingual pre-training task (translation language modeling (TLM) proposed in XLM [11]), and the multimodal pretraining task (masked language modeling (MLM) generally used in V+L pre-training models [9,23,28]), we propose to combine them for the multimodal multilingual scenario as the multimodal translation language modeling (MTLM) task for the PMT.\n\nThe goal of MTLM is to predict the masked words in both languages with the context information of images, the surrounding words in the same language, and all the words in the other language. We randomly choose 15% word tokens in both languages for prediction. Each chosen word is replaced with a special [MASK] token 80% of the time, another random word 10% of the time and the original word 10% of the time. Note that the random words could be foreign words. The MTLM task takes the fused feature from cross encoder as the input and predict the original word with an output softmax layer, which is tied with input embeddings. We share the vocabulary and softmax prediction layer across languages. The training objective of MTLM can be expressed as follows:\n\nwhere D denotes the whole training set, and denote the masked words in and , and \u0398 denotes all learnable parameters of the pre-training model. Note that and are not semantically aligned words. With the MTLM pre-training task, the model learns the semantic alignments between source and target language words. However, since the translation guidance from the images is much weaker than that from the source words, the model tends to ignore the visual modality with only the MTLM task for the target translation. Therefore, we further propose to enhance the cross-modal fusion between images and texts via cross-modal pre-training tasks. Task #2: Image Source-Sentence Matching (ISM) The crossmodal matching task has been widely",
            "score": 0.5274677709909157,
            "section_title": "Pre-training Tasks",
            "char_start_offset": 14729,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 594,
                    "end": 597,
                    "matchedPaperCorpusId": "201058752"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "199453025"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "266567429",
            "title": "Modeling Multimodal Uncertainties via Probability Distribution Encoders Included Vision-Language Models",
            "text": "After the cross-modal transformer, fine-grained interaction is enabled on each token across different modalities. We propose two methods to handle the fine-grained multimodal pre-training, which are Distribution-based Masked Language Modeling (D-MLM) and Distribution-based Image Text Matching (D-ITM). D-MLM necessitates the model to predict masked words by interpreting the text in conjunction with an image. The conventional Masked Language Modeling task, initially employed as a pre-training task for BERT [39], aims at enhancing contextual modeling capabilities. In the VLP scenario, missing words are reconstructed using information from other features and modalities. According to the configurations from several multimodal models [15], [35], the model masks text tokens at a probability of 15%, with 80% of them replaced by the [MASK] token, 10% substituted with random words, and the remaining 10% left unchanged. To predict the masked words, we sample the points from distribution representations, wherein D-MLM minimizes a Cross-Entropy (CE) loss across \u00b5 vectors and other sample point vectors: \n\nwhere K denotes the sample number and y represents the label of the masked word. \u00b5 is indicative of a mean vector, whereas z (i) stands for stochastic sample point vectors; these vectors are subsequently channeled into the classifier \u03c6. In the inference phase, the final output is derived by averaging the prediction results of all samples: \n\nD-ITM is a binary classification task that predicts whether a pair of image-text is matched or not. In detail, we extract the point vectors from w [CLS] distributions of multimodal representations, and merge them to generate the results. \n\nwhere v \u00b5 and w \u00b5 represent the mean vectors of vision and language [CLS] distributions, respectively, while v (i)  and w (i) denote the sampled points. The D-ITM classifier is denoted by \u03c6. The matched image-text pairs serve as positive examples. Negative examples are generated through the random substitution of either images or text descriptions.",
            "score": 0.5274483052634514,
            "section_title": "2) FINE-GRAINED PRE-TRAINING TASKS",
            "char_start_offset": 17176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 510,
                    "end": 514,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 738,
                    "end": 742,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "241033425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "277857505",
            "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
            "text": "Large Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023;Team et al., 2023;Guo et al., 2025) are trained on large-scale text corpora to predict token sequences in an autoregressive manner. Given a sequence of tokens x = (x 1 , x 2 , . . . , x t ), an LLM models the conditional probability of the next token as: \n\nwhere h t denotes the hidden state at time step t, and W is the learned weight matrix mapping hidden states to token logits. The model generates text by sampling iteratively from this distribution. Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023a;Yan et al., 2024), audio (Chu et al., 2023), graph (Tang et al., 2024), and structured charts (Han et al., 2023), alongside text. These non-text inputs are embedded into a shared representation space as e V , while the textual prompt is embedded as e T . The MLLM then generates output tokens y = (y 1 , y 2 , . . . , y L ) according to: \n\nwhere y <t denotes previously generated tokens. By integrating multimodal contexts, MLLMs significantly expand the capabilities of LLMs-enabling tasks such as image captioning, visual question answering, and multimodal reasoning that are beyond the scope of text-only models.",
            "score": 0.5264903022528368,
            "section_title": "B.2 LLMs & MLLMs",
            "char_start_offset": 57083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1276
                }
            ],
            "ref_mentions": [
                {
                    "start": 642,
                    "end": 661,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 661,
                    "end": 678,
                    "matchedPaperCorpusId": "264590333"
                },
                {
                    "start": 712,
                    "end": 731,
                    "matchedPaperCorpusId": "264405943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "To comprehensively assess the model performance across image, text, and multimodal dimensions, we employ a diverse set of metrics. For evaluating the quality and diversity of generated images, we utilize the Inception Score (IS) (Salimans et al., 2016), and Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017). Textual performance is gauged through metrics such as BLEU (Papineni et al., 2002), Rouge-L (Lin, 2004), METEOR (Banerjee & Lavie, 2005), and Sentence-BERT (S-BERT) (Reimers & Gurevych, 2019) scores. \n\nFrom the multimodal perspective, we leverage CLIP-based metrics (Rombach et al., 2022b) to assess the similarities between generated content and ground truth. CLIP-I evaluates the similarity between generated and ground-truth image features. To address potential misalignments in the multimodal generation, such as when the ground truth is text-only, but the output is multimodal, we utilize MM-Relevance (Feng et al., 2022). This metric calculates the F1 score based on CLIP similarities, providing a nuanced evaluation of multimodal coherence. \n\nRecognizing that the generated multimodal output might be meaningful yet differ from the ground truth, we also incorporate human evaluation to assess the model's performance. We examine the model's effectiveness from three perspectives: (1) Language Continuity: assessing if the produced text aligns seamlessly with the provided context; (2) Image Quality: evaluating the clarity and relevance of the generated image; and (3) Multimodal Coherence: determining if the combined textimage output is consistent with the initial context.",
            "score": 0.5263565859632429,
            "section_title": "Metrics",
            "char_start_offset": 18823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1596
                }
            ],
            "ref_mentions": [
                {
                    "start": 229,
                    "end": 252,
                    "matchedPaperCorpusId": "1687220"
                },
                {
                    "start": 373,
                    "end": 396,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 406,
                    "end": 417,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 426,
                    "end": 450,
                    "matchedPaperCorpusId": "7164502"
                },
                {
                    "start": 580,
                    "end": 603,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "276249222",
            "title": "Large Multimodal Models for Low-Resource Languages: A Survey",
            "text": "References are clickable links to papers. \n\nthe scarcity of paired data across modalities (e.g. image-text pairs, audio-text alignments) poses additional challenges. \n\nThe motivation for developing multimodal capabilities for LR languages is compelling. First, multimodal processing better reflects how humans naturally communicate and understand information through multiple sensory channels. Second, visual and audio cues can provide crucial contextual information that helps to overcome the limitations of scarce textual data. Third, many LR languages are primarily spoken rather than written, making multimodal approaches particularly relevant for their digital preservation and processing. However, developing multimodal systems for LR languages faces several significant challenges, including: (1) the scarcity of high-quality multimodal datasets in these languages, (2) the lack of standardized evaluation benchmarks, (3) the computational cost of training large-scale models with limited resources, and (4) the complexity of handling different writing systems, dialects, and cultural contexts. Moreover, the problem of catastrophic forgetting when adapting pre-trained models to new languages and the challenge of maintaining performance across different modalities pose significant technical hurdles. In this context, we survey research articles from 2018 to the end of 2024 that specifically study LMMs for LR languages. We focus on works that go beyond simple cross-lingual transfer or translation, examining techniques that leverage multiple modalities to improve model performance. Our analysis reveals several interesting patterns in how researchers approached multimodal learning for LR languages. As shown in Figure 1, text-image combinations dominate the research landscape, appearing in 67 papers (63% of surveyed works), while more complex combinations incorporating audio and video remain less explored. In addition, the distribution of research focus across languages is notably uneven, as illustrated in Figure 2, with Hindi (26 papers), Arabic (18 papers) and Bengali (16 papers) receiving significant focus, whereas 40 other languages are each represented by a single study. This disparity highlights the need for a broader coverage of understudied languages in multimodal research. Some recent surveys have explored various aspects of multimodal language models.",
            "score": 0.5262047129351434,
            "section_title": "Introduction",
            "char_start_offset": 1875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2306
                },
                {
                    "start": 2307,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "274305717",
            "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
            "text": "Unified Visual Language Models Pioneering efforts have made significant strides by enabling multimodal understanding and generation within language models. In the realm of generating visual content using VLM, many works [18,19,25,50,72,74] have integrated VLMs with diffusion models [48] to achieve high-quality visual outputs. It is important to note that VLMs inherently lack the capability to directly produce visual content, and the quality of the generated images heavily relies on the performance of the diffusion models. For example, Emu [50] uses the output of the LLM as a condition for the pretrained diffusion model and then generates images with the diffusion model. Transfusion [74] combines the language modeling loss function with diffusion to train a single transformer. \n\nOther works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks. Janus [60] uses separate encoders for understanding and generation, but it increases the complexity of the model. VILA-U [61] combines contrastive and reconstruction loss to align visual and text tokens, but it has convergence problems, requiring a specific training recipe and large-scale image-text pairs from COYO-700M [5] dataset. \n\nIn this work, we explore a semantic-aware discrete encoding method for image reconstruction and generation. Our work reconstructs Siglip's visual features, which are well aligned with text, making the training process simpler and demonstrating outstanding performance in both visual understanding and generation tasks.",
            "score": 0.5257998490820142,
            "section_title": "Related Work",
            "char_start_offset": 6699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "271909855"
                },
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "271909855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "259765944",
            "title": "Generative Pretraining in Multimodality",
            "text": "The unified auto-regressive modeling of different modalities endows Emu with a powerful ability to serve as a multimodal generalist that can perform any types of completion in a multimodal sequence, i.e., accepting multimodal sequence as input, and outputting signals across vision and language modalities. For example, given two examples as the prompt, Emu automatically infers and completes the corresponding task given a new input, as shown in the second block of Figure 1. \n\nSpecifically, given a multimodal context, if the expected output format is text, Emu will use the language modeling head to generate discrete text tokens. If the desired output is image, we will append a [IMG] token at the end of the input sequence, then Emu will autoregressively generate N visual embeddings that will then be sent to the visual decoder for decoding into a real-world image.",
            "score": 0.5250123047538691,
            "section_title": "GENERALIST INTERFACE",
            "char_start_offset": 8407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 871
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "273641462",
            "title": "Multi-modal Auto-regressive Modeling via Visual Tokens",
            "text": "Previous work integrate images and text into a unified structure, enabling the powerful reasoning capabilities of the LLM to generalize from the text space to the multi-modal space. They essentially blur the modality differences in the encoding process of different modalities, as they all participate in the encoding of information in a consistent embedding form. \n\nTo further explore the connection between visual features and text embeddings, we search for the token id semantically closest to each image patch. For each feature   \u2208  image , its semantically closest token id   can be obtained as: \n\nin which   \u2208 [ 0 ,  1 , . . . ,   \u22121 ] is the pre-trained text embedding of LLM and Cosine(, ) is the cosine similarity of vector  and . \n\nWe show the semantically closest tokens of image regions in figure 3(c), it can be observed that for the LMM with multi-modal alignment training, there is a certain correlation between the information contained in each vector of the image features and the actual content of the image. Moreover, this correlation can be explicitly expressed by the vocabulary of the LMM. This finding, on the one hand, proves that the visual features within the LMM are in a similar semantic space as the text embedding, and on the other hand, it provides a new perspective for the interpretability analysis of the LMM. Furthermore, existing LLM methods have shown that the hidden states of text features can be mapped to the model's vocabulary through linear projection, enabling the extraction of interpretable semantics. Therefore, we propose using the linear projection to also map the visual features to the probability distribution over the model's vocabulary, which we called visual tokens, to further strengthen the correlation between visual features and text embeddings. \n\nIn this case, the classic language modeling loss can be represented as \n\nwhere  MM is the optimizable parameters of MM head,\u03a6 represents the multi-modal decoder. \n\nThe visual tokens of visual features can be represented as \n\nin which  image is the visual embeddings within  input and  VM is the optimizable parameters of VM head. To perform visual modeling on image information using a unified classification format, we design the  VM as follows: \n\nwhere set  VM is the set of visual output index of model.",
            "score": 0.5249934899637947,
            "section_title": "Visual Tokens",
            "char_start_offset": 7726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 1968
                },
                {
                    "start": 1971,
                    "end": 2029
                },
                {
                    "start": 2032,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2253
                },
                {
                    "start": 2256,
                    "end": 2313
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "268384865",
            "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
            "text": "\"Hyde Park\" User:\n\n(c)\n\nUser:\n\n(d)\n\nFig. 1: MM1 can perform in-context predictions thanks to its large-scale multimodal pre-training.This allows MM1 to (a) count objects and follow custom formatting, (b) refer to parts of the images and perform OCR, (c) demonstrate common-sense and word knowledge about everyday objects, and (d) perform basic math functions.Images are from the COCO 2014 validation set [72].\n\nGiven the above developments, an area of multimodal foundation models has emerged that marries the above advances into a single model achieving superior capabilities.In particular, Multimodal Large Language Models (MLLMs) are large-scale foundation models that consume image and text data and produce text [28, 67,79,110].After the rise of LLMs, MLLMs are emerging as the next frontier in foundation models.\n\nWhen it comes to transparency, existing MLLMs fall into two categories: closed models [1,106] and open models [3-5, 77, 90].In the former category, the models might be available for use, but little to nothing is known about the data, model architecture, and training details.In the latter category, the model parameters might be released together with a detailed description of data, model, and training configurations, thus allowing the community to build upon.However, most of the works, both open and closed, release close to nothing about the process they have undergone to arrive at their algorithmic design choices, especially regarding multimodal pre-training.\n\nTo further research in this area, we believe it is imperative to distill principles and lessons of how to build such models that might outlive concrete component implementations.Thus, in this paper, we document the MLLM building process and attempt to formulate design lessons, that we hope are of use to the community.\n\nIn particular, our contributions are as follows.First, we perform ablations at small scale across (1) model architecture decisions and (2) pre-training data choices.We identify several interesting trends.On the modeling side, we see that Fig. 2: MM1 can follow instructions and reason across images.",
            "score": 0.5249581912824036,
            "section_title": "MM1-30B (Ours):",
            "char_start_offset": 630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 19,
                    "end": 22
                },
                {
                    "start": 24,
                    "end": 29
                },
                {
                    "start": 31,
                    "end": 34
                },
                {
                    "start": 36,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 409
                },
                {
                    "start": 411,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 733
                },
                {
                    "start": 733,
                    "end": 818
                },
                {
                    "start": 820,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1282
                },
                {
                    "start": 1282,
                    "end": 1487
                },
                {
                    "start": 1489,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1808
                },
                {
                    "start": 1810,
                    "end": 1858
                },
                {
                    "start": 1858,
                    "end": 1975
                },
                {
                    "start": 1975,
                    "end": 2014
                },
                {
                    "start": 2014,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 728,
                    "end": 732,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.806640625
        },
        {
            "corpus_id": "272986599",
            "title": "Multimodal Pragmatic Jailbreak on Text-to-image Models",
            "text": "Text-to-image (T2I) models have shown unprecedented capabilities in synthesizing high-quality images that closely adhere to textual prompts [49,45,51]. However, recently the community has raised concerns regarding the potential for these models to generate unsafe content, i.e., images that might be offensive, disturbing, hateful, sexually explicit, or otherwise inappropriate [47,52,42]. Concurrently, a growing body of work focused on the jailbreak on Large Language Models (LLMs) [68,76,37,1] to generate harmful texts through bypassing existing safety protocols. Moreover, Multimodal Large Language Models (MLLMs) [34,24,75], have been shown to be more vulnerable to jailbreak and generate unsafe texts when integrating the visual modality [17,57,41]. \n\nIn this work, we introduce a novel form of jailbreak for T2I models, which we term multimodal pragmatic jailbreak, which triggers T2I models to generate the image with visual typographic texts on it, so-called visual text rendering [25], using prompts like \"an image of <image-generation prompt>, with a sign that says, '<visual text prompt>'\". In this scenario, the image and text content may each be considered safe in isolation, yet their combination can lead to unsafeness. This form of jailbreak is also inspired by pragmatics, which as defined in linguistics [5,18], is considered as the context-based interpretation of language use, where meanings derive not only from the linguistic system itself, but also from contextual factors, like background information, or moving beyond a focus on language (e.g. visual resources), a concept known as multimodal pragmatics [38]. In image generation with visual text by T2I models, the integration of visual and textual information can introduce complex interactions between modalities, such as allusion, exaggeration, sarcasm, and contrast, leading the resulting images to be unsafe, as demonstrated in Fig. 1. This underscores the need for careful consideration of both modalities in the generation process to ensure the safety of the outputs.",
            "score": 0.5248959689439214,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 382,
                    "end": 385,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "258841623"
                },
                {
                    "start": 491,
                    "end": 494,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 749,
                    "end": 752,
                    "matchedPaperCorpusId": "260203143"
                },
                {
                    "start": 752,
                    "end": 755,
                    "matchedPaperCorpusId": "259244034"
                },
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "254877579"
                },
                {
                    "start": 1324,
                    "end": 1327,
                    "matchedPaperCorpusId": "153411858"
                },
                {
                    "start": 1327,
                    "end": 1330,
                    "matchedPaperCorpusId": "11089401"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "274597680",
            "title": "A Self-Learning Multimodal Approach for Fake News Detection",
            "text": "Concurrently, with regard to the text content, the Byte-Pair Encoding algorithm is initially employed as a tokenizer for the purpose of transforming sentences into tokens. Subsequently, the multimodal model predicated on MiniGPT-4 amalgamates the text along with the image features that have been encoded during the pre-training phase. Given that the large language model (LLM) utilized in our paper has already undergone pre-training with an extensive volume of data in advance, the downstream task merely necessitates fine-tuning with a relatively small quantity of data to fulfill the specified task requirements. \n\nInstead of using the image features from contrastive learning module, we would like to leverage Q-Former [18], a core component of MiniGPT-4 [22], to bridge the relationship between image and text. It extracts features from a frozen image encoder and aligns them with a large language model. The key component is a learnable query which is a set of vectors Fig. 4. Q-Former structure adopted from [18] that are designed to interact with the input data in a way that helps extract relevant information and establish meaningful connections. The structure is illustrated in Fig. 4. Q-Former's alignment is divided into two phases, the first phase mainly involves the learning of image features, through which Q-Former learns the most relevant image feature representation to the current text by the learnable queries. The second stage is generative learning, which combines the output of Q-Former with a pre-trained frozen large language model to achieve visual-to-language generative learning. The LLM (Vicuna [6]) is used to understand and describe the visual expression features of the Q-Former output, thus building a relationship between visual information and linguistic description. \n\nIn the end of multimodal fusion module, text feature e text is obtained through the text embedding layer, image feature e img is obtained in a pre-trained image encoder, and function f that can be interpreted by the large language model are obtained by using Q-Former with approapriate prompt, and these features are connected together to obtain the final mixed feature representation E. The equations are shown in Equation 3 and 4.",
            "score": 0.5239091611171077,
            "section_title": "B. Multimodal Fusion Module",
            "char_start_offset": 13279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "207852737"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "277043335",
            "title": "Small Vision-Language Models: A Survey on Compact Architectures and Techniques",
            "text": "For the textual modality, the input text is tokenized into subword units, and the resulting embeddings are processed similarly through the transformer layers. The fusion of these modalities typically occurs in one of two ways: early fusion or late fusion. A key innovation in these models is the use of cross-attention mechanisms, where the textual features attend to the visual features and vice versa. This enables the model to focus on the most relevant parts of each modality when performing tasks such as visual question answering (VQA) or image captioning. By allowing the model to learn both intra-modal (within the same modality) and inter-modal (between modalities) relationships, transformers can effectively handle complex multimodal data. The transformer architecture in sVLMs provides a powerful and flexible framework for integrating visual and textual information. \n\nIts ability to model long-range dependencies, capture contextual relationships, and fuse multimodal data efficiently makes it an ideal choice for vision-language tasks. TinyGPT-V [103] introduces several key technical contributions, including an efficient architecture that integrates the compact Phi-2 [73] language model with pre-trained vision encoders, a novel training methodology focused on small pre-trained backbones, and resource-efficient training and inference capabilities. The model addresses challenges such as the closed-source nature and high computational demands of large MLLMs, aiming to achieve comparable performance to larger models while using significantly fewer resources. TinyGPT-V's architecture consists of a visual encoder using EVA (ViT) [25], projection layers including Q-Former layers and linear projections, and the Phi-2 language model as the backbone, incorporating various normalization techniques and LoRA [35] for efficient fine-tuning. The model is evaluated on benchmarks such as GQA Hudson [39], VSR [109], IconVQ [62], VizWiz [32], and Hateful Memes (HM) [44], using a diverse set of datasets across its training stages. The ablation study highlights the potential challenges in training smaller language models for multimodal tasks, which is one of the possible drawbacks.",
            "score": 0.5238933414462343,
            "section_title": "Transformer-Based Models",
            "char_start_offset": 16745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 1938,
                    "end": 1942,
                    "matchedPaperCorpusId": "239885946"
                },
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "150368646"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6845703125
        },
        {
            "corpus_id": "274981694",
            "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
            "text": "Visual Language Models (VLMs) have gained significant attention due to their capacity to bridge the gap between visual and textual modalities, facilitating more intuitive interactions between humans and machines. These models are valuable in tasks like visual question answering (VQA), which may involve yes/no answers, multiple-choice questions, or even generating image descriptions. With advances in deep learning, VLMs can now effectively handle visual scenes and provide meaningful textual outputs that explain or describe those scenes in natural language. \n\nRecent advancements in VLMs, such as CLIP [43], have enabled Large Language Models (LLMs) to process images and text simultaneously [3,6,45]. For example, Flamingo is a VLM that can handle both modalities and excels at tasks like image captioning and VQA [3]. Similarly, BLIP-2 connects a visual encoder to LLMs using a querying transformer, creating a more efficient multimodal model [29]. There are several VLM models, such as LLava [33], LocVLM [46], and LISA [27], that further improve model predictions through reasoning and prompting techniques. \n\nDespite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions. However, there is still a lack of exploration in the open-source community on building such speech interaction models based on foundation models. \n\nTo enable speech interaction with LLMs, speech instruction models such as Qwen2-Audio [14], SALMONN [50], and Llama-Omni [19] have been developed to process speech instead of text-based instructions. Although these models enable speech instruction, they are not capable of understanding both images and text simultaneously. Inspired by VLMs and ARS models, we propose SilVar, a multimodal model that can understand both images and audio or images and text, at the same time.",
            "score": 0.523569737443122,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 561
                },
                {
                    "start": 564,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2273
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "259991075"
                },
                {
                    "start": 819,
                    "end": 822,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "269043025"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "matchedPaperCorpusId": "260351258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "237417089",
            "title": "Multimodal Conditionality for Natural Language Generation",
            "text": "Language representation model BERT (Devlin et al., 2018) demonstrated that transformer models trained with masked language modeling and next sentence prediction objective can lead to state-of-the-art performance for a variety of NLU tasks. VilBERT (Lu et al., 2019) extended the approach to-wards multimodality with separate transformer streams for image and text with cross-modality interaction though coattention between the two streams. Other methods (Li et al., 2020;Su et al., 2019b;Chen et al., 2020;Li et al., 2019) showed that single stream transformer models can learn the relationship between image and text. These models are pretrained on vision and language data, however Kiela et al. (2019) proposed a different approach where a pretrained unimodal (text) BERT model is fine-tuned together with a different modality (image), skipping the multimodal pretraining step. These methods are effective for understanding tasks like classification, but have not been studied for multimodal conditional generative tasks.",
            "score": 0.5232336662290183,
            "section_title": "Transfer Learning in Multimodal Models",
            "char_start_offset": 3935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1023
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 471,
                    "matchedPaperCorpusId": "201058752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "271533594",
            "title": "GP-VLS: A general-purpose vision language model for surgery",
            "text": "VLMs are language models designed to process both visual and textual information. These models combine LLMs with vision processing to enable interactions between image and text data using a language backbone 15 . Typically, VLMs consist of a vision encoder f v (I) for processing images, a text encoder f t (T ) for handling language inputs, and a multimodal fusion component f m (v,t) to integrate information from both sources. The vision encoder often employs convolutional neural networks or transformer-based architectures to extract features, while the text encoder typically uses transformer",
            "score": 0.5232191499439512,
            "section_title": "Vision Language Models",
            "char_start_offset": 6187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 598
                }
            ],
            "ref_mentions": [
                {
                    "start": 208,
                    "end": 210,
                    "matchedPaperCorpusId": "257913547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47021484375
        },
        {
            "corpus_id": "254275172",
            "title": "M-VADER: A Model for Diffusion with Multimodal Context",
            "text": "We introduce M-VADER: a diffusion model (DM) for image generation where the output can be specified using arbitrary combinations of images and text. We show how M-VADER enables the generation of images specified using combinations of image and text, and combinations of multiple images. Previously, a number of successful DM image generation algorithms have been introduced that make it possible to specify the output image using a text prompt. Inspired by the success of those models, and led by the notion that language was already developed to describe the elements of visual contexts that humans find most important, we introduce an embedding model closely related to a vision-language model. Specifically, we introduce the embedding model S-MAGMA: a 13 billion parameter multimodal decoder combining components from an autoregressive vision-language model MAGMA and biases finetuned for semantic search.",
            "score": 0.5229367553268154,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71923828125
        },
        {
            "corpus_id": "270094821",
            "title": "Multi-Modal Generative Embedding Model",
            "text": "In recent years, the multi-modal learning field has witnessed a unifying trend [3,17,30,33,54].This trend is driven by the advanced understanding ability and more efficient computation brought by shared representations.Moreover, the simplification of the model structure makes it much more direct to perform various downstream tasks [30,40].\n\nMost cutting-edge multi-modal models can be categorized into two paradigms: embedding models and generative models.Embedding models [20,40,50] typically utilize a dual encoder structure.This framework projects distinct modalities into a unified latent space, thereby facilitating efficient cross-modal retrieval and classification tasks.Generative models [2,32,55] forge a connection between visual representations and Large Language Models (LLMs).This integration enables the models to harness capabilities such as instruction following [32,55] or in-context learning [2].These two paradigms intersect in the visual modality, i.e., the vision module of the generative model is usually derived from a powerful embedding model [40].However, the textual modality reveals a divergence in approach.While generative models commonly employ an auto-regressive text decoder, embedding models favor a text encoder to extract a global representation of the text.\n\nThe divergence in textual modality remains as a pivotal obstacle to achieving the goal of unification, namely, using only one model per modality.several works step in different directions towards this goal.BLIP [30] shares all parameters in the text encoder and decoder except for the self-attention layers.CoCa [52] splits the text decoder into unimodal and multimodal components, by removing the cross-attention module in the unimodal decoder layers.These methods differentiate the forward path of unimodal and multimodal, introducing a hindrance to the direct use of pre-trained text models.FROMAGe [24] truly achieves unification by grounding the image feature to the inputs and outputs of a frozen large language model.However, the lack of joint training with the visual modality results in a performance deficiency.",
            "score": 0.5228466067046919,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 341
                },
                {
                    "start": 343,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1295
                },
                {
                    "start": 1297,
                    "end": 1442
                },
                {
                    "start": 1442,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1749
                },
                {
                    "start": 1749,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "241035439"
                },
                {
                    "start": 82,
                    "end": 85,
                    "matchedPaperCorpusId": "239011704"
                },
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 88,
                    "end": 91,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 91,
                    "end": 94,
                    "matchedPaperCorpusId": "202734445"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 337,
                    "end": 340,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 475,
                    "end": 479,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 479,
                    "end": 482,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 698,
                    "end": 701,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 885,
                    "end": 888,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 912,
                    "end": 915,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "258947258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "257078916",
            "title": "Que2Engage: Embedding-based Retrieval for Relevant and Engaging Products at Facebook Marketplace",
            "text": "Besides encoding the contextual information, we use text encoder to convert the textual fields, i.e., product title and description, into a sequence of word tokens and feed them into the transformer to get the textual embedding. A special [CLS] token is used to encode the whole sentence representation. For the variable number of images attached to the document, we take the pre-trained image representations [1] for each of the attached images, apply a shared MLP layer and deep sets [19] fusion to get the image dense representation as image modality token. We borrow the transformer-fusion architecture used in [18], where we feed the concatenation of the text tokens, image token as well as context token to the multimodal fusion encoder. Our text encoder and multimodal fusion encoder are initialized from 6-layer XLM-R [3], an multilingual language model. As in [18], the text encoder inherits its first K layers and the multimodal fusion model inherits its remaining M layers. We extract the hidden output of the [CLS] token at the last layer of multimodal fusion encoder and project it to the desired dimension as the final document embedding.",
            "score": 0.5228322413162922,
            "section_title": "Multimodal fusion.",
            "char_start_offset": 4374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1152
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "219177697"
                },
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "4870287"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "246863399"
                },
                {
                    "start": 826,
                    "end": 829,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "246863399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "266521081",
            "title": "VCoder: Versatile Vision Encoders for Multimodal Large Language Models",
            "text": "Humans possess the remarkable skill of Visual Perception, the ability to see and understand the seen, helping them make sense of the visual world and, in turn, reason. Multimodal Large Language Models (MLLM) have recently achieved impressive performance on vision-language tasks ranging from visual question-answering and image captioning to visual reasoning and image generation. However, when prompted to identify or count (perceive) the entities in a given image, existing MLLM systems fail. Working towards developing an accurate MLLM system for perception and reasoning, we propose using Versatile vision encoders (VCoder) as perception eyes for Multimodal LLMs. We feed the VCoder with perception modalities such as segmentation or depth maps, improving the MLLM's perception abilities. Secondly, we leverage the images from COCO and outputs from off-the-shelf vision perception models to create our COCO Segmentation Text (COST) dataset for training and evaluating MLLMs on the object perception task. Thirdly, we introduce metrics to assess the object perception abilities in MLLMs on our COST dataset. Lastly, we provide extensive experimental evidence proving the VCoder's improved object-level perception skills over existing Multimodal LLMs, including GPT-4V. We open-source our dataset, code, and models to promote research.",
            "score": 0.5227198264342526,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424560546875
        },
        {
            "corpus_id": "271213320",
            "title": "ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report",
            "text": "To bridge this gap, the research and development of Large Vision Language Models (LVLMs) [9,2,10,15,14] have gained momentum. LVLMs are designed to perceive and understand both textual and visual information, thereby broadening the scope of applications for AI models. This integration allows for a more holistic understanding of content, enabling models to perform tasks that involve both text and images, such as image captioning, visual question answering, and visual grounding. \n\nLVLMs are built upon the foundation of LLMs, incorporating additional components that allow them to process and understand visual data. These components typically include visual receptors for image processing, input-output interfaces that enable the model to handle multimodal data, and training pipelines that are tailored to accommodate the complexities of learning from both text and images. Furthermore, these models are often trained on multilingual multimodal cleaned corpora, enhancing their ability to function across different languages and modalities. \n\nThe development of LVLMs represents a significant step forward in the field of AI, offering new possibilities for applications that require an understanding of the world that extends beyond text. By combining the capabilities of LLMs with the ability to process visual information, LVLMs are paving the way for more advanced and versatile AI systems. \n\nThese models have set new benchmarks in various visualcentric tasks, demonstrating their potential in enhancing the performance and applicability of AI across a broad range of domains.",
            "score": 0.522020029731476,
            "section_title": "Related Work",
            "char_start_offset": 4643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1585
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "207905405",
            "title": "Data Augmentation to Stabilize Image Caption Generation Models in Deep Learning",
            "text": "This section presents an overview on the research papers that are related to the image caption generation models. The authors of [3] adopts the retrieval based protocol in which image query is checked to get a sentence from a pool of reference sentences associated with the image. This technique can be used in any system that stores both, images and sentences. The main idea in that study is the retrieval of image-to-sentence. To establish the results, they use the Kernel Canonicals Correlation Analysis along with many linguistic and visual Kernels to map images and sentence into space where the similarity between them can be computed directly. They train the model using 6,000 images with real-world captions. For further enhancements in the results, they introduce an algorithm which is called Stacked Auxiliary Embedding that can transfer tens of thousands of annotated images every week to improve the accuracy of the retrieval-based Image caption. \n\nIn [5], they use the encoder-decoder pipeline to learn based on the work of generating the descriptions of the images using the Multimodal. A multimodal is based on the embedding space of the images and text and a novel language modal that matches distributed representation of the text and images. This multimodal consists of two trained models, which are as follows. \n\n\u2022 Neural Language Modal",
            "score": 0.5218973365256981,
            "section_title": "II. LITERATURE REVIEW",
            "char_start_offset": 3641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1355
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "2241593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "273346517",
            "title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
            "text": "\"True evaluation lies in the seamless interweaving of diverse modalities.\" \n\nMultimodal learning has made remarkable progress with the development of Large Vision-Language Models (LVLMs) (Liu et al., 2023a;Zhu et al., 2023;Dai et al., 2023), which are capable of handling diverse tasks that involve both images and text. Despite their advancements, most of these models are limited to multimodal tasks for text generation, such as visual question answering (VQA) and image captioning, which do not fully reflect the potential of multimodal capacity. To broaden their application, interleaved text-and-image generation has emerged as a critical area of research (Liu et al., 2024). It requires models to generate images and text in any sequence, thereby enhancing the versatility and effectiveness of multimodal systems. It opens up possibilities for various complex applications, such as multi-step inference (Lu et al., 2024;Kazemi et al., 2024), multimodal situational analysis (Yang et al., 2021), and visual storytelling (Huang et al., 2016). \n\nWhile recent LVLMs are evolving to support interleaved text-and-image generation (Team, 2024;Xie et al., 2024;Chern et al., 2024;Zhou et al., 2024), a comprehensive evaluation benchmark is still falling behind due to the following two challenges: Figure 1: Typical samples from the MMIE Benchmark showcase its support for multiple image inputs and outputs, with ground truth provided for every query. MMIE evaluates models across diverse fields, ensuring a comprehensive evaluation of their capabilities. \n\n\u2022 Difficulty in Constructing Modality-Coherent Benchmarks. The first challenge lies in the difficulty of constructing modality-aligned multimodal datasets, where both the input and output contain images and text.",
            "score": 0.5217708241029939,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 926,
                    "matchedPaperCorpusId": "264491155"
                },
                {
                    "start": 1025,
                    "end": 1045,
                    "matchedPaperCorpusId": "2574224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "272911245",
            "title": "MIO: A Foundation Model on Multimodal Tokens",
            "text": "For example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images. GPT-4 (OpenAI et al., 2023) can utilize either an image generator like DALL-E 3 (Betker et al., 2024) or a text-to-speech (TTS) tool like Whisper (Radford et al., 2022) to support multimodal generation.1 However, these methods are not end-to-end, relying on the text modality as an interface. \n\nRecently, the release of GPT-4o has demonstrated the capabilities of any-to-any and end-to-end foundation models. complex multimodal instruction-following, reasoning, planning, and other generalist capabilities. Furthermore, as the continuous scaling up of LLMs in the community depletes high-quality language tokens, GPT-4o verifies a new source of data for LLM training: multimodal tokens. This approach suggests that the next generation AGI could derive more knowledge from multimodal tokens when language tokens are exhausted. However, GPT-4o is closed source and focuses primarily on end-toend support for speech I/O, image I/O, 3D generation, and video understanding. Its recent open-source \"alternatives\", such as VITA (Fu et al., 2024), still lack the ability to generate data of all supported modalities, particularly for the generation of multimodal interleaved sequences. \n\nTo address the aforementioned issues, we introduce MIO (Multimodal Input and Output, or Multimodal Interleaved Output), the first open-source any-to-any foundation model that unifies multimodal understanding and generation across four modalities-text, image, speech (with voice), and video, while enabling the generation of multimodal interleaved sequences. Specifically, MIO is built on discrete multimodal tokens that capture both semantic representations through contrastive loss and low-level features via reconstruction loss (Ge et al., 2023a;Zhang et al., 2023b) from raw multimodal data.",
            "score": 0.5214686270984334,
            "section_title": "Models",
            "char_start_offset": 4895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 301,
                    "matchedPaperCorpusId": "252923993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "262084220",
            "title": "ContextRef: Evaluating Referenceless Metrics For Image Description Generation",
            "text": "Frozen One approach to permit a text-only language model to operate as a multimodal model with no additional multimodal fine-tuning is to use a frozen language model (e.g., GPT-2; Radford et al. 2019) and a multimodal embedding model (e.g., CLIP; Radford et al. 2021) to map images to linear combinations of token embeddings. For example, consider an image of a \"pluot\" that is represented in the multimodal model's embedding space as a linear combination of its embeddings for the words plum and apricot: i.e., encode_image(pluot_image) = \u03b1 * encode_text(plum) + \u03b2 * encode_text(apricot). Then, a new token would be created in the language model's vocabulary corresponding to the same linear combination of the language model embeddings for plum and apricot: new_token(pluot_image) = \u03b1 * embed_token(plum) + \u03b2 * embed_token(apricot). Then, the image can be passed into the language model as if it is a token. This combines ideas from Tsimpoukelli et al. (2021) and Norouzi et al. (2014) and was first introduced by dzryk (2023). \n\nBLIP The BLIP models that we consider (more precisely, BLIP-2 models; Li et al. 2023) use a ViT image encoder (Dosovitskiy et al., 2021), similar to the Flamingo models. Both OpenFlamingo and BLIP support a variety of Transformer-based autoregressive text encoders, some of which are instruction-tuned (including InstructBLIP, which is instruction-tuned to follow directions; Dai et al. 2023). Unlike the other models, they are trained with both a likelihood-based and similarity-based objective. We analyze both their likelihood-based and similarity-based metric outputs.",
            "score": 0.5214394593390594,
            "section_title": "LIKELIHOOD-BASED METRICS",
            "char_start_offset": 9387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 247,
                    "end": 267,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 935,
                    "end": 961,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 966,
                    "end": 987,
                    "matchedPaperCorpusId": "1926319"
                },
                {
                    "start": 1102,
                    "end": 1116,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "263333914",
            "title": "Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips",
            "text": "GPT4 [11] has sparked a research frenzy by highlighting the impressive capabilities of the model in not only processing text, but also the ability to understand and explain images, such as being able to generate a fully functioning website from a hand drawn picture of its design or explaining why a picture is funny. While the vision capabilities of GPT4 are still not accessible to the public, there have been various open-source efforts to develop capable models [15]. Multimodal large language models (MLLMs) are neural networks that have displayed impressive capabilities in processing both natural language and visual information. In other words, they facilitate having conversations about images. We conduct our experiments on the LLaVA model [6], which is a state-of-the-art MLLM, but our findings can be generalized to other similar models. We believe that an MLLM based approach is suitable for this task as it is a conversational interface that can not only generate a description but further allow users to interact with the comic strips, such as asking questions about their humor or meaning. \n\nTo the best of our knowledge, this is the first work that explores using MLLMs for the task of generating accessible text descriptions for newspaper comic strips. This is a challenging task because there are no clear rules for describing comic strips, there are few annotated datasets available for training on this domain, and comic strips have a lot of variety in their layouts, styles, and fonts, which make image processing difficult. Our main contributions are: \n\n1. We propose a novel application of MLLMs for generating comic descriptions that can benefit people with visual impairments. \n\n2. We identify the key elements of comic strips that are essential for generating good descriptions and propose a training-free, prompt-based approach that can improve the performance of MLLMs in this task. \n\n3. We evaluate our approach and provide insights into the strengths and weaknesses of our method.",
            "score": 0.5212656758103824,
            "section_title": "Multimodal large language model (MLLM)",
            "char_start_offset": 1382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1911
                },
                {
                    "start": 1914,
                    "end": 2011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "253510668",
            "title": "Few-shot Multimodal Sentiment Analysis Based on Multimodal Probabilistic Fusion Prompts",
            "text": "In the field of Natural Language Processing (NLP), prompt-based language modeling has emerged as a powerful approach for solving different few-shot tasks using pre-trained language models (PLM) [15]. Prompt-based methods treat the classification task as a masked language modeling (MLM) task, where the model is fine-tuned with a set of prompts to guide its predictions. In the beginning, promptbased approaches are introduced to handle text few-shot classification task, including LM-BFF [6], LM-SC [9], and so on. Ehsan et al. [7] propose a generative language model (GFSC) that reformulates the task as a language generation problem for text classification. However, the above-mentioned models only handle text-related tasks. Recently, there has been an increasing interest in designing models to handle few-shot multimodal tasks. Existing models for few-shot multimodal tasks, such as Frozen [22], PVLM [30], and UP-MPF [31], primarily rely on introducing image tokens to a pretrained language model for prompt-based fine-tuning. However, these approaches face the challenge of discrepancy between different modalities since image features are agnostic to language models. To this end, we propose a novel unified multimodal prompt that allows for the joint processing of both text and image modalities in a coherent manner.",
            "score": 0.5212317757648631,
            "section_title": "Few-shot Learning with PLM",
            "char_start_offset": 9750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1327
                }
            ],
            "ref_mentions": [
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "248506039"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "248119039"
                },
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "251848020"
                },
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "252782278"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "259287020",
            "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
            "text": "Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELICS, we train vision and language models of 9 and 80 billion parameters named IDEFICS, and obtain competitive performance on different multimodal benchmarks. We release our dataset, models and code.",
            "score": 0.5211709651779229,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13525390625
        },
        {
            "corpus_id": "271533621",
            "title": "Data Processing Techniques for Modern Multimodal Models",
            "text": "1) Multimodal Generative models: Multimodal models have seen significant development in recent years. While multimodal models often refer to those that incorporate more than one modality, vision-language models are the most extensively studied areas [4], [5]. In this work, we focus on data processing techniques for vision-language models, and some of these concepts can be also applied to models involving other modalities. \n\nThere are two major categories of vision-language models: discriminative models and generative models. Discriminative models typically use early fusion or late fusion architectures to learn meaningful representations for downstream tasks such as classification and ranking. Most early work centered on discriminative models, including ALBEF [6], ALIGN [7], and CLIP [8]. \n\nGenerative models, on the other hand, aim to generate text or image outputs. Generative vision-language models can be further divided into image generation models (e.g., diffusion models) [9] and text generation models (e.g., MLLMs) [10]. Diffusion models, for instance, utilize U-Net [11] and transformer architectures [12] for text-based image generation. MLLMs have emerged following the introduction of large language models (LLMs) [13]. With the success of LLMs, researchers have explored the possibility of incorporating visual information as tokens into LLM models to leverage the previously learned knowledge in LLMs. Influential works in this area include LLaVA [14] and miniGPT-4 [15]. \n\nIn this work, we will focus on the data processing techniques used in diffusion models and MLLMs, because these models represent the forefront of multimodal machine learning, with significant advancements and wide-ranging applications. While our mainly focus on these recent models, we also cover some early discriminative models to provide a comprehensive overview of data processing techniques in the field. \n\n2) Multimodal Dataset: Training datasets play a critical role in both model pretraining and finetuning. A typical dataset for vision-language models consists of pairs of images and text.",
            "score": 0.5207368398676077,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87158203125
        },
        {
            "corpus_id": "254685730",
            "title": "CLIPPO: Image-and-Language Understanding from Pixels Only",
            "text": "Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications.",
            "score": 0.5202155166175365,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "270319618",
            "title": "A survey on advancements in image-text multimodal models: From general techniques to biomedical implementations",
            "text": "Within those Transformer-based models, some mix image and text data right from the input stage, while others provide two separate encoders (or streams) for visual and textual information.Therefore, according to the model architecture, they can be divided into single-stream and dual-stream models.Common single-stream models include: VisualBERT [34] integrates visual information into the text sequence in the form of specific tokens directly on top of BERT.Unicoder-VL [35] is a universal vision-language pre-training model, which also employs a single transformer structure to handle visual and language inputs.VL-BERT [36] is similar to VisualBERT, also processing visual and textual information in a single-stream manner.Oscar [190] enhances the alignment of visual and textual information by using object tags as anchors.ALIGN [191], by pre-training on a large-scale dataset of images and noisy text, achieves outstanding performance on multiple downstream visionlanguage tasks.FILIP [192] aims to further strengthen the model's ability to correlate image and text information by performing fine-grained contrastive learning.Common dualstream models include: ViLBERT (Visual and Language BERT) [25], which uses a dual-stream structure to separately process visual and textual information and implements their interaction through a co-attention mechanism at the top layer.LXMERT [33], specifically designed for visual question answering tasks, uses a dual-stream architecture to encode images and text separately before interacting and merging them.ERNIE-ViL [193] incorporates knowledge graphs into the semantic interaction of images and text, adopting a dualstream structure.Pixel-BERT [194], unlike the original dualstream design of ViLBERT, aligns the features of images and text directly at the pixel level, but the dual-stream feature processing is its characteristic.",
            "score": 0.5201912284785354,
            "section_title": "The Introduction of Attention Mechanism and the Rise of Transformers",
            "char_start_offset": 12703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1681
                },
                {
                    "start": 1681,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "201058752"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 731,
                    "end": 736,
                    "matchedPaperCorpusId": "202734445"
                },
                {
                    "start": 832,
                    "end": 837,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "201103729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "277104480",
            "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM",
            "text": "Using Creation-MMBench, we evaluate various Multimodal Large Language Models (MLLMs), with a focus on image-based MLLMs that support multiple image inputs. Additionally, we adapted our benchmark into a text-only version (Creation-MMBench-TO) by replacing the visual inputs with corresponding textual descriptions and tested multiple Large Language Models (LLMs) to gain deeper insights into their creative capabilities. All evaluations were conducted based on VLMEvalKit [5], employing greedy decoding during inference with the maximum output tokens set to 4096.",
            "score": 0.5201256591068673,
            "section_title": "Experiment",
            "char_start_offset": 16389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 562
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "271218736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416259765625
        },
        {
            "corpus_id": "271946807",
            "title": "Foundational Model for Electron Micrograph Analysis: Instruction-Tuning Small-Scale Language-and-Vision Assistant for Enterprise Adoption",
            "text": "Our novel encoder-decoder multimodal framework efficiently processes and aligns images and text to generate textual responses to questions across image captioning and open-ended VQA tasks. Key components of MAEMI for the zero-shot image captioning task are illustrated in Figure 2. The multimodal model, MAEMI, integrates visual processing and language modeling for answering questions about specific image features. It includes: (a) The vision encoder, using a vision transformer (Dosovitskiy et al., 2020), analyzes the microscopic images by splitting them into patches and using self-attention mechanism to capture beyond pair-wise patch relationships. This allows for understanding the global context and highlighting relevant visual regions and relationships. A <cls> token attends to and aggregates information from all patches, resulting in a higher-level visual semantic representation to capture the overall context or summary of the input image. (b)The text encoder, crucial for analyzing end-user questions, takes as input an interleaved multimodal prompt. We insert <image> token in the prompt at the image location and append an <Encode> token to facilitate multimodal integration, with its output embedding symbolizing the fused representation. The text encoder leverages instruction-tuned Llama-2-7b, a pretrained language model, to capture language nuances and context. The language-only model is customized using parameter-efficient fine tuning technique, enhancing its ability to interpret end-user questions. Both the vision and language-only unimodal encoders synergize to interpret end-user questions and analyze visual input for generating answers consistent with the visual context. (c) It utilizes a multi-layered structure with multiple blocks, alternating between self-attention and gated cross-attention blocks. This design facilitates complex interactions between visual and textual modalities. By extracting and refining information from both modalities at each level, the framework progressively builds a comprehensive understanding, enabling coherent and contextually relevant answers to the end-user questions. Gated cross-attention blocks integrate visual features with textual features.",
            "score": 0.5199079625997123,
            "section_title": "Introduction",
            "char_start_offset": 8846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378662109375
        },
        {
            "corpus_id": "270703735",
            "title": "MR-MLLM: Mutual Reinforcement of Multimodal Comprehension and Vision Perception",
            "text": "The proposed Mutually Reinforced Multimodal Large Language Model (MR-MLLM) framework, shown in Figure 4, represents a groundbreaking approach to multimodal learning.It integrates the strengths of visual perception and language comprehension, pioneering a shared query mechanism that synergistically enhances both tasks of multimodal comprehension and vision perception.This framework leverages the rich perceptual information from vision perception models and the contextual insights of large language models, harmonizing them to elevate multimodal comprehension.\n\nCentral to MR-MLLM is the dual-stream architecture, composed of a visual perception stream and a linguistic processing stream.The visual stream, powered by pre-trained visual encoders, f V , transforms raw images I into visual feature spaces V f :\n\nIn parallel, the linguistic stream, realized through a Large Language Model, f L , processes textual input T into rich contextual embeddings L e , adept at capturing the nuanced interdependencies within the text:\n\nWe introduce a novel shared query structure, Q shared , which is formulated by integrating the outputs from two pre-trained visual encoders, which are perception forward and visual forward.This integration, achieved through a sophisticated fusion function F, leverages multi-knowledge interaction to enhance mutual understanding:\n\nFurthermore, MR-MLLM innovatively incorporates perception outputs as a new modality, enriching multimodal interactions.This is realized by embedding these perception outputs, P o , into the existing multimodal framework G:\n\nAdditionally, MR-MLLM enhances traditional prompt generation by integrating perception-enhanced prompts into the Large Language Model.This integration leads to a more informed and contextually aware language model, significantly enhancing its response generation capabilities.The enhanced prompt generation manner is represented as E prompt , which refines the model's ability to generate relevant and contextually rich responses.This is mathematically articulated as:\n\nIn this equation, P represents the function responsible for encoding the enhanced prompts, P o denotes the perception outputs integrated into the prompt, and T orig is the original textual prompt.",
            "score": 0.5197906844467949,
            "section_title": "Overall pipeline of MR-MLLM",
            "char_start_offset": 10749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 563
                },
                {
                    "start": 565,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 812
                },
                {
                    "start": 814,
                    "end": 1026
                },
                {
                    "start": 1028,
                    "end": 1217
                },
                {
                    "start": 1217,
                    "end": 1357
                },
                {
                    "start": 1359,
                    "end": 1478
                },
                {
                    "start": 1478,
                    "end": 1581
                },
                {
                    "start": 1583,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1859
                },
                {
                    "start": 1859,
                    "end": 2013
                },
                {
                    "start": 2013,
                    "end": 2051
                },
                {
                    "start": 2053,
                    "end": 2249
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62646484375
        },
        {
            "corpus_id": "253510668",
            "title": "Few-shot Multimodal Sentiment Analysis Based on Multimodal Probabilistic Fusion Prompts",
            "text": "We compare our model with three groups of baselines 6 . The first group consists of previous text-based models, including RoBERTa [16], Prompt Tuning (PT) only uses a single textual prompt based on the multimodal prompt, such as [<s> [T] It was <mask>. </s>] for coarse-grained datasets and [<s> [T] [A] It was <mask>. </s>] for fine-grained datasets, LM-BFF [6] utilizes generated text prompts based on each specific dataset and text demonstrations to solve few-shot text classification tasks, LM-SC [9] introduces supervised contrastive learning based on LM-BFF to few-shot text tasks, and GFSC [7] converts the classification task into a generation task to solve text classification tasks in the few-shot setting through the pre-trained generation model, i.e., GPT2 [20]. \n\nThe second group consists of multimodal approaches that are trained in full MSA datasets from published papers. For the coarsegrained MSA task: Multimodal Fine Tuning (MFN) is a baseline that doesn't use any designed prompts and employs the representation of the \"<s>\" token for classification. CLMLF [13] is the state-ofthe-art model for coarse-grained MSA. For the fine-grained MSA task: TomBERT [29] is a multimodal BERT for the fine-grained MSA task. EF-CapTrBERT [12] translates images in input space to construct an auxiliary sentence that provides multimodal information to BERT. KEF [14] exploits adjective-noun pairs extracted from the image for the fine-grained MSA task. FITE [24] is the state-of-the-art model for fine-grained MSA, which leverages facial information from the image modality. VLP-MABSA [14] designs  The last group includes multimodal approaches that have been trained for few-shot MSA. PVLM [30] directly introduces image features to pre-trained language models to solve the MAS task in a few-shot scenario.",
            "score": 0.5197327525949534,
            "section_title": "Baselines",
            "char_start_offset": 19952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1813
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "248506039"
                },
                {
                    "start": 597,
                    "end": 600,
                    "matchedPaperCorpusId": "248119039"
                },
                {
                    "start": 1175,
                    "end": 1179,
                    "matchedPaperCorpusId": "199465957"
                },
                {
                    "start": 1245,
                    "end": 1249,
                    "matchedPaperCorpusId": "236912847"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "248227577"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "256461316"
                },
                {
                    "start": 1591,
                    "end": 1595,
                    "matchedPaperCorpusId": "248227577"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76416015625
        },
        {
            "corpus_id": "259164940",
            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
            "text": "Our proposed dataset and task introduce two key innovations compared to previous methods. Firstly, our multimodal dataset comprises image-text inputs and outputs, which facilitates the development of more effective multimodal generative models. Secondly, our datasets incorporate predefined rules that teach the model to reject certain human instructions, even if they are technically feasible. This gives rise to the concept of accountable text-based visual re-creation (ATVC) task, which can be defined as follows: \n\nGiven a visual input (V) and a text-based query (T), the multimodal generative model is expected to produce a re-created image (M) and provide a language-based explanation (A) for its decisions. A successful ATVC model should possess the ability to comprehend the user's query and employ language cues to reason about the visual input, while also generating appropriate textual feedback and re-created visual results.",
            "score": 0.5196799715572054,
            "section_title": "Definition of ATVC",
            "char_start_offset": 2711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 936
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "253258526",
            "title": "Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content Dilutions",
            "text": "Multimodal classifier: We design a fusion-based multimodal classification model (M mm ) following widely adopted architectures in both academic research and industrial applications (Agarwal et al., 2020;Dataminr, 2020). M mm takes the concatenation of modality-specific representations as input and makes a joint classification. To model individual modalities, we first train an image-only classifier M image and a text-only classifier M text for the same classification task. We then concatenate the output of the penultimate layers of the modalityspecific models to feed them into a fully-connected network that is trained to fuse the modality-specific representations to perform joint classification based on the multimodal input. Keyword extraction: Our dilution generation approach is centered around keywords in the original image and text as that will ensure semantic relatedness of the dilution text with both the associated modalities. We use Yet Another Keyword Extractor (YAKE) (Campos et al., 2018) to extract the most important keywords from the textual description for each example. For extracting keywords from the image, we consider the top 150 objects in the Visual Genome dataset (Krishna et al., 2017) and identify these objects in our dataset using a pre-trained image to Scene Graph generator (Tang et al., 2020). We further filtered the list of all identified objects by only considering objects with a bounding box that occupies at least 10% of the total image area to ensure prominence in the image. These objects are considered the keywords of the image. We denote the keywords from text and image as K text and K image , respectively. Constrained text generation: Once we have the keywords from text and image for each of the examples, the goal is to generate dilution around these keywords. For this, we extend the constrained text generation approach proposed by Zhang et al. (2020). We fine-tune a BERT language model to progressively predict [MASK] tokens around the initial set of keywords until only a special token (i.e., no-insertion token [NOI]) is predicted at all places to indicate no further insertions. We consider the original descriptions of the training examples in our target dataset and fine-tune the pretrained",
            "score": 0.5193180695307015,
            "section_title": "Method for Generating Dilutions",
            "char_start_offset": 10002,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1198,
                    "end": 1220,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 1891,
                    "end": 1910,
                    "matchedPaperCorpusId": "226604173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "258865230",
            "title": "Visual Programming for Text-to-Image Generation and Evaluation",
            "text": "Text-to-image generation models. In the T2I generation task, models generate images from text. Early deep learning models used the Generative Adversarial Networks (GAN) [38] framework for this task [39; 40; 41; 31]. More recently, multimodal language models [42; 22] and diffusion models [43; 44; 21; 45] have gained popularity. Recent advances in multimodal language models such as Parti [46] and MUSE [47], and diffusion models like Stable Diffusion [21], UnCLIP [23], and Imagen [24], have demonstrated a high level of photorealism in zero-shot image generation. \n\nBridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions. \n\nLanguage models with visual modules. Although large language models (LLMs) have shown a broad range of commonsense knowledge, most of them are trained only on text corpus and cannot understand image inputs to tackle vision-and-language (VL) tasks. Thus, recent works explore tackling VL tasks by solving sub-tasks with external visual modules and combining their outputs to obtain the final response [9; 10; 11; 12; 13; 14; 15]. The visual sub-tasks include describing images as text, finding image regions relevant to the text, editing images with text guidance, and obtaining answers from a VQA model. However, existing work focuses on converting visual inputs into text format so that LLMs can understand them. Our work is the first work using visual programming for interpretable and explainable T2I generation and evaluation. \n\nEvaluation of text-to-image generation models.",
            "score": 0.5191247090590297,
            "section_title": "Related Works",
            "char_start_offset": 6589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "269983697",
            "title": "From Text to Pixel: Advancing Long-Context Understanding in MLLMs",
            "text": "The rapid progress in Multimodal Large Language Models (MLLMs) has significantly advanced their ability to process and understand complex visual and textual information. However, the integration of multiple images and extensive textual contexts remains a challenge due to the inherent limitation of the models' capacity to handle long input sequences efficiently. In this paper, we introduce SEEKER, a multimodal large language model designed to tackle this issue. SEEKER aims to optimize the compact encoding of long text by compressing the text sequence into the visual pixel space via images, enabling the model to handle long text within a fixed token-length budget efficiently. Our empirical experiments on six long-context multimodal tasks demonstrate that SEEKER can leverage fewer image tokens to convey the same amount of textual information compared with the OCR-based approach, and is more efficient in understanding long-form multimodal input and generating long-form textual output, outperforming all existing proprietary and open-source MLLMs by large margins.",
            "score": 0.5185045445755023,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "Multimodal understanding models refer to LLM-based architectures capable of receiving, reasoning over, and gener- ating outputs from multimodal inputs [47]. These models extend the generative and reasoning capabilities of LLMs beyond textual data, enabling rich semantic understanding across diverse information modalities [42], [48]. Most efforts of existing methods focus on vision-language understanding (VLU), which integrates both visual (e.g., images and videos) and textual inputs to support a more comprehensive understanding of spatial relationships, objects, scenes, and abstract concepts [49], [50], [51]. A typical architecture of multimodal understanding models is illustrated in Fig. 2. These models operate within a hybrid input space, where textual data are represented discretely, while visual signals are encoded as continuous representations [52]. Similar to traditional LLMs, their outputs are generated as discrete tokens derived from internal representations, using classification-based language modeling and task-specific decoding strategies [8], [53]. \n\nEarly VLU models primarily focused on aligning visual and textual modalities using dual-encoder architectures, wherein images and text are first encoded separately and then jointly reasoned over via aligned latent representations, including CLIP [22], ViLBERT [54], VisualBERT [55], and UNITER [56]. Although these pioneering models established key principles for multimodal reasoning, they depended heavily on region-based visual preprocessing and separate encoders, limiting the scalability and generality of the mode. With the emergence of powerful LLMs, VLU models have progressively shifted toward decoder-only architectures that incorporate frozen or minimally fine-tuned LLM backbones. These methods primarily transform image embeddings through a connector with different structures, as illustrated in Fig. 2. Specifically, MiniGPT-4 [57] utilized a single learnable layer to project CLIP-derived image embeddings into the token space of Vicuna [58].",
            "score": 0.5184101786560351,
            "section_title": "Multimodal Understanding Model",
            "char_start_offset": 4530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 155,
                    "matchedPaperCorpusId": "259243718"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "271750640"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "268249175"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1324,
                    "end": 1328,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1372,
                    "end": 1376,
                    "matchedPaperCorpusId": "216080982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.841796875
        },
        {
            "corpus_id": "270357697",
            "title": "Leveraging Generative AI for Extracting Process Models from Multimodal Documents",
            "text": "Additionally, we leverage multimodal LLMs, diverging from the methods highlighted in the preceding papers.This approach offers greater versatility since an LLM can perform multiple tasks using fundamental reasoning skills without being exclusively trained on a specific type of image or text.\n\nTo the best of our knowledge, no previous work has investigated the capabilities of multimodal generative models to auto-generate graphical process models from intersected text-and image-based inputs in detail.The paper aims to contribute towards closing this gap.",
            "score": 0.5183838901379794,
            "section_title": "Background & Related Work",
            "char_start_offset": 3753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 292
                },
                {
                    "start": 294,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 558
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "249062795",
            "title": "Multimodal Knowledge Alignment with Reinforcement Learning",
            "text": "ESPER consists of three components: 1) CLIP's non-generative image/text encoders (Radford et al., 2021); \n\n1 2) GPT-2 (Radford et al., 2019), a left-toright language generator; and 3) an encoder that projects multimodal inputs into the word embedding space of GPT-2. Encoder. The encoder F \u03c6 is the only module with trainable parameters in ESPER. Given the vector representation of an image x i extracted using CLIP, the module outputs a series of vectors of length k to be passed on to the language model. The output image representations h i work as the multimodal prompt and are concatenated to the embedded word representations. We fix the visual token length in all experiments to k = 10. \n\nFor fair comparison in later experiments, we use the same multimodal encoder architecture as CLIP-Cap (Mokady et al., 2021): a lightweight, twolayer Multi-Layer Perceptron (MLP). The first layer maps the CLIP encoding dimensions to GPT-2's dimensions and the second layer expands the single vector representation to a series of vector representations of length k. We use tanh as the nonlinear activation function between these two layers. By employing a less expressive encoder architecture (than, e.g., a transformer), we aim to demonstrate that the contribution of ESPER does not rely on the structure/capacity of the encoder itself. \n\nPretrained Language Model. ESPER employs a pretrained deep autoregressive language model such as GPT-2 (Radford et al., 2019) as the backbone. Autoregressive language models parameterize likelihood of a text sequence y comprised of text tokens y j with length l using autoregressive decomposition. \n\nInspired by prompt tuning in the text-only domain (Liu et al., 2021b), we treat the encoded image vector sequence h i as a multimodal prompt and concatenate it with the text prompt representation output by GPT-2's embedding lookup layer given previous tokens y i j \u2032 <j to build the prefix for the conditioned text generation:",
            "score": 0.5182445449130195,
            "section_title": "Method",
            "char_start_offset": 4291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 103,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 118,
                    "end": 140,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1437,
                    "end": 1459,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "260125105",
            "title": "An intelligent remote sensing image quality inspection system",
            "text": "In addition to traditional fixed-label image classification, the recently proposed image-text multimodal models can also be used for image classification. Although their classification accuracy may not be as high as dedicated classification models for specific tasks, they provide more comprehensive information and have greater versatility. They hold the potential for \"one model for all tasks\" and may be capable of addressing all image-related problems in remote sensing. Therefore, the application of multimodal models in the field of remote sensing is promising. \n\nThe structure of multimodal models is similar to generative language models, but with the addition of an image encoder that embeds the image as a vector alongside the textual prompt. The output of the model is identical to that of a language model. The training methodology for multimodal models is largely similar to language models and will not be elaborated upon in this paper. \n\nA comparative experiment is conducted between BLIP [15], which possesses the ability to convert images into text, and VisualGLM [16], which supports long conversations. We do not need to make any structural changes to the pretrained models. For BLIP [15], a Chinese pre-trained version is utilized, and the dataset mentioned in Appendix A.2., consisting of images and their corresponding textual descriptions, is used for fine-tuning all parameters. As for the VisualGLM [16], due to its large size, a lora [24] training approach is employed. Only the layers with even numbers in the ChatGLM [16] part of Visu-alGLM [16] underwent lora training, while no modifications are made to other parts such as the image encoder. Subsequently, the Q&A dataset mentioned in Appendix A. 4. is used to train the model's ability to provide detailed descriptions of distortions in remote sensing images.",
            "score": 0.5179084683690326,
            "section_title": "Exploration of Multimodal Models",
            "char_start_offset": 26900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1728,
                    "end": 1729,
                    "matchedPaperCorpusId": "14542261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7666015625
        },
        {
            "corpus_id": "271039870",
            "title": "HEMM: Holistic Evaluation of Multimodal Foundation Models",
            "text": "Multimodal machine learning brings unique challenges for ML research due to the heterogeneity between modalities and the interconnections found between them [69].It has inspired many theoretical studies in data heterogeneity and interactions [25], as well as diverse applications in multimedia [44,14,88], affective computing [86], robotics [47], finance [39], HCI [25,82], education [12] and healthcare [80,110].\n\nEvaluation frameworks for multimodal models have significantly shaped the multimodal research landscape, through holistic [57,66] and domain-specific benchmarks [31,28].Recent benchmarks have focused on testing the capabilities of multimodal foundation models, such as MME [29], MMBench [73], LVLM-ehub [111], SEED-Bench [59], Touchstone [7], Mm-vet [120], ReForm-Eval [65], VisIT-Bench [11], FLAVA [45].Other benchmarks focus on evaluating hallucination [21] and applications in medicine [113] and autonomous driving [107].These benchmarks contain many tasks, but without the systematic taxonomy and comprehensiveness that HEMM provides.\n\nMultimodal foundation models are promising foundations for the future of AI, with impressive reasoning [75], interactive dialogue [49], and few-shot generalization abilities [100].These models can be pre-trained (typically with image-text self-supervised learning) and fine-tuned for downstream tasks [63,74,91,67], or based on adapting language models with vision to enable text generation conditioned on images [61,105].Cross-modal transformer architectures have emerged as a popular backbone due to their suitability for both language and image data [17,99].Additionally, composable diffusion models [96] can be used to further generate combinations of output modalities.\n\nAdapting language models for multimodality is another promising approach where frozen models are aligned on both vision and language to generate text from multimodal inputs [127,62,118,109].",
            "score": 0.5178214928313215,
            "section_title": "Related Work",
            "char_start_offset": 18335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 413
                },
                {
                    "start": 415,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1053
                },
                {
                    "start": 1055,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1729
                },
                {
                    "start": 1731,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "16540884"
                },
                {
                    "start": 294,
                    "end": 298,
                    "matchedPaperCorpusId": "244954623"
                },
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "249375461"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "680978"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "156051062"
                },
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "149720066"
                },
                {
                    "start": 365,
                    "end": 369,
                    "matchedPaperCorpusId": "16540884"
                },
                {
                    "start": 369,
                    "end": 372,
                    "matchedPaperCorpusId": "14879124"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "13858994"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "53110712"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "235899386"
                },
                {
                    "start": 576,
                    "end": 580,
                    "matchedPaperCorpusId": "224960953"
                },
                {
                    "start": 580,
                    "end": 583,
                    "matchedPaperCorpusId": "2044324"
                },
                {
                    "start": 814,
                    "end": 818,
                    "matchedPaperCorpusId": "252782958"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 1229,
                    "end": 1234,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 1360,
                    "end": 1363,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1366,
                    "end": 1369,
                    "matchedPaperCorpusId": "259275338"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1612,
                    "end": 1615,
                    "matchedPaperCorpusId": "173990158"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "270440191",
            "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models",
            "text": "A.1 Visual-Language Models (VLMs) \n\nHumans possess the remarkable ability to seamlessly integrate information from various sources concurrently. For instance, in conversations, we adeptly interpret verbal cues, body language, facial expressions, and intonation. Similarly, VLMs demonstrate proficiency in processing such multimodal signals, allowing machines to comprehend and generate image-related content that seamlessly merges visual and textual components. Contemporary VLM architectures such as CLIP [Radford et al., 2021] predominantly leverage transformer-based models [Vaswani et al., 2023, Dosovitskiy et al., 2021] for processing both images and text due to their effectiveness in capturing long-range dependencies. At the heart of the transformers lies the multi-head attention mechanism, which plays a pivotal role in these models' functionality. \n\nTo enable multimodal comprehension, VLMs typically comprise three key components: (i) an Image Model responsible for extracting meaningful visual features from visual data, (ii) a Text Model designed to process natural language, and (iii) a Fusion Mechanism to integrate representations from both modalities. Encoders in VLMs can be categorized based on their fusion mechanisms into Fusion encoders [Li et al., 2020, 2021b, 2019b, Su et al., 2019], which directly combine image and text embeddings, Dual encoders [Radford et al., 2021, Li et al., 2022, 2023b, Jia et al., 2021], which process modalities separately before interaction, and Hybrid methods [Singh et al., 2021, Bao et al., 2022] that leverage both approaches. Furthermore, fusion schemes for cross-modal interaction can be classified into single-stream [Li et al., 2020, 2019b, Su et al., 2019, Bao et al., 2022, Singh et al., 2021] and dual-stream [Li et al., 2021b] architectures.",
            "score": 0.5177669123569697,
            "section_title": "A Related Work",
            "char_start_offset": 31083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 36,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 528,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1261,
                    "end": 1277,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 1277,
                    "end": 1284,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1516,
                    "end": 1535,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1535,
                    "end": 1554,
                    "matchedPaperCorpusId": "241035439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "259287020",
            "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
            "text": "The recent most performant vision and language models are trained on large sets of multimodal web documents. For instance, Flamingo (Alayrac et al., 2022), an 80 billion multimodal model, was trained on a mix of 2.1 billion image-text pairs, 27 million video-text pairs, and 43 million multimodal web documents. The latter called M3W, includes 185 million images. Similarly, KOSMOS-1 (Huang et al., 2023) was trained on a mixture containing 71 million multimodal web documents. However, in both cases, the dataset is not publicly available, and little information is accessible as to the dataset's content, the strategies employed to create that dataset (including filtering strategies), and the quality of the resulting web documents, which ultimately hinders further research.\n\nConcurrently to our work, the Multimodal C4 (mmc4) dataset  was recently made accessible. It consists of 103 million multimodal web documents that include 585 million images. Although there are similarities between our datasets, it is important to highlight particular distinctions. First, our dataset is based on more recent documents from February 2020 to February 2023, whereas mmc4 uses documents from April 2019. Additionally, our filtering heuristics appear to be more comprehensive: we leverage the HTML DOM trees to filter out undesirable texts and images, whereas mmc4 uses the HTML to find images in order to merge them with the original C4 dataset by solving a bipartite assignment problem based on a CLIP model similarities. Last, we implement additional deduplication steps at the image, document, and paragraph levels.",
            "score": 0.5176775874078139,
            "section_title": "Multimodal web document datasets",
            "char_start_offset": 4926,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 154,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "269005642",
            "title": "GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling",
            "text": "We consider a generative vision-language model (GVLM) G that can reason over the text and visual input and generate an output text.Here, the GVLM is capable of capturing the fine-grained details in the event task description with image(s) in its context.We illustrate this stage in Figure 2 (left).\n\nFor the input instance Q i we prompt the GVLM with a multimodal prompt P = T (Q i ) where T is a predefined template that converts the input query details into a multimodal prompt (Appendix Table 8).We highlight the example multimodal input prompt in Figure 2 (left).Subsequently, we generate the event-centric object role description d o i from the GLVM with the context P.",
            "score": 0.5176416478206358,
            "section_title": "Generative Vision-Language Model",
            "char_start_offset": 8408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 298
                },
                {
                    "start": 300,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64208984375
        },
        {
            "corpus_id": "270458639",
            "title": "Unveiling the evolution of generative AI (GAI): a comprehensive and investigative analysis toward LLM models (2021\u20132024) and beyond",
            "text": "In the realm of machine learning (ML), the focus has historically been on models dealing with a single modality of data, such as language models, image classifiers, or speech recognition models.However, the future of ML holds even more promise with the emergence of multimodal models capable of handling diverse modalities simultaneously.\n\nRather than relying on individual models tailored to specific tasks or domains, the next generation of multimodal models aims to activate only the relevant model pathways for a given problem, allowing flexible handling of different modalities both as inputs and outputs.Building effective multimodal models involves addressing two key questions: how much modality-specific processing should be done before merging learned representations, and what is the most effective way to mix these representations.\n\nRecent work on \"Multi-modal bottleneck transformers\" and \"Attention bottlenecks for multimodal fusion\" explores these trade-offs, revealing that merging modalities after a few layers of modality-specific processing and then mixing features through a bottleneck layer is particularly effective.This approach significantly improves accuracy in various video classification tasks by leveraging multiple modalities for decision-making.\n\nCombining modalities not only enhances performance on multimodal tasks but can also improve accuracy on single-modality tasks.Notable examples include the DeViSE framework, which improves image classification accuracy by combining image and word-embedding representations, and Locked-image Tuning (LiT), a method that adds language understanding to pretrained image models, demonstrating substantial improvements in zero-shot image classification.\n\nThe utility of multimodal models extends to co-training on related modalities, such as images and videos, leading to enhanced accuracy on video action classification tasks.Additionally, the integration of language with other modalities, like vision, opens avenues for more natural human-computer interactions.\n\nThe \"PaLI: scaling language-image learning\" model, for instance, combines vision transformers with text-based transformers to achieve state-of-the-art results across various language-related tasks and benchmarks.\n\nIn applications like FindIt, a unified model for visual grounding, natural language questions about visual images can be answered through a general-purpose visual grounding model.This flexibility enables the model to handle different types of queries related to grounding and detection.",
            "score": 0.5174961626900701,
            "section_title": "Multimodal models: a case study analysis",
            "char_start_offset": 28715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 338
                },
                {
                    "start": 340,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 843
                },
                {
                    "start": 845,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1276
                },
                {
                    "start": 1278,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1725
                },
                {
                    "start": 1727,
                    "end": 1899
                },
                {
                    "start": 1899,
                    "end": 2036
                },
                {
                    "start": 2038,
                    "end": 2250
                },
                {
                    "start": 2252,
                    "end": 2431
                },
                {
                    "start": 2431,
                    "end": 2538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479736328125
        },
        {
            "corpus_id": "272368331",
            "title": "CV-Probes: Studying the interplay of lexical and world knowledge in visually grounded verb understanding",
            "text": "BLIP (Li et al., 2022) employs a visual transformer and BERT, integrating these through a Multimodal Mixture of Encoder-Decoder (MED) architecture. The MED operates in three modes: an unimodal encoder for separate image and text encoding, an image-grounded text encoder that incorporates visual information into text encoding via a cross-attention layer, and an image-grounded text decoder with causal self-attention layers for generating textual descriptions from images. Pretraining involves three objectives: contrastive learning, image-text matching, and language modeling. The CapFilt method enhances training data quality by generating and filtering synthetic captions to reduce noise in web-sourced image-text pairs. BLIP is pre-trained on MS-COCO, Visual Genome, Conceptual Captions, Conceptual 12M, and SBU Captions. Hard negative mining is performed, similarly to BLIP. \n\nFLAVA (Singh et al., 2022) uses a unified framework for processing unimodal and multimodal data. The architecture comprises a visual and textual transformer integrated through a shared multimodal encoder. As input to the multimodal encoder, the model uses learned linear projections of the hidden state vectors from the two modalities, concatenating these with an additional [CLS M] token. FLAVA's pre-training involves multiple objectives: unimodal objectives like masked image modeling and masked language modeling, and multimodal objectives, including image-text matching, contrastive learning, and masked multimodal modeling. The model is pre-trained on datasets like MS-COCO, SBU Captions, Localized Narratives, Wikipedia Image Text Visual Genome, ImageNet, Conceptual Captions, Conceptual Captions 12M, Red Caps, and YFCC100M. The model is intended for unimodal (vision-only, text-only) and multimodal tasks. \n\nBLIP2 (Li et al., 2023) This model introduces a Querying Transformer (Q-Former) to bridge the gap between a frozen image encoder and a frozen large language model (LLM).",
            "score": 0.5169788468008126,
            "section_title": "Models",
            "char_start_offset": 14120,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 5,
                    "end": 22,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 888,
                    "end": 908,
                    "matchedPaperCorpusId": "244954250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80908203125
        },
        {
            "corpus_id": "10137425",
            "title": "Multimodal Machine Learning: A Survey and Taxonomy",
            "text": "Generative approaches to multimodal translation construct models that can perform multimodal translation given a unimodal source instance. It is a challenging problem as it requires the ability to both understand the source modality and to generate the target sequence or signal. As discussed in the following section, this also makes such methods much more difficult to evaluate, due to large space of possible correct answers. \n\nIn this survey we focus on the generation of three modalities: language, vision, and sound. Language generation has been explored for a long time [170], with a lot of recent attention for tasks such as image and video description [19]. Speech and sound generation has also seen a lot of work with a number of historical [88] and modern approaches [157], [209]. Photo-realistic image generation has been less explored, and is still in early stages [132], [171], however, there have been a number of attempts at generating abstract scenes [253], computer graphics [45], and talking heads [6]. \n\nWe identify three broad categories of generative models: grammar-based, encoder-decoder, and continuous generation models. Grammar based models simplify the task by restricting the target domain by using a grammar, e.g., by generating restricted sentences based on a subject, object, verb template. Encoder-decoder models first encode the source modality to a latent representation which is then used by a decoder to generate the target modality. Continuous generation models generate the target modality continuously based on a stream of source modality inputs and are most suited for translating between temporal sequences -such as text-to-speech. Grammar-based models rely on a pre-defined grammar for generating a particular modality. They start by detecting high level concepts from the source modality, such as objects in images and actions from videos. These detections are then incorporated together with a generation procedure based on a pre-defined grammar to result in a target modality. \n\nKojima et al. [107] proposed a system to describe human behavior in a video using the detected position of the person's head and hands and rule based natural language generation that incorporates a hierarchy of concepts and actions.",
            "score": 0.5169037052292091,
            "section_title": "Generative approaches",
            "char_start_offset": 36606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2022
                },
                {
                    "start": 2025,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 582,
                    "matchedPaperCorpusId": "59940"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "14621185"
                },
                {
                    "start": 778,
                    "end": 783,
                    "matchedPaperCorpusId": "1697911"
                },
                {
                    "start": 885,
                    "end": 890,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 968,
                    "end": 973,
                    "matchedPaperCorpusId": "10554419"
                },
                {
                    "start": 993,
                    "end": 997,
                    "matchedPaperCorpusId": "3842372"
                },
                {
                    "start": 1017,
                    "end": 1020,
                    "matchedPaperCorpusId": "1075172"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "278327487",
            "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
            "text": "One major direction in unified multimodal understanding and generation models adopts autoregressive (AR) architectures, where both vision and language tokens are typically serialized and modeled sequentially. In these models, a backbone Transformer, typically adapted from large language models (LLMs) such as LLaMA family [1], [2], [133], Vicuna [58], Gemma series [134], [135], [136], and Qwen series [5], [6], [9], [10], serves as the unified modality-fusion module to autoregressively predict multimodal outputs. \n\nTo integrate visual information into the AR framework, as shown in Fig. 5, existing methods propose different strategies for image tokenization during modality encoding. These approaches can be broadly categorized into four types: pixel-based, semantic-based, learnable query-based, hybrid-based encoding methods. 1) Pixel-based Encoding. As shown in Fig. 5 (b-1), pixelbased encoding typically refers to the representation of images as continuous or discrete tokens obtained from pretrained autoencoders supervised purely by image reconstruction, such as VQGAN-like models [32], [137], [138], [139]. These encoders compress the high-dimensional pixel space into a compact latent space, where each spatial patch corresponds to an image token. In unified multimodal autoregressive models, image tokens serialized from such encoders are processed analogously to text tokens, allowing both modalities to be modeled within a single sequence. \n\nRecent works have adopted and enhanced pixel-based tokenization with various encoder designs. LWM [29] employs a VQGAN tokenizer [32] to encode images into discrete latent codes without requiring semantic supervision. It proposes a multimodal world modeling framework, wherein visual and textual tokens are serialized together for unified autoregressive modeling. By learning world dynamics purely through reconstruction-based visual tokens and textual descriptions, LWM demonstrates that largescale multimodal generation is feasible without specialized semantic tokenization.",
            "score": 0.5162129093322342,
            "section_title": "Auto-Regressive Models",
            "char_start_offset": 27036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1099,
                    "end": 1104,
                    "matchedPaperCorpusId": "252367709"
                },
                {
                    "start": 1106,
                    "end": 1111,
                    "matchedPaperCorpusId": "263671912"
                },
                {
                    "start": 1113,
                    "end": 1118,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1588,
                    "end": 1592,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "269449129",
            "title": "Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification",
            "text": "Vision-Language Models.\n\nVision-Language Models (VLMs) represent a class of multimodal models adept at correlating textual and visual information.Prominent models in this domain include CLIP [16], ALIGN [6], FLAVA [18] and Florence [30], among others.These models typically comprise two main components: an image encoder and a text encoder.Some models also have multimodal fusion modules [8,22].VLMs are trained on extensive text-image pairs through tasks like image-text contrastive learning, endowing them with powerful text-image matching capabilities.In this paper, we leverage these capabilities of CLIP to generate our cross-modal neighbor representations (CODER) for images.Using LLMs as Experts to Improve VLMs.Large Language Models (LLMs) are widely used in many tasks, such as in-context learning.LLMs can serve as experts by outputting their knowledge in text form.This text-based knowledge can be harnessed by VLMs to enhance their capabilities.Previous work has explored using LLMs' knowledge to optimize VLMs' pre-training [1], prediction interpretability [12,26,28], and classifier quality [2,11,12,15].Recently, some work [29,35] uses LLMs to select the best VLMs from a VLM zoo for specific target tasks.In this paper, we use the semantic information provided by LLMs to optimize CLIP's features during the inference stage in a training-free manner.",
            "score": 0.5162085600918156,
            "section_title": "Related Work",
            "char_start_offset": 5462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 25,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 395
                },
                {
                    "start": 395,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 876
                },
                {
                    "start": 876,
                    "end": 957
                },
                {
                    "start": 957,
                    "end": 1118
                },
                {
                    "start": 1118,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 195,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "231879586"
                },
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 388,
                    "end": 391,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "260068316"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "258987272"
                },
                {
                    "start": 1070,
                    "end": 1074,
                    "matchedPaperCorpusId": "252872997"
                },
                {
                    "start": 1074,
                    "end": 1077,
                    "matchedPaperCorpusId": "260682240"
                },
                {
                    "start": 1077,
                    "end": 1080,
                    "matchedPaperCorpusId": "253735286"
                },
                {
                    "start": 1105,
                    "end": 1108,
                    "matchedPaperCorpusId": "254247317"
                },
                {
                    "start": 1108,
                    "end": 1111,
                    "matchedPaperCorpusId": "254591376"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "252872997"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "259165681"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "corpus_id": "260334248",
            "title": "Unified Model for Image, Video, Audio and Language Tasks",
            "text": "While many previous works have attempted to build unified models, they still have some customization in terms of architectures and tasks. Our work tries to unify most aspects of the model, following a recent line of work (Wang et al., 2022c). In the following, we detail the 4 unification axes that distinguish our work from previous ones. \n\nUnified input/output. To have a unified model, it is important to have the same input and output format across all tasks and modalities. The common approach is to cast everything to sequence of tokens as in language models. Multimodal inputs, such as images, videos and audios can be transformed to tokens by patchifying or using shallow modality-specific projections. Multimodal outputs can also be discritized, by using VQ-GAN for images and discrete pixel locations for visual grounding. A unified vocabulary is used when training the model. \n\nUnified model. The unified input/output representation allows to use a single model to solve all tasks, without the need to any adaptation when transitioning from the pretraining to the finetuning phase (e.g., no need for task-specific heads). In addition, the current advances in LLMs, especially their generalization to new tasks, make it a good choice to leverage these models to solve multimodal tasks. The common approach is to have a language model as the core model, with light-weight modality-specific input projections.",
            "score": 0.5160605775934404,
            "section_title": "C Unified Foundation Models: 4 Unification axes",
            "char_start_offset": 46087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 339
                },
                {
                    "start": 342,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "267068435",
            "title": "Text-to-Image Cross-Modal Generation: A Systematic Review",
            "text": "Limited research output notwithstanding, significant progress has recently been made in this domain. \n\nThe image-to-text and text-to-image problems have been significantly expanded and are both at the forefront of research focusing on cross-modal generation. They have also seen the incorporation of research lines from other areas of deep learning. \n\nIn this work, we specifically focus on the text-to-image problem and derivative tasks. With the increased interest and research output on the rise for this field, there is a need for a comprehensive review of the various strands of research. \n\nTo the best of our knowledge, the existing research on text-to-image generation lacks such a review and therein lies the main contribution of this work. We aim to establish links between various areas within text-to-image generation, as well as with other areas of deep learning, bringing together disjoint lines of research. It is our intention to unify the discussion from an overarching cross-modal generation perspective. \n\nThe starting points of this review are the research papers published at 8 machine learning conferences: \n\n\u2022 Conference on Neural Information Processing Systems (NeurIPS) \n\n\u2022 International Conference on Machine Learning (ICML) \n\n\u2022 International Conference on Learning Representations (ICLR) \n\n\u2022 AAAI Conference on Artificial Intelligence (AAAI) \n\n\u2022 International Joint Conference on Artificial Intelligence (IJCAI) \n\n\u2022 International Conference on Computer Vision (ICCV) \n\n\u2022 European Conference on Computer Vision (ECCV) \n\n\u2022 Conference on Computer Vision and Pattern Recognition (CVPR). \n\nMore concretely, we consider papers published in the time frame 2016-2022 -the most recent publication years at the moment of writing. We have searched the conference proceedings for the following terms: cross-modal, multi-modal (multimodal), generative and diffusion. From papers conforming to these criteria, we select those that actually cover text-to-image generation. We also add a number of papers which do not meet the outlined search criteria but are still relevant as far as text-to-image generation is concerned, in particular, works on text-to-image diffusion models. We strive to provide a comprehensive review of cross-modal text-to-image generation, focusing on both the common elements of various methods, and their distinctive characteristics.",
            "score": 0.5157540054482418,
            "section_title": "INTRODUCTION",
            "char_start_offset": 8703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 103,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 349
                },
                {
                    "start": 352,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2370
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "268033147",
            "title": "Probing Multimodal Large Language Models for Global and Local Semantic Representations",
            "text": "Recently, Large Language Models (LLMs) have achieved remarkable advancements in various natural language processing applications (Touvron et al., 2023;OpenAI, 2024), owing to pre-training on massive text corpus. It becomes a popular topic nowadays to transfer the powerful capacity of LLMs to Multimodal Large Language Models (MLLMs) through image-caption corpus (Alayrac et al., 2022;Li et al., 2023). These MLLMs show an impressive ability to handle multimodal tasks, including Image Captioning (IC, Plummer et al., 2015) and Visual Question Answering (VQA, Goyal et al., 2017). However, existing research has predominantly focused on the ability of MLLMs to generate single tokens one by one, while lacking investigations about how their representation vectors can encode global multimodal information. In generation tasks like IC and VQA, when predicting the next token, the models may only need to focus on a local part of the image and a subsequence of the text to handle the task. But in tasks like image-text retrieval (Xie et al., 2019), the MLLMs should aim to encode the global semantic information of the entire image and text, when predicting whether they have correlation. \n\nIn this work, we focus on understanding and uncovering how the global and local semantic information is encoded in the decoder-only MLLMs. To track the representing ability of each layer in \u2020 This work was done when Mingxu Tao and Quzhe \n\nHuang were interns at Kuaishou Technology. \n\nMLLMs, we use probing study, a popular tool to investigate model interpretability (Tenney et al., 2019;Jawahar et al., 2019). Previous probing studies of pure-text language models have explored the representing ability of models in various levels, from local to global semantics (Liu et al., 2019;Talmor et al., 2020). However, to the best of our knowledge, existing works about vision-language models sorely focus on investigating the ability to represent local semantic information, for instance, from a lexical perspective (Dahlgren Lindstr\u00f6m et al., 2020).",
            "score": 0.5156210441992712,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 385,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 502,
                    "end": 523,
                    "matchedPaperCorpusId": "6941275"
                },
                {
                    "start": 560,
                    "end": 579,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 1555,
                    "end": 1576,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1576,
                    "end": 1597,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 1752,
                    "end": 1770,
                    "matchedPaperCorpusId": "84841767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "262459251",
            "title": "FaceGemma: Enhancing Image Captioning with Facial Attributes for Portrait Images",
            "text": "PaliGemma [33] is a vision-language model that processes both image and text inputs, inspired by PaLI-3 [30] and incorporating SigLIP [29] and Gemma [28] components. It answers questions about images, generates captions for visual media, detects objects, and reads embedded text. It offers general-purpose pre-trained models adaptable through fine-tuning for diverse applications, alongside research-oriented models specialized for specific datasets. Fine-tuning optimizes PaliGemma for specific tasks, enhancing accuracy and relevance in both general and research contexts, making it versatile for detailed image analysis and multimodal understanding.",
            "score": 0.5155225295403666,
            "section_title": "PaliGemma",
            "char_start_offset": 11452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 652
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.546875
        },
        {
            "corpus_id": "258865377",
            "title": "Text encoders bottleneck compositionality in contrastive vision-language models",
            "text": "Building models capable of reasoning jointly about visual and textual inputs is a long-standing goal of AI (Winograd, 1971), with potential applications in the fields of vision-language navigation (Anderson et al., 2018), human-robot interaction (Matuszek et al., 2012), accessible image captioning (Gurari et al., 2020), etc. Recent challenge datasets have been designed to probe the capacity of multimodal models to represent descriptions of precise visual compositions (Johnson et al., 2017;Suhr et al., 2019;Hudson and Manning, 2019;Thrush et al., 2022). Yuksekgonul et al. (2023) and Yamada et al. (2022) study CLIP specifically, demonstrating its shortcomings (and some potential fixes) in terms of modeling syntax. Ma et al. (2022) study OpenCLIP models for various types of compositional reasoning, with programmatically sourced hard negatives. Different from these works, our textual probe does not require access to images. \n\nOur image-and-text evaluation most closely resembles Thrush et al. (2022). However, we stratify the examples based on type of input (e.g., temporal relations) to provide more detailed insights. We also keep our prompts relatively simple, never having more than two objects or two attributes in the input. We believe this is a more realistic goal for our current vision-language models. The word order shuffling aspect is also discussed in Yuksekgonul et al. (2023). However, as their proposed benchmark does not provide pairs of images with corresponding captions, it is possible to achieve state-of-the-art with a text-only model (specifically, 2-shot ChatGPT6 (Ouyang et al., 2022), details in Appendix and the recent Hsieh et al. (2023)). While this does not detract from their finding that visionlanguage models ignore word order, our benchmarks have an additional advantage of being insensitive to text-only priors.",
            "score": 0.5154955818633917,
            "section_title": "Related work",
            "char_start_offset": 19045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 220,
                    "matchedPaperCorpusId": "4673790"
                },
                {
                    "start": 246,
                    "end": 269,
                    "matchedPaperCorpusId": "2408319"
                },
                {
                    "start": 299,
                    "end": 320,
                    "matchedPaperCorpusId": "211204968"
                },
                {
                    "start": 472,
                    "end": 494,
                    "matchedPaperCorpusId": "15458100"
                },
                {
                    "start": 494,
                    "end": 512,
                    "matchedPaperCorpusId": "53178856"
                },
                {
                    "start": 512,
                    "end": 537,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 537,
                    "end": 557,
                    "matchedPaperCorpusId": "248006414"
                },
                {
                    "start": 722,
                    "end": 738,
                    "matchedPaperCorpusId": "254685851"
                },
                {
                    "start": 989,
                    "end": 1009,
                    "matchedPaperCorpusId": "248006414"
                },
                {
                    "start": 1656,
                    "end": 1675,
                    "matchedPaperCorpusId": "259251493"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "276929408",
            "title": "Recognition-Synergistic Scene Text Editing",
            "text": "In response to the notable progress of large language models in natural language processing [1,2,6], the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models [15], but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,38,40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs [3,13,29] offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation. Inspired by these methods, our approach integrates the multi-modal language model RS-STE, which is specialized in scene text editing.",
            "score": 0.5152441020817581,
            "section_title": "MLLM for Image Generation and Editing",
            "char_start_offset": 6474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 99,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "253581838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93994140625
        },
        {
            "corpus_id": "254129024",
            "title": "Multimodal Context Carryover",
            "text": "Thanks to the advent of Transformer-based models over the past few years, multimodal representations have seen significant advances. The types of multimodal models can be roughly categorized into three, a) Single Encoder (Girshick et al., 2013;Long et al., 2014;Simonyan and Zisserman, 2014;Tan and Bansal, 2019;Chen et al., 2019b;Li et al., 2020;Wanigasekara et al., 2022), b) Dual Encoder (Radford et al., 2021;Li et al., 2021a;Zhang et al., 2020;Jia et al., 2021;, and c) Encoder-Decoder models (Vinyals et al., 2014;Piergiovanni et al., 2022;. Attempts at unifying these foundational models have also been made in (Yu et al., 2022;Singh et al., 2021).\n\nSingle Encoder models appear early in the multimodal literature and pave the way for the other 2 types of models. For this family of models, usually the image and text representations exist in separate spaces and there is an ensuing fusion layer. Dual Encoder models leverage image-text contrastive loss (Oord et al., 2018;He et al., 2019;Chen et al., 2020;Tian et al., 2019) during training, exhibit higher image-to-text alignment and bring the image, text representations to a common more aligned representation space. They perform well on image-text retrieval tasks but underperform in vision-language understanding tasks requiring higher reasoning, e.g., Visual Question Answering (VQA), and Natural Language Inference (NLI). Our current task of Multimodal Context Carryover uses the latest advances in the multimodal representation learning space and injects state-ofthe-art components with minimal changes into a framework for dialog tracking and slot selection, and results in a system that can handle multimodal user-system dialog interaction. Our current multimodal use cases are set up to be similar to a textto-image retrieval task that occur within the context of a user-system dialog interaction. Thus, we incorporate the latest developments in",
            "score": 0.5152441020817581,
            "section_title": "Related work",
            "char_start_offset": 2889,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 373,
                    "matchedPaperCorpusId": "249306408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "278032951",
            "title": "Symbolic Representation for Any-to-Any Generative Tasks",
            "text": "Recent years have witnessed remarkable advances in large language models (LLMs), which have demonstrated exceptional capabilities across various natural language tasks, from basic comprehension to complex reasoning [3, 6-8, 16, 21, 24, 29-31, 43, 44]. Building on this success, multimodal large language models (MLLMs) have extended these capabilities to integrate multiple forms of input and output, covering data modalities such as images, audio, video, and 3D structures [1, 4, 5, 10, 14, 18-20, 22, 32, 34-37, 46, 47, 51-54, 56]. The field has progressed from isolated single-modality models to sophisticated any-to-any frameworks [25,26,28,31,40,41,55] that can handle diverse input-output combinations within a single model architecture. However, these unified multimodal frameworks face significant challenges in practice. The scarcity of high-quality, diverse multimodal datasets remains a fundamental bottleneck, particularly for complex cross-modal tasks. Moreover, different modalities often require distinct processing approaches and representations, making it challenging to achieve optimal performance across all possible modality combinations in a single model. The need to align disparate modalities into a coherent unified representation while preserving their unique characteristics continues to be a core challenge in advancing these frameworks.",
            "score": 0.5152441020817581,
            "section_title": "Related work 2.1. Unified multi-modal framework",
            "char_start_offset": 7160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1364
                }
            ],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 657,
                    "matchedPaperCorpusId": "267750101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "270619856",
            "title": "Improving Visual Commonsense in Language Models via Multiple Image Generation",
            "text": "Commonsense reasoning is fundamentally based on multimodal knowledge. However, existing large language models (LLMs) are primarily trained using textual data only, limiting their ability to incorporate essential visual information. In contrast, Visual Language Models, which excel at visually-oriented tasks, often fail at non-visual tasks such as basic commonsense reasoning. This divergence highlights a critical challenge - the integration of robust visual understanding with foundational text-based language reasoning. To this end, we introduce a method aimed at enhancing LLMs' visual commonsense. Specifically, our method generates multiple images based on the input text prompt and integrates these into the model's decision-making process by mixing their prediction probabilities. To facilitate multimodal grounded language modeling, we employ a late-fusion layer that combines the projected visual features with the output of a pre-trained LLM conditioned on text only. This late-fusion layer enables predictions based on comprehensive image-text knowledge as well as text only when this is required. We evaluate our approach using several visual commonsense reasoning tasks together with traditional NLP tasks, including common sense reasoning and reading comprehension. Our experimental results demonstrate significant superiority over existing baselines. When applied to recent state-of-the-art LLMs (e.g., Llama3), we observe improvements not only in visual common sense but also in traditional NLP benchmarks. Code and models are available under https://github.com/guyyariv/vLMIG.",
            "score": 0.5152114797545315,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "271050132",
            "title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?",
            "text": "Recent advancements in multimodal foundation models (FMs) have witnessed a proliferation of image generation models such as DALLE-3 [58,59], Stable Diffusion [62] and many others [34,65,83,53].However, these text-to-image models often suffer from issues such as (1) text-image misalignment, where the model generates plausible entities in the image that contradict the instruction (often known Figure 1: We evaluate a large variety of multimodal judges on MJ-BENCH dataset.We compare their feedback over four comprehensive perspectives, each decomposed into multiple sub-categories.Additionally, we study the effectiveness of the feedback under different scales and input modes.as hallucination) [61,94,78]; (2) unsafe content, where the model produces harmful or inappropriate output, including toxic, sexual, or violent concepts [77]; (3) low-quality generation, where the model generates images with blurry or unnatural artifacts [40]; and (4) biased and stereotypical output, where the model produces biased output that either favors or opposes certain demographic groups [76,93].\n\nTo address these underlying issues and improve the reliability of text-to-image models, it is important to inform the model when it performs poorly.This necessitates providing feedback on the model's generation using a multimodal judge [11,96,79].This feedback can be used for inference-time guidance [87,13] or training-based alignment for text-to-image models [10,55].The judges can be categorized into two types: (1) CLIP-based scoring models [56], where the feedback is directly a text-image alignment score from the vision-language pretrained models.These models are typically smaller in size yet unbalanced-aligned across different evaluation objectives (e.g. while these models are better at text-vision alignment, they could be extremely unsafe or biased) [66]; (2) VLMs, which are larger in scale yet more capable and comprehensive, typically incorporate a Chain-of-Thought (CoT) step and can provide feedback on various scales, such as numerical or Likert scales [16].",
            "score": 0.5149629665785754,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 1084
                },
                {
                    "start": 1086,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "257427461"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "259991581"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "259108247"
                },
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "233346984"
                },
                {
                    "start": 831,
                    "end": 835,
                    "matchedPaperCorpusId": "259202782"
                },
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "265051037"
                },
                {
                    "start": 1387,
                    "end": 1391,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "271310477",
            "title": "Urban Visual Appeal According to ChatGPT: Contrasting AI and Human Insights",
            "text": "The advent of Large Language Models (LLMs) such as GPT-3 (28) and BERT (29) has not only revolutionised AI with sophisticated text generation and understanding, but also democratised interactions with AI technologies (30).These models enable users to execute commands, optimise and fine-tune AI responses, and engage in nuanced interactions without requiring deep AI expertise.This suite of LLMs illustrates the leap towards intuitive, accessible technology, transforming user interactions across various domains.This capability is particularly crucial for our study, allowing for the customisation of analysis criteria.Nonetheless, the application of LLMs is limited, because it lacks the ability to process visual media, essential for assessing urban visual appeal.\n\nThe emergence of MLLMs like GPT-4 (31) and Kosmos-2.5 (32,33), which integrate the textual interaction capability of LLMs with image analysis, presents a novel solution.Early studies in the field were dedicated to understanding and generating text based on multimodal inputs, focusing on how models interpret the relationship between visual elements and text.This research area benefited greatly from projects like BLIP-2 (Bootstrapping Language-Image Pre-training-2) (34), CLIP (Contrastive Language-Image Pretraining) (35), and LLaVA (Large Language-and-Vision Assistant) (36).As the field evolved, the scope of MLLMs broadened to include generating outputs specific to various modalities.These models emphasise the use of modality encoders, LLM backbones, and modality generators to process and generate multimodal content efficiently (37).\n\nNotable in this development is the flexibility in input representation, allowing for seamless integration of several data types into the LLM framework.\n\nThis advancement provides new options for applications requiring comprehensive analysis of both visual and textual information, such as an urban area's visual assessment, that can potentially benefit spatial planning.Some early studies have explored this field, including the study by Jongwiriyanurak et al. (38), which used LLaVA by prompting six questions to gather information on various factors considered critical in assessing motorcycle crash risks.",
            "score": 0.51458505793964,
            "section_title": "AI in Sentiment and Multimodal Analysis",
            "char_start_offset": 5160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 767
                },
                {
                    "start": 769,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1460
                },
                {
                    "start": 1460,
                    "end": 1612
                },
                {
                    "start": 1614,
                    "end": 1765
                },
                {
                    "start": 1767,
                    "end": 1984
                },
                {
                    "start": 1984,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "258520642"
                },
                {
                    "start": 823,
                    "end": 827,
                    "matchedPaperCorpusId": "257219775"
                },
                {
                    "start": 1289,
                    "end": 1293,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "253735233",
            "title": "CL-Cross VQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering",
            "text": "In recent years, pre-trained models have progressed at a dizzying speed in tandem with the evolution of transformers. Due to the significance of single-modal language/vision pre-training, some pioneering works [11,30,34,[44][45][46] have recently attempted to explore the joint representation of language and vision by pre-training large-scale models on vision and language modalities, which are referred to as Vision-and-Language Pre-trained Models (VLPMs). There are three primary components in VLPMs, namely vision encoder (VE), text encoder (TE), and modality fusion module (MF). VE and TE are pre-trained with images and texts, respectively. MF is pre-trained using image-text pairs and amalgamates the output embeddings of VE and TE. While almost all VLPMs extract text embeddings from a pretrained language model, e.g., BERT [14], the pre-trained model used on the vision side is different. Unlike most existing VLPMs, which are built upon region-level features extracted from pre-trained object detectors, e.g., Faster RCNN [38]. All VLPMs evaluated in our work directly utilize patch features without bounding box annotations, which allows them to escape from capacity limitations due to the imperfections of the object detectors. \n\nTo thoroughly investigate the effects of model design in a CL setting, we evaluate 4 different types of VLPMs, which are summarized in Table 1. Specifically, ViLT [24] directly feeds image patch features and text token embeddings into a pre-trained ViT [15] model that learns the visionlanguage alignment with self-attention across both modalities. To address the impoverished language representations of ViLT, VAuLT [12], an extension of ViLT, propagates the output representations of a large language model, i.e., BERT, to the language input of ViLT. FLAVA [42], a foundational language and vision alignment model, learns strong representations through joint pre-training on both unimodal and multimodal data while encompassing crossmodal alignment objectives and multimodal fusion objectives.",
            "score": 0.5145808376641346,
            "section_title": "Vision-and-Language Pre-trained Models",
            "char_start_offset": 4166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "202889174"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 1032,
                    "end": 1036,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "231839613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "274610552",
            "title": "RoboMM: All-in-One Multimodal Large Model for Robotic Manipulation",
            "text": "Multimodal Large Language Models (MLLMs) typically consist of three main components: the modality encoder (Enc), the adapter (Adapter), and the large language model (LLM), mathematically expressed as follows: \n\nHere, WE denotes the word embedding layer. The modality encoder transforms inputs from single modality into appropriate representations. For instance, the image encoder extracts features F I from input images I. Common visual encoders like CLIP [53] are pre-trained on image-text pairs, aligning visual and textual semantics for easier integration with LLMs. The adapter maps features from visual and other modalities into inputs U that the LLM can understand. For example, Blip2 [32] uses Q-Former for feature interaction; LLaVA [37] employs MLPs to align visual features with text features. The large language model is the core component of our framework, referred to in this paper as the Feature Fusion Decoder. It typically employs auto-regressive models such as LLaMA [59] or GPT [1], as well as cross-attention models like Flamingo [2] or LLaMA3.2 [44]. This model fuses the feature representations U with text features F T extracted from the word embedding layer to generate the final textual output O T . This integration of features enhances the model's ability to produce contextually relevant responses.",
            "score": 0.5143386476187911,
            "section_title": "Review",
            "char_start_offset": 7736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1325
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 691,
                    "end": 695,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 741,
                    "end": 745,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "257622925",
            "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing",
            "text": "As shown in Fig. 3, the reward model takes in an input image c I , a text instruction c E , and an edited image x and outputs a scalar value. Inspired by the recent work on the vision-language model, especially BLIP [28], we employ a visual transformer [13] as our image encoder and an image-grounded text encoder as the multimodal encoder for images and text. Finally, we set a linear layer on top of the image-grounded text encoder to map the multimodal embedding to a scalar value. \n\n(1) Visual transformer. We encode both the input image c I and edited image x with the same visual transformer. Then we obtain the joint image embedding by concatenating the two image embeddings vit(c I ), vit(x). (2) Image-grounded text encoder. The image-grounded text encoder is a multimodal encoder that inserts one additional cross-attention layer between the self-attention layer and the feed-forward network for each transformer block of BERT [11]. The additional cross-attention layer incorporates visual information into the text model. The output embedding of the image-grounded text encoder is used as the multimodal representation of the (c I , c E , x) triplet. \n\nWe gather a dataset comprising 3,634 images for the purpose of ranking. For each image, we generate five variant edited images, and ask an annotator to rank images from best to worst. Additionally, we ask annotators to indicate if any of the following scenarios apply: (1) all edited images are edited but none of them follow the instruction; (2) all edited images are visually the same as the original image; (3) all images are edited beyond the scope of instruction; (4) edited images have harmful content containing sex, violence, porn, etc; and (5) all edited images look similar to each other. We compare training reward models by filtering some/all of these options. \n\nWe note that a considerable portion of the collected data falls under at least one of the aforementioned categories, indicating that even for humans, ranking these images is challenging.",
            "score": 0.5140916028544519,
            "section_title": "B.2. Human Feedback Rewards Learning",
            "char_start_offset": 26376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 2025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10968017578125
        },
        {
            "corpus_id": "248240054",
            "title": "Opal: Multimodal Image Generation for News Illustration",
            "text": "Our system adds to a long tradition of work within multimodal authoring tools. [11] and Wordseye [14] were systems that connected language to 3D space, allowing users to use text prompts to generate 3D scenes. [12] allowed users to define 3D shapes based on adjectives and emotive cues, parameterizing creature shapes by qualities like \"cute\" or \"dangerous\". As tools moved away from statistical-based techniques to GANs, a number of text-to-image architectures [39,40,52,53] were proposed. However, many of these architectures were class-conditional, meaning they were constrained to the single-class datasets they were trained upon. \n\nRecently, multimodal systems have found renewed momentum from machine learning advancements in multimodal representation learning. CLIP, one of the latest advancements, demonstrated that images and text pairs can be contrastively learned and optimized such that the image and text embeddings can share a multimodal embedding space. [41] Many of the earliest open-source text-to-image generative frameworks to gain traction used CLIP in a discriminator-like fashion, using it in conjunction with a host of generative models from BigGan to VQGAN. [4,16,17,22] Newer methods such as diffusion models have also increased output quality. [15,19,36,37] DALL\u2022E 2 [42] showed by conditioning on a CLIP text prior, generating CLIP image representations, and decoding those representations either autoregressively or with diffusion, DALL\u2022E 2 could create a text-to-image generator with novel functionalities like image remixing, inpainting, and interpolation. Other state-of-the-art text-to-image architectures have come out in rapid succession. Make-a-Scene offers users control in the form of semantic layout sketches.[1]",
            "score": 0.5139191366906484,
            "section_title": "Multimodality",
            "char_start_offset": 8429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "10046351"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "202783730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "270216205",
            "title": "Effectiveness of Vision Language Models for Open-world Single Image Test Time Adaptation",
            "text": "CLIP [24] is a multimodal VLM consisting of two modules: Vision encoder and Text encoder denoted as F V and F T respectively.During pre-training, the two modules are jointly trained in a contrastive self-supervised fashion to align massive amounts of web scrapped image-text pairs.CLIP has demonstrated impressive zero-shot generalization ability across a wide variety of datasets.\n\nMaPLe [29] is a multimodal prompt learner model that simultaneously adapts both the vision and text encoders while finetuning CLIP for downstream tasks.They use learnable text prompts p T and bridge the two modalities using visual prompts obtained as p V = Proj(p T ).Learnable tokens are also introduced in the deeper layers of both image and text encoders, to enable progressive adaptation of the features.As in [26], we use MaPLe as an additional VLM backbone to test our approach.We now review the baselines developed based on CLIP and MaPLe for zero-shot evaluation.\n\nZSEval: Given a test image x t , the image feature is extracted from the vision encoder as\n\nFor a C-class classification problem, the classifier is obtained by prepending a predefined text prompt p T =\"A photo of a\", with the class names {c TPT [25] aims to improve the zero shot generalization ability of CLIP by providing custom adaptable context for each image.This is done by prepending learnable text prompts p T to the class names instead of a predefined text prompt.The text classifiers t i = F T ({p T ; c i }), i \u2208 {1, 2, . . .C} are now a function of these learnable prompts, which are specially adapted for each test image using an entropy minimization objective as arg min p T L ent .The entropy is obtained using the average score vector of the filtered augmented views.\n\nPromptAlign [26] (PAlign) leverages multimodal prompt learner model MaPLe [29] to facilitate the adaptation of both vision and language encoders for each test sample.",
            "score": 0.5127877679929147,
            "section_title": "Vision Language Models",
            "char_start_offset": 7298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 381
                },
                {
                    "start": 383,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 954
                },
                {
                    "start": 956,
                    "end": 1046
                },
                {
                    "start": 1048,
                    "end": 1320
                },
                {
                    "start": 1320,
                    "end": 1429
                },
                {
                    "start": 1429,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1652
                },
                {
                    "start": 1652,
                    "end": 1739
                },
                {
                    "start": 1741,
                    "end": 1907
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "252873086",
            "title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting",
            "text": "MAPL is a vision-language (VL) multimodal model capable of generating text from a combination of visual and textual inputs. Our model builds on top of pre-trained vision-only and language-only models and leverages their strong generalization capabilities (e.g., zero-shot transfer, in-context learning) to tackle unseen VL tasks. MAPL is agnostic to the choice of these pre-trained unimodal models as long as they show such capabilities (Sec. 4.5). Concretely, MAPL maps the image representations from a vision encoder's output embedding space to a LM's token embedding space, so that the LM can be conditioned both on visual and textual information. To this end, we train a mapping network with an image captioning objective (Sec. 3.1, 3.2), while keeping the weights of the vision encoder and the LM frozen. Once the mapping network is trained, MAPL can be prompted with a few examples of unseen VL tasks and predict the response via text generation (Sec. 3.3). The overall model architecture is depicted in Figure 1.",
            "score": 0.5126573083404339,
            "section_title": "Method",
            "char_start_offset": 8157,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5595703125
        },
        {
            "corpus_id": "237605078",
            "title": "MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks",
            "text": "-world multimodal observations that need to be classified are not simply a caption describing what is happening in an image, additional pretraining is needed using the data of interest in order for the model to adapt to the target domain. Pretraining is also computationally expensive, requiring computational resources not available to most social scientists.\n\nTo this end, we propose multimodal representations from modality translation, or MARMOT, a novel transformers-based architecture that produces multimodal representations. MARMOT aims to solve the two issues that make state-of-the-art multimodal models unusable for political science and communications research. First, we use attention masks to handle missing modalities explicitly. Attention masks, typically used in machine translation and question-answering tasks, are used to prevent the self-attention mechanism of the transformer from attending to missing modalities, meaning that representations can be constructed even for observations missing modalities. Second, to capture the spirit of pretraining while avoiding expensive computational costs, we propose modality translation. Instead of pretraining our model on an image annotation dataset, we directly generate captions for each observation containing an image using a pretrained image captioner. To avoid having to adapt the underlying transformers-based pretrained language model to accept both text and image features, we use a transformer decoder initialized with pretrained BERT weights (Devlin, Chang, Lee, & Toutanova, 2018). The image captions, derived from a pretrained image captioner such as self-critical sequence training (Rennie, Marcheret, Mroueh, Ross, & Goel, 2016), are inputted into the BERT decoder, and the image features, derived from a pretrained image network such as ResNet-152 (He, Zhang, Ren, & Sun, 2015), are inputted at the encoder-decoder attention layer. The BERT decoder constructs what we call the translated image. Modality translation maps image features to the relevant parts of the text feature space. The last step jointly inputs the text, image captions, and translated image features into a transformer encoder initialized with pretrained BERT weights. The output of the transformer encoder is the joint image-text",
            "score": 0.5122639070468471,
            "section_title": "Introduction",
            "char_start_offset": 3822,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "222141733",
            "title": "Fine-Grained Grounding for Multimodal Speech Recognition",
            "text": "Inspired by studies of human perception, multimodal processing is spreading into many traditional areas of research, e.g., machine translation (Sulubacak et al., 2019) and ASR . It has become an important part of new areas of research such as image captioning (Bernardi et al., 2016), visual question-answering (VQA; (Antol et al., 2015)), and multimodal summarization .\n\nThe representation and integration of visual context in multimodal ASR systems is an active area of research. Previous approaches incorporate image representations either in the acoustic model (Miao and Metze, 2016), the language model (Gupta et al., 2017;Naszadi et al., 2018), or in end-to-end models .  and Moriya and Jones (2018) explore different types of multimodal representations such as imagescene representations and titles of instructional videos respectively. Although all these integration methods show improvements over unimodal baselines, it is not clear when such approaches perform better, and which representations are best.\n\nIt has been argued that traditional multimodal architectures do not necessarily take advantage of image semantics in different tasks.  showed that multimodal ASR models trained with shift adaptation (Miao and Metze, 2016) 6 use the image as a regularization signal. In a similar direction, Elliott (2018) showed that misaligment between image and text representations do not affect multimodal MT models. Ramakrishnan et al. (2018) and Grand and Belinkov (2019) showed that traditional VQA neural architectures ignore the visual context and focus on linguistic biases of the dataset. More related to our work are the studies of Srinivasan et al. (2020) and , which explore how multimodal models use image information under noisy scenarios. These studies conclude that when certain nouns are dropped from the dominant language modality, multimodal models are capable of properly using the semantics provided by the image. However, unlike this work, their explorations are limited to nouns and not expanded to other types of words.\n\nFrom an image representation perspective, previous works have studied the utility of using local representations, rather than global ones for multimodal language processing tasks. For instance, Xu et al. (2015) show",
            "score": 0.5122477297385111,
            "section_title": "Related Work",
            "char_start_offset": 23401,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 283,
                    "matchedPaperCorpusId": "1770055"
                },
                {
                    "start": 317,
                    "end": 337,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 608,
                    "end": 628,
                    "matchedPaperCorpusId": "12332372"
                },
                {
                    "start": 628,
                    "end": 649,
                    "matchedPaperCorpusId": "59248806"
                },
                {
                    "start": 682,
                    "end": 705,
                    "matchedPaperCorpusId": "61806470"
                },
                {
                    "start": 1306,
                    "end": 1320,
                    "matchedPaperCorpusId": "53082631"
                },
                {
                    "start": 1420,
                    "end": 1446,
                    "matchedPaperCorpusId": "52946763"
                },
                {
                    "start": 1451,
                    "end": 1476,
                    "matchedPaperCorpusId": "140080210"
                },
                {
                    "start": 1643,
                    "end": 1667,
                    "matchedPaperCorpusId": "211097001"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "269448756",
            "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?",
            "text": "Decoder vision and language models (VLMs), such as GPT-4V (OpenAI, 2023), Gemini 1.5 (Reid et al., 2024), Grok-1.5 Vision (xAI, 2024), and open-source VLMs (Koh et al., 2023;Dai et al., 2024;Liu et al., 2024a) predict the next language token in a sequence of text and image inputs. \n\nHowever, the multimodal nature of VLMs raises questions about how much they are using the available vision and text modalities in multimodal tasks, a concept referred to as the multimodal degree of a VLM (Parcalabescu & Frank, 2023). By examining this question, one determines if a VLM relies more on vision or text when giving answers. Prior work has developed MM-SHAP to measure the multimodal degree of VL encoders (Parcalabescu & Frank, 2023) and found that different architectures tend to be either balanced, or rely more on the vision or text modality. But so far, we have not seen any analysis of the multimodal degree of recent autoregressive VLM decoders. \n\nSince VLM decoders can produce natural language explanations (NLEs) for their own answers (e.g., Q: 'What color is the street paint in the image?' Answer: 'Yellow.' NLE: 'Due to construction'), this raises the further question as to the degree to which they use either input modality when generating an answer versus an explanation. So, we aim to explore whether VLMs rely more on the image or text when generating answers or explanations, respectively. \n\nWhen it comes to NLEs, it is not only crucial to evaluate how much a VLM uses the different modalities when generating the NLE, but also to assess the self-consistency of a VLM's self-explanation.",
            "score": 0.5120533269195158,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1603
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 174,
                    "matchedPaperCorpusId": "258947258"
                },
                {
                    "start": 174,
                    "end": 191,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70458984375
        },
        {
            "corpus_id": "271533538",
            "title": "Large Language Models for Human-Like Autonomous Driving: A Survey",
            "text": "LLMs and MLLMs have revolutionized the field of natural language processing and beyond, enabling machines to understand, generate, and reason with natural language at an unprecedented level. LLMs are trained on vast amounts of text data, allowing them to capture intricate linguistic patterns, contextual dependencies, and semantic relationships. The input to an LLM is typically represented as a set of tokens X = {x i,j | i \u2208 1, . . . , N , j \u2208 1, . . . , T i }, where i denotes the i-th sentence, j denotes the j-th token within the sentence. The output of the LLM, denoted as y i,j , is usually a probability distribution over the next token. Thus, an LLM can be represented as a function f that maps the input to the output: \n\nBuilding upon the foundation of LLMs, MLLMs have been proposed to handle multimodal inputs, such as images, videos, text, audio, point clouds, and even depth or thermal information [18]. Mathematically, an MLLM can be formulated as a function f \u2032 that maps a set of multimodal inputs x \n\ni,j , ..., x (n) i,j to an output y i,j : \n\ni,j , ..., x \n\ni,j \u2208 X (k) represents the input from modality k (e.g., image I, text T , point cloud P) for the (i, j)-th example, and y i,j is the model's output (e.g., a control command for AVs) for the same example. \n\nThe key components of an MLLM include modality encoders E (k) , a multimodal fusion module F , a language model f , and an output projector P . Formally, the MLLM's output can be expressed as: \n\nSeveral influential MLLMs, such as BLIP-2 [19], LLaVA [20], and Flamingo [21], have laid the foundation for the development of this field. These models have demonstrated impressive performance in tasks like Visual Question Answering (VQA), image captioning, and multimodal reasoning.",
            "score": 0.5119872342612194,
            "section_title": "II. LARGE LANGUAGE MODELS AND MULTIMODAL LARGE LANGUAGE MODELS",
            "char_start_offset": 4571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1477
                },
                {
                    "start": 1480,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "258564264"
                },
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "257663583",
            "title": "MAGVLT: Masked Generative Vision-and-Language Transformer",
            "text": "In this multimodal generative modeling, the model often ignores the cross-modal context and produces an output that is biased to the within-modal statistics. For example, in the I2T task of Figure 4, the model should predict the masked word token as 'cat' by the given image, however, the model often rather outputs 'dog' since 'dog jumps' is more likely occurred than 'cat jumps' before the text of 'on grass field' in the set of training text sequences.\n\nThus, in order to reduce such bias, we propose a simple yet effective additional learning task, named selective prediction on the mixed context (MixSel), which is described Figure 5. MixSel learning tasks corresponding to three multimodal tasks. in Figure 5. As shown in the figure, two different input contexts are mixed in a half-and-half concatenated manner, and one of them is randomly selected to be the target context in generation. Here, a special token is appended to inform the selected context, for instance <LEFT> or <RIGHT> is used for the horizontally combined image or the concatenated text sequence while <TOP> or <BOTTOM> is used for the vertically combined image. Also, when two different text sequences are concatenated, another special token, <SEP>, is inserted between them. The MixSel objective is denoted as L MS , and for instance L MS on the I2T task can then be defined as\n\nwhere \u03c6 is the mixture function on the two images X 1 and X 2 , and \u2208 {1, 2} represents the selected context. From this MixSel training task, the model is able to attend more carefully to the appropriate span of the crossmodal context and improve the accuracy of the cross-modal attention by mixing the original cross-modal content with randomly unrelated one. This could make the model to utilize the cross-modal context more trustfully and hence more often in generation as the training progresses. Therefore, MixSel indeed helps in reducing the overlooking of the cross-modal context and circumventing the within-modal bias problem in test-time generation. Note that it is different from the previous mix-based data augmentation techniques [26,69,70] in that we retain the information of the original contexts entirely and randomly select",
            "score": 0.5119748902099835,
            "section_title": "Selective Prediction on Mixed Context",
            "char_start_offset": 15278,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414306640625
        },
        {
            "corpus_id": "273185450",
            "title": "Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative Approach Yolo With Video-llava",
            "text": "Large language models (LLMs) employ transformer-based neural network architecture and are trained on vast amounts of text data to identify patterns and relationships in natural language. These models have demonstrated the ability to both interpret and produce text like human language. However, LLMs are primarily focused on processing text inputs and outputs, lacking the ability to directly perceive and reason about visual information like images and videos. This limitation has motivated research into extending LLMs to handle multi-modal inputs beyond just text. Vision-language models (VLMs), also known as large vision-language models (LVLMs), aim to bridge this gap by enabling LLMs to understand and generate responses based on both textual and visual inputs. These models typically involve integrating computer vision components like convolutional neural networks (CNNs) or vision transformers (ViTs) [11] to extract visual features, which are then aligned and fused with the text representations within the LLM architecture. Approaches like pre-training on large image-text or video-text datasets and instruction tuning [17] have shown promising results in grounding LLMs in visual contexts for tasks like visual question answering, image/video captioning, and multimodal reasoning. Recent visual-language models like Video-LLaVA [4] take this a step further by introducing a unified approach to visual-language modeling, integrating both image and video data into a single, coherent representation for large language models. Video-LLaVA is based on an auto-regressive language model, which is fine-tuned on multimodal instruction-following data generated by Llava1.5 [10] and VideoChat [12]. \n\nAs shown in Figure 2 the structure of Video-LLaVA is designed to effectively integrate visual and textual data into a unified representation for enhanced multi-modal reasoning. Central to its structure are the LanguageBind encoders [18], which extract features from both images and videos and map these visual signals into a shared textual feature space, facilitating a unified visual representation. These encoders are initialized from OpenCLIP [14], ensuring that the visual features are aligned with the language features, thereby creating a unified visual representation.",
            "score": 0.5119704064148036,
            "section_title": "Large Vision-Language Model",
            "char_start_offset": 6479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "262055614",
            "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants",
            "text": "This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.",
            "score": 0.5119531663144481,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "241583700",
            "title": "Investigating Negation in Pre-trained Vision-and-language Models",
            "text": "Following the success of pre-trained language models (Devlin et al., 2019), multimodal tasks such as visual question answering (Antol et al., 2015) and visual commonsense reasoning (Zellers et al., 2019) have also recently been approached using a pretrain-and-finetune method. Pre-trained vision-and-language models fall into two categories: single-stream models (e.g., Li et al. 2019;Chen et al. 2020) and two-stream models (e.g., Lu et al. 2019;Tan and Bansal 2019). While two-stream models have separate encoders for the visual and textual modalities and then jointly process them using a third encoder, single-stream models process both the textual and visual features together from the start. \n\nWhile these models have been very successful, pushing state-of-the-art results up across tasks, not much is known about their capabilities and limitations. From that perspective, the closest works to ours are by Li et al. (2020) and Cao et al. (2020). Li et al. (2020) analyse attention heads in the model and show that words in the text are correctly mapped to image regions which correspond to them. Cao et al. (2020) probe pre-trained visionand-language models and report similar observations. Their work focuses more on the grounding aspect of vision-and-language models, whereas our work focuses more on linguistic and reasoning abilities. Cao et al. (2020) also show that multimodal pre-trained models learn some linguistic knowledge, however, they do not analyse this any deeper than providing results on several probing tasks, none of which involves negation.",
            "score": 0.5118893415079083,
            "section_title": "Vision-and-language models",
            "char_start_offset": 4596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 74,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 127,
                    "end": 147,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 181,
                    "end": 203,
                    "matchedPaperCorpusId": "53734356"
                },
                {
                    "start": 370,
                    "end": 385,
                    "matchedPaperCorpusId": "199528533"
                },
                {
                    "start": 385,
                    "end": 401,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 432,
                    "end": 447,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 912,
                    "end": 928,
                    "matchedPaperCorpusId": "218610661"
                },
                {
                    "start": 933,
                    "end": 950,
                    "matchedPaperCorpusId": "218665405"
                },
                {
                    "start": 952,
                    "end": 968,
                    "matchedPaperCorpusId": "218610661"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "218665405"
                },
                {
                    "start": 1345,
                    "end": 1362,
                    "matchedPaperCorpusId": "218665405"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "277112744",
            "title": "SemEval-2025 Task 1: AdMIRe - Advancing Multimodal Idiomaticity Representation",
            "text": "In order to support participation in the shared task by teams who wish to work only with text models (which lowers complexity and computational costs), we generated descriptive captions for each image. These were obtained by using the LLaVA-HF/v1.6-mistral-7b-hf3 (LLaVA) model, a large vision-language model specifically designed for tasks requiring multimodal reasoning (Liu et al., 2024). LLaVA integrates a vision encoder to extract semantic features from images and a large language model to process these features and generate text. By employing the prompt \"What is shown in this image?\", the model generates captions that describe the content of the input images. The workflow ensures that the visual and textual components of the model work in harmony to produce accurate and contextually relevant descriptions. To ensure the quality of the generated captions, all outputs were reviewed and verified by human evaluators. \n\nFor items in Portuguese, we provide captions in both English and Portuguese. \n\n3 Task Description",
            "score": 0.511646012627006,
            "section_title": "Caption Generation",
            "char_start_offset": 7299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1028
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64501953125
        },
        {
            "corpus_id": "265220875",
            "title": "MMoE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts",
            "text": "We introduce the models and multimodal prediction tasks that we consider for experiments in this section. More information related to experimental settings is available in Appendix \u00a7I. \n\nModel We implement MMOE on top of three categories of multimodal language models to show its widespread applicability on top of many base models (see Figure 5 for an illustration). Detailed model information is available in Appendix \u00a7A. These three model categories include: 1. Fusion-based vision language models (VLM) uses cross-attention to learn multimodal interactions between all regions of the image with all words in the input text. Examples of such models include ALBEF (Li et al., 2021), LXMERT (Tan and Bansal, 2019) and BLIP (Li et al., 2022). \n\n2. Multimodal-extended LLMs (MLLM) includes models like BLIP2 (Li et al., 2023) (Zeng et al., 2022) and the video understanding model (Zhang et al., 2023).",
            "score": 0.5115110422804224,
            "section_title": "Experimental Setup",
            "char_start_offset": 16391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 666,
                    "end": 683,
                    "matchedPaperCorpusId": "236034189"
                },
                {
                    "start": 692,
                    "end": 714,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 724,
                    "end": 741,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 807,
                    "end": 824,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 879,
                    "end": 899,
                    "matchedPaperCorpusId": "259370676"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "245117353",
            "title": "Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation",
            "text": "Inspired by the success of large-scale pre-training model in natural language processing, various vision-language pre-training methods that aim to learn multimodal representations from large-scale image-text pairs have been proposed. Once pretrained, these pre-trained checkpoints can be fine-tuned to perform various downstream tasks. This simple pretrain-and-finetune paradigm has recently shown great potential in many challenging vision-and-language tasks such as visual question answering, image-text retrieval, image captioning and visual entailment [6,26,30,31,41]. \n\nVision-language downstream tasks fall into two categories: understanding and generation. The understanding tasks include visual question answering (VQA), visual entailment (VE), image classification and image-text retrieval. Most existing vision-language pre-training methods formulate tasks in this category as discriminative tasks, and they require the model to select an answer from a predefined answer list, e.g. for VQA, existing methods formulate it as a multi-answer classification task and feed the [CLS] output representation to an additional linear classifier [6,14,23,25,31]. These tasks typically require the model to have a coarse understanding of the semantic information of the image and text, e.g., is the text describing the content of the image? what is the relationship between the image and text? entailment, neutral, or contradictory? In contrast, generation tasks usually require the model to generate a complete sentence conditioned on an image. A typical example is image captioning which requires the model to output a sentence describing the content of the image. Existing vision-language methods employ BERT-like objectives, such as masked language modeling and image-text matching [6,23,26,41] to learn multimodal representations. They perform well on understanding tasks but cannot be directly applied to generative tasks. \n\nWe propose Unified Multimodal Pre-training for Vision-Language Understanding and Generation (UniVL) that handles understanding and generative tasks with shared parameters. We first use an image encoder and a text encoder to encode the image and text separately. Then we use a multimodal encoder to fuse the image and text features with cross-attention.",
            "score": 0.5114226423320577,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 562,
                    "end": 565,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "208637516"
                },
                {
                    "start": 568,
                    "end": 571,
                    "matchedPaperCorpusId": "201317624"
                },
                {
                    "start": 1145,
                    "end": 1148,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1148,
                    "end": 1151,
                    "matchedPaperCorpusId": "219573512"
                },
                {
                    "start": 1151,
                    "end": 1154,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 1157,
                    "end": 1160,
                    "matchedPaperCorpusId": "208637516"
                },
                {
                    "start": 1784,
                    "end": 1787,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 1787,
                    "end": 1790,
                    "matchedPaperCorpusId": "231839613"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 1793,
                    "end": 1796,
                    "matchedPaperCorpusId": "201317624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "258947627",
            "title": "Semantic Composition in Visually Grounded Language Models",
            "text": "Vision-language deep learning research [46] synthesizes advances in computer vision and natural language processing to build models capable of solving multimodal tasks like imagetext matching, image2text captioning, and text2image generation. There are a number of methods for learning joint representations of vision and language, but most leverage the transformer architecture [90]. \n\nEncoder models generally either use a dual unimodal encoder architecture [70], a single cross-modal encoder [17], or a combination [80]. Image-conditioned text generation models often consist of an image encoder that feeds into a causal transformer decoder [43]. Textbased image generation models are quite varied but often condition an image diffusion model on the output of a text encoder [73].",
            "score": 0.5112723849247478,
            "section_title": "Vision-Language Deep Learning",
            "char_start_offset": 6163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 783
                }
            ],
            "ref_mentions": [
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "253510668",
            "title": "Few-shot Multimodal Sentiment Analysis Based on Multimodal Probabilistic Fusion Prompts",
            "text": "Inspired by recent works, such as GPT-3 [3] and LM-BFF [6], we further design multimodal demonstrations chosen by similarity scores, as shown on the right side of Figure 1. Specifically, we first feed the raw text input  and image prompt  from the image input , that can be regarded as text description of image, into a pre-trained language model, such as SBERT [21], to obtain embeddings . \n\nwhere \u2295 is the concatenation operation.  represents the aspect term and is optional. For the fine-grained task, we combine the text with the aspect term, while for the coarse-grained task, there is no aspect term. Next, we compute the similarity scores between each query instance   = (  ,   ,   ) and support set with   instances for the -th label category,  ( \n\nWe then select the multimodal support instance with the highest similarity score for each label category . \n\nFinally, we convert the multimodal support instances with the highest similarity scores into P  templates, with <mask> tokens replaced by different labels from L. These resulting multimodal prompts are denoted as P , and we concatenate them with the query instance   . \n\nwhere |L| is the number of sentiment categories in each dataset.",
            "score": 0.511111117224351,
            "section_title": "Multimodal Demonstrations",
            "char_start_offset": 15952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1201
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "229923710"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "270765005",
            "title": "MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data",
            "text": "We train a model to generate images from multimodal prompts of interleaved text and images such as\"aman and hisdog in ananimated style.\"We bootstrap a multimodal dataset by extracting semantically meaningful image crops corresponding to words in the image captions of synthetically generated and publicly available text-image data. Our model, MUMU, is composed of a vision-language model encoder with a diffusion decoder and is trained on a single 8xH100 GPU node. Despite being only trained on crops from the same image, MUMU learns to compose inputs from different images into a coherent output. For example, an input of a realistic person and a cartoon will output the same person in the cartoon style, and an input of a standing subject and a scooter will output the subject riding the scooter. As a result, our model generalizes to tasks such as style transfer and character consistency. Our results show the promise of using multimodal models as general purpose controllers for image generation.",
            "score": 0.5110899812987143,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "270067594",
            "title": "Multi-modal Generation via Cross-Modal In-Context Learning",
            "text": "Problem Statement: Given a sequence of text prompts or interleaved imagetext prompts (eg.story sequence) presented over multiple instances from t 1 to t n\u22121 (Fig. 2), our task is to generate the image at time t n while maintaining the context of the earlier text and image prompt sequence.Here, n represents the length of the sequence.Formally, our aim is to process interleaved sequences of text y = {y a } n a=1 and images x = {x a } n\u22121 a=1 pairs, where y and x represent text and image respectively.Then, our objective is to generate a novel image at time t n , retaining the context of earlier prompts.In this work, we leverage the capabilities of pre-trained and frozen large language models [31,35,48] and diffusion models [37,38] to generate these images with minimal training efforts.Baseline: Our method builds upon the recent GILL approach [22].In contrast to conventional diffusion models utilizing clip text encoders [38], GILL adopts a pre-trained LLM.This fusion of LLM with the diffusion model enables image generation within extensive multimodal input.In processing text-image sequences, GILL [22] initially transforms the image into LLM embedding space.Additionally, it introduces specialized image tokens within the LLM's vocabulary to represent the final image to be generated by the model.These image tokens are aligned with the clip text encoder through a learnable transformer module named GILLMapper.Subsequently, GILLMapper's output serves as input to the diffusion model [22] during inference.\n\nWhile our baseline GILL enables using lengthy multimodal story sequences, it faces several limitations: (a) The output of GILLMapper, serving as the pretrained diffusion model's input, tends to generate holistic images representing all prompts in a story sequence.This results in the loss of fine-grained details specific to the current prompt at t n .For instance, as shown in (Fig. 1 row 1), the baseline generates a holistic image combining information from various prompts, such as party foods and fruits, even when the final prompt corresponds solely to a pasta salad.",
            "score": 0.5110618008096133,
            "section_title": "Method",
            "char_start_offset": 9267,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1519
                },
                {
                    "start": 1521,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1873
                },
                {
                    "start": 1873,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 705,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "258959284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87939453125
        },
        {
            "corpus_id": "271974534",
            "title": "Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis",
            "text": "In this zero-shot setting, the task is to answer the question about the microscopic image solely based on the small-scale model's internal parametric knowledge. sLAVA comprises a vision encoder to capture the global context of microscopic images, and a text encoder that interprets end-user questions and the auxiliary user-provided image information. The image-grounded text encoder facilitates cross-modal learning by integrating visual information directly into text understanding, thereby generating a comprehensive multimodal representation grounded in the image's visual content. The image-grounded text decoder then synthesizes coherent and contextually relevant textual outputs based on the generated multimodal representations. Finally, the framework is jointly optimized using the binary cross-entropy loss for positive image-text matching and language modeling loss for contextually relevant text generation to answer end-user questions. \n\nimage, effectively highlighting more relevant visual regions along with their contextual relationships to understand and ground the questions within visual concepts. We incorporate a <cls> token to attend to and aggregate information from all image regions. The higher-level visual semantic representation of the global (<cls>) token represent the summary of the input image. (b) The text encoder plays a crucial role in analyzing and interpreting user input to understand the nature of the question. It leverages supporting text descriptions associated with the image to extract key details and provide accurate and relevant answers. We insert <image> token at the image location in the interleaved multimodal input. We append a <Encode> token to the tokenized text to facilitate multimodal integration, with its output embedding representing the fused image-text representation. To better capture the nuances of language and context, the text encoder leverages a pre-trained language model, Llama-2-7b (Touvron et al., 2023), to compute a high-level representation that captures the semantic meaning and relationships within the end-user question. We fine-tune Llama-2-7b using Dynamic Adaptation of Mixture of Quantized Parameter-Efficient Experts (DyA-MOQPEs) technique (details in the technical appendix) using the instruction-following dataset generated by GPT-4.",
            "score": 0.5104967811361092,
            "section_title": "Processed text",
            "char_start_offset": 9236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2320
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426025390625
        },
        {
            "corpus_id": "270924310",
            "title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation",
            "text": "Similarly, Lu et al. [27] introduced a transformer sequence-to-sequence model that performs a variety of vision and language tasks without requiring task-specific branches, trained on over 90 datasets related to vision and language. \n\nFurthering the multimodal approach, Singh et al. [41] created a model with separate encoders for images, text, and multimodal integration, which was pretrained on unimodal and multimodal losses for 35 tasks across vision, language, and vision-language areas. Another contribution by Singh et al. [49] includes a multimodal vision-language model with a shared multiway transformer backbone, trained on masked data modeling across modalities, achieving state-of-the-art performance on various vision and vision-language benchmarks. Li et al. [26] explored cross-modal contrastive learning to unify representations across modalities with a unified transformer on image and text data, enhancing the synergy between these modalities. \n\nIn this research, we harness the domain adaptation capabilities of generative models to customize the sensor decoder for specific scenarios of sensor data representation. This approach is designed to significantly enhance the visualization quality of the sensor model's outputs.",
            "score": 0.5104482886178727,
            "section_title": "RELATED WORK",
            "char_start_offset": 14529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1244
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 284,
                    "end": 288,
                    "matchedPaperCorpusId": "244954250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "274597781",
            "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models",
            "text": "Multimodal Vision Language Models. Many approaches have been proposed to adapt decoder-only LLMs trained with autoregressive loss for image understanding [10-13, 32, 33] and multimodal generation tasks [16, 19-21, 24-26, 34-37]. These models consist of an LLM, an image encoder, an image decoder (for multimodal-output VLMs), and adapters between the LLM and visual components. The success of diffusion-based image generation has led to the widespread use of stable diffusion as the decoding architecture in multimodal-output models [16,19,21,24,25,35]. For the image encoding part, most architectures use a vision transformer (ViT), often that of CLIP [16,19,21,24,25,[34][35][36], due to the effectiveness of ViT embeddings in encoding image semantics [38]. The outputs from the visual encoder are either directly projected into the LLM embedding space [21,24,25,38] or, in the case of multimodal generation, often mapped to a discrete set of tokens [16,19,34]. A drawback of these methods is the complexity of encoding architecture. Moreover, the image-text contrastive loss can make the image representations resemble textual ones excessively [33], with text often favored as the preferred modality [17,19,25]. Alternative approaches use VQ-VAEs [39,40] to encode and decode the images and train the LLMs from scratch on multimodal datasets [20,26]. This strategy fuses the visual and textual representations early in training instead of using complex pipelines to bridge the modality gap [32,38,41,42] and eliminates the need for adapters, greatly simplifying the architecture design. \n\nSpecial Tokens, Memory Tokens, Registers. The importance of special tokens to store and redistribute global information was emphasized by Burtsev et al. [31].",
            "score": 0.5104062609238976,
            "section_title": "Related Works",
            "char_start_offset": 4131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 537,
                    "end": 540,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 549,
                    "end": 552,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 657,
                    "end": 660,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 669,
                    "end": 673,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 855,
                    "end": 859,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 862,
                    "end": 865,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "263605855"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "270703300"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "258822817"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "267750101"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "261696650"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1493,
                    "end": 1497,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 1500,
                    "end": 1503,
                    "matchedPaperCorpusId": "270357343"
                },
                {
                    "start": 1503,
                    "end": 1506,
                    "matchedPaperCorpusId": "247244904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "274860024",
            "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
            "text": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.",
            "score": 0.5102010513923,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "248227296",
            "title": "MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration",
            "text": "Research in multimodal understanding and generation brings together the subfields of vision and language in AI. Significant progress has been made on imagetext understanding and generation tasks, such as CLIP [54] for image-text retrieval and DALL-E [55] for text-to-image generation. This progress has been made possible with large-scale image-text datasets [6,53,61,64,72] that are collected from the web. However, progress in the video-text domain lags due to challenges in data collection and modeling of spatiotemporal information. \n\nMugen walks to the right and collects a few coins, and jumps to the right and killed a snail, and walks a bit.",
            "score": 0.5101380598406559,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 649
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 362,
                    "end": 365,
                    "matchedPaperCorpusId": "14579301"
                },
                {
                    "start": 365,
                    "end": 368,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 368,
                    "end": 371,
                    "matchedPaperCorpusId": "232092726"
                },
                {
                    "start": 371,
                    "end": 374,
                    "matchedPaperCorpusId": "3104920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40185546875
        },
        {
            "corpus_id": "274305717",
            "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
            "text": "The main objective of this work is to establish a simple and unified autoregressive transformer for both visual and language modalities. In this model, visual and language data can be input and output in the same discrete encoding format. For the language modality, there are already well-developed text tokenizers and large language models (LLMs) [23,64,66] that have been extensively trained on massive text data. However, how to construct an effective tokenizer in the visual modality remains to be explored. \n\nTherefore, we propose semantic discrete encoding as a tokenizer for the visual modality to generate visual tokens that are well-aligned with language. Based on this, we propose MUSE-VL, a model capable of handling mixed visual and language tokens, supporting both visual understanding and generation tasks. This section first introduces the visual tokenizer proposed in our work, followed by the unified vision-language model built upon it.",
            "score": 0.5101122198941723,
            "section_title": "Method",
            "char_start_offset": 8554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1512451171875
        },
        {
            "corpus_id": "265351653",
            "title": "Multimodal Large Language Models: A Survey",
            "text": "The exploration of multimodal language models integrates multiple data types, such as images, text, language, audio, and other heterogeneity. While the latest large language models excel in text-based tasks, they often struggle to understand and process other data types. Multimodal models address this limitation by combining various modalities, enabling a more comprehensive understanding of diverse data. This paper begins by defining the concept of multimodal and examining the historical development of multimodal algorithms. Furthermore, we introduce a range of multimodal products, focusing on the efforts of major technology companies. A practical guide is provided, offering insights into the technical aspects of multimodal models. Moreover, we present a compilation of the latest algorithms and commonly used datasets, providing researchers with valuable resources for experimentation and evaluation. Lastly, we explore the applications of multimodal models and discuss the challenges associated with their development. By addressing these aspects, this paper aims to facilitate a deeper understanding of multimodal models and their potentiality in various domains.",
            "score": 0.5101122198941723,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "273502896",
            "title": "CausalGraph2LLM: Evaluating LLMs for Causal Queries",
            "text": "In this work we focus on textual encodings into LLMs, however with the developments of multimodal models, we can test LLM's ability to answer causal queries when presented with image inputs. We performed our experiment on GPT-4 model with T=0. Future works can be built upon to test better image inputs for multimodal models.",
            "score": 0.5101122198941723,
            "section_title": "C.6 Multimodal models",
            "char_start_offset": 38073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 325
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "276782523",
            "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
            "text": "In this section, we provide an introduction to the base unified generative model, which serves as the foundation for our approach to multimodal understanding and generation. It integrates both auto-regressive and diffusion modeling techniques to achieve joint multimodal understanding and generation [74]. \n\nThe base unified generative model is built upon a pretrained large language model Phi-1.5 [38]. The architecture of the base model is largely inherited from it with minimal modifications to accommodate multimodal input. Specifically, a QK-Norm operation is added to each attention layer [13,70] to enhance training stability. The embedding layer is expanded by adding learnable embeddings for discrete image tokens, enabling the joint encoding of text and image modalities. The final model consists of 24 transformer layers with a total of 1.5 billion parameters. In the proposed DoraCycle, the parameters of the base model are fixed and the LoRA components introduce 4.7 million trainable parameters, accounting for approximately 0.32% of the total model parameters. \n\nThe model tokenizes both text and image data into discrete tokens to create a unified space, maintaining a unified vocabulary. Text data is tokenized using a pre-trained text tokenizer of Phi-1.5 [38]. The codebook size of text tokens is 58498. For images, a quantizer like MAGVIT-v2 [78] is used. This quantizer maintains a codebook of size K = 8, 192 and encodes images at a resolution of 512\u00d7512 into 32 \u00d7 32 discrete tokens. The model utilizes the unified tokenization strategy ensuring that both modalities can be processed consistently, allowing the model to handle multimodal inputs within a shared framework. Besides the text and image tokens, the model also involves different special tokens, like <sot>, <eot>, <soi>, and <soi>, which are used to denote the start and end of the text and image tokens. Among them, there are some special tokens that indicate the task to be executed, where the <mmu> indicates the model should do the understanding task and the <t2i> indicates the model should generate image tokens based on the given text.",
            "score": 0.5101122198941723,
            "section_title": "A. Model Details",
            "char_start_offset": 31031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 599,
                    "matchedPaperCorpusId": "256808367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "270562151",
            "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model",
            "text": "Vision-Language Models (VLMs). VLMs are a type of multimodal model designed to process both visual and textual data. These models generate sentences in an autoregressive manner, predicting the probability distribution of the next token based on the context provided. In this framework, we consider a VLM as a policy model \u03c0 \u03b8 (y|x) parameterized by \u03b8. The policy \u03c0 \u03b8 is constructed to handle input prompts x \u2208 X , which include both image and text, and to generate a test response y \u2208 Y. Given an input x, the VLM \u03c0 \u03b8 generates a text response y in an autoregressive manner: \n\n(1)",
            "score": 0.5101122198941723,
            "section_title": "B.1. Vision-Language Models",
            "char_start_offset": 31020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 580
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "258676569",
            "title": "OneCAD: One Classifier for All image Datasets using multimodal learning",
            "text": "The models which use text rendered on image are vocabulary-free models.These model are an attempt towards removing the vocabulary bottleneck.PIXEL (Rust et al., 2022) was inspired from (Salesky et al., 2021).It is trained to perform language modeling from image-pixels.The input text is rendered on image-pixels and given as an input to the model.Model reconstructs the input image to predict masked text to perform multiple language tasks.CLIPPO (Tschannen et al., 2022) is a multi-  Multimodal: DONUT (Kim et al., 2021) learns text in the input image in various formats to peform documents understanding tasks.Other such multi-modal models involving text are Dessurt (Davis et al., 2023), GIT2 Imageto-text generation (Wang et al., 2022b), Pali (Chen et al., 2022), CLIP (Shen et al., 2021), ALIGN (Jia et al., 2021), LIMoE (Mustafa et al., 2022), Zero-shot text-to-image generation (Ramesh et al., 2021), and Pix2Struct (Lee et al., 2022).",
            "score": 0.5101122198941723,
            "section_title": "Multimodal and Visual text:",
            "char_start_offset": 4034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 71,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 942
                }
            ],
            "ref_mentions": [
                {
                    "start": 669,
                    "end": 689,
                    "matchedPaperCorpusId": "247839830"
                },
                {
                    "start": 800,
                    "end": 818,
                    "matchedPaperCorpusId": "231879586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "263608981",
            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
            "text": "Recent advancements in multimodal Large Language Models, such as MiniGPT-4 (Zhu et al., 2023), have primarily concentrated on multimodal comprehension, enabling the processing of images as sequential input. The Integrated Vision-Language Encoding Module is designed to extend the capabilities of LLMs from mere comprehension to active generation in multimodal contexts. Generative  vokens play a crucial role in this module, enabling the translation of raw visual inputs into a format that LLMs can process and utilize for subsequent generation tasks. \n\nMultimodal Encoding Each text token is embedded into a vector e text \u2208 R d , while the pretrained visual encoder transforms each input image into the feature e img \u2208 R 32\u00d7d . These embeddings are concatenated to create the input prompt features. \n\nGenerative Vokens Since the original LLM's V vocabulary only includes the textual tokens, we need to construct a bridge between the LLM and the generative model. Therefore, we introduce a set of special tokens V img = {[IMG1], [IMG2], . . . , [IMGn]} (by default n = 8) as generative vokens into the LLM's vocabulary V . The LLM's output hidden state for these vokens is harnessed for subsequent image generation, and the positions of these vokens can represent the insertion of the interleaved images. With all pretrained weights \u03b8 pretrained in MiniGPT-4 fixed, the trainable parameters include extra input embedding \u03b8 voken input and output embedding \u03b8 voken output . \n\nParameter-Efficient Fine-Tuning (PEFT) Parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019;Hu et al., 2021;Li & Liang, 2021) is critical in training Large Language Models (LLMs), employed to adapt LLMs to downstream tasks without the need for extensive retraining. In PEFT, rather than updating all the parameters of a model, only a small subset of parameters is trained.",
            "score": 0.5100476846332972,
            "section_title": "MULTIMODAL UNDERSTANDING MODULE",
            "char_start_offset": 9102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 1553,
                    "end": 1575,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "248721934",
            "title": "Deep learning and synthetic media",
            "text": "increasingly easy to guide image generation directly with text. DALL-E, a new multimodal Transformer model trained on a dataset of text-image pairs, is capable of generating plausible images in a variety of styles simply from a text description of the desired output . DALL-E's outputs can exhibit complex compositional structure corresponding to that of the input text sequences, such as \"An armchair in the shape of an avocado\", \"a small red block sitting on a large green block\", or \"an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants\". DALL-E has been developed jointly with another multimodal model, called CLIP, capable of producing a natural language caption for any input image . Using CLIP to steer the generation process, it is also possible to produce images with GANs from natural language descriptions of the desired output (Galatolo et al., 2021;Patashnik et al., 2021;see Fig. 5).\n\nIf these trends continue-and there is no reason for them to slow down significantly as hardware improvement and architectural breakthroughs continue to spur larger and more efficient models, it is only a matter of time before DL algorithms allow us to generate high-resolution stylized or photorealistic samples of arbitrary scenes that are consistently indistinguishable from human-made outputs. In the domain of static visual media, that goal is already within sight for medium to large resolutions (around 1024x1024 pixels at the time of writing). The domain of dynamic visual media presents a significantly greater challenge, as the spatiotemporal consistency of the scene needs to be taken into account. Nonetheless, it is plausible that we will be able to synthesize realistic and coherent video scenes at relatively high resolution in the short to medium term, beyond mere face swapping in existing videos.",
            "score": 0.5098108585499005,
            "section_title": "Totally synthetic DLSAM",
            "char_start_offset": 21091,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 898,
                    "matchedPaperCorpusId": "231749958"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "265302359",
            "title": "Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report",
            "text": "This approach is highly effective for capturing the context and meaning of each token in the input text, enabling the model to better understand the overall meaning of the text. When processing an input text T , the text encoder generates a sequence of embeddings w cls , w 1 , ..., w N , where w cls represents the embedding of the [CLS] token, and the remaining w i represent the embeddings of the individual input tokens. This sequence of embeddings is then passed to the multimodal encoder to be combined with the visual embeddings generated by the vision encoder. \n\nMultimodal Encoder The multimodal encoder is a complex module that plays a critical role in enabling the model to learn the relationships between the visual and textual inputs. To achieve this, it incorporates an additional cross-attention (CA) layer that sits between the self-attention (SA) layer and the feed-forward network (FFN) for each transformer block of the text encoder. By doing so, the model can attend to both the textual and visual inputs and build better representations of the image-text pair. To create a multimodal representation of the image-text pair, the text input is modified by appending a taskspecific [Encode] token at the end of the sequence, which is then fed into the multimodal encoder. The output embedding of this token is used as the final representation of the image-text pair. The embedding layers, CA layers, and FFN share similar functionality between encoding and decoding tasks, which means that they can be shared to improve training efficiency and benefit from multi-task learning. Additionally, the cross-attention layer introduces another set of attention weights to the model, which requires additional computation and increases the number of parameters to be learned. However, this additional complexity is necessary to enable the model to learn Figure 3: The overall architecture of our model, which comprises a vision encoder responsible for processing visual data, a language encoder that focuses on comprehending textual information, and a multimodal encoder that combines the input from both the vision and language encoders to fuse comprehensive representations.",
            "score": 0.5098058096807393,
            "section_title": "Retrieval System",
            "char_start_offset": 15648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2185
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "268091222",
            "title": "EAMA : Entity-Aware Multimodal Alignment Based Approach for News Image Captioning",
            "text": "MLLMs (Multimodal Large Language Models) have been proved highly effective across diverse multimodal tasks, so we craft our methodology based on MLLMs. Figure 2 offers a concise overview, delineating the alignment training and caption generation stages. Typically, MLLM architectures comprise an image encoder, a Vision-Language (V-L) Connector, and a large language model. \n\nDuring our alignment training stage, we commence by simultaneously training the MLLM on three tasks: Entity-Aware Sentence Selection, Entity Selection, and News Image Captioning. This alignment aims to enhance the MLLM's capability to handle multimodal entity information. \n\nWe employ a language modeling loss as the optimization objective across all tasks. Typically, within a given dataset D, each data instance comprises three components: the input image I, the textual input context T , and the target output text O = (o 1 , ..., o n ), with o i denoting a token. Thus, the general multimodal language modeling loss can be formulated as: \n\n(1) where the conditional probability P is calculated by neural network with multimodal features. \n\nFor example, in News Image Captioning task, the input image I is the news image, the textual input context T is the news article, while the target output text O is the given caption. Thus for a News Image Captioning dataset D, the corresponding News Image Captioning loss can be denoted as: \n\nIn our caption generation stage, we leverage our aligned MLLM to extract key entities and entityrelated sentences from news articles using multimodal input, without incurring additional modules. Subsequently, our aligned MLLM integrates this information to supplement the original textual input context and generate the news image caption.",
            "score": 0.5097879853746508,
            "section_title": "Overview",
            "char_start_offset": 8380,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1752
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "270094821",
            "title": "Multi-Modal Generative Embedding Model",
            "text": "The Multi-Modal Generative Embedding Model (MM-GEM) presents a unified approach to multimodal learning by integrating generative and embedding objectives within a single Large Language Model (LLM).Our experiments demonstrate that these two objectives do not significantly conflict with each other.MM-GEM achieves competitive performance across a range of tasks, including cross-modal retrieval, zero-shot classification, and image captioning.A key contribution is the PoolAggregator, enhancing the model's ability to handle fine-grained tasks.Additionally, MM-GEM's advanced text module significantly improves performance on long-form text retrieval, showcasing the benefits of leveraging a robust LLM for text processing.\n\nMM-GEM represents a significant step towards unified multi-modal models, yet there are still many subsequent potential directions: 1) We mainly focus on image captioning for generative tasks in this work, the performance impact of adding plain language data needs to be further investigated.2) MM-GEM enables LLMs generate discriminative outputs besides language tokens, this may benefit multi-modal large language model by retrieving or grounding visual information efficiently.Further investigation into these aspects will be explored in future work.",
            "score": 0.5097448186012934,
            "section_title": "Conclusion",
            "char_start_offset": 25786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 722
                },
                {
                    "start": 724,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "264490812",
            "title": "M2C: Towards Automatic Multimodal Manga Complement",
            "text": "Vision-Language Models. The success of visionlanguage models can be attributed to the Transformer architecture (Vaswani et al., 2017;Radford et al., 2021;Li et al., 2020;Sharma et al., 2018;Miech et al., 2019;Bai et al., 2023b;Zhu et al., En: \n\n\u2022 \"A golden ... \" \n\n\u2022 \"flame ... \" \n\n\u2022 \"Can you really do this, Wendy? \" \u2022 \"Lacrima 6 \" Fr: 2023; Guo et al., 2022;Liu et al., 2022b,a) and large-scale training data (Liu et al., 2022c;Guo et al., 2023a,c;Bai et al., 2023a;Wang et al., 2023a;Guo et al., 2023b;Wang et al., 2023b;Yu et al., 2023). Transformer-based multimodal models incorporate both textual and visual information in a shared representation, which first preprocesses images using object detection models to extract information about specific regions or objects within the images. These region features are then projected into a common embedding space, which allows for seamless integration with the textual input. The multimodal Transformer architecture is then used to jointly process both the text and image inputs, using multi-head attention mechanisms to capture relationships and dependencies between the two modalities. This process results in the learning of a unified representation that captures the relationships between the textual and visual aspects of the data.",
            "score": 0.5095770540105155,
            "section_title": "Related Works",
            "char_start_offset": 4087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1286
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 154,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 154,
                    "end": 170,
                    "matchedPaperCorpusId": "201058752"
                },
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 343,
                    "end": 360,
                    "matchedPaperCorpusId": "253157438"
                },
                {
                    "start": 411,
                    "end": 430,
                    "matchedPaperCorpusId": "250562776"
                },
                {
                    "start": 524,
                    "end": 540,
                    "matchedPaperCorpusId": "267026765"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "10137425",
            "title": "Multimodal Machine Learning: A Survey and Taxonomy",
            "text": "A big part of multimodal machine learning is concerned with translating (mapping) from one modality to another. \n\nGiven an entity in one modality the task is to generate the same entity in a different modality. For example given an image we might want to generate a sentence describing it or given a textual description generate an image matching it. Multimodal translation is a long studied problem, with early work in speech synthesis [88], visual speech generation [136] video description [107], and cross-modal retrieval [169]. \n\nMore recently, multimodal translation has seen renewed interest due to combined efforts of the computer vision and natural language processing (NLP) communities [19] and recent availability of large multimodal datasets [38], [205]. A particularly popular problem is visual scene description, also known as image [214] and video captioning [213], which acts as a great test bed for a number of computer vision and NLP problems. To solve it, we not only need to fully understand the visual scene and to identify its salient parts, but also to produce grammatically correct and comprehensive yet concise sentences describing it. \n\nWhile the approaches to multimodal translation are very broad and are often modality specific, they share a number of unifying factors. We categorize them into two typesexample-based, and generative. Example-based models use a dictionary when translating between the modalities. Generative models, on the other hand, construct a model that is able to produce a translation. This distinction is similar to the one between non-parametric and parametric machine learning approaches and is illustrated in Figure 2, with representative examples summarized in Table 3. \n\nGenerative models are arguably more challenging to build as they require the ability to generate signals or sequences of symbols (e.g., sentences). This is difficult for any modality -visual, acoustic, or verbal, especially when temporally and structurally consistent sequences need to be generated. This led to many of the early multimodal translation systems relying on example-based translation. However,  this has been changing with the advent of deep learning models that are capable of generating images [171], [210], sounds [157], [209], and text [12].",
            "score": 0.5093999446180097,
            "section_title": "TRANSLATION",
            "char_start_offset": 28885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "14621185"
                },
                {
                    "start": 468,
                    "end": 473,
                    "matchedPaperCorpusId": "13632053"
                },
                {
                    "start": 525,
                    "end": 530,
                    "matchedPaperCorpusId": "10347107"
                },
                {
                    "start": 846,
                    "end": 851,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 873,
                    "end": 878,
                    "matchedPaperCorpusId": "52316421"
                },
                {
                    "start": 2237,
                    "end": 2242,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 2244,
                    "end": 2249,
                    "matchedPaperCorpusId": "8142135"
                },
                {
                    "start": 2258,
                    "end": 2263,
                    "matchedPaperCorpusId": "1697911"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "275757603",
            "title": "MASS: Overcoming Language Bias in Image-Text Matching",
            "text": "For each pair of images and text (c, x), an intertwined image-text encoder f \u03d5,\u03c8 processes both to yield a multimodal vector representation. Then, a linear classifier h \u03c9 is applied on top of the representation to build a binomial logit for the cross-entropy loss. At inference time, the probability of the image-text pair being true is used as the similarity function output S ITM (c, x). In our setting, all baselines marked ITM fall into this category. Note that OFA uses the text generation head to obtain the ITM score instead of a separate linear classifier. Refer to the corresponding paper (Wang et al. 2022b) for details. \n\n(2) \n\n2.2 Token-Level Similarity Functions \n\n3 Multimodal ASsociation Score \n\nDirectly using likelihood from autoregressive visuallanguage models as an image-text similarity function is problematic since this does not represent pure image-text similarity. For instance, image captioning models are also required to generate linguistically plausible captions. Whether the token x t matches the given image c or not, the model can assign a high likelihood if it was exposed frequently with the prefix x <t in the training process. To address this ignorance of the context, pointwise mutual information P M I(x; c) has been utilized in NLP tasks such as the conversation model (Li et al. 2015) and text summarization (Van der Poel, Cotterell, and Meister 2022). However, its application to the visual-language model has yet to be investigated, and we propose it as an effective method for reducing language bias in image-text matching.",
            "score": 0.5093667662054062,
            "section_title": "Sequence-Level Similarity Functions",
            "char_start_offset": 7506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1565
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 617,
                    "matchedPaperCorpusId": "246634906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333984375
        },
        {
            "paperId": "2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75",
            "corpusId": 256416164,
            "title": "Grounding Language Models to Images for Multimodal Generation",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 58,
            "citationCount": 93,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.13823",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2301.13823?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2301.13823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                }
            ],
            "abstract": "We propose an ef\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "corpus_id": "256416164",
            "text": "We propose an ef\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95556640625
        },
        {
            "paperId": "409961b88a3c7495afd21a9a4183c0bfaa858da0",
            "corpusId": 263620748,
            "title": "Kosmos-G: Generating Images in Context with Multimodal Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "referenceCount": 54,
            "citationCount": 68,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.02992",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.02992, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158877024",
                    "name": "Xichen Pan"
                },
                {
                    "authorId": "145307652",
                    "name": "Li Dong"
                },
                {
                    "authorId": "3110003",
                    "name": "Shaohan Huang"
                },
                {
                    "authorId": "2087004998",
                    "name": "Zhiliang Peng"
                },
                {
                    "authorId": "2253811183",
                    "name": "Wenhu Chen"
                },
                {
                    "authorId": "2253471545",
                    "name": "Furu Wei"
                }
            ],
            "abstract": "Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of\"image as a foreign language in image generation.\"This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of\"image as a foreign language in image generation.\"The code can be found at https://aka.ms/Kosmos-G",
            "corpus_id": "263620748",
            "text": "Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the ultimate goal of\"image as a foreign language in image generation.\"This paper presents Kosmos-G, a model that leverages the advanced multimodal perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates an impressive capability of zero-shot subject-driven generation with interleaved multi-image and text input. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of\"image as a foreign language in image generation.\"The code can be found at https://aka.ms/Kosmos-G",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.86962890625
        },
        {
            "paperId": "b331f211357d76c019d606b68c372c40b57a4998",
            "corpusId": 271769402,
            "title": "Mol2Lang-VLM: Vision- and Text-Guided Generative Pre-trained Language Models for Advancing Molecule Captioning through Multimodal Fusion",
            "venue": "LANGMOL",
            "year": 2024,
            "referenceCount": 19,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.langmol-1.12, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315308375",
                    "name": "Duong Tran"
                },
                {
                    "authorId": "2298324850",
                    "name": "Nhat Truong Pham"
                },
                {
                    "authorId": "2315500028",
                    "name": "Nguyen Doan Hieu Nguyen"
                },
                {
                    "authorId": "49557292",
                    "name": "Balachandran Manavalan"
                }
            ],
            "abstract": "This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the encoder and decoder blocks of the Transformer-based architecture by introducing third sub-layers into both. Specifically, we insert sub-layers in the encoder to fuse features from SELFIES strings and molecular images, while the decoder fuses features from SMILES strings and their corresponding descriptions. Moreover, cross multi-head attention is employed instead of common multi-head attention to enable the decoder to attend to the encoder\u2019s output, thereby integrating the encoded contextual information for better and more accurate caption generation. Performance evaluation on the CheBI-20 and L+M-24 benchmark datasets demonstrates Mol2Lang-VLM\u2019s superiority, achieving higher accuracy and quality in caption generation compared to existing methods. Our code and pre-processed data are available at https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang/.",
            "corpus_id": "271769402",
            "text": "This paper introduces Mol2Lang-VLM, an enhanced method for refining generative pre-trained language models for molecule captioning using multimodal features to achieve more accurate caption generation. Our approach leverages the encoder and decoder blocks of the Transformer-based architecture by introducing third sub-layers into both. Specifically, we insert sub-layers in the encoder to fuse features from SELFIES strings and molecular images, while the decoder fuses features from SMILES strings and their corresponding descriptions. Moreover, cross multi-head attention is employed instead of common multi-head attention to enable the decoder to attend to the encoder\u2019s output, thereby integrating the encoded contextual information for better and more accurate caption generation. Performance evaluation on the CheBI-20 and L+M-24 benchmark datasets demonstrates Mol2Lang-VLM\u2019s superiority, achieving higher accuracy and quality in caption generation compared to existing methods. Our code and pre-processed data are available at https://github.com/nhattruongpham/mol-lang-bridge/tree/mol2lang/.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.67333984375
        },
        {
            "paperId": "d3301ce2a9046c1b5acd51b79c7d1b2a6af690bf",
            "corpusId": 274445941,
            "title": "ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 55,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.02368, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333522984",
                    "name": "Leixin Zhang"
                },
                {
                    "authorId": "2264186499",
                    "name": "Steffen Eger"
                },
                {
                    "authorId": "2333620039",
                    "name": "Yinjie Cheng"
                },
                {
                    "authorId": "2333424820",
                    "name": "Weihe Zhai"
                },
                {
                    "authorId": "2138207755",
                    "name": "Jonas Belouadi"
                },
                {
                    "authorId": "66122857",
                    "name": "Christoph Leiter"
                },
                {
                    "authorId": "2029669151",
                    "name": "Simone Paolo Ponzetto"
                },
                {
                    "authorId": "2333424310",
                    "name": "Fahimeh Moafian"
                },
                {
                    "authorId": "2333625440",
                    "name": "Zhixue Zhao"
                }
            ],
            "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for accelerating scientific progress--remains underexplored. In this work, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal capabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as their combinations, focusing on the relationships between scientific objects (e.g., squares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E, and StableDiffusion, using two modes of output generation: code-based outputs (Python, TikZ) and direct raster image generation. Additionally, we examine four different input languages: English, German, Farsi, and Chinese. Our evaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific accuracy), reveals that while GPT-4o produces outputs of decent quality for simpler prompts involving individual dimensions such as spatial, numeric, or attribute understanding in isolation, all models face challenges in this task, especially for more complex prompts.",
            "corpus_id": "274445941",
            "text": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images--a critical application for accelerating scientific progress--remains underexplored. In this work, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal capabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as their combinations, focusing on the relationships between scientific objects (e.g., squares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E, and StableDiffusion, using two modes of output generation: code-based outputs (Python, TikZ) and direct raster image generation. Additionally, we examine four different input languages: English, German, Farsi, and Chinese. Our evaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific accuracy), reveals that while GPT-4o produces outputs of decent quality for simpler prompts involving individual dimensions such as spatial, numeric, or attribute understanding in isolation, all models face challenges in this task, especially for more complex prompts.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91796875
        },
        {
            "paperId": "220a6688eb533d071702a6a52b9cfc5a03188ca7",
            "corpusId": 273707022,
            "title": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding",
            "venue": "IEEE International Symposium on Biomedical Imaging",
            "year": 2024,
            "referenceCount": 20,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2410.23822",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.23822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2221231272",
                    "name": "Jinlong He"
                },
                {
                    "authorId": "2278383802",
                    "name": "Pengfei Li"
                },
                {
                    "authorId": "2192649954",
                    "name": "Gang Liu"
                },
                {
                    "authorId": "2053866414",
                    "name": "Shenjun Zhong"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) inherit the superior text understanding capabilities of LLMs and extend these capabilities to multimodal scenarios. These models achieve excellent results in the general domain of multimodal tasks. However, in the medical domain, the substantial training costs and the requirement for extensive medical data pose challenges to the development of medical MLLMs. Furthermore, due to the free-text form of answers, tasks such as visual grounding that need to produce output in a prescribed form become difficult for MLLMs. So far, there have been no medical MLLMs works in medical visual grounding area. For the medical vision grounding task, which involves identifying locations in medical images based on short text descriptions, we propose Parameter-efficient Fine-tuning medical multi-modal large language models for Medcial Visual Grounding (PFMVG). To validate the performance of the model, we evaluate it on a public benchmark dataset for medical visual grounding, where it achieves competitive results, and significantly outperforming GPT-4v.",
            "corpus_id": "273707022",
            "text": "Multimodal Large Language Models (MLLMs) inherit the superior text understanding capabilities of LLMs and extend these capabilities to multimodal scenarios. These models achieve excellent results in the general domain of multimodal tasks. However, in the medical domain, the substantial training costs and the requirement for extensive medical data pose challenges to the development of medical MLLMs. Furthermore, due to the free-text form of answers, tasks such as visual grounding that need to produce output in a prescribed form become difficult for MLLMs. So far, there have been no medical MLLMs works in medical visual grounding area. For the medical vision grounding task, which involves identifying locations in medical images based on short text descriptions, we propose Parameter-efficient Fine-tuning medical multi-modal large language models for Medcial Visual Grounding (PFMVG). To validate the performance of the model, we evaluate it on a public benchmark dataset for medical visual grounding, where it achieves competitive results, and significantly outperforming GPT-4v.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6572265625
        },
        {
            "paperId": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f",
            "corpusId": 261101015,
            "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
            "venue": "",
            "year": 2023,
            "referenceCount": 86,
            "citationCount": 939,
            "influentialCitationCount": 172,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.12966, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41211611",
                    "name": "Jinze Bai"
                },
                {
                    "authorId": "3768186",
                    "name": "Shuai Bai"
                },
                {
                    "authorId": null,
                    "name": "Shusheng Yang"
                },
                {
                    "authorId": "2217429986",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2110171536",
                    "name": "Sinan Tan"
                },
                {
                    "authorId": "2155302144",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "35996608",
                    "name": "Junyang Lin"
                },
                {
                    "authorId": "2192678144",
                    "name": "Chang Zhou"
                },
                {
                    "authorId": "1709595",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
            "corpus_id": "261101015",
            "text": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.233154296875
        },
        {
            "paperId": "3d28d20238165e18d305acfdd70e8eb9ac2a2722",
            "corpusId": 273819944,
            "title": "LLM4CAD: Multi-Modal Large Language Models for 3D Computer-Aided Design Generation",
            "venue": "Volume 6: 36th International Conference on Design Theory and Methodology (DTM)",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 7,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1115/detc2024-143740?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1115/detc2024-143740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155446759",
                    "name": "Xingang Li"
                },
                {
                    "authorId": "2329315291",
                    "name": "Yuewan Sun"
                },
                {
                    "authorId": "2166239124",
                    "name": "Z. Sha"
                }
            ],
            "abstract": "\n The evolution of multimodal large language models (LLMs) capable of processing diverse input modalities (e.g., text and images) holds new prospects for their application in engineering design, such as the generation of 3D computer-aided design (CAD) models. However, little is known about the ability of multimodal LLMs to generate 3D design objects, and there is a lack of quantitative assessment. In this study, we develop an approach to enable two LLMs, GPT-4 and GPT-4V, to generate 3D CAD models (i.e., LLM4CAD) and perform experiments to evaluate their efficacy. To address the challenge of data scarcity for multimodal LLM studies, we created a data synthesis pipeline to generate CAD models, sketches, and image data of typical mechanical components (e.g., gears and springs) and collect their natural-language descriptions with dimensional information using Amazon Mechanical Turk. We positioned the CAD program (programming script for CAD design) as a bridge, facilitating the conversion of LLMs\u2019 textual output into tangible CAD design objects. We focus on two critical capabilities: the generation of syntactically correct CAD programs (Cap1) and the accuracy of the parsed 3D shapes (Cap2) quantified by intersection over union. The results show that both GPT-4 and GPT-4V demonstrate potential in 3D CAD generation. Specifically, on average, GPT-4V outperforms when processing only text-based input, exceeding the results obtained using multimodal inputs, such as text with image, for Cap 1 and Cap 2. However, when examining category-specific results of mechanical components, while the same trend still holds for Cap 2, the prominence of multimodal inputs is increasingly evident for more complex geometries (e.g., springs and gears) in Cap 1. The potential of multimodal LLMs in enhancing 3D CAD generation is clear, but their application must be carefully calibrated to the complexity of the target CAD models to be generated.",
            "corpus_id": "273819944",
            "text": "\n The evolution of multimodal large language models (LLMs) capable of processing diverse input modalities (e.g., text and images) holds new prospects for their application in engineering design, such as the generation of 3D computer-aided design (CAD) models. However, little is known about the ability of multimodal LLMs to generate 3D design objects, and there is a lack of quantitative assessment. In this study, we develop an approach to enable two LLMs, GPT-4 and GPT-4V, to generate 3D CAD models (i.e., LLM4CAD) and perform experiments to evaluate their efficacy. To address the challenge of data scarcity for multimodal LLM studies, we created a data synthesis pipeline to generate CAD models, sketches, and image data of typical mechanical components (e.g., gears and springs) and collect their natural-language descriptions with dimensional information using Amazon Mechanical Turk. We positioned the CAD program (programming script for CAD design) as a bridge, facilitating the conversion of LLMs\u2019 textual output into tangible CAD design objects. We focus on two critical capabilities: the generation of syntactically correct CAD programs (Cap1) and the accuracy of the parsed 3D shapes (Cap2) quantified by intersection over union. The results show that both GPT-4 and GPT-4V demonstrate potential in 3D CAD generation. Specifically, on average, GPT-4V outperforms when processing only text-based input, exceeding the results obtained using multimodal inputs, such as text with image, for Cap 1 and Cap 2. However, when examining category-specific results of mechanical components, while the same trend still holds for Cap 2, the prominence of multimodal inputs is increasingly evident for more complex geometries (e.g., springs and gears) in Cap 1. The potential of multimodal LLMs in enhancing 3D CAD generation is clear, but their application must be carefully calibrated to the complexity of the target CAD models to be generated.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7373046875
        },
        {
            "paperId": "7ec9a50b9a91b2d0ec60a335396d0d2daedb5c6b",
            "corpusId": 275921533,
            "title": "A Causality-Aware Paradigm for Evaluating Creativity of Multimodal Large Language Models",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2025,
            "referenceCount": 107,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109670338",
                    "name": "Zhongzhan Huang"
                },
                {
                    "authorId": "2053866242",
                    "name": "Shan Zhong"
                },
                {
                    "authorId": "2272787003",
                    "name": "Pan Zhou"
                },
                {
                    "authorId": "2269765109",
                    "name": "Shanghua Gao"
                },
                {
                    "authorId": "2095762",
                    "name": "M. Zitnik"
                },
                {
                    "authorId": "2280319348",
                    "name": "Liang Lin"
                }
            ],
            "abstract": "Recently, numerous benchmarks have been developed to evaluate the logical reasoning abilities of large language models (LLMs). However, assessing the equally important creative capabilities of LLMs is challenging due to the subjective, diverse, and data-scarce nature of creativity, especially in multimodal scenarios. In this paper, we consider the comprehensive pipeline for evaluating the creativity of multimodal LLMs, with a focus on suitable evaluation platforms and methodologies. First, we find the Oogiri game\u2014a creativity-driven task requiring humor, associative thinking, and the ability to produce unexpected responses to text, images, or both. This game aligns well with the input-output structure of modern multimodal LLMs and benefits from a rich repository of high-quality, human-annotated creative responses, making it an ideal platform for studying LLM creativity. Next, beyond using the Oogiri game for standard evaluations like ranking and selection, we propose LoTbench, an interactive, causality-aware evaluation framework, to further address some intrinsic risks in standard evaluations, such as information leakage and limited interpretability. The proposed LoTbench not only quantifies LLM creativity more effectively but also visualizes the underlying creative thought processes. Our results show that while most LLMs exhibit constrained creativity, the performance gap between LLMs and humans is not insurmountable. Furthermore, we observe a strong correlation between results from the multimodal cognition benchmark MMMU and LoTbench, but only a weak connection with traditional creativity metrics. This suggests that LoTbench better aligns with human cognitive theories, highlighting cognition as a critical foundation in the early stages of creativity and enabling the bridging of diverse concepts. Project Page.",
            "corpus_id": "275921533",
            "text": "Recently, numerous benchmarks have been developed to evaluate the logical reasoning abilities of large language models (LLMs). However, assessing the equally important creative capabilities of LLMs is challenging due to the subjective, diverse, and data-scarce nature of creativity, especially in multimodal scenarios. In this paper, we consider the comprehensive pipeline for evaluating the creativity of multimodal LLMs, with a focus on suitable evaluation platforms and methodologies. First, we find the Oogiri game\u2014a creativity-driven task requiring humor, associative thinking, and the ability to produce unexpected responses to text, images, or both. This game aligns well with the input-output structure of modern multimodal LLMs and benefits from a rich repository of high-quality, human-annotated creative responses, making it an ideal platform for studying LLM creativity. Next, beyond using the Oogiri game for standard evaluations like ranking and selection, we propose LoTbench, an interactive, causality-aware evaluation framework, to further address some intrinsic risks in standard evaluations, such as information leakage and limited interpretability. The proposed LoTbench not only quantifies LLM creativity more effectively but also visualizes the underlying creative thought processes. Our results show that while most LLMs exhibit constrained creativity, the performance gap between LLMs and humans is not insurmountable. Furthermore, we observe a strong correlation between results from the multimodal cognition benchmark MMMU and LoTbench, but only a weak connection with traditional creativity metrics. This suggests that LoTbench better aligns with human cognitive theories, highlighting cognition as a critical foundation in the early stages of creativity and enabling the bridging of diverse concepts. Project Page.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2093505859375
        },
        {
            "paperId": "bcc4f07e9d5e04752c28bbd6c7e1a5848a076a83",
            "corpusId": 279318410,
            "title": "A Survey of Generative Categories and Techniques in Multimodal Large Language Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 170,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.10016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2367673004",
                    "name": "Longzhen Han"
                },
                {
                    "authorId": "2336634387",
                    "name": "Awes Mubarak"
                },
                {
                    "authorId": "46204344",
                    "name": "Almas Baimagambetov"
                },
                {
                    "authorId": "3045368",
                    "name": "Nikolaos Polatidis"
                },
                {
                    "authorId": "2366564385",
                    "name": "Thar Baker"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Architectural innovations like transformers and diffusion models underpin this convergence, enabling cross-modal transfer and modular specialization. We highlight emerging patterns of synergy, and identify open challenges in evaluation, modularity, and structured reasoning. This survey offers a unified perspective on MLLM development and identifies critical paths toward more general-purpose, adaptive, and interpretable multimodal systems.",
            "corpus_id": "279318410",
            "text": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Architectural innovations like transformers and diffusion models underpin this convergence, enabling cross-modal transfer and modular specialization. We highlight emerging patterns of synergy, and identify open challenges in evaluation, modularity, and structured reasoning. This survey offers a unified perspective on MLLM development and identifies critical paths toward more general-purpose, adaptive, and interpretable multimodal systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7998046875
        },
        {
            "paperId": "7a8547311298fedc248182c135fb75f0c92cb023",
            "corpusId": 278788650,
            "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 22,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.16774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2363449602",
                    "name": "Yiming Gao"
                },
                {
                    "authorId": "2306845506",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "1780581845",
                    "name": "Chengwei Wei"
                },
                {
                    "authorId": "2307389481",
                    "name": "Shuo Sun"
                },
                {
                    "authorId": "2308032939",
                    "name": "AiTi Aw"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated strong instruction-following capabilities in text-based tasks. However, this ability often deteriorates in multimodal models after alignment with non-text modalities such as images or audio. While several recent efforts have investigated instruction-following performance in text and vision-language models, instruction-following in audio-based large language models remains largely unexplored. To bridge this gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess the ability to follow instructions in an audio LLM. IFEval-Audio contains 280 audio-instruction-answer triples across six diverse dimensions: Content, Capitalization, Symbol, List Structure, Length, and Format. Each example pairs an audio input with a text instruction, requiring the model to generate an output that follows a specified structure. We benchmark state-of-the-art audio LLMs on their ability to follow audio-involved instructions. The dataset is released publicly to support future research in this emerging area.",
            "corpus_id": "278788650",
            "text": "Large language models (LLMs) have demonstrated strong instruction-following capabilities in text-based tasks. However, this ability often deteriorates in multimodal models after alignment with non-text modalities such as images or audio. While several recent efforts have investigated instruction-following performance in text and vision-language models, instruction-following in audio-based large language models remains largely unexplored. To bridge this gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess the ability to follow instructions in an audio LLM. IFEval-Audio contains 280 audio-instruction-answer triples across six diverse dimensions: Content, Capitalization, Symbol, List Structure, Length, and Format. Each example pairs an audio input with a text instruction, requiring the model to generate an output that follows a specified structure. We benchmark state-of-the-art audio LLMs on their ability to follow audio-involved instructions. The dataset is released publicly to support future research in this emerging area.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.36669921875
        },
        {
            "paperId": "7f265fd9a036b2f049a430a4d0f44ee200a68deb",
            "corpusId": 264405680,
            "title": "Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 44,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13361, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186606523",
                    "name": "Wenyu Guo"
                },
                {
                    "authorId": "2159548678",
                    "name": "Qingkai Fang"
                },
                {
                    "authorId": "2261095515",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2261199971",
                    "name": "Yang Feng"
                }
            ],
            "abstract": "Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process.Experimental results show that our approach achieves state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while remaining independent of authentic images during inference.",
            "corpus_id": "264405680",
            "text": "Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process.Experimental results show that our approach achieves state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while remaining independent of authentic images during inference.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.424560546875
        },
        {
            "paperId": "58d0fc95c6436342f560570860271f87f90b70bf",
            "corpusId": 255417390,
            "title": "Multilingual Text-Based Image Search Using Multimodal Embeddings",
            "venue": "Conference Information and Communication Technology",
            "year": 2022,
            "referenceCount": 20,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CICT56698.2022.9997911?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CICT56698.2022.9997911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2179952839",
                    "name": "Kristen Pereira"
                },
                {
                    "authorId": "2199256164",
                    "name": "Aman Parikh"
                },
                {
                    "authorId": "2157821154",
                    "name": "P. Kumar"
                },
                {
                    "authorId": "144088162",
                    "name": "V. Hole"
                }
            ],
            "abstract": "The explosion of data and information on the Internet calls for efficient and reliable information retrieval methods. While textual information retrieval systems have significantly improved, content-based image retrieval using text inputs requires further study and optimization. This research paper proposes a system that uses CLIP (Contrastive Language-Image Pre-Training) model, which projects images and text into a multimodal embedding space to provide representations to compare the semantic meaning of words to embeddings of the image in a dataset. The output is a set of images closely resembling the text input, which is achieved through cosine similarity based on matrix operations. The models have also been optimized for use in production with the use of ONNX runtime is done to speed up inference timing. The application is full-stack and easily accessible, with a ReactJS frontend hosted on Netlify and Flask based Python backend hosted on AWS.",
            "corpus_id": "255417390",
            "text": "The explosion of data and information on the Internet calls for efficient and reliable information retrieval methods. While textual information retrieval systems have significantly improved, content-based image retrieval using text inputs requires further study and optimization. This research paper proposes a system that uses CLIP (Contrastive Language-Image Pre-Training) model, which projects images and text into a multimodal embedding space to provide representations to compare the semantic meaning of words to embeddings of the image in a dataset. The output is a set of images closely resembling the text input, which is achieved through cosine similarity based on matrix operations. The models have also been optimized for use in production with the use of ONNX runtime is done to speed up inference timing. The application is full-stack and easily accessible, with a ReactJS frontend hosted on Netlify and Flask based Python backend hosted on AWS.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.76953125
        },
        {
            "paperId": "d7552c281caf7e2be5818e8f74119600644a41cb",
            "corpusId": 270924351,
            "title": "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02814, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309477916",
                    "name": "Zhaotian Weng"
                },
                {
                    "authorId": "2309569903",
                    "name": "Zijun Gao"
                },
                {
                    "authorId": "2309481746",
                    "name": "Jerone Andrews"
                },
                {
                    "authorId": "2310387763",
                    "name": "Jieyu Zhao"
                }
            ],
            "abstract": "Vision-language models (VLMs) pre-trained on extensive datasets can inadvertently learn biases by correlating gender information with specific objects or scenarios. Current methods, which focus on modifying inputs and monitoring changes in the model\u2019s output probability scores, often struggle to comprehensively understand bias from the perspective of model components. We propose a framework that incorporates causal mediation analysis to measure and map the pathways of bias generation and propagation within VLMs. Our framework is applicable to a wide range of vision-language and multimodal tasks. In this work, we apply it to the object detection task and implement it on the GLIP model. This approach allows us to identify the direct effects of interventions on model bias and the indirect effects of interventions on bias mediated through different model components. Our results show that image features are the primary contributors to bias, with significantly higher impacts than text features, specifically accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE datasets, respectively. Notably, the image encoder\u2019s contribution surpasses that of the text encoder and the deep fusion encoder. Further experimentation confirms that contributions from both language and vision modalities are aligned and non-conflicting. Consequently, focusing on blurring gender representations within the image encoder which contributes most to the model bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal performance loss or increased computational demands.",
            "corpus_id": "270924351",
            "text": "Vision-language models (VLMs) pre-trained on extensive datasets can inadvertently learn biases by correlating gender information with specific objects or scenarios. Current methods, which focus on modifying inputs and monitoring changes in the model\u2019s output probability scores, often struggle to comprehensively understand bias from the perspective of model components. We propose a framework that incorporates causal mediation analysis to measure and map the pathways of bias generation and propagation within VLMs. Our framework is applicable to a wide range of vision-language and multimodal tasks. In this work, we apply it to the object detection task and implement it on the GLIP model. This approach allows us to identify the direct effects of interventions on model bias and the indirect effects of interventions on bias mediated through different model components. Our results show that image features are the primary contributors to bias, with significantly higher impacts than text features, specifically accounting for 32.57% and 12.63% of the bias in the MSCOCO and PASCAL-SENTENCE datasets, respectively. Notably, the image encoder\u2019s contribution surpasses that of the text encoder and the deep fusion encoder. Further experimentation confirms that contributions from both language and vision modalities are aligned and non-conflicting. Consequently, focusing on blurring gender representations within the image encoder which contributes most to the model bias, reduces bias efficiently by 22.03% and 9.04% in the MSCOCO and PASCAL-SENTENCE datasets, respectively, with minimal performance loss or increased computational demands.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10302734375
        }
    ],
    "quotes": {
        "cost": 0.177267,
        "quotes": [
            {
                "idx": 0,
                "key": "[233168627 | Franceschelli et al. | 2021 | Citations: 42]",
                "snippets": "The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) has an image encoder pre-trained together with a text encoder to generate a caption for an image.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                },
                "metadata": [
                    {
                        "section_title": "Examples of Models.",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 421,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 116
                            },
                            {
                                "start": 117,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 421
                            }
                        ],
                        "ref_mentions": [
                            "231591445"
                        ],
                        "quote": "The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) has an image encoder pre-trained together with a text encoder to generate a caption for an image."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[237417089 | Sollami et al. | 2021 | Citations: 10]",
                "snippets": "We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[256416164 | Koh et al. | 2023 | Citations: 93]",
                "snippets": "This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[258865230 | Cho et al. | 2023 | Citations: 51]",
                "snippets": "Bridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 568,
                        "end": 1164,
                        "sentence_offsets": [
                            {
                                "start": 568,
                                "end": 615
                            },
                            {
                                "start": 616,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 958
                            },
                            {
                                "start": 959,
                                "end": 1164
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Bridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[258947258 | Koh et al. | 2023 | Citations: 122]",
                "snippets": "and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 170,
                        "end": 643,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[258959284 | Koh et al. | 2023 | Citations: 259]",
                "snippets": "In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 693,
                        "end": 1000,
                        "sentence_offsets": [
                            {
                                "start": 693,
                                "end": 791
                            },
                            {
                                "start": 792,
                                "end": 1000
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[259164940 | Zhang et al. | 2023 | Citations: 0]",
                "snippets": "Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for multimodal dialogue...FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for multimodal dialogue",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Multimodal Dialogue Models",
                        "pdf_hash": "",
                        "start": 917,
                        "end": 1103,
                        "sentence_offsets": [
                            {
                                "start": 917,
                                "end": 1102
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[262898422 | Aiello et al. | 2023 | Citations: 31]",
                "snippets": "To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[263608981 | Zheng et al. | 2023 | Citations: 98]",
                "snippets": "However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 126,
                        "end": 443,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[265498814 | Li et al. | 2023 | Citations: 69]",
                "snippets": "Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 428,
                        "end": 785,
                        "sentence_offsets": [
                            {
                                "start": 428,
                                "end": 785
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[267412060 | Jing et al. | 2024 | Citations: 38]",
                "snippets": "The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models...CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Multimodality of RAG",
                        "pdf_hash": "",
                        "start": 100,
                        "end": 505,
                        "sentence_offsets": [
                            {
                                "start": 100,
                                "end": 203
                            },
                            {
                                "start": 204,
                                "end": 342
                            },
                            {
                                "start": 343,
                                "end": 506
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models"
                    },
                    {
                        "section_title": "A. Multimodality of RAG",
                        "pdf_hash": "",
                        "start": 821,
                        "end": 953,
                        "sentence_offsets": [
                            {
                                "start": 821,
                                "end": 952
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[267637150 | Tsai et al. | 2024 | Citations: 9]",
                "snippets": "Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORKS 2.1 Multimodal Foundation Models",
                        "pdf_hash": "",
                        "start": 1324,
                        "end": 1950,
                        "sentence_offsets": [
                            {
                                "start": 1324,
                                "end": 1478
                            },
                            {
                                "start": 1479,
                                "end": 1636
                            },
                            {
                                "start": 1637,
                                "end": 1697
                            },
                            {
                                "start": 1698,
                                "end": 1848
                            },
                            {
                                "start": 1849,
                                "end": 1950
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[269354784 | Bazi et al. | 2024 | Citations: 44]",
                "snippets": "The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Vision-Language Models in General Computer Vision",
                        "pdf_hash": "",
                        "start": 1868,
                        "end": 2154,
                        "sentence_offsets": [
                            {
                                "start": 1868,
                                "end": 2005
                            },
                            {
                                "start": 2005,
                                "end": 2154
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[270094821 | Ma et al. | 2024 | Citations: 4]",
                "snippets": "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[270285703 | Basu et al. | 2024 | Citations: 12]",
                "snippets": "We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 35,
                        "end": 139,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 139
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2]."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271218251 | Li et al. | 2024 | Citations: 6]",
                "snippets": "Since humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35]...models designed for input in one modality and output in another, such as text-to-image (Rombach et al., 2021)[27] and image-to-text [37] models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                },
                "metadata": [
                    {
                        "section_title": "Multimodal Vision Language Models",
                        "pdf_hash": "",
                        "start": 652,
                        "end": 884,
                        "sentence_offsets": [
                            {
                                "start": 652,
                                "end": 885
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Since humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35]"
                    },
                    {
                        "section_title": "Multimodal Vision Language Models",
                        "pdf_hash": "",
                        "start": 170,
                        "end": 295,
                        "sentence_offsets": [
                            {
                                "start": 92,
                                "end": 295
                            }
                        ],
                        "ref_mentions": [
                            "245335280"
                        ],
                        "quote": "models designed for input in one modality and output in another, such as text-to-image (Rombach et al., 2021)[27] and image-to-text [37] models."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271245054 | Jiang et al. | 2024 | Citations: 34]",
                "snippets": "Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[271924334 | Xie et al. | 2024 | Citations: 229]",
                "snippets": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822817 | Tang et al. | 2023 | Citations: 188]": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io",
                    "[261975252 | Dong et al. | 2023 | Citations: 201]": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                },
                "metadata": [
                    {
                        "section_title": "UNIFIED VISION-LANGUAGE FOUNDATION MODEL",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 414,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 253
                            },
                            {
                                "start": 254,
                                "end": 414
                            }
                        ],
                        "ref_mentions": [
                            "258822817",
                            "261975252"
                        ],
                        "quote": "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[272911245 | Wang et al. | 2024 | Citations: 12]",
                "snippets": "For example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 132,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 132
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273403986 | Fang et al. | 2024 | Citations: 15]",
                "snippets": "Recent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 573,
                        "end": 1025,
                        "sentence_offsets": [
                            {
                                "start": 573,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 887
                            },
                            {
                                "start": 888,
                                "end": 1025
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[274023126 | Liu et al. | 2024 | Citations: 11]",
                "snippets": "The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output (Liu et al., 2023)- (Kong et al., 2024), (Zhu et al., 2023)- [30].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                    "[258291930 | Zhu et al. | 2023 | Citations: 2067]": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.",
                    "[267412182 | Kong et al. | 2024 | Citations: 94]": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo."
                },
                "metadata": [
                    {
                        "section_title": "A. Multimodal Generative Models",
                        "pdf_hash": "",
                        "start": 95,
                        "end": 332,
                        "sentence_offsets": [
                            {
                                "start": 95,
                                "end": 332
                            }
                        ],
                        "ref_mentions": [
                            "258179774",
                            "267412182",
                            "258291930"
                        ],
                        "quote": "The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output (Liu et al., 2023)- (Kong et al., 2024), (Zhu et al., 2023)- [30]."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[274305717 | Xie et al. | 2024 | Citations: 10]",
                "snippets": "Other works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 789,
                        "end": 1188,
                        "sentence_offsets": [
                            {
                                "start": 789,
                                "end": 960
                            },
                            {
                                "start": 961,
                                "end": 1188
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Other works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274981694 | Pham et al. | 2024 | Citations: 1]",
                "snippets": "Despite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1118,
                        "end": 1650,
                        "sentence_offsets": [
                            {
                                "start": 1118,
                                "end": 1313
                            },
                            {
                                "start": 1314,
                                "end": 1472
                            },
                            {
                                "start": 1473,
                                "end": 1650
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[275820726 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "Multimodal Large Language Models (MLLMs) [1,(Alayrac et al., 2022)(Liu et al., 2023)40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL (Chen et al., 2023) can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248476411 | Alayrac et al. | 2022 | Citations: 3607]": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                    "[266521410 | Chen et al. | 2023 | Citations: 1215]": "The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 44,
                        "end": 809,
                        "sentence_offsets": [
                            {
                                "start": 34,
                                "end": 373
                            },
                            {
                                "start": 374,
                                "end": 571
                            },
                            {
                                "start": 572,
                                "end": 809
                            }
                        ],
                        "ref_mentions": [
                            "248476411",
                            "258179774",
                            "266521410"
                        ],
                        "quote": "Multimodal Large Language Models (MLLMs) [1,(Alayrac et al., 2022)(Liu et al., 2023)40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL (Chen et al., 2023) can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276929408 | Fang et al. | 2025 | Citations: 1]",
                "snippets": "In response to the notable progress of large language models in natural language processing [1,2,(Chowdhery et al., 2022), the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models (Ho et al., 2020), but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219955663 | Ho et al. | 2020 | Citations: 18352]": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
                    "[247951931 | Chowdhery et al. | 2022 | Citations: 6293]": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
                    "[253581213 | Brooks et al. | 2022 | Citations: 1833]": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
                    "[266374640 | Sun et al. | 2023 | Citations: 290]": "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                },
                "metadata": [
                    {
                        "section_title": "MLLM for Image Generation and Editing",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1202,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 199
                            },
                            {
                                "start": 200,
                                "end": 350
                            },
                            {
                                "start": 351,
                                "end": 560
                            },
                            {
                                "start": 561,
                                "end": 757
                            },
                            {
                                "start": 758,
                                "end": 941
                            },
                            {
                                "start": 942,
                                "end": 1202
                            }
                        ],
                        "ref_mentions": [
                            "247951931",
                            "219955663",
                            "266374640",
                            "253581213",
                            "253581838"
                        ],
                        "quote": "In response to the notable progress of large language models in natural language processing [1,2,(Chowdhery et al., 2022), the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models (Ho et al., 2020), but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[277313749 | Yu et al. | 2025 | Citations: 2]",
                "snippets": "Recent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2023), which integrate additional encoders into textual LLMs to process inputs from other modalities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248476411 | Alayrac et al. | 2022 | Citations: 3607]": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
                    "[256390509 | Li et al. | 2023 | Citations: 4653]": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Multimodal Large Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 374,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 163
                            },
                            {
                                "start": 164,
                                "end": 374
                            }
                        ],
                        "ref_mentions": [
                            "256390509",
                            "248476411",
                            "258179774"
                        ],
                        "quote": "Recent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2023), which integrate additional encoders into textual LLMs to process inputs from other modalities."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[277857505 | Xia et al. | 2025 | Citations: 2]",
                "snippets": "Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023)(Yan et al., 2023), audio (Chu et al., 2023), graph (Tang et al., 2023), and structured charts (Han et al., 2023), alongside text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
                    "[264405943 | Tang et al. | 2023 | Citations: 145]": "Graph Neural Networks (GNNs) have evolved to understand graph structures through recursive exchanges and aggregations among nodes. To enhance robustness, self-supervised learning (SSL) has become a vital tool for data augmentation. Traditional methods often depend on fine-tuning with task-specific labels, limiting their effectiveness when labeled data is scarce. Our research tackles this by advancing graph model generalization in zero-shot learning environments. Inspired by the success of large language models (LLMs), we aim to create a graph-oriented LLM capable of exceptional generalization across various datasets and tasks without relying on downstream graph data. We introduce the GraphGPT framework, which integrates LLMs with graph structural knowledge through graph instruction tuning. This framework includes a text-graph grounding component to link textual and graph structures and a dual-stage instruction tuning approach with a lightweight graph-text alignment projector. These innovations allow LLMs to comprehend complex graph structures and enhance adaptability across diverse datasets and tasks. Our framework demonstrates superior generalization in both supervised and zero-shot graph learning tasks, surpassing existing benchmarks. The open-sourced model implementation of our GraphGPT is available at https://github.com/HKUDS/GraphGPT.",
                    "[264590333 | Yan et al. | 2023 | Citations: 60]": "Urban region profiling from web-sourced data is of utmost importance for urban computing. We are witnessing a blossom of LLMs for various fields, especially in multi-modal data research such as vision-language learning, where text modality serves as a supplement for images. As textual modality has rarely been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions: i) Can text modality enhance urban region profiling? ii) and if so, in what ways and which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of text modality into urban imagery, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP ). Specifically, it first generates a detailed textual description for each satellite image by Image-to-Text LLMs. Then, the model is trained on image-text pairs, seamlessly unifying language supervision for urban visual representation learning, jointly with contrastive loss and language modeling loss. Results on urban indicator prediction in four major metropolises show its superior performance, with an average improvement of 6.1% on R2 compared to the state-of-the-art methods. Our code and dataset are available at https://github.com/StupidBuluchacha/UrbanCLIP."
                },
                "metadata": [
                    {
                        "section_title": "B.2 LLMs & MLLMs",
                        "pdf_hash": "",
                        "start": 526,
                        "end": 790,
                        "sentence_offsets": [
                            {
                                "start": 526,
                                "end": 790
                            }
                        ],
                        "ref_mentions": [
                            "258179774",
                            "264590333",
                            "264405943"
                        ],
                        "quote": "Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023)(Yan et al., 2023), audio (Chu et al., 2023), graph (Tang et al., 2023), and structured charts (Han et al., 2023), alongside text."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.021018,
        "cot": "For the query \"are papers that multilmodal language models output text images\", I need to organize the information about multimodal language models (MLLMs) and their output capabilities. Let me plan the dimensions that would best address this query:\n\n1. \"Introduction to Multimodal Language Models\" - This should be a synthesis paragraph that introduces what MLLMs are and provides context for understanding their capabilities. This will help set the foundation for discussing their output modalities.\n\n2. \"MLLMs with Text Output Capabilities\" - Since the query asks about output modalities, this dimension will focus on models that generate text outputs. This should be a synthesis paragraph to explain the various approaches and architectures.\n\n3. \"MLLMs with Image Output Capabilities\" - This dimension will focus on models that can generate images. Again, a synthesis would be appropriate to explain the various techniques and models.\n\n4. \"MLLMs with Combined Text and Image Output\" - This dimension will specifically address models that can output both text and images together, which seems to be the core of the query. A synthesis format would work best to explain the relationships between text and image generation in these models.\n\n5. \"Applications and Examples of Multimodal Outputs\" - This would be a list format to showcase various use cases and examples of how these multimodal outputs are being utilized in research and applications.\n\nThe quotes contain information about various MLLMs, their architectures, and capabilities, including those that can output text, images, or both. I'll assign each quote to the most appropriate dimension based on its content.",
        "plan": {
            "Introduction to Multimodal Language Models (synthesis)": [
                14,
                15,
                16,
                23,
                24,
                25,
                26
            ],
            "MLLMs with Text Output Capabilities (synthesis)": [
                11,
                12,
                13,
                20,
                22
            ],
            "MLLMs with Image Output Capabilities (synthesis)": [
                3,
                10,
                18,
                19
            ],
            "MLLMs with Combined Text and Image Output (synthesis)": [
                0,
                1,
                2,
                4,
                5,
                6,
                7,
                8,
                9,
                17,
                21
            ],
            "Applications and Examples of Multimodal Outputs (list)": [
                13,
                15,
                24
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Multimodal Language Models",
                "tldr": "Multimodal Large Language Models (MLLMs) extend traditional language models by incorporating multiple types of data like images, audio, and graphs alongside text. This evolution enables more human-like interaction with AI systems and opens up diverse applications ranging from visual question answering to image generation and editing. (11 sources)",
                "text": "\nMultimodal Large Language Models (MLLMs) represent a significant advancement in artificial intelligence by combining the processing of multiple data modalities\u2014primarily text and images\u2014within a single model architecture. At their core, MLLMs are designed to take both visual and textual inputs and generate text outputs <Paper corpusId=\"270285703\" paperTitle=\"(Basu et al., 2024)\" isShortName></Paper>. This multimodal capability marks an important evolution from text-only large language models (LLMs), significantly expanding their potential applications and use cases <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe development of MLLMs is motivated by the human experience of interacting with the world through multiple sensory inputs simultaneously <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. While traditional approaches focused on single-modality models or unidirectional transformations (such as text-to-image or image-to-text), modern MLLMs are designed to process, understand, and manipulate visual and textual information in tandem <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. These models create a more nuanced integration of linguistic and visual information, enabling more sophisticated understanding and generation capabilities <Paper corpusId=\"276929408\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n\nSeveral pioneering architectures have shaped the MLLM landscape. Flamingo introduced key innovations to bridge pretrained vision and language models, handle interleaved visual and textual data, and process various visual inputs <Paper corpusId=\"277313749\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"248476411\" paperTitle=\"(Alayrac et al., 2022)\" isShortName></Paper>. BLIP-2 demonstrated an efficient pre-training strategy that bootstraps vision-language learning from frozen pre-trained components <Paper corpusId=\"277313749\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"256390509\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. LLaVA pioneered the use of GPT-4 to generate multimodal instruction-following data, creating an end-to-end trained model for general-purpose visual and language understanding <Paper corpusId=\"277313749\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nMore recent models like GPT-4o and InternVL have further advanced the field by demonstrating strong performance across multimodal tasks such as visual understanding, language generation, and visual question answering <Paper corpusId=\"275820726\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>. Beyond images and text, researchers have begun incorporating additional modalities into MLLMs, including audio, graph structures, and even structured charts <Paper corpusId=\"277857505\" paperTitle=\"(Xia et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, the representation of multimodal information within MLLMs remains an active area of research <Paper corpusId=\"271245054\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. These models typically leverage joint embedding spaces where images and their corresponding textual descriptions are closely aligned, allowing for more effective cross-modal understanding and generation <Paper corpusId=\"275820726\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>. This architectural approach enables MLLMs to not only understand multimodal inputs but also potentially generate outputs across different modalities.",
                "citations": [
                    {
                        "id": "(Basu et al., 2024)",
                        "snippets": [
                            "We consider a MLLM to be a model that takes an image and text as input, and generates a text output [2]."
                        ],
                        "paper": {
                            "corpus_id": 270285703,
                            "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2114710333",
                                    "name": "Samyadeep Basu"
                                },
                                {
                                    "authorId": "2052886960",
                                    "name": "Martin Grayson"
                                },
                                {
                                    "authorId": "121927341",
                                    "name": "C. Morrison"
                                },
                                {
                                    "authorId": "2571049",
                                    "name": "Besmira Nushi"
                                },
                                {
                                    "authorId": "34389431",
                                    "name": "S. Feizi"
                                },
                                {
                                    "authorId": "2286878667",
                                    "name": "Daniela Massiceti"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 12
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Since humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35]",
                            "models designed for input in one modality and output in another, such as text-to-image (Rombach et al., 2021)[27] and image-to-text [37] models."
                        ],
                        "paper": {
                            "corpus_id": 271218251,
                            "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
                            "authors": [
                                {
                                    "authorId": "2116785555",
                                    "name": "Zhimin Li"
                                },
                                {
                                    "authorId": "2253579462",
                                    "name": "Haichao Miao"
                                },
                                {
                                    "authorId": "2251997858",
                                    "name": "Valerio Pascucci"
                                },
                                {
                                    "authorId": "2289268771",
                                    "name": "Shusen Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2025)",
                        "snippets": [
                            "In response to the notable progress of large language models in natural language processing [1,2,(Chowdhery et al., 2022), the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models (Ho et al., 2020), but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."
                        ],
                        "paper": {
                            "corpus_id": 276929408,
                            "title": "Recognition-Synergistic Scene Text Editing",
                            "authors": [
                                {
                                    "authorId": "2313679002",
                                    "name": "Zhengyao Fang"
                                },
                                {
                                    "authorId": "10344582",
                                    "name": "Pengyuan Lyu"
                                },
                                {
                                    "authorId": "2313920076",
                                    "name": "Jingjing Wu"
                                },
                                {
                                    "authorId": "2248958848",
                                    "name": "Chengquan Zhang"
                                },
                                {
                                    "authorId": "2349736964",
                                    "name": "Jun Yu"
                                },
                                {
                                    "authorId": "2261251059",
                                    "name": "Guan-Lun Lu"
                                },
                                {
                                    "authorId": "1678473",
                                    "name": "Wenjie Pei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.93994140625
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "Recent advances in multimodal large language models (MLLMs) have led to the development of models that can receive, reason, and output with multimodal information. Notable examples include BLIP-2 (Li et al., 2023), Flamingo (Alayrac et al., 2022), and LLaVA (Liu et al., 2023), which integrate additional encoders into textual LLMs to process inputs from other modalities."
                        ],
                        "paper": {
                            "corpus_id": 277313749,
                            "title": "CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning",
                            "authors": [
                                {
                                    "authorId": "2352021201",
                                    "name": "Hao Yu"
                                },
                                {
                                    "authorId": "2352021017",
                                    "name": "Zhuokai Zhao"
                                },
                                {
                                    "authorId": "2334461697",
                                    "name": "Shen Yan"
                                },
                                {
                                    "authorId": "2352020894",
                                    "name": "Lukasz Korycki"
                                },
                                {
                                    "authorId": "2352039047",
                                    "name": "Jianyu Wang"
                                },
                                {
                                    "authorId": "2334709723",
                                    "name": "Baosheng He"
                                },
                                {
                                    "authorId": "2340394857",
                                    "name": "Jiayi Liu"
                                },
                                {
                                    "authorId": "2340399373",
                                    "name": "Lizhu Zhang"
                                },
                                {
                                    "authorId": "2323788930",
                                    "name": "Xiangjun Fan"
                                },
                                {
                                    "authorId": "2352014487",
                                    "name": "Hanchao Yu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.943359375
                    },
                    {
                        "id": "(Alayrac et al., 2022)",
                        "snippets": [
                            "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."
                        ],
                        "paper": {
                            "corpus_id": 248476411,
                            "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
                            "authors": [
                                {
                                    "authorId": "2285263",
                                    "name": "Jean-Baptiste Alayrac"
                                },
                                {
                                    "authorId": "7408951",
                                    "name": "Jeff Donahue"
                                },
                                {
                                    "authorId": "152831141",
                                    "name": "Pauline Luc"
                                },
                                {
                                    "authorId": "19200186",
                                    "name": "Antoine Miech"
                                },
                                {
                                    "authorId": "2159207795",
                                    "name": "Iain Barr"
                                },
                                {
                                    "authorId": "66535271",
                                    "name": "Yana Hasson"
                                },
                                {
                                    "authorId": "3257286",
                                    "name": "Karel Lenc"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47447264",
                                    "name": "Malcolm Reynolds"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "12159303",
                                    "name": "Serkan Cabi"
                                },
                                {
                                    "authorId": "22237490",
                                    "name": "Tengda Han"
                                },
                                {
                                    "authorId": "48398849",
                                    "name": "Zhitao Gong"
                                },
                                {
                                    "authorId": "2412073",
                                    "name": "Sina Samangooei"
                                },
                                {
                                    "authorId": "49601928",
                                    "name": "Marianne Monteiro"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "3208081",
                                    "name": "Aida Nematzadeh"
                                },
                                {
                                    "authorId": "7782886",
                                    "name": "Sahand Sharifzadeh"
                                },
                                {
                                    "authorId": "9961753",
                                    "name": "Mikolaj Binkowski"
                                },
                                {
                                    "authorId": "2026369796",
                                    "name": "Ricardo Barreira"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "1688869",
                                    "name": "Andrew Zisserman"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 3607
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
                        ],
                        "paper": {
                            "corpus_id": 256390509,
                            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
                            "authors": [
                                {
                                    "authorId": "49299019",
                                    "name": "Junnan Li"
                                },
                                {
                                    "authorId": "2981509",
                                    "name": "Dongxu Li"
                                },
                                {
                                    "authorId": "1702137",
                                    "name": "S. Savarese"
                                },
                                {
                                    "authorId": "2184854289",
                                    "name": "Steven C. H. Hoi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 4653
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                        ],
                        "paper": {
                            "corpus_id": 258179774,
                            "title": "Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2109737569",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "31060482",
                                    "name": "Qingyang Wu"
                                },
                                {
                                    "authorId": "144756076",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4921
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "Multimodal Large Language Models (MLLMs) [1,(Alayrac et al., 2022)(Liu et al., 2023)40,41] have shown significant performance improvements in multi-modal problems such as visual question answering and image captioning by leveraging large-scale datasets to learn a joint embedding space where images and their corresponding textual descriptions are closely aligned. GPT-4o [1] has the ability to get the context and has a humanlike text generation ability, showing strong performance not only in the natural language processing area but also in multi-modal tasks. InternVL (Chen et al., 2023) can address both text and image data and shows better performances in various multimodal tasks (such as visual understanding, language generation, and visual QA) while using fewer computing resources compared to other MLLMs."
                        ],
                        "paper": {
                            "corpus_id": 275820726,
                            "title": "Multi-aspect Knowledge Distillation with Large Language Model",
                            "authors": [
                                {
                                    "authorId": "2238019356",
                                    "name": "Taegyeong Lee"
                                },
                                {
                                    "authorId": "2341715815",
                                    "name": "Jinsik Bang"
                                },
                                {
                                    "authorId": "2294517321",
                                    "name": "Soyeong Kwon"
                                },
                                {
                                    "authorId": "2174161125",
                                    "name": "Taehwan Kim"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Xia et al., 2025)",
                        "snippets": [
                            "Multimodal Large Language Models (MLLMs) extend this paradigm by incorporating additional modalities, e.g., imagery (Liu et al., 2023)(Yan et al., 2023), audio (Chu et al., 2023), graph (Tang et al., 2023), and structured charts (Han et al., 2023), alongside text."
                        ],
                        "paper": {
                            "corpus_id": 277857505,
                            "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2279954226",
                                    "name": "Yutong Xia"
                                },
                                {
                                    "authorId": "2283843071",
                                    "name": "Ao Qu"
                                },
                                {
                                    "authorId": "97527544",
                                    "name": "Yunhan Zheng"
                                },
                                {
                                    "authorId": "2243088943",
                                    "name": "Yihong Tang"
                                },
                                {
                                    "authorId": "1580217088",
                                    "name": "Dingyi Zhuang"
                                },
                                {
                                    "authorId": "2261965063",
                                    "name": "Yuxuan Liang"
                                },
                                {
                                    "authorId": "2334035234",
                                    "name": "Cathy Wu"
                                },
                                {
                                    "authorId": "2249532794",
                                    "name": "Roger Zimmermann"
                                },
                                {
                                    "authorId": "2283883570",
                                    "name": "Jinhua Zhao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9033203125
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored."
                        ],
                        "paper": {
                            "corpus_id": 271245054,
                            "title": "E5-V: Universal Embeddings with Multimodal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2259012212",
                                    "name": "Ting Jiang"
                                },
                                {
                                    "authorId": "2260342581",
                                    "name": "Minghui Song"
                                },
                                {
                                    "authorId": "2260851231",
                                    "name": "Zihan Zhang"
                                },
                                {
                                    "authorId": "2146285313",
                                    "name": "Haizhen Huang"
                                },
                                {
                                    "authorId": "2066621592",
                                    "name": "Weiwei Deng"
                                },
                                {
                                    "authorId": "2247156451",
                                    "name": "Feng Sun"
                                },
                                {
                                    "authorId": "2256972722",
                                    "name": "Qi Zhang"
                                },
                                {
                                    "authorId": "2145351849",
                                    "name": "Deqing Wang"
                                },
                                {
                                    "authorId": "2162961864",
                                    "name": "Fuzhen Zhuang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.89306640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "MLLMs with Text Output Capabilities",
                "tldr": "Multimodal Large Language Models primarily generate text outputs from various input modalities including images, videos, and audio. These models align visual representations with language model input spaces to enable coherent textual responses to multimodal inputs. (7 sources)",
                "text": "\nMultimodal Large Language Models (MLLMs) predominantly function as \"Any-to-Text\" systems, where they process various input modalities but produce textual outputs <Paper corpusId=\"274023126\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This architecture represents the most common approach to multimodality in current systems, where visual, audio, or other data types are encoded and projected into the word embedding space of the language model for generating coherent text responses <Paper corpusId=\"274023126\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nA key technical challenge in these systems involves effectively aligning visual representations with the input space of the language model. LLaVA exemplifies this approach by using the LLM's native self-attention mechanism to process visual information alongside textual input <Paper corpusId=\"269354784\" paperTitle=\"(Bazi et al., 2024)\" isShortName></Paper>. This alignment enables the model to generate descriptive text about visual content, answer questions about images, and provide detailed explanations based on visual inputs <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nMany MLLMs achieve this multimodal integration by pre-processing input modalities into textual descriptions that can be understood by the language model. For example, several approaches convert images into captions and object detection results into text descriptions to create multimodal training data <Paper corpusId=\"267637150\" paperTitle=\"(Tsai et al., 2024)\" isShortName></Paper>. This technique is particularly evident in specialized domains: OphGLM extracts information from fundus images to create diagnostic reports, while ChatCAD transforms X-ray outputs from CAD models into natural language <Paper corpusId=\"267637150\" paperTitle=\"(Tsai et al., 2024)\" isShortName></Paper>.\n\nThe architectural approach to text generation in MLLMs typically involves decoupling language modules into a text decoder for generation and a text encoder for embedding <Paper corpusId=\"270094821\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. This separation allows models to efficiently process multiple modalities while maintaining strong text generation capabilities. Similar principles extend beyond visual inputs to other modalities, as demonstrated by Audio Flamingo, which enables LLMs to understand and respond to non-speech sounds and non-verbal speech <Paper corpusId=\"267412182\" paperTitle=\"(Kong et al., 2024)\" isShortName></Paper>.\n\nRecent developments have expanded beyond text-only outputs to include speech-based interactions, enhancing user experience compared to traditional text-based interfaces. Models like GPT-4o now enable voice interaction with LLMs, representing an important advance in making multimodal systems more accessible and natural to use <Paper corpusId=\"274981694\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The first category includes Any-to-Text (Any Modality to Text) Models, which integrate inputs from multiple modalities, encode them, and project into the word embedding space of the LLM for generating textual output (Liu et al., 2023)- (Kong et al., 2024), (Zhu et al., 2023)- [30]."
                        ],
                        "paper": {
                            "corpus_id": 274023126,
                            "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2263703053",
                                    "name": "Xuannan Liu"
                                },
                                {
                                    "authorId": "2212020693",
                                    "name": "Xing Cui"
                                },
                                {
                                    "authorId": "2253583840",
                                    "name": "Peipei Li"
                                },
                                {
                                    "authorId": "2265220595",
                                    "name": "Zekun Li"
                                },
                                {
                                    "authorId": "2268583100",
                                    "name": "Huaibo Huang"
                                },
                                {
                                    "authorId": "2306060428",
                                    "name": "Shuhan Xia"
                                },
                                {
                                    "authorId": "2330734159",
                                    "name": "Miaoxuan Zhang"
                                },
                                {
                                    "authorId": "2305465534",
                                    "name": "Yueying Zou"
                                },
                                {
                                    "authorId": "2261958972",
                                    "name": "Ran He"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.91357421875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                        ],
                        "paper": {
                            "corpus_id": 258179774,
                            "title": "Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2109737569",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "31060482",
                                    "name": "Qingyang Wu"
                                },
                                {
                                    "authorId": "144756076",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4921
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bazi et al., 2024)",
                        "snippets": [
                            "The open-source project LLaVA [15] aims to replicate this performance by aligning visual representations with the input space of the LLM.By leveraging the original self-attention mechanism within the LLM, LLaVA enables effective processing of visual information alongside textual input."
                        ],
                        "paper": {
                            "corpus_id": 269354784,
                            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
                            "authors": [
                                {
                                    "authorId": "1795469",
                                    "name": "Y. Bazi"
                                },
                                {
                                    "authorId": "25206458",
                                    "name": "Laila Bashmal"
                                },
                                {
                                    "authorId": "2267672536",
                                    "name": "Mohamad Mahmoud Al Rahhal"
                                },
                                {
                                    "authorId": "2212706249",
                                    "name": "Riccardo Ricci"
                                },
                                {
                                    "authorId": "1774633",
                                    "name": "F. Melgani"
                                }
                            ],
                            "year": 2024,
                            "venue": "Remote Sensing",
                            "n_citations": 44
                        },
                        "score": 0.90234375
                    },
                    {
                        "id": "(Tsai et al., 2024)",
                        "snippets": [
                            "Several works involve processing images with foundation models and combining the results into text for LLM training, linking visual information with text. For example, LLaVA [26] utilizes GPT4 to transfer images into captions and object detection results into text descriptions as their multimodal training data. VideoChat-Text [25] encodes video into textual descriptions. In medicine, OphGLM [15] uses classification and segmentation models to extract information from fundus images and create diagnostic reports for LLMs. Similarly, ChatCAD [43] transforms X-ray outputs from CAD models into natural language for LLM input."
                        ],
                        "paper": {
                            "corpus_id": 267637150,
                            "title": "Text-centric Alignment for Multi-Modality Learning",
                            "authors": [
                                {
                                    "authorId": "3328096",
                                    "name": "Yun-Da Tsai"
                                },
                                {
                                    "authorId": "2283934604",
                                    "name": "Ting-Yu Yen"
                                },
                                {
                                    "authorId": "2256994069",
                                    "name": "Pei-Fu Guo"
                                },
                                {
                                    "authorId": "2283982736",
                                    "name": "Zhe-Yan Li"
                                },
                                {
                                    "authorId": "2237949904",
                                    "name": "Shou-De Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding."
                        ],
                        "paper": {
                            "corpus_id": 270094821,
                            "title": "Multi-Modal Generative Embedding Model",
                            "authors": [
                                {
                                    "authorId": "2218044713",
                                    "name": "Feipeng Ma"
                                },
                                {
                                    "authorId": "1878753365",
                                    "name": "Hongwei Xue"
                                },
                                {
                                    "authorId": "2303832848",
                                    "name": "Guangting Wang"
                                },
                                {
                                    "authorId": "2141747950",
                                    "name": "Yizhou Zhou"
                                },
                                {
                                    "authorId": "2276428924",
                                    "name": "Fengyun Rao"
                                },
                                {
                                    "authorId": "2303910455",
                                    "name": "Shilin Yan"
                                },
                                {
                                    "authorId": "2240587447",
                                    "name": "Yueyi Zhang"
                                },
                                {
                                    "authorId": "2303712729",
                                    "name": "Siying Wu"
                                },
                                {
                                    "authorId": "2303652188",
                                    "name": "Mike Zheng Shou"
                                },
                                {
                                    "authorId": "2303715824",
                                    "name": "Xiaoyan Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Kong et al., 2024)",
                        "snippets": [
                            "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo."
                        ],
                        "paper": {
                            "corpus_id": 267412182,
                            "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
                            "authors": [
                                {
                                    "authorId": "2229373549",
                                    "name": "Zhifeng Kong"
                                },
                                {
                                    "authorId": "2282529324",
                                    "name": "Arushi Goel"
                                },
                                {
                                    "authorId": "3433816",
                                    "name": "Rohan Badlani"
                                },
                                {
                                    "authorId": "2253664013",
                                    "name": "Wei Ping"
                                },
                                {
                                    "authorId": "2281035249",
                                    "name": "Rafael Valle"
                                },
                                {
                                    "authorId": "2264406909",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 94
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pham et al., 2024)",
                        "snippets": [
                            "Despite the significant advancements in VLMs, most of them currently support only text-based interactions, limiting their application to scenarios where text input is inconvenient or unavailable. In addition, while reasoning and prompting techniques for LLMs have been explored, these techniques for speech-based instruction remain largely underexplored. Recently, models like GPT-4o [42] have enabled interaction with LLMs through speech, significantly enhancing the user experience compared to traditional text-based interactions."
                        ],
                        "paper": {
                            "corpus_id": 274981694,
                            "title": "SilVar: Speech Driven Multimodal Model for Reasoning Visual Question Answering and Object Localization",
                            "authors": [
                                {
                                    "authorId": "2312271163",
                                    "name": "Tan-Hanh Pham"
                                },
                                {
                                    "authorId": "2337102747",
                                    "name": "Hoang-Nam Le"
                                },
                                {
                                    "authorId": "2337398686",
                                    "name": "Phu-Vinh Nguyen"
                                },
                                {
                                    "authorId": "2336865725",
                                    "name": "Chris Ngo"
                                },
                                {
                                    "authorId": "2308040088",
                                    "name": "Truong-Son Hy"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9013671875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "MLLMs with Image Output Capabilities",
                "tldr": "While many MLLMs primarily generate text, newer architectures have expanded to enable image generation capabilities through various approaches including text-to-layout generation, visual token production, and integration with diffusion models. (4 sources)",
                "text": "\nMultimodal Large Language Models have evolved beyond text-only outputs to incorporate image generation capabilities through several innovative approaches. One significant development has been the integration of text-to-image generation with layout planning. VPGEN exemplifies this approach by leveraging language models to handle layout generation, producing textual descriptions of objects, their counts, and positions, which enables more flexible adaptation of pretrained LMs to understand diverse region descriptions <Paper corpusId=\"258865230\" paperTitle=\"(Cho et al., 2023)\" isShortName></Paper>. This approach offers advantages over previous methods that were limited to predefined object classes and required training new layout predictor modules from scratch.\n\nThe development of image generation capabilities in MLLMs has been significantly influenced by pioneering models like DALL-E, which introduced a Transformer-based approach for converting text to images by treating images as sequences of discrete tokens <Paper corpusId=\"267412060\" paperTitle=\"(Jing et al., 2024)\" isShortName></Paper>. Building upon this foundation, researchers have achieved subsequent improvements through model scaling, pre-training, and enhanced image quantization techniques. Models like CM3 have demonstrated the ability to generate both images and text by training on large HTML corpora with randomly masked content <Paper corpusId=\"267412060\" paperTitle=\"(Jing et al., 2024)\" isShortName></Paper>.\n\nRecent research has expanded MLLMs' visual generation capabilities beyond the previously well-explored areas of visual understanding and reasoning <Paper corpusId=\"273403986\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. These newer approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. For image generation specifically, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders <Paper corpusId=\"273403986\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nSome MLLMs achieve image generation through integration with external models rather than generating images directly. For instance, HuggingGPT generates textual image descriptions that are then passed to external diffusion models to synthesize the actual images <Paper corpusId=\"272911245\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This cooperative approach leverages the strengths of specialized models while maintaining the multimodal capabilities of MLLMs.",
                "citations": [
                    {
                        "id": "(Cho et al., 2023)",
                        "snippets": [
                            "Bridging text-to-image generation with layouts. One line of research decomposes the T2I generation task into two stages: text-to-layout generation and layout-to-image generation [25; 26; 27; 28]. However, the previous approaches focus on a set of predefined object classes by training a new layout predictor module from scratch and therefore cannot place new objects unseen during training. In contrast, our VPGEN uses an LM to handle layout generation by generating objects/counts/positions in text, allowing flexible adaptation of pretrained LMs that can understand diverse region descriptions."
                        ],
                        "paper": {
                            "corpus_id": 258865230,
                            "title": "Visual Programming for Text-to-Image Generation and Evaluation",
                            "authors": [
                                {
                                    "authorId": "2706729",
                                    "name": "Jaemin Cho"
                                },
                                {
                                    "authorId": "2008198436",
                                    "name": "Abhaysinh Zala"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 51
                        },
                        "score": 0.8916015625
                    },
                    {
                        "id": "(Jing et al., 2024)",
                        "snippets": [
                            "The impressive achievements of LLMs have inspired significant advancements in vision-language research. DALL-E from OpenAI introduced a Transformer-based approach for converting text to images, treating images as sequences of discrete tokens. Subsequent improvements in the text-to-image area [38] have been achieved through methods like model scaling, pre-training, and enhanced image quantization models",
                            "CM3 [41] trained a randomly masked model on a large HTML corpus and showed that the model is capable of generating images and text."
                        ],
                        "paper": {
                            "corpus_id": 267412060,
                            "title": "When Large Language Models Meet Vector Databases: A Survey",
                            "authors": [
                                {
                                    "authorId": "15217571",
                                    "name": "Zhi Jing"
                                },
                                {
                                    "authorId": "2278407298",
                                    "name": "Yongye Su"
                                },
                                {
                                    "authorId": "2260277757",
                                    "name": "Yikun Han"
                                },
                                {
                                    "authorId": "2282533661",
                                    "name": "Bo Yuan"
                                },
                                {
                                    "authorId": "2260396712",
                                    "name": "Haiyun Xu"
                                },
                                {
                                    "authorId": "2260828894",
                                    "name": "Chunjiang Liu"
                                },
                                {
                                    "authorId": "2266796043",
                                    "name": "Kehai Chen"
                                },
                                {
                                    "authorId": "2267153486",
                                    "name": "Min Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "2025 Conference on Artificial Intelligence x Multimedia (AIxMM)",
                            "n_citations": 38
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Recent studies (Sun et al., 2023;Ge et al., 2024b) have explored MLLM's potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders."
                        ],
                        "paper": {
                            "corpus_id": 273403986,
                            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
                            "authors": [
                                {
                                    "authorId": "144484147",
                                    "name": "Rongyao Fang"
                                },
                                {
                                    "authorId": "2326302007",
                                    "name": "Chengqi Duan"
                                },
                                {
                                    "authorId": "2267252383",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2274232642",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "2274118690",
                                    "name": "Hao Tian"
                                },
                                {
                                    "authorId": "2257433221",
                                    "name": "Xingyu Zeng"
                                },
                                {
                                    "authorId": "2319817758",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2292283383",
                                    "name": "Jifeng Dai"
                                },
                                {
                                    "authorId": "2268799275",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "2286520347",
                                    "name": "Xihui Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 15
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "For example, HuggingGPT (Shen et al., 2023) generates textual image descriptions for external diffusion models to synthesize images."
                        ],
                        "paper": {
                            "corpus_id": 272911245,
                            "title": "MIO: A Foundation Model on Multimodal Tokens",
                            "authors": [
                                {
                                    "authorId": "2252413754",
                                    "name": "Z. Wang"
                                },
                                {
                                    "authorId": "2323111394",
                                    "name": "King Zhu"
                                },
                                {
                                    "authorId": "2280285922",
                                    "name": "Chunpu Xu"
                                },
                                {
                                    "authorId": "2284803168",
                                    "name": "Wangchunshu Zhou"
                                },
                                {
                                    "authorId": "2294523552",
                                    "name": "Jiaheng Liu"
                                },
                                {
                                    "authorId": "2277149577",
                                    "name": "Yibo Zhang"
                                },
                                {
                                    "authorId": "2310651349",
                                    "name": "Jiashuo Wang"
                                },
                                {
                                    "authorId": "2322957423",
                                    "name": "Ning Shi"
                                },
                                {
                                    "authorId": "2323142735",
                                    "name": "Siyu Li"
                                },
                                {
                                    "authorId": "2129449392",
                                    "name": "Yizhi Li"
                                },
                                {
                                    "authorId": "2249762724",
                                    "name": "Haoran Que"
                                },
                                {
                                    "authorId": "2322607797",
                                    "name": "Zhaoxiang Zhang"
                                },
                                {
                                    "authorId": "2279778502",
                                    "name": "Yuanxing Zhang"
                                },
                                {
                                    "authorId": "2307558672",
                                    "name": "Ge Zhang"
                                },
                                {
                                    "authorId": "2249845515",
                                    "name": "Ke Xu"
                                },
                                {
                                    "authorId": "2253966125",
                                    "name": "Jie Fu"
                                },
                                {
                                    "authorId": "2239245627",
                                    "name": "Wenhao Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.89697265625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "MLLMs with Combined Text and Image Output",
                "tldr": "Recent advances in multimodal language models have enabled systems that can generate both text and images simultaneously. These models use various approaches including interleaved generation, joint autoregressive frameworks, and visual token integration to produce coherent multimodal outputs. (14 sources)",
                "text": "\nMultimodal Large Language Models (MLLMs) have evolved beyond single-modality outputs to systems capable of generating both text and images in a unified manner. Early steps toward this integration began with alignment of latent representations between modalities. For instance, Contrastive Language-Image Pretraining (CLIP) aligned image and text encoders to enable caption generation from images <Paper corpusId=\"233168627\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nSignificant advancements have been made in creating MLLMs that can process and generate interleaved image-text content. Models like Flamingo demonstrated the ability to \"process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images\" <Paper corpusId=\"256416164\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258947258\" paperTitle=\"(Koh et al._1, 2023)\" isShortName></Paper>. This approach maintains the language model's core functionality while adding cross-modality interactions through fine-tuned input and output layers.\n\nBuilding on this foundation, GILL (Generating Images with Large Language Models) expanded multimodal capabilities to include novel image generation alongside text, establishing a more comprehensive multimodal system <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al._2, 2023)\" isShortName></Paper>. Similarly, FROMAGe implemented image-text inputs and outputs for multimodal dialogue while keeping the pre-trained language model frozen <Paper corpusId=\"259164940\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nFor coherent multimodal content generation, some approaches use specialized representations that bridge visual and textual elements. The concept of \"generative vokens\" enables interleaved vision-and-language generation with coherent image-text outputs <Paper corpusId=\"263608981\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. Other systems like MAnTiS use modality-specific encoders that project to a common textual token space to form a conditionality prefix guiding the generation process <Paper corpusId=\"237417089\" paperTitle=\"(Sollami et al., 2021)\" isShortName></Paper>.\n\nMore recent research has focused on creating unified frameworks specifically designed for mixed-modal generation. The Joint Autoregressive Mixture (JAM) framework systematically fuses existing text and image generation models with a specialized instruction-tuning strategy for mixed-modal tasks <Paper corpusId=\"262898422\" paperTitle=\"(Aiello et al., 2023)\" isShortName></Paper>. This approach has demonstrated high-quality multimodal outputs and represents one of the first models explicitly designed for this purpose.\n\nThe trend toward unified multimodal language models continues with systems capable of both comprehension and generation <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. Some approaches like Chameleon, Show-o, and Emu3 directly adopt VQ tokenizers to encode images for multimodal understanding and generation, though aligning visual tokens with language tokens remains challenging <Paper corpusId=\"274305717\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>. Other models use continuous representations interleaved with text tokens for autoregressive modeling to generate images <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\nThese advances represent significant progress toward the goal of creating MLLMs that can seamlessly process multimodal inputs and generate coherent multimodal outputs, treating text and images as interchangeable elements within a unified representation space <Paper corpusId=\"265498814\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Franceschelli et al., 2021)",
                        "snippets": [
                            "The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance, Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) has an image encoder pre-trained together with a text encoder to generate a caption for an image."
                        ],
                        "paper": {
                            "corpus_id": 233168627,
                            "title": "Creativity and Machine Learning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2067291198",
                                    "name": "Giorgio Franceschelli"
                                },
                                {
                                    "authorId": "1806767",
                                    "name": "Mirco Musolesi"
                                }
                            ],
                            "year": 2021,
                            "venue": "ACM Computing Surveys",
                            "n_citations": 42
                        },
                        "score": 0.97021484375
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Koh et al., 2023)",
                        "snippets": [
                            "This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images."
                        ],
                        "paper": {
                            "corpus_id": 256416164,
                            "title": "Grounding Language Models to Images for Multimodal Generation",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 93
                        },
                        "score": 0.95556640625
                    },
                    {
                        "id": "(Koh et al._1, 2023)",
                        "snippets": [
                            "and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images."
                        ],
                        "paper": {
                            "corpus_id": 258947258,
                            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 122
                        },
                        "score": 0.966796875
                    },
                    {
                        "id": "(Koh et al._2, 2023)",
                        "snippets": [
                            "In this work, we tackle the task of extending multimodal language models to generate novel images. Our approach, Generating Images with Large Language Models (GILL), is capable of processing arbitrarily interleaved image-and-text inputs to generate text, retrieve images, and generate novel images (Fig. 1)."
                        ],
                        "paper": {
                            "corpus_id": 258959284,
                            "title": "Generating Images with Multimodal Language Models",
                            "authors": [
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "47070750",
                                    "name": "Daniel Fried"
                                },
                                {
                                    "authorId": "145124475",
                                    "name": "R. Salakhutdinov"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 259
                        },
                        "score": 0.984375
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Another relevant work, FROMAGe (Jing Yu Koh, 2023), also involves image-text inputs and outputs for multimodal dialogue",
                            "FROMAGe (Jing Yu Koh, 2023) also involves image-text inputs and outputs for multimodal dialogue, with a few linear layers fine-tuned while keeping the pre-trained language model frozen."
                        ],
                        "paper": {
                            "corpus_id": 259164940,
                            "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
                            "authors": [
                                {
                                    "authorId": "2187404140",
                                    "name": "Zhiwei Zhang"
                                },
                                {
                                    "authorId": "2108353180",
                                    "name": "Yuliang Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 0
                        },
                        "score": 0.927734375
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs."
                        ],
                        "paper": {
                            "corpus_id": 263608981,
                            "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
                            "authors": [
                                {
                                    "authorId": "2253444275",
                                    "name": "Kaizhi Zheng"
                                },
                                {
                                    "authorId": "2149253467",
                                    "name": "Xuehai He"
                                },
                                {
                                    "authorId": "2256599634",
                                    "name": "Xin Eric Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 98
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Sollami et al., 2021)",
                        "snippets": [
                            "We propose MAnTiS, Multimodal Adaptation for Text Synthesis, a general approach for multimodal conditionality in transformer-based NLG models. In this method, we pass inputs from each modality through modality-specific encoders, project to textual token space, and finally join to form a conditionality prefix. We fine-tune the pretrained language model and encoders with the conditionality prefix guiding the generation. We apply MAnTiS to the task of product description generation, conditioning a network on both product images and titles to generate descriptive text."
                        ],
                        "paper": {
                            "corpus_id": 237417089,
                            "title": "Multimodal Conditionality for Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "3423000",
                                    "name": "Michael Sollami"
                                },
                                {
                                    "authorId": "10818273",
                                    "name": "Aashish Jain"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.95361328125
                    },
                    {
                        "id": "(Aiello et al., 2023)",
                        "snippets": [
                            "To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose."
                        ],
                        "paper": {
                            "corpus_id": 262898422,
                            "title": "Jointly Training Large Autoregressive Multimodal Models",
                            "authors": [
                                {
                                    "authorId": "2185505465",
                                    "name": "Emanuele Aiello"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2247227174",
                                    "name": "Yixin Nie"
                                },
                                {
                                    "authorId": "2201435",
                                    "name": "Armen Aghajanyan"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 31
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Xie et al., 2024)",
                        "snippets": [
                            "In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images."
                        ],
                        "paper": {
                            "corpus_id": 271924334,
                            "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "2306603347",
                                    "name": "Jinheng Xie"
                                },
                                {
                                    "authorId": "2258958963",
                                    "name": "Weijia Mao"
                                },
                                {
                                    "authorId": "2237427303",
                                    "name": "Zechen Bai"
                                },
                                {
                                    "authorId": "2248388634",
                                    "name": "David Junhao Zhang"
                                },
                                {
                                    "authorId": "2316668014",
                                    "name": "Weihao Wang"
                                },
                                {
                                    "authorId": "2298569956",
                                    "name": "Kevin Qinghong Lin"
                                },
                                {
                                    "authorId": "2248678348",
                                    "name": "Yuchao Gu"
                                },
                                {
                                    "authorId": "2316662510",
                                    "name": "Zhijie Chen"
                                },
                                {
                                    "authorId": "2309246065",
                                    "name": "Zhenheng Yang"
                                },
                                {
                                    "authorId": "2244623976",
                                    "name": "Mike Zheng Shou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 229
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Xie et al._1, 2024)",
                        "snippets": [
                            "Other works like Chameleon [52], Show-o [62] and Emu3 [59] have tried to directly adopt the VQ tokenizer to encode images for both multimodal understanding and generation. However, since these visual tokenizers do not contain semantic information, aligning visual tokens with language tokens becomes difficult, and these models usually yield suboptimal performance in multimodal understanding tasks."
                        ],
                        "paper": {
                            "corpus_id": 274305717,
                            "title": "MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding",
                            "authors": [
                                {
                                    "authorId": "2332574955",
                                    "name": "Rongchang Xie"
                                },
                                {
                                    "authorId": "2332709350",
                                    "name": "Chen Du"
                                },
                                {
                                    "authorId": "2332538297",
                                    "name": "Ping Song"
                                },
                                {
                                    "authorId": "2333419477",
                                    "name": "Chang Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Tang et al., 2023)",
                        "snippets": [
                            "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"
                        ],
                        "paper": {
                            "corpus_id": 258822817,
                            "title": "Any-to-Any Generation via Composable Diffusion",
                            "authors": [
                                {
                                    "authorId": "151270642",
                                    "name": "Zineng Tang"
                                },
                                {
                                    "authorId": "2155459391",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                },
                                {
                                    "authorId": "48262024",
                                    "name": "Michael Zeng"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."
                        ],
                        "paper": {
                            "corpus_id": 261975252,
                            "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
                            "authors": [
                                {
                                    "authorId": "2056965063",
                                    "name": "Runpei Dong"
                                },
                                {
                                    "authorId": "2118643247",
                                    "name": "Chunrui Han"
                                },
                                {
                                    "authorId": "2211415443",
                                    "name": "Yuang Peng"
                                },
                                {
                                    "authorId": "3424017",
                                    "name": "Zekun Qi"
                                },
                                {
                                    "authorId": "2242581956",
                                    "name": "Zheng Ge"
                                },
                                {
                                    "authorId": "2161319190",
                                    "name": "Jinrong Yang"
                                },
                                {
                                    "authorId": "48096671",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "26913717",
                                    "name": "Jian\u2010Yuan Sun"
                                },
                                {
                                    "authorId": "2157473950",
                                    "name": "Hongyu Zhou"
                                },
                                {
                                    "authorId": "134085586",
                                    "name": "Hao-Ran Wei"
                                },
                                {
                                    "authorId": "2069029697",
                                    "name": "Xiangwen Kong"
                                },
                                {
                                    "authorId": "2185865433",
                                    "name": "Xiangyu Zhang"
                                },
                                {
                                    "authorId": "2244131405",
                                    "name": "Kaisheng Ma"
                                },
                                {
                                    "authorId": "2242612318",
                                    "name": "Li Yi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 201
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer."
                        ],
                        "paper": {
                            "corpus_id": 265498814,
                            "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1491798621",
                                    "name": "Bohao Li"
                                },
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2243452799",
                                    "name": "Guangzhi Wang"
                                },
                                {
                                    "authorId": "2151036422",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "2268726753",
                                    "name": "Ruimao Zhang"
                                },
                                {
                                    "authorId": "2265579883",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 69
                        },
                        "score": 0.95654296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Examples of Multimodal Outputs",
                "tldr": "Multimodal language models with combined text and image output capabilities enable diverse real-world applications spanning creative content generation, visual editing, education, and healthcare. These systems demonstrate practical utility through text-guided image generation, interactive visual content creation, multimodal document production, and specialized domain applications. (8 sources)",
                "text": "\nMultimodal Large Language Models (MLLMs) with output capabilities across both text and visual modalities have enabled numerous practical applications:\n\n## Content Creation and Editing\n- **Text-to-Image Generation**: Systems built on diffusion models can create highly detailed images from textual descriptions, allowing users to generate artwork, illustrations, and visual content without specialized skills <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"219955663\" paperTitle=\"(Ho et al., 2020)\" isShortName></Paper>.\n- **Image Editing Through Language**: Advanced MLLMs enable users to manipulate existing images through natural language instructions rather than pixel-level editing, such as changing attributes or inserting new elements <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>.\n- **Subject-Driven Generation**: State-of-the-art MLLMs can perform targeted visual creation where specific subjects or themes are maintained across multiple generated images <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\n## Multimodal Communication\n- **Visual Storytelling**: Models can generate coherent narratives combining text and supporting imagery, enhancing engagement and comprehension <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n- **Interactive Assistants**: MLLMs power conversational agents that can both explain concepts textually and visualize them on demand <Paper corpusId=\"276929408\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n- **Document Creation**: Systems can produce multimodal documents including reports, presentations, and educational materials with integrated text and visual elements <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n## Specialized Domain Applications\n- **Educational Tools**: MLLMs create interactive learning materials that combine explanatory text with supporting visualizations, enhancing understanding of complex concepts <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n- **Healthcare Visualization**: Medical applications where textual diagnoses are paired with visual representations of conditions or anatomical structures <Paper corpusId=\"270094821\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n- **Design Assistance**: Architecture and design applications where textual specifications can be transformed into visual mock-ups and iteratively refined through language <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n- **Accessibility Solutions**: Systems that can transform visual information into descriptive text and vice versa, improving information access for users with different needs <Paper corpusId=\"271218251\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n## Emerging Applications\n- **Visual Reasoning**: MLLMs demonstrate capabilities to solve problems requiring reasoning about visual information and expressing the solution process in text with supporting visuals <Paper corpusId=\"247951931\" paperTitle=\"(Chowdhery et al., 2022)\" isShortName></Paper>.\n- **Multimodal Brainstorming**: Creative applications where text prompts generate diverse visual variations to inspire designers and creators <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n- **Visual Prompting**: Advanced models can understand visual inputs and generate appropriate responses with both textual and visual components <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ho et al., 2020)",
                        "snippets": [
                            "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"
                        ],
                        "paper": {
                            "corpus_id": 219955663,
                            "title": "Denoising Diffusion Probabilistic Models",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                },
                                {
                                    "authorId": "1623995772",
                                    "name": "Ajay Jain"
                                },
                                {
                                    "authorId": "1689992",
                                    "name": "P. Abbeel"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 18352
                        },
                        "score": 0
                    },
                    {
                        "id": "(Brooks et al., 2022)",
                        "snippets": [
                            "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."
                        ],
                        "paper": {
                            "corpus_id": 253581213,
                            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                            "authors": [
                                {
                                    "authorId": "2679394",
                                    "name": "Tim Brooks"
                                },
                                {
                                    "authorId": "2248172435",
                                    "name": "Aleksander Holynski"
                                },
                                {
                                    "authorId": "1763086",
                                    "name": "Alexei A. Efros"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1833
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2023)",
                        "snippets": [
                            "The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."
                        ],
                        "paper": {
                            "corpus_id": 266374640,
                            "title": "Generative Multimodal Models are In-Context Learners",
                            "authors": [
                                {
                                    "authorId": "2264040642",
                                    "name": "Quan Sun"
                                },
                                {
                                    "authorId": "2263702215",
                                    "name": "Yufeng Cui"
                                },
                                {
                                    "authorId": "2264589602",
                                    "name": "Xiaosong Zhang"
                                },
                                {
                                    "authorId": "2264274388",
                                    "name": "Fan Zhang"
                                },
                                {
                                    "authorId": "23716915",
                                    "name": "Qiying Yu"
                                },
                                {
                                    "authorId": "2275808712",
                                    "name": "Zhengxiong Luo"
                                },
                                {
                                    "authorId": "2217456303",
                                    "name": "Yueze Wang"
                                },
                                {
                                    "authorId": "2275170369",
                                    "name": "Yongming Rao"
                                },
                                {
                                    "authorId": "2222717281",
                                    "name": "Jingjing Liu"
                                },
                                {
                                    "authorId": "2257137554",
                                    "name": "Tiejun Huang"
                                },
                                {
                                    "authorId": "2263959521",
                                    "name": "Xinlong Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 290
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Since humans interact with their environment through multi-modal sensory input, the evolution from text-only LLM systems to multimodal LLMs significantly increases the potential usage cases for LLMs, prompting many applications [35]",
                            "models designed for input in one modality and output in another, such as text-to-image (Rombach et al., 2021)[27] and image-to-text [37] models."
                        ],
                        "paper": {
                            "corpus_id": 271218251,
                            "title": "Visualization Literacy of Multimodal Large Language Models: A Comparative Study",
                            "authors": [
                                {
                                    "authorId": "2116785555",
                                    "name": "Zhimin Li"
                                },
                                {
                                    "authorId": "2253579462",
                                    "name": "Haichao Miao"
                                },
                                {
                                    "authorId": "2251997858",
                                    "name": "Valerio Pascucci"
                                },
                                {
                                    "authorId": "2289268771",
                                    "name": "Shusen Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Fang et al., 2025)",
                        "snippets": [
                            "In response to the notable progress of large language models in natural language processing [1,2,(Chowdhery et al., 2022), the field of multimodal large language models (MLLM) has made significant strides in recent years. MLLMs leverage both natural language and visual inputs, allowing these models to understand and manipulate visual data guided by textual descriptions. This dual-modality capability builds upon foundational image generation models, such as GANs [12] and diffusion models (Ho et al., 2020), but advances them by incorporating language as a critical component in model design. Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."
                        ],
                        "paper": {
                            "corpus_id": 276929408,
                            "title": "Recognition-Synergistic Scene Text Editing",
                            "authors": [
                                {
                                    "authorId": "2313679002",
                                    "name": "Zhengyao Fang"
                                },
                                {
                                    "authorId": "10344582",
                                    "name": "Pengyuan Lyu"
                                },
                                {
                                    "authorId": "2313920076",
                                    "name": "Jingjing Wu"
                                },
                                {
                                    "authorId": "2248958848",
                                    "name": "Chengquan Zhang"
                                },
                                {
                                    "authorId": "2349736964",
                                    "name": "Jun Yu"
                                },
                                {
                                    "authorId": "2261251059",
                                    "name": "Guan-Lun Lu"
                                },
                                {
                                    "authorId": "1678473",
                                    "name": "Wenjie Pei"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.93994140625
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding."
                        ],
                        "paper": {
                            "corpus_id": 270094821,
                            "title": "Multi-Modal Generative Embedding Model",
                            "authors": [
                                {
                                    "authorId": "2218044713",
                                    "name": "Feipeng Ma"
                                },
                                {
                                    "authorId": "1878753365",
                                    "name": "Hongwei Xue"
                                },
                                {
                                    "authorId": "2303832848",
                                    "name": "Guangting Wang"
                                },
                                {
                                    "authorId": "2141747950",
                                    "name": "Yizhou Zhou"
                                },
                                {
                                    "authorId": "2276428924",
                                    "name": "Fengyun Rao"
                                },
                                {
                                    "authorId": "2303910455",
                                    "name": "Shilin Yan"
                                },
                                {
                                    "authorId": "2240587447",
                                    "name": "Yueyi Zhang"
                                },
                                {
                                    "authorId": "2303712729",
                                    "name": "Siying Wu"
                                },
                                {
                                    "authorId": "2303652188",
                                    "name": "Mike Zheng Shou"
                                },
                                {
                                    "authorId": "2303715824",
                                    "name": "Xiaoyan Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Chowdhery et al., 2022)",
                        "snippets": [
                            "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                        ],
                        "paper": {
                            "corpus_id": 247951931,
                            "title": "PaLM: Scaling Language Modeling with Pathways",
                            "authors": [
                                {
                                    "authorId": "2841893",
                                    "name": "Aakanksha Chowdhery"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2159632445",
                                    "name": "Gaurav Mishra"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "152399055",
                                    "name": "P. Barham"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "152549864",
                                    "name": "Charles Sutton"
                                },
                                {
                                    "authorId": "3159346",
                                    "name": "Sebastian Gehrmann"
                                },
                                {
                                    "authorId": "2620528",
                                    "name": "Parker Schuh"
                                },
                                {
                                    "authorId": "2362367",
                                    "name": "Kensen Shi"
                                },
                                {
                                    "authorId": "2160888237",
                                    "name": "Sasha Tsvyashchenko"
                                },
                                {
                                    "authorId": "2124977868",
                                    "name": "Joshua Maynez"
                                },
                                {
                                    "authorId": "1484043592",
                                    "name": "Abhishek Rao"
                                },
                                {
                                    "authorId": "80940648",
                                    "name": "Parker Barnes"
                                },
                                {
                                    "authorId": "144447820",
                                    "name": "Yi Tay"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3331141",
                                    "name": "Vinodkumar Prabhakaran"
                                },
                                {
                                    "authorId": "49849144",
                                    "name": "Emily Reif"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "2161431901",
                                    "name": "Reiner Pope"
                                },
                                {
                                    "authorId": "2065251344",
                                    "name": "James Bradbury"
                                },
                                {
                                    "authorId": "2058365883",
                                    "name": "Jacob Austin"
                                },
                                {
                                    "authorId": "2090818",
                                    "name": "M. Isard"
                                },
                                {
                                    "authorId": "2284681044",
                                    "name": "Guy Gur-Ari"
                                },
                                {
                                    "authorId": "38253388",
                                    "name": "Pengcheng Yin"
                                },
                                {
                                    "authorId": "2145151992",
                                    "name": "Toju Duke"
                                },
                                {
                                    "authorId": "6639036",
                                    "name": "Anselm Levskaya"
                                },
                                {
                                    "authorId": "1780892",
                                    "name": "S. Ghemawat"
                                },
                                {
                                    "authorId": "50991767",
                                    "name": "Sunipa Dev"
                                },
                                {
                                    "authorId": "47407464",
                                    "name": "H. Michalewski"
                                },
                                {
                                    "authorId": "143936294",
                                    "name": "Xavier Garc\u00eda"
                                },
                                {
                                    "authorId": "40055795",
                                    "name": "Vedant Misra"
                                },
                                {
                                    "authorId": "2148473059",
                                    "name": "Kevin Robinson"
                                },
                                {
                                    "authorId": "2096916416",
                                    "name": "L. Fedus"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                },
                                {
                                    "authorId": "7975935",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "150970919",
                                    "name": "D. Luan"
                                },
                                {
                                    "authorId": "8939217",
                                    "name": "Hyeontaek Lim"
                                },
                                {
                                    "authorId": "2368067",
                                    "name": "Barret Zoph"
                                },
                                {
                                    "authorId": "1572884723",
                                    "name": "A. Spiridonov"
                                },
                                {
                                    "authorId": "35474601",
                                    "name": "Ryan Sepassi"
                                },
                                {
                                    "authorId": "35363891",
                                    "name": "David Dohan"
                                },
                                {
                                    "authorId": "3504647",
                                    "name": "Shivani Agrawal"
                                },
                                {
                                    "authorId": "3175815",
                                    "name": "Mark Omernick"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2598683",
                                    "name": "Thanumalayan Sankaranarayana Pillai"
                                },
                                {
                                    "authorId": "97905921",
                                    "name": "Marie Pellat"
                                },
                                {
                                    "authorId": "102549875",
                                    "name": "Aitor Lewkowycz"
                                },
                                {
                                    "authorId": "2057453483",
                                    "name": "Erica Moreira"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "2636739",
                                    "name": "Oleksandr Polozov"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "2198519",
                                    "name": "Zongwei Zhou"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "4125424",
                                    "name": "Brennan Saeta"
                                },
                                {
                                    "authorId": "2152965375",
                                    "name": "Mark D\u00edaz"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "1754926",
                                    "name": "Michele Catasta"
                                },
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1398655031",
                                    "name": "K. Meier-Hellstern"
                                },
                                {
                                    "authorId": "2396681",
                                    "name": "D. Eck"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                },
                                {
                                    "authorId": "1754497",
                                    "name": "Slav Petrov"
                                },
                                {
                                    "authorId": "22640071",
                                    "name": "Noah Fiedel"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 6293
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.125322
    }
}