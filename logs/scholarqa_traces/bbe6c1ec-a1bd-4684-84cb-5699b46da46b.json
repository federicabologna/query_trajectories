{
    "query": "What are the latest implementation-level techniques, evaluations, and challenges of fine-tuning both retriever and generator components (including dual fine-tuning) in Retrieval-Augmented Generation (RAG) systems, with a focus on methods like RAFT, CRAFT, Self-RAG, and Finetune-RAG?",
    "user_id": "lib_user",
    "task_id": "bbe6c1ec-a1bd-4684-84cb-5699b46da46b",
    "timestamp": "2025-06-23T21:41:57.196857",
    "n_retrieval": 256,
    "n_retrieved": 255,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.38608800000000004,
    "decomposed_query": {
        "rewritten_query": "Implementation-level techniques, evaluations, and challenges of fine-tuning both retriever and generator components (including dual fine-tuning) in Retrieval-Augmented Generation (RAG) systems, with a focus on methods like RAFT, CRAFT, Self-RAG, and Finetune-RAG.",
        "keyword_query": "implementation techniques evaluations challenges fine-tuning retriever generator components dual fine-tuning Retrieval-Augmented Generation RAG RAFT CRAFT Self-RAG Finetune-RAG",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.011181,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "2290870875",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "corpus_id": 275993994,
            "sentences": [
                {
                    "corpus_id": "275993994",
                    "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
                    "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
                    "score": 0.8173813341354541,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.955078125
                }
            ],
            "relevance_judgement": 0.955078125,
            "relevance_judgment_input_expanded": "# Title: RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects\n# Venue: arXiv.org\n# Authors: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai\n## Abstract\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.\n",
            "reference_string": "[275993994 | Tu et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273537815",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2308276345",
                    "name": "Zhenghua Wang"
                },
                {
                    "authorId": "2292070745",
                    "name": "Xuan Gao"
                },
                {
                    "authorId": "2308226671",
                    "name": "Feiran Zhang"
                },
                {
                    "authorId": "2308043953",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "2308044030",
                    "name": "Zhibo Xu"
                },
                {
                    "authorId": "2308036711",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2309182278",
                    "name": "Zhengyuan Wang"
                },
                {
                    "authorId": "2309656885",
                    "name": "Shizheng Li"
                },
                {
                    "authorId": "2309176521",
                    "name": "Qi Qian"
                },
                {
                    "authorId": "2292032843",
                    "name": "Ruicheng Yin"
                },
                {
                    "authorId": "2220896023",
                    "name": "Changze Lv"
                },
                {
                    "authorId": "2257315404",
                    "name": "Xiaoqing Zheng"
                },
                {
                    "authorId": "2257129987",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
            "corpus_id": 270870251,
            "sentences": [
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
                    "score": 0.9393830879509706,
                    "section_title": "Retriever and Generator Fine-tuning",
                    "char_start_offset": 7670,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 94,
                            "end": 242
                        },
                        {
                            "start": 242,
                            "end": 345
                        },
                        {
                            "start": 345,
                            "end": 551
                        },
                        {
                            "start": 553,
                            "end": 739
                        },
                        {
                            "start": 739,
                            "end": 910
                        },
                        {
                            "start": 910,
                            "end": 1037
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 181,
                            "end": 185,
                            "matchedPaperCorpusId": "258865283"
                        },
                        {
                            "start": 658,
                            "end": 660,
                            "matchedPaperCorpusId": "250340214"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9306640625
                },
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.",
                    "score": 0.7511701234507469,
                    "section_title": "Related Work",
                    "char_start_offset": 4844,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 127,
                            "end": 296
                        },
                        {
                            "start": 296,
                            "end": 491
                        },
                        {
                            "start": 491,
                            "end": 665
                        },
                        {
                            "start": 665,
                            "end": 829
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 227,
                            "end": 230,
                            "matchedPaperCorpusId": "266164171"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.818359375
                }
            ],
            "relevance_judgement": 0.9306640625,
            "relevance_judgment_input_expanded": "# Title: Searching for Best Practices in Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang\n## Abstract\nRetrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.\n## Related Work\nEnsuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.\n\n## Retriever and Generator Fine-tuning\nFine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
            "reference_string": "[270870251 | Wang et al. | 2024 | Citations: 61]"
        },
        {
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "venue": "",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362089035",
                    "name": "Zhan Peng Lee"
                },
                {
                    "authorId": "2362188632",
                    "name": "Andre Lin"
                },
                {
                    "authorId": "2363425126",
                    "name": "Calvin Tan"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.",
            "corpus_id": 278714952,
            "sentences": [
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
                    "score": 0.7431968047566778,
                    "section_title": "Methodology",
                    "char_start_offset": 8999,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 632
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9287109375
                },
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "In this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in Retrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on retrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while ignoring misleading information, with no architectural changes required. \n\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages GPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts robustness, with less structured formats sometimes aiding discrimination. \n\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully improve hallucination resistance in noisy retrieval environments. We release our code, dataset, and checkpoints to support further research in this direction, and highlight future extensions including multi-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.",
                    "score": 0.7957693704376294,
                    "section_title": "Conclusion",
                    "char_start_offset": 23325,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 364
                        },
                        {
                            "start": 367,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1143
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85400390625
                }
            ],
            "relevance_judgement": 0.9287109375,
            "relevance_judgment_input_expanded": "# Title: Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation\n# Venue: \n# Authors: Zhan Peng Lee, Andre Lin, Calvin Tan\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.\n## Methodology\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.\n\n## Conclusion\nIn this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in Retrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on retrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while ignoring misleading information, with no architectural changes required. \n\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages GPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts robustness, with less structured formats sometimes aiding discrimination. \n\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully improve hallucination resistance in noisy retrieval environments. We release our code, dataset, and checkpoints to support further research in this direction, and highlight future extensions including multi-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.",
            "reference_string": "[278714952 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "venue": "ACM Trans. Inf. Syst.",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 40,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3701228",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187857206",
                    "name": "Yuanjie Lyu"
                },
                {
                    "authorId": "2268429641",
                    "name": "Zhiyu Li"
                },
                {
                    "authorId": "2268393907",
                    "name": "Simin Niu"
                },
                {
                    "authorId": "2268399953",
                    "name": "Feiyu Xiong"
                },
                {
                    "authorId": "2268400606",
                    "name": "Bo Tang"
                },
                {
                    "authorId": "2117833477",
                    "name": "Wenjin Wang"
                },
                {
                    "authorId": "2282083454",
                    "name": "Hao Wu"
                },
                {
                    "authorId": "2304320758",
                    "name": "Huan Liu"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.",
            "corpus_id": 267320876,
            "sentences": [
                {
                    "corpus_id": "267320876",
                    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                    "text": "In this paper, we have introduced an innovative framework (CRUD-RAG) for evaluating retrievalaugmented generation (RAG) systems that is both comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD-Create, Read, Update, and Delete-types provides a structured approach to assess the capabilities and limitations of RAG systems in handling a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed largescale datasets for each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources. \n\nOur study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results presented in this paper will spur further exploration and innovation in the realm of RAG technologies. Our work aims to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.",
                    "score": 0.6406703796399257,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 67227,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 850
                        },
                        {
                            "start": 853,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1473
                        },
                        {
                            "start": 1474,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1832
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9140625
                }
            ],
            "relevance_judgement": 0.9140625,
            "relevance_judgment_input_expanded": "# Title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models\n# Venue: ACM Trans. Inf. Syst.\n# Authors: Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huan Liu, Tong Xu, Enhong Chen\n## Abstract\nRetrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.\n## CONCLUSION\nIn this paper, we have introduced an innovative framework (CRUD-RAG) for evaluating retrievalaugmented generation (RAG) systems that is both comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD-Create, Read, Update, and Delete-types provides a structured approach to assess the capabilities and limitations of RAG systems in handling a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed largescale datasets for each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources. \n\nOur study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results presented in this paper will spur further exploration and innovation in the realm of RAG technologies. Our work aims to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.",
            "reference_string": "[267320876 | Lyu et al. | 2024 | Citations: 40]"
        },
        {
            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.03075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2195381022",
                    "name": "Zhengliang Shi"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "2153198380",
                    "name": "Weiwei Sun"
                },
                {
                    "authorId": "2326805997",
                    "name": "Yue Feng"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                },
                {
                    "authorId": "2265517632",
                    "name": "Xinyu Ma"
                },
                {
                    "authorId": "2237948548",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2331316040",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2265490493",
                    "name": "M. D. Rijke"
                },
                {
                    "authorId": "2261862546",
                    "name": "Zhaochun Ren"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) integrates large language models ( LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most previous work independently fine-tunes the retriever to adapt to frozen LLM s or trains the LLMs to use documents retrieved by off-the-shelf retrievers, lacking end-to-end training supervision. Recent work addresses this limitation by jointly training these two components but relies on overly simplifying assumptions of document independence, which has been criticized for being far from real-world scenarios. Thus, effectively optimizing the overall RAG performance remains a critical challenge. We propose a direct retrieval-augmented optimization framework, named DRO, that enables end-to-end training of two key components: (i) a generative knowledge selection model and (ii) an LLM generator. DRO alternates between two phases: (i) document permutation estimation and (ii) re-weighted maximization, progressively improving RAG components through a variational approach. In the estimation step, we treat document permutation as a latent variable and directly estimate its distribution from the selection model by applying an importance sampling strategy. In the maximization step, we calibrate the optimization expectation using importance weights and jointly train the selection model and LLM generator. Our theoretical analysis reveals that DRO is analogous to policy-gradient methods in reinforcement learning. Extensive experiments conducted on five datasets illustrate that DRO outperforms the best baseline with 5%-15% improvements in EM and F1. We also provide in-depth experiments to qualitatively analyze the stability, convergence, and variance of DRO.",
            "corpus_id": 278339057,
            "sentences": [
                {
                    "corpus_id": "278339057",
                    "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                    "text": "Retrieval Fig. 1. Overview of DRO objective. The selection model directly estimate a document permutation for the generator to predict an answer, with both components trained jointly. \n\n1 Introduction Large language models (LLMs) have shown remarkable text generation abilities; however, they often provide factually incorrect content [4,53,73] due to the hallucination [16] or out-of-date information [9]. To mitigate these limitations, retrieval-augmented generation (RAG) is proposed to integrate external retrievers with LLMs, which enables the model to access extensive corpora and retrieve relevant documents for references, thereby enhancing factuality. By integrating the retriever with LLMs, RAG has shown superior performance in knowledge-intensive tasks such as question answering [49,61] and conversational information seeking [5,24,68]. \n\nFollowing the most widely used architecture [9,11,23], RAG typically includes two components to answer an input query: (i) knowledge selection, where retrieval and re-ranking models select target documents, (ii) answer generation, where an LLM generator generates correct answers conditioned on the selected documents. To enhance coverage and improve answer quality, RAG models often provide multiple retrieved documents as input to the generator. The interrelationships among these documents are crucial for final performance [15,28,32,72]. We refer to a specific selection of retrieved documents as a document permutation. \n\nImproving RAG performance. To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45]. \n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51].",
                    "score": 0.7196434701907461,
                    "section_title": "Generative selector",
                    "char_start_offset": 138,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 17
                        },
                        {
                            "start": 18,
                            "end": 44
                        },
                        {
                            "start": 45,
                            "end": 183
                        },
                        {
                            "start": 186,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 849
                        },
                        {
                            "start": 852,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2044
                        },
                        {
                            "start": 2047,
                            "end": 2158
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 338,
                            "end": 341,
                            "matchedPaperCorpusId": "271162228"
                        },
                        {
                            "start": 370,
                            "end": 374,
                            "matchedPaperCorpusId": "265067168"
                        },
                        {
                            "start": 402,
                            "end": 405,
                            "matchedPaperCorpusId": "269740933"
                        },
                        {
                            "start": 796,
                            "end": 799,
                            "matchedPaperCorpusId": "270370889"
                        },
                        {
                            "start": 842,
                            "end": 845,
                            "matchedPaperCorpusId": "267406766"
                        },
                        {
                            "start": 845,
                            "end": 848,
                            "matchedPaperCorpusId": "269983269"
                        },
                        {
                            "start": 896,
                            "end": 899,
                            "matchedPaperCorpusId": "269740933"
                        },
                        {
                            "start": 902,
                            "end": 905,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1379,
                            "end": 1383,
                            "matchedPaperCorpusId": "252568176"
                        },
                        {
                            "start": 1389,
                            "end": 1392,
                            "matchedPaperCorpusId": "269605438"
                        },
                        {
                            "start": 1645,
                            "end": 1649,
                            "matchedPaperCorpusId": "10986612"
                        },
                        {
                            "start": 1753,
                            "end": 1757,
                            "matchedPaperCorpusId": "270199429"
                        },
                        {
                            "start": 2040,
                            "end": 2043,
                            "matchedPaperCorpusId": "269293655"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.904296875
                }
            ],
            "relevance_judgement": 0.904296875,
            "relevance_judgment_input_expanded": "# Title: Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models\n# Venue: arXiv.org\n# Authors: Zhengliang Shi, Lingyong Yan, Weiwei Sun, Yue Feng, Pengjie Ren, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, M. D. Rijke, Zhaochun Ren\n## Abstract\nRetrieval-augmented generation (RAG) integrates large language models ( LLM s) with retrievers to access external knowledge, improving the factuality of LLM generation in knowledge-grounded tasks. To optimize the RAG performance, most previous work independently fine-tunes the retriever to adapt to frozen LLM s or trains the LLMs to use documents retrieved by off-the-shelf retrievers, lacking end-to-end training supervision. Recent work addresses this limitation by jointly training these two components but relies on overly simplifying assumptions of document independence, which has been criticized for being far from real-world scenarios. Thus, effectively optimizing the overall RAG performance remains a critical challenge. We propose a direct retrieval-augmented optimization framework, named DRO, that enables end-to-end training of two key components: (i) a generative knowledge selection model and (ii) an LLM generator. DRO alternates between two phases: (i) document permutation estimation and (ii) re-weighted maximization, progressively improving RAG components through a variational approach. In the estimation step, we treat document permutation as a latent variable and directly estimate its distribution from the selection model by applying an importance sampling strategy. In the maximization step, we calibrate the optimization expectation using importance weights and jointly train the selection model and LLM generator. Our theoretical analysis reveals that DRO is analogous to policy-gradient methods in reinforcement learning. Extensive experiments conducted on five datasets illustrate that DRO outperforms the best baseline with 5%-15% improvements in EM and F1. We also provide in-depth experiments to qualitatively analyze the stability, convergence, and variance of DRO.\n## Generative selector\nRetrieval Fig. 1. Overview of DRO objective. The selection model directly estimate a document permutation for the generator to predict an answer, with both components trained jointly. \n\n1 Introduction Large language models (LLMs) have shown remarkable text generation abilities; however, they often provide factually incorrect content [4,53,73] due to the hallucination [16] or out-of-date information [9]. To mitigate these limitations, retrieval-augmented generation (RAG) is proposed to integrate external retrievers with LLMs, which enables the model to access extensive corpora and retrieve relevant documents for references, thereby enhancing factuality. By integrating the retriever with LLMs, RAG has shown superior performance in knowledge-intensive tasks such as question answering [49,61] and conversational information seeking [5,24,68]. \n\nFollowing the most widely used architecture [9,11,23], RAG typically includes two components to answer an input query: (i) knowledge selection, where retrieval and re-ranking models select target documents, (ii) answer generation, where an LLM generator generates correct answers conditioned on the selected documents. To enhance coverage and improve answer quality, RAG models often provide multiple retrieved documents as input to the generator. The interrelationships among these documents are crucial for final performance [15,28,32,72]. We refer to a specific selection of retrieved documents as a document permutation. \n\nImproving RAG performance. To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45]. \n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51].",
            "reference_string": "[278339057 | Shi et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 229,
            "citation_count": 1819,
            "influential_citation_count": 106,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.10997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2275341478",
                    "name": "Xinyu Gao"
                },
                {
                    "authorId": "2275191447",
                    "name": "Kangxiang Jia"
                },
                {
                    "authorId": "2275530552",
                    "name": "Jinliu Pan"
                },
                {
                    "authorId": "2275171009",
                    "name": "Yuxi Bi"
                },
                {
                    "authorId": "2276187454",
                    "name": "Yi Dai"
                },
                {
                    "authorId": "2275540959",
                    "name": "Jiawei Sun"
                },
                {
                    "authorId": "2258800561",
                    "name": "Qianyu Guo"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
            "corpus_id": 266359151,
            "sentences": [
                {
                    "corpus_id": "266359151",
                    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "text": "The following three sections explore core components-\"Retrieval\", \"Generation\" and \"Augmentation\", respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG's downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII.",
                    "score": 0.6523434194955634,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 4384,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 536
                        },
                        {
                            "start": 537,
                            "end": 582
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88720703125
                },
                {
                    "corpus_id": "266359151",
                    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
                    "text": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
                    "score": 0.6756885509871785,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82421875
                }
            ],
            "relevance_judgement": 0.88720703125,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang\n## Abstract\nLarge Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\n## I. INTRODUCTION\nThe following three sections explore core components-\"Retrieval\", \"Generation\" and \"Augmentation\", respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG's downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII.",
            "reference_string": "[266359151 | Gao et al. | 2023 | Citations: 1819]"
        },
        {
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203079037",
                    "name": "Sakhinana Sagar Srinivas"
                },
                {
                    "authorId": "2139833562",
                    "name": "Venkataramana Runkana"
                }
            ],
            "abstract": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
            "corpus_id": 277501853,
            "sentences": [
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "For simple questions, it reduces unnecessary overhead, enabling faster responses and minimizing hallucinations. For complex or multi-faceted tasks, it increases reasoning depth to improve accuracy and better integrate retrieved context, enabling LLMs to effectively process and reason with augmented context. This adaptive approach mimics human-like deliberative reasoning for knowledgeintensive tasks without costly retraining, enhancing efficiency and performance while maintaining accuracy and reducing hallucinations. Together, RAFT enhances RAG by integrating retrieval with supervised fine-tuning, enabling models to dynamically leverage external knowledge and prioritize relevant content while ignoring distractors. DRA-GIN dynamically determines when and what to retrieve during text generation, minimizing redundant retrievals and improving efficiency. KV Caching optimizes memory usage by storing intermediate hidden states, reducing computational overhead in RAG, while Test-Time Scaling dynamically allocates resources based on task complexity. These advancements enable RAG systems to integrate external knowledge more accurately, efficiently, and at scale, ensuring faster and more effective utilization of retrieved data within the LLM framework. While these recent advancements have enhanced retrieval integration in LLMs, significant challenges remain in balancing retrieval fidelity, response quality, and computational efficiency. Current methods often struggle to dynamically determine when and how much external information to incorporate, sometimes overwhelming the model or sacrificing the coherence of its responses. Motivated by these persistent challenges, our work seeks to refine the synergy between retrieval and generation through a dual approach. First, we fine-tune language models via policy optimization, enabling them to more effectively integrate and utilize retrieved content. This refinement not only improves factual alignment but also enhances overall response quality. Second, we introduce a mechanism that selectively triggers external retrieval based on the model's internal state, ensuring that additional information is incorporated only when necessary. This targeted strategy optimizes computational resources while preserving the language model's coherence. In the following sections, we outline our contributions that extend state-of-the-art methods by addressing both the optimization of retrieval-augmented generation and the efficient management of computational overhead. Our contributions are as follows:",
                    "score": 0.6421374390812464,
                    "section_title": "Introduction",
                    "char_start_offset": 4240,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1777
                        },
                        {
                            "start": 1778,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 2009
                        },
                        {
                            "start": 2010,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2304
                        },
                        {
                            "start": 2305,
                            "end": 2523
                        },
                        {
                            "start": 2524,
                            "end": 2557
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.884765625
                },
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "Policy-Optimized Retrieval-Augmented Generation (PORAG) and Retrieval-Augmented Fine-Tuning (RAFT) (Zhang et al., 2024c) offer fundamentally different strategies for optimizing RAG systems. RAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently. In contrast, PORAG employs Group Relative Policy Optimization (GRPO), an advanced reinforcement learning method, to directly optimize multiple generation quality dimensions simultaneously through specialized reward models. PORAG dynamically generates policy-driven training samples, directly optimizing retrieval fidelity-how faithfully retrieved information is reflected-and response quality, including coherence, fluency, and helpfulness. Unlike RAFT, PORAG implicitly and dynamically handles distractors through reward modeling and advantage estimation rather than explicitly embedding distractors in supervised training sets. Additionally, PORAG incorporates explicit advantage estimation and KL-divergence regularization during policy updates to maintain controlled adaptation in retrievalaugmented generation. This stabilizes training, prevents drastic policy shifts, and balances retrieval fidelity with the model's inherent parametric knowledge, enhancing robustness and generalization across retrieval scenarios. In contrast, RAFT provides robustness primarily within domain-specific scenarios due to its explicit distractoraware fine-tuning but lacks dynamic adaptability beyond its predefined training context. In summary, PORAG offers greater deployment flexibility, nuanced generation optimization, and dynamic adaptability, addressing key limitations of RAFT related to static supervision, singlestrategy optimization, and the lack of direct optimization of retrieval fidelity and response quality.",
                    "score": 0.6255071202389232,
                    "section_title": "B. Comparing PORAG and RAFT Methodologies",
                    "char_start_offset": 68668,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 386
                        },
                        {
                            "start": 389,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 1036
                        },
                        {
                            "start": 1037,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1629
                        },
                        {
                            "start": 1630,
                            "end": 1835
                        },
                        {
                            "start": 1836,
                            "end": 2035
                        },
                        {
                            "start": 2036,
                            "end": 2326
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 99,
                            "end": 120,
                            "matchedPaperCorpusId": "268510197"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83251953125
                },
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "Retrieval-Augmented Generation (RAG, (Lewis et al., 2020;Su et al.;Wang et al., 2025)) has gained significant interest in Natural Language Processing for enhancing large language models (LLMs) on knowledge-intensive tasks through external information retrieval, with applications across search engines, conversational agents, chatbots, and many other applications. RAG addresses key LLM limitations, including hallucinations, outdated information, and insufficient domain-specific knowledge, particularly in open-domain question answering. Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering). Building on advancements in LLM training methodologies, DeepSeek has enhanced its AI models, notably DeepSeek-R1 (Liu et al., 2024;Guo et al., 2025;Shao et al., 2024), by implementing Group Relative Policy Optimization (GRPO), an advanced reinforcement learning algorithm that improves training efficiency and model performance beyond traditional supervised fine-tuning. GRPO reduces computational overhead by eliminating the value function, using group-based advantage estimation for simplified reward computation, lowering memory usage, and integrating Kullback-Leibler (KL) divergence regularization for stable, efficient training. It outperforms standard Rejection Sampling Fine-Tuning (RFT), which relies on offline sampling, and Online RFT, which dynamically samples from an evolving policy. GRPO also supports process supervision (GRPO+PS), providing step-by-step feedback for improved reasoning, surpassing outcome supervision (GRPO+OS), which evaluates only final answers. Addressing the limitations of static retrieval in traditional RAG, DRAGIN (Dynamic Retrieval-Augmented Generation based on Information Needs, (Su et al.)) is an advanced framework that dynamically determines when and what to retrieve during text generation.",
                    "score": 0.6962971368264554,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1857
                        },
                        {
                            "start": 1858,
                            "end": 2041
                        },
                        {
                            "start": 2042,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 57,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 578,
                            "end": 599,
                            "matchedPaperCorpusId": "268510197"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8291015625
                }
            ],
            "relevance_judgement": 0.884765625,
            "relevance_judgment_input_expanded": "# Title: Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding\n# Venue: arXiv.org\n# Authors: Sakhinana Sagar Srinivas, Venkataramana Runkana\n## Abstract\nWe present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.\n## Introduction\nRetrieval-Augmented Generation (RAG, (Lewis et al., 2020;Su et al.;Wang et al., 2025)) has gained significant interest in Natural Language Processing for enhancing large language models (LLMs) on knowledge-intensive tasks through external information retrieval, with applications across search engines, conversational agents, chatbots, and many other applications. RAG addresses key LLM limitations, including hallucinations, outdated information, and insufficient domain-specific knowledge, particularly in open-domain question answering. Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering). Building on advancements in LLM training methodologies, DeepSeek has enhanced its AI models, notably DeepSeek-R1 (Liu et al., 2024;Guo et al., 2025;Shao et al., 2024), by implementing Group Relative Policy Optimization (GRPO), an advanced reinforcement learning algorithm that improves training efficiency and model performance beyond traditional supervised fine-tuning. GRPO reduces computational overhead by eliminating the value function, using group-based advantage estimation for simplified reward computation, lowering memory usage, and integrating Kullback-Leibler (KL) divergence regularization for stable, efficient training. It outperforms standard Rejection Sampling Fine-Tuning (RFT), which relies on offline sampling, and Online RFT, which dynamically samples from an evolving policy. GRPO also supports process supervision (GRPO+PS), providing step-by-step feedback for improved reasoning, surpassing outcome supervision (GRPO+OS), which evaluates only final answers. Addressing the limitations of static retrieval in traditional RAG, DRAGIN (Dynamic Retrieval-Augmented Generation based on Information Needs, (Su et al.)) is an advanced framework that dynamically determines when and what to retrieve during text generation.\n...\nFor simple questions, it reduces unnecessary overhead, enabling faster responses and minimizing hallucinations. For complex or multi-faceted tasks, it increases reasoning depth to improve accuracy and better integrate retrieved context, enabling LLMs to effectively process and reason with augmented context. This adaptive approach mimics human-like deliberative reasoning for knowledgeintensive tasks without costly retraining, enhancing efficiency and performance while maintaining accuracy and reducing hallucinations. Together, RAFT enhances RAG by integrating retrieval with supervised fine-tuning, enabling models to dynamically leverage external knowledge and prioritize relevant content while ignoring distractors. DRA-GIN dynamically determines when and what to retrieve during text generation, minimizing redundant retrievals and improving efficiency. KV Caching optimizes memory usage by storing intermediate hidden states, reducing computational overhead in RAG, while Test-Time Scaling dynamically allocates resources based on task complexity. These advancements enable RAG systems to integrate external knowledge more accurately, efficiently, and at scale, ensuring faster and more effective utilization of retrieved data within the LLM framework. While these recent advancements have enhanced retrieval integration in LLMs, significant challenges remain in balancing retrieval fidelity, response quality, and computational efficiency. Current methods often struggle to dynamically determine when and how much external information to incorporate, sometimes overwhelming the model or sacrificing the coherence of its responses. Motivated by these persistent challenges, our work seeks to refine the synergy between retrieval and generation through a dual approach. First, we fine-tune language models via policy optimization, enabling them to more effectively integrate and utilize retrieved content. This refinement not only improves factual alignment but also enhances overall response quality. Second, we introduce a mechanism that selectively triggers external retrieval based on the model's internal state, ensuring that additional information is incorporated only when necessary. This targeted strategy optimizes computational resources while preserving the language model's coherence. In the following sections, we outline our contributions that extend state-of-the-art methods by addressing both the optimization of retrieval-augmented generation and the efficient management of computational overhead. Our contributions are as follows:\n\n## B. Comparing PORAG and RAFT Methodologies\nPolicy-Optimized Retrieval-Augmented Generation (PORAG) and Retrieval-Augmented Fine-Tuning (RAFT) (Zhang et al., 2024c) offer fundamentally different strategies for optimizing RAG systems. RAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently. In contrast, PORAG employs Group Relative Policy Optimization (GRPO), an advanced reinforcement learning method, to directly optimize multiple generation quality dimensions simultaneously through specialized reward models. PORAG dynamically generates policy-driven training samples, directly optimizing retrieval fidelity-how faithfully retrieved information is reflected-and response quality, including coherence, fluency, and helpfulness. Unlike RAFT, PORAG implicitly and dynamically handles distractors through reward modeling and advantage estimation rather than explicitly embedding distractors in supervised training sets. Additionally, PORAG incorporates explicit advantage estimation and KL-divergence regularization during policy updates to maintain controlled adaptation in retrievalaugmented generation. This stabilizes training, prevents drastic policy shifts, and balances retrieval fidelity with the model's inherent parametric knowledge, enhancing robustness and generalization across retrieval scenarios. In contrast, RAFT provides robustness primarily within domain-specific scenarios due to its explicit distractoraware fine-tuning but lacks dynamic adaptability beyond its predefined training context. In summary, PORAG offers greater deployment flexibility, nuanced generation optimization, and dynamic adaptability, addressing key limitations of RAFT related to static supervision, singlestrategy optimization, and the lack of direct optimization of retrieval fidelity and response quality.",
            "reference_string": "[277501853 | Srinivas et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.17648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322992640",
                    "name": "Isaac Chung"
                },
                {
                    "authorId": "2322982756",
                    "name": "Phat Vo"
                },
                {
                    "authorId": "2322991957",
                    "name": "Arman Kizilkale"
                },
                {
                    "authorId": "2322982549",
                    "name": "Aaron Reite"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
            "corpus_id": 272911196,
            "sentences": [
                {
                    "corpus_id": "272911196",
                    "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                    "text": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
                    "score": 0.6435781957712307,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88232421875
                },
                {
                    "corpus_id": "272911196",
                    "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                    "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
                    "score": 0.8268299276906443,
                    "section_title": "Introduction",
                    "char_start_offset": 4339,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 422
                        },
                        {
                            "start": 425,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1753
                        },
                        {
                            "start": 1756,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 2090
                        },
                        {
                            "start": 2091,
                            "end": 2235
                        },
                        {
                            "start": 2236,
                            "end": 2383
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82568359375
                }
            ],
            "relevance_judgement": 0.88232421875,
            "relevance_judgment_input_expanded": "# Title: Efficient In-Domain Question Answering for Resource-Constrained Environments\n# Venue: arXiv.org\n# Authors: Isaac Chung, Phat Vo, Arman Kizilkale, Aaron Reite\n## Abstract\nRetrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.\n## Introduction\nWe release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "reference_string": "[272911196 | Chung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256382094",
                    "name": "Kushagra Bhushan"
                },
                {
                    "authorId": "1392630568",
                    "name": "Yatin Nandwani"
                },
                {
                    "authorId": "2345003462",
                    "name": "Dinesh Khandelwal"
                },
                {
                    "authorId": "2320314900",
                    "name": "Sonam Gupta"
                },
                {
                    "authorId": "2345005348",
                    "name": "Gaurav Pandey"
                },
                {
                    "authorId": "1916865",
                    "name": "Dinesh Raghu"
                },
                {
                    "authorId": "2243011716",
                    "name": "Sachindra Joshi"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.",
            "corpus_id": 276287820,
            "sentences": [
                {
                    "corpus_id": "276287820",
                    "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                    "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
                    "score": 0.8321421183923233,
                    "section_title": "Related Work",
                    "char_start_offset": 5971,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1677
                        },
                        {
                            "start": 1680,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8818359375
                }
            ],
            "relevance_judgement": 0.8818359375,
            "relevance_judgment_input_expanded": "# Title: Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.\n## Related Work\nRetrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "reference_string": "[276287820 | Bhushan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.02545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296788981",
                    "name": "Daniel Fleischer"
                },
                {
                    "authorId": "2077591838",
                    "name": "Moshe Berchansky"
                },
                {
                    "authorId": "2134755",
                    "name": "Moshe Wasserblat"
                },
                {
                    "authorId": "2477428",
                    "name": "Peter Izsak"
                }
            ],
            "abstract": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
            "corpus_id": 271710111,
            "sentences": [
                {
                    "corpus_id": "271710111",
                    "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
                    "text": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
                    "score": 0.7092603199637795,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87109375
                }
            ],
            "relevance_judgement": 0.87109375,
            "relevance_judgment_input_expanded": "# Title: RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak\n## Abstract\nImplementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.\n",
            "reference_string": "[271710111 | Fleischer et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "venue": "Conference on Multimedia Information Processing and Retrieval",
            "year": 2024,
            "reference_count": 14,
            "citation_count": 57,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003089508",
                    "name": "Kunal Sawarkar"
                },
                {
                    "authorId": "2295990127",
                    "name": "Abhilasha Mangal"
                },
                {
                    "authorId": "2295990033",
                    "name": "S. R. Solanki"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
            "corpus_id": 269043117,
            "sentences": [
                {
                    "corpus_id": "269043117",
                    "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
                    "text": "RAG represents an approach to text generation that is based not only on patterns learned during training but also on dynamically retrieved external knowledge [1]. This method combines the creative flair of generative models with the encyclopedic recall of a search engine. The efficacy of the RAG system relies fundamentally on two components: the Retriever (R) and the Generator (G), the latter representing the size and type of LLM. \n\nThe language model can easily craft sentences, but it might not always have all the facts. This is where the Retriever (R) steps in, quickly sifting through vast amounts of documents to find relevant information that can be used to inform and enrich the language model's output. Think of the retriever as a researcher part of the AI, which feeds the contextually grounded text to generate knowledgeable answers to Generator (G). Without the retriever, RAG would be like a well-spoken individual who delivers irrelevant information.",
                    "score": 0.6790958811904939,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 434
                        },
                        {
                            "start": 437,
                            "end": 527
                        },
                        {
                            "start": 528,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 968
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86767578125
                },
                {
                    "corpus_id": "269043117",
                    "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
                    "text": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
                    "score": 0.6571550227143843,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80859375
                }
            ],
            "relevance_judgement": 0.86767578125,
            "relevance_judgment_input_expanded": "# Title: Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers\n# Venue: Conference on Multimedia Information Processing and Retrieval\n# Authors: Kunal Sawarkar, Abhilasha Mangal, S. R. Solanki\n## Abstract\nRetrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.\n## I. INTRODUCTION\nRAG represents an approach to text generation that is based not only on patterns learned during training but also on dynamically retrieved external knowledge [1]. This method combines the creative flair of generative models with the encyclopedic recall of a search engine. The efficacy of the RAG system relies fundamentally on two components: the Retriever (R) and the Generator (G), the latter representing the size and type of LLM. \n\nThe language model can easily craft sentences, but it might not always have all the facts. This is where the Retriever (R) steps in, quickly sifting through vast amounts of documents to find relevant information that can be used to inform and enrich the language model's output. Think of the retriever as a researcher part of the AI, which feeds the contextually grounded text to generate knowledgeable answers to Generator (G). Without the retriever, RAG would be like a well-spoken individual who delivers irrelevant information.",
            "reference_string": "[269043117 | Sawarkar et al. | 2024 | Citations: 57]"
        },
        {
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 251,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491233507",
                    "name": "Mingyue Cheng"
                },
                {
                    "authorId": "2208917508",
                    "name": "Yucong Luo"
                },
                {
                    "authorId": "2322501286",
                    "name": "Ouyang Jie"
                },
                {
                    "authorId": "2332691115",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2312648865",
                    "name": "Huijie Liu"
                },
                {
                    "authorId": "2291070758",
                    "name": "Li Li"
                },
                {
                    "authorId": "2322429208",
                    "name": "Shuo Yu"
                },
                {
                    "authorId": "2351226328",
                    "name": "Bohou Zhang"
                },
                {
                    "authorId": "2350426005",
                    "name": "Jiawei Cao"
                },
                {
                    "authorId": "2350427710",
                    "name": "Jie Ma"
                },
                {
                    "authorId": "2322524150",
                    "name": "Daoyu Wang"
                },
                {
                    "authorId": "2258714945",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "corpus_id": 277043297,
            "sentences": [
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Training RAG models requires balancing the optimization of both retrieval and generation components to achieve optimal performance. Effective training strategies ensure the retriever fetches relevant information while the generator produces coherent and accurate outputs. This section reviews various methods in RAG training, including static training, unidirectional guided training, and collaborative training, shown in Figure 8. Each approach offers distinct benefits and challenges, affecting the effectiveness and adaptability of RAG models across applications. By exploring these paradigms, we can enhance the integration of retrieval and generation processes, ultimately improving RAG performance. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption. \n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance. \n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information. to better integrate and utilize external information. For example, RETRO [23] employs a pre-trained BERT model as the retriever to provide relevant context that enhances the generator's output. Similarly, RALMs [264] use a pre-trained COLBERTV2 retriever to fine-tune large language models (LLMs), thereby improving the generator's capacity to incorporate retrieved data effectively. ITER-RTGEN [217] utilizes S-BERT to guide the fine-tuning of a T5 generator, ensuring that the generated text aligns closely with the relevant retrieved information. Moreover, SMALLCAP [201] integrates CLIP as a retriever to direct a GPT-2 model in generating accurate and contextually appropriate captions. \n\nThis guided approach ensures that the generator benefits from a steady stream of relevant information, enhancing the overall quality and relevance of the generated content. \n\nGenerator-Guided Retriever Training.",
                    "score": 0.7080869647052384,
                    "section_title": "RAG Training",
                    "char_start_offset": 74282,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1174
                        },
                        {
                            "start": 1177,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2224
                        },
                        {
                            "start": 2225,
                            "end": 2366
                        },
                        {
                            "start": 2369,
                            "end": 2541
                        },
                        {
                            "start": 2544,
                            "end": 2580
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 994,
                            "end": 999,
                            "matchedPaperCorpusId": "207178704"
                        },
                        {
                            "start": 1748,
                            "end": 1752,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 2244,
                            "end": 2249,
                            "matchedPaperCorpusId": "252668790"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86669921875
                },
                {
                    "corpus_id": "277043297",
                    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                    "text": "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively. This bidirectional influence ensures that the retriever evolves in tandem with the generator, fostering a more integrated and efficient RAG system.",
                    "score": 0.6874143116102047,
                    "section_title": "RAG Training",
                    "char_start_offset": 76826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 36
                        },
                        {
                            "start": 37,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1136
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8466796875
                }
            ],
            "relevance_judgement": 0.86669921875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Knowledge-Oriented Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong Chen\n## Abstract\nRetrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.\n## RAG Training\nTraining RAG models requires balancing the optimization of both retrieval and generation components to achieve optimal performance. Effective training strategies ensure the retriever fetches relevant information while the generator produces coherent and accurate outputs. This section reviews various methods in RAG training, including static training, unidirectional guided training, and collaborative training, shown in Figure 8. Each approach offers distinct benefits and challenges, affecting the effectiveness and adaptability of RAG models across applications. By exploring these paradigms, we can enhance the integration of retrieval and generation processes, ultimately improving RAG performance. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption. \n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance. \n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information. to better integrate and utilize external information. For example, RETRO [23] employs a pre-trained BERT model as the retriever to provide relevant context that enhances the generator's output. Similarly, RALMs [264] use a pre-trained COLBERTV2 retriever to fine-tune large language models (LLMs), thereby improving the generator's capacity to incorporate retrieved data effectively. ITER-RTGEN [217] utilizes S-BERT to guide the fine-tuning of a T5 generator, ensuring that the generated text aligns closely with the relevant retrieved information. Moreover, SMALLCAP [201] integrates CLIP as a retriever to direct a GPT-2 model in generating accurate and contextually appropriate captions. \n\nThis guided approach ensures that the generator benefits from a steady stream of relevant information, enhancing the overall quality and relevance of the generated content. \n\nGenerator-Guided Retriever Training.\n...\nGenerator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively. This bidirectional influence ensures that the retriever evolves in tandem with the generator, fostering a more integrated and efficient RAG system.",
            "reference_string": "[277043297 | Cheng et al. | 2025 | Citations: 6]"
        },
        {
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 153,
            "citation_count": 51,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10981, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2260272949",
                    "name": "Yizheng Huang"
                },
                {
                    "authorId": "2259653248",
                    "name": "Jimmy X. Huang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
            "corpus_id": 269188036,
            "sentences": [
                {
                    "corpus_id": "269188036",
                    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
                    "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
                    "score": 0.6399868251610288,
                    "section_title": "Introduction",
                    "char_start_offset": 2330,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 610
                        },
                        {
                            "start": 613,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 833
                        },
                        {
                            "start": 834,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1212
                        },
                        {
                            "start": 1215,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1535
                        },
                        {
                            "start": 1536,
                            "end": 1708
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8662109375
                }
            ],
            "relevance_judgement": 0.8662109375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Retrieval-Augmented Text Generation for Large Language Models\n# Venue: arXiv.org\n# Authors: Yizheng Huang, Jimmy X. Huang\n## Abstract\nRetrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.\n## Introduction\nTo provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "reference_string": "[269188036 | Huang et al. | 2024 | Citations: 51]"
        },
        {
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 159,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296715370",
                    "name": "Benjue Weng"
                }
            ],
            "abstract": "With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.",
            "corpus_id": 269149041,
            "sentences": [
                {
                    "corpus_id": "269149041",
                    "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
                    "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
                    "score": 0.805642954216085,
                    "section_title": "XIII. RAG-MEMORY-FINETUNING",
                    "char_start_offset": 145958,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 116,
                            "end": 216
                        },
                        {
                            "start": 216,
                            "end": 453
                        },
                        {
                            "start": 453,
                            "end": 528
                        },
                        {
                            "start": 530,
                            "end": 781
                        },
                        {
                            "start": 783,
                            "end": 979
                        },
                        {
                            "start": 981,
                            "end": 1179
                        },
                        {
                            "start": 1181,
                            "end": 1547
                        },
                        {
                            "start": 1549,
                            "end": 1721
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1296,
                            "end": 1301,
                            "matchedPaperCorpusId": "252735056"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies\n# Venue: arXiv.org\n# Authors: Benjue Weng\n## Abstract\nWith the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.\n## XIII. RAG-MEMORY-FINETUNING\nFor every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "reference_string": "[269149041 | Weng | 2024 | Citations: 10]"
        },
        {
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 11,
            "citation_count": 23,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311997786",
                    "name": "Shailja Gupta"
                },
                {
                    "authorId": "2311893279",
                    "name": "Rajesh Ranjan"
                },
                {
                    "authorId": "2321535962",
                    "name": "Surya Narayan Singh"
                }
            ],
            "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
            "corpus_id": 273403982,
            "sentences": [
                {
                    "corpus_id": "273403982",
                    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                    "text": "Retrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the limitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism, which retrieves relevant documents or information from an external knowledge source, and (ii) a generation module, which processes this information to generate human-like text (Lewis et al. 2020). This combination allows RAG models to not only generate fluent text but also ground their outputs in real-world, up-to-date data. \n\nThe retrieval module in RAG typically leverages dense vector representations to identify relevant documents from large datasets, such as Wikipedia or proprietary databases. Once retrieved, these documents are passed to the generative module, often built using transformer-based architectures, to generate responses grounded in the retrieved knowledge. This methodology helps mitigate the hallucination problem and ensures that the generated text is more factual and contextually appropriate (Thakur et al. 2021). Over the period, RAG models have seen applications in various domains, including open-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), and personalized recommendations. \n\nFigure 2: A basic flow of the RAG system along with its component",
                    "score": 0.6821715402840021,
                    "section_title": "Overview of Retrieval-Augmented Generation (RAG)",
                    "char_start_offset": 2068,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 523
                        },
                        {
                            "start": 526,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1251
                        },
                        {
                            "start": 1254,
                            "end": 1319
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 373,
                            "end": 392,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.859375
                },
                {
                    "corpus_id": "273403982",
                    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                    "text": "Retrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
                    "score": 0.6307701953852428,
                    "section_title": "Overview of RAG Models",
                    "char_start_offset": 10564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 913
                        },
                        {
                            "start": 916,
                            "end": 962
                        },
                        {
                            "start": 965,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1248
                        },
                        {
                            "start": 1251,
                            "end": 1457
                        },
                        {
                            "start": 1460,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 2056
                        },
                        {
                            "start": 2057,
                            "end": 2239
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 755,
                            "end": 773,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.818359375
                },
                {
                    "corpus_id": "273403982",
                    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                    "text": "RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including PubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO (Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models in tasks like open-domain question answering and fact verification, addresses issues of over-or under-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated outputs. The method improves context quality by identifying useful context through lexical and information-theoretic approaches and training context filtering models to refine retrieved contexts during test time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG) (Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models (LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate a fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens to evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific needs and has shown superior performance in open-domain question-answering, reasoning, fact verification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on the quality of retrieval and more meta-data understanding of the repository would enhance the effectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow advances beyond the traditional retrieve-then-read mode and employs a prepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually relevant, time-critical, or domain-specific information. Key innovations include generating metadata, synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary) for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems.",
                    "score": 0.7653593308625986,
                    "section_title": "Recent Advancement in the field:",
                    "char_start_offset": 25127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1365
                        },
                        {
                            "start": 1366,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 2037
                        },
                        {
                            "start": 2038,
                            "end": 2241
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81005859375
                },
                {
                    "corpus_id": "273403982",
                    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
                    "text": "In Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role in producing the final output by integrating retrieved information with the input query. After the retrieval component pulls relevant knowledge from external sources, the generator synthesizes this information into coherent, contextually appropriate responses. The Large Language Model (LLM) serves as the backbone of the generator, which ensures the generated text is fluent, accurate, and aligned with the original query.",
                    "score": 0.6386118488806979,
                    "section_title": "Generator Mechanisms in RAG Systems",
                    "char_start_offset": 17627,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 517
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80224609375
                }
            ],
            "relevance_judgement": 0.859375,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions\n# Venue: arXiv.org\n# Authors: Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh\n## Abstract\nThis paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.\n## Overview of Retrieval-Augmented Generation (RAG)\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the limitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism, which retrieves relevant documents or information from an external knowledge source, and (ii) a generation module, which processes this information to generate human-like text (Lewis et al. 2020). This combination allows RAG models to not only generate fluent text but also ground their outputs in real-world, up-to-date data. \n\nThe retrieval module in RAG typically leverages dense vector representations to identify relevant documents from large datasets, such as Wikipedia or proprietary databases. Once retrieved, these documents are passed to the generative module, often built using transformer-based architectures, to generate responses grounded in the retrieved knowledge. This methodology helps mitigate the hallucination problem and ensures that the generated text is more factual and contextually appropriate (Thakur et al. 2021). Over the period, RAG models have seen applications in various domains, including open-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), and personalized recommendations. \n\nFigure 2: A basic flow of the RAG system along with its component\n\n## Overview of RAG Models\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues\n\n## Generator Mechanisms in RAG Systems\nIn Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role in producing the final output by integrating retrieved information with the input query. After the retrieval component pulls relevant knowledge from external sources, the generator synthesizes this information into coherent, contextually appropriate responses. The Large Language Model (LLM) serves as the backbone of the generator, which ensures the generated text is fluent, accurate, and aligned with the original query.\n\n## Recent Advancement in the field:\nRAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including PubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO (Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models in tasks like open-domain question answering and fact verification, addresses issues of over-or under-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated outputs. The method improves context quality by identifying useful context through lexical and information-theoretic approaches and training context filtering models to refine retrieved contexts during test time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG) (Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models (LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate a fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens to evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific needs and has shown superior performance in open-domain question-answering, reasoning, fact verification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on the quality of retrieval and more meta-data understanding of the repository would enhance the effectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow advances beyond the traditional retrieve-then-read mode and employs a prepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually relevant, time-critical, or domain-specific information. Key innovations include generating metadata, synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary) for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems.",
            "reference_string": "[273403982 | Gupta et al. | 2024 | Citations: 23]"
        },
        {
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 401,
            "citation_count": 282,
            "influential_citation_count": 11,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19473, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268718776",
                    "name": "Penghao Zhao"
                },
                {
                    "authorId": "2288557803",
                    "name": "Hailin Zhang"
                },
                {
                    "authorId": "2289597580",
                    "name": "Qinhan Yu"
                },
                {
                    "authorId": "2288675277",
                    "name": "Zhengren Wang"
                },
                {
                    "authorId": "2288532368",
                    "name": "Yunteng Geng"
                },
                {
                    "authorId": "46182701",
                    "name": "Fangcheng Fu"
                },
                {
                    "authorId": "2249513224",
                    "name": "Ling Yang"
                },
                {
                    "authorId": "2277807793",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2277742543",
                    "name": "Bin Cui"
                }
            ],
            "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "corpus_id": 268091298,
            "sentences": [
                {
                    "corpus_id": "268091298",
                    "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
                    "text": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
                    "score": 0.6871429220980978,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.857421875
                }
            ],
            "relevance_judgement": 0.857421875,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for AI-Generated Content: A Survey\n# Venue: arXiv.org\n# Authors: Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui\n## Abstract\nAdvancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.\n",
            "reference_string": "[268091298 | Zhao et al. | 2024 | Citations: 282]"
        },
        {
            "title": "Synergizing RAG and Reasoning: A Systematic Review",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 87,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291322497",
                    "name": "Yijie Zhong"
                },
                {
                    "authorId": "2275171009",
                    "name": "Yuxi Bi"
                },
                {
                    "authorId": "2356716546",
                    "name": "Ming Xue"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Recent breakthroughs in large language models (LLMs), particularly in reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to unprecedented levels. By synergizing retrieval mechanisms with advanced reasoning, LLMs can now tackle increasingly complex problems. This paper presents a systematic review of the collaborative interplay between RAG and reasoning, clearly defining\"reasoning\"within the RAG context. It construct a comprehensive taxonomy encompassing multi-dimensional collaborative objectives, representative paradigms, and technical implementations, and analyze the bidirectional synergy methods. Additionally, we critically evaluate current limitations in RAG assessment, including the absence of intermediate supervision for multi-step reasoning and practical challenges related to cost-risk trade-offs. To bridge theory and practice, we provide practical guidelines tailored to diverse real-world applications. Finally, we identify promising research directions, such as graph-based knowledge integration, hybrid model collaboration, and RL-driven optimization. Overall, this work presents a theoretical framework and practical foundation to advance RAG systems in academia and industry, fostering the next generation of RAG solutions.",
            "corpus_id": 277994112,
            "sentences": [
                {
                    "corpus_id": "277994112",
                    "title": "Synergizing RAG and Reasoning: A Systematic Review",
                    "text": "Tuning-Based. The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-ofthought mechanism within LLMs. Current research mainly targets three goals: retrieval pathway optimization, structured generation enhancement, and collaborative training with external modules. \n\nFor retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy. \n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning. \n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
                    "score": 0.7179822655630566,
                    "section_title": "5.2.2",
                    "char_start_offset": 63935,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 13
                        },
                        {
                            "start": 14,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 356
                        },
                        {
                            "start": 359,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1119
                        },
                        {
                            "start": 1122,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1442
                        },
                        {
                            "start": 1443,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 2065
                        },
                        {
                            "start": 2068,
                            "end": 2224
                        },
                        {
                            "start": 2225,
                            "end": 2315
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1183,
                            "end": 1186,
                            "matchedPaperCorpusId": "264288947"
                        },
                        {
                            "start": 1947,
                            "end": 1951,
                            "matchedPaperCorpusId": "258841328"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85693359375
                }
            ],
            "relevance_judgement": 0.85693359375,
            "relevance_judgment_input_expanded": "# Title: Synergizing RAG and Reasoning: A Systematic Review\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, Haofen Wang\n## Abstract\nRecent breakthroughs in large language models (LLMs), particularly in reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to unprecedented levels. By synergizing retrieval mechanisms with advanced reasoning, LLMs can now tackle increasingly complex problems. This paper presents a systematic review of the collaborative interplay between RAG and reasoning, clearly defining\"reasoning\"within the RAG context. It construct a comprehensive taxonomy encompassing multi-dimensional collaborative objectives, representative paradigms, and technical implementations, and analyze the bidirectional synergy methods. Additionally, we critically evaluate current limitations in RAG assessment, including the absence of intermediate supervision for multi-step reasoning and practical challenges related to cost-risk trade-offs. To bridge theory and practice, we provide practical guidelines tailored to diverse real-world applications. Finally, we identify promising research directions, such as graph-based knowledge integration, hybrid model collaboration, and RL-driven optimization. Overall, this work presents a theoretical framework and practical foundation to advance RAG systems in academia and industry, fostering the next generation of RAG solutions.\n## 5.2.2\nTuning-Based. The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-ofthought mechanism within LLMs. Current research mainly targets three goals: retrieval pathway optimization, structured generation enhancement, and collaborative training with external modules. \n\nFor retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy. \n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning. \n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
            "reference_string": "[277994112 | Gao et al. | 2025 | Citations: 7]"
        },
        {
            "title": "Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344750815",
                    "name": "Yan Weng"
                },
                {
                    "authorId": "31734386",
                    "name": "Fengbin Zhu"
                },
                {
                    "authorId": "2344749912",
                    "name": "Tong Ye"
                },
                {
                    "authorId": "2344792659",
                    "name": "Haoyan Liu"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG), which integrates external knowledge into Large Language Models (LLMs), has proven effective in enabling LLMs to produce more accurate and reliable responses. However, it remains a significant challenge how to effectively integrate external retrieved knowledge with internal parametric knowledge in LLMs. In this work, we propose a novel Self-Selection RAG framework, where the LLM is made to select from pairwise responses generated with internal parametric knowledge solely and with external retrieved knowledge together to achieve enhanced accuracy. To this end, we devise a Self-Selection-RGP method to enhance the capabilities of the LLM in both generating and selecting the correct answer, by training the LLM with Direct Preference Optimization (DPO) over a curated Retrieval Generation Preference (RGP) dataset. Experimental results with two open-source LLMs (i.e., Llama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our approach over other baseline methods on Natural Questions (NQ) and TrivialQA datasets.",
            "corpus_id": 276249310,
            "sentences": [
                {
                    "corpus_id": "276249310",
                    "title": "Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection",
                    "text": "In this section, we elaborate on the proposed Self-Selection framework for enhanced Retrieval-Augmented Generation (RAG). Before explaining our method, we first revisit two preliminary concepts, i.e. Large Language Model (LLM) and Retrieval-Augmented Generation (RAG). Then, we present the formulation of our Self-Selection framework with detailed notations. In order to strengthen the capabilities of the LLM in accurately generating and selecting responses, we develop a novel Self-Selection-RGP method, which is essentially fining tuning the LLM over a newly built Retrieval-Generation Preference (RGP) dataset.",
                    "score": 0.747969535489106,
                    "section_title": "SELF-SELECTION FRAMEWORK",
                    "char_start_offset": 6140,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 358
                        },
                        {
                            "start": 359,
                            "end": 614
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85400390625
                }
            ],
            "relevance_judgement": 0.85400390625,
            "relevance_judgment_input_expanded": "# Title: Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection\n# Venue: arXiv.org\n# Authors: Yan Weng, Fengbin Zhu, Tong Ye, Haoyan Liu, Fuli Feng, Tat-Seng Chua\n## Abstract\nRetrieval-Augmented Generation (RAG), which integrates external knowledge into Large Language Models (LLMs), has proven effective in enabling LLMs to produce more accurate and reliable responses. However, it remains a significant challenge how to effectively integrate external retrieved knowledge with internal parametric knowledge in LLMs. In this work, we propose a novel Self-Selection RAG framework, where the LLM is made to select from pairwise responses generated with internal parametric knowledge solely and with external retrieved knowledge together to achieve enhanced accuracy. To this end, we devise a Self-Selection-RGP method to enhance the capabilities of the LLM in both generating and selecting the correct answer, by training the LLM with Direct Preference Optimization (DPO) over a curated Retrieval Generation Preference (RGP) dataset. Experimental results with two open-source LLMs (i.e., Llama2-13B-Chat and Mistral-7B) well demonstrate the superiority of our approach over other baseline methods on Natural Questions (NQ) and TrivialQA datasets.\n## SELF-SELECTION FRAMEWORK\nIn this section, we elaborate on the proposed Self-Selection framework for enhanced Retrieval-Augmented Generation (RAG). Before explaining our method, we first revisit two preliminary concepts, i.e. Large Language Model (LLM) and Retrieval-Augmented Generation (RAG). Then, we present the formulation of our Self-Selection framework with detailed notations. In order to strengthen the capabilities of the LLM in accurately generating and selecting responses, we develop a novel Self-Selection-RGP method, which is essentially fining tuning the LLM over a newly built Retrieval-Generation Preference (RGP) dataset.",
            "reference_string": "[276249310 | Weng et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "venue": "ACM Transactions on Knowledge Discovery from Data",
            "year": 2024,
            "reference_count": 90,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.08680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2656889",
                    "name": "Teo Su\u0161njak"
                },
                {
                    "authorId": "2296719088",
                    "name": "Peter Hwang"
                },
                {
                    "authorId": "1783269",
                    "name": "N. Reyes"
                },
                {
                    "authorId": "3312622",
                    "name": "A. Barczak"
                },
                {
                    "authorId": "11430146",
                    "name": "Timothy R. McIntosh"
                },
                {
                    "authorId": "143976433",
                    "name": "Surangika Ranathunga"
                }
            ],
            "abstract": "This research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.",
            "corpus_id": 269149146,
            "sentences": [
                {
                    "corpus_id": "269149146",
                    "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                    "text": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
                    "score": 0.6385772840635362,
                    "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
                    "char_start_offset": 18167,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 182,
                            "end": 432
                        },
                        {
                            "start": 432,
                            "end": 654
                        },
                        {
                            "start": 654,
                            "end": 819
                        },
                        {
                            "start": 819,
                            "end": 978
                        },
                        {
                            "start": 978,
                            "end": 1288
                        },
                        {
                            "start": 1288,
                            "end": 1529
                        },
                        {
                            "start": 1529,
                            "end": 1839
                        },
                        {
                            "start": 1839,
                            "end": 2073
                        },
                        {
                            "start": 2075,
                            "end": 2150
                        },
                        {
                            "start": 2152,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8505859375
                }
            ],
            "relevance_judgement": 0.8505859375,
            "relevance_judgment_input_expanded": "# Title: Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning\n# Venue: ACM Transactions on Knowledge Discovery from Data\n# Authors: Teo Su\u0161njak, Peter Hwang, N. Reyes, A. Barczak, Timothy R. McIntosh, Surangika Ranathunga\n## Abstract\nThis research pioneers the use of fine-tuned Large Language Models (LLMs) to automate Systematic Literature Reviews (SLRs), presenting a significant and novel contribution in integrating AI to enhance academic research methodologies. Our study employed advanced fine-tuning methodologies on open sourced LLMs, applying textual data mining techniques to automate the knowledge discovery and synthesis phases of an SLR process, thus demonstrating a practical and efficient approach for extracting and analyzing high-quality information from large academic datasets. The results maintained high fidelity in factual accuracy in LLM responses, and were validated through the replication of an existing PRISMA-conforming SLR. Our research proposed solutions for mitigating LLM hallucination and proposed mechanisms for tracking LLM responses to their sources of information, thus demonstrating how this approach can meet the rigorous demands of scholarly research. The findings ultimately confirmed the potential of fine-tuned LLMs in streamlining various labor-intensive processes of conducting literature reviews. As a scalable proof-of-concept, this study highlights the broad applicability of our approach across multiple research domains. The potential demonstrated here advocates for updates to PRISMA reporting guidelines, incorporating AI-driven processes to ensure methodological transparency and reliability in future SLRs. This study broadens the appeal of AI-enhanced tools across various academic and research fields, demonstrating how to conduct comprehensive and accurate literature reviews with more efficiency in the face of ever-increasing volumes of academic studies while maintaining high standards.\n## RAG for Enhanced Factual Accuracy in SLRs\nRetrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
            "reference_string": "[269149146 | Susnjak et al. | 2024 | Citations: 26]"
        },
        {
            "title": "Toward Optimal Search and Retrieval for RAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284988281",
                    "name": "Alexandria Leto"
                },
                {
                    "authorId": "2276606286",
                    "name": "Cecilia Aguerrebere"
                },
                {
                    "authorId": "3286327",
                    "name": "I. Bhati"
                },
                {
                    "authorId": "2276606593",
                    "name": "Ted Willke"
                },
                {
                    "authorId": "2276606934",
                    "name": "Mariano Tepper"
                },
                {
                    "authorId": "3369353",
                    "name": "Vy A. Vo"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.",
            "corpus_id": 273969566,
            "sentences": [
                {
                    "corpus_id": "273969566",
                    "title": "Toward Optimal Search and Retrieval for RAG",
                    "text": "Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.",
                    "score": 0.7017168120876025,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8466796875
                }
            ],
            "relevance_judgement": 0.8466796875,
            "relevance_judgment_input_expanded": "# Title: Toward Optimal Search and Retrieval for RAG\n# Venue: arXiv.org\n# Authors: Alexandria Leto, Cecilia Aguerrebere, I. Bhati, Ted Willke, Mariano Tepper, Vy A. Vo\n## Abstract\nRetrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.\n",
            "reference_string": "[273969566 | Leto et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3333168",
                    "name": "Ali Anaissi"
                },
                {
                    "authorId": "3069261",
                    "name": "Ali Braytee"
                },
                {
                    "authorId": "1992906806",
                    "name": "Junaid Akram"
                }
            ],
            "abstract": "We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.",
            "corpus_id": 273501949,
            "sentences": [
                {
                    "corpus_id": "273501949",
                    "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                    "text": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
                    "score": 0.7235959696138269,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 3739,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1163
                        },
                        {
                            "start": 1166,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84521484375
                }
            ],
            "relevance_judgement": 0.84521484375,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning LLMs for Reliable Medical Question-Answering Services\n# Venue: 2024 IEEE International Conference on Data Mining Workshops (ICDMW)\n# Authors: Ali Anaissi, Ali Braytee, Junaid Akram\n## Abstract\nWe present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.\n## II. RELATED WORK\nRetrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
            "reference_string": "[273501949 | Anaissi et al. | 2024 | Citations: 3]"
        },
        {
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 5,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326248349",
                    "name": "Jintao Liu"
                },
                {
                    "authorId": "2058085406",
                    "name": "Ruixue Ding"
                },
                {
                    "authorId": "2326165573",
                    "name": "Linhao Zhang"
                },
                {
                    "authorId": "2326115683",
                    "name": "Pengjun Xie"
                },
                {
                    "authorId": "2326114691",
                    "name": "Fie Huang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.",
            "corpus_id": 273375021,
            "sentences": [
                {
                    "corpus_id": "273375021",
                    "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
                    "text": "Retrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.",
                    "score": 0.6770586978311228,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8408203125
                },
                {
                    "corpus_id": "273375021",
                    "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
                    "text": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
                    "score": 0.6697642054979545,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1946
                        },
                        {
                            "start": 1947,
                            "end": 2027
                        },
                        {
                            "start": 2028,
                            "end": 2201
                        },
                        {
                            "start": 2202,
                            "end": 2392
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 380,
                            "matchedPaperCorpusId": "251371732"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8271484375
                }
            ],
            "relevance_judgement": 0.8408203125,
            "relevance_judgment_input_expanded": "# Title: CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity\n# Venue: arXiv.org\n# Authors: Jintao Liu, Ruixue Ding, Linhao Zhang, Pengjun Xie, Fie Huang\n## Abstract\nRetrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.\n## Introduction\nIn recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
            "reference_string": "[273375021 | Liu et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.21059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280046531",
                    "name": "Yunfan Gao"
                },
                {
                    "authorId": "2275320371",
                    "name": "Yun Xiong"
                },
                {
                    "authorId": "2291409458",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2256769434",
                    "name": "Haofen Wang"
                }
            ],
            "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
            "corpus_id": 271571401,
            "sentences": [
                {
                    "corpus_id": "271571401",
                    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "text": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
                    "score": 0.7684576637459273,
                    "section_title": "E. Tuning Pattern",
                    "char_start_offset": 55399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 768
                        },
                        {
                            "start": 771,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 920
                        },
                        {
                            "start": 921,
                            "end": 997
                        },
                        {
                            "start": 998,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1502
                        },
                        {
                            "start": 1503,
                            "end": 1617
                        },
                        {
                            "start": 1620,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2144
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84033203125
                }
            ],
            "relevance_judgement": 0.84033203125,
            "relevance_judgment_input_expanded": "# Title: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n# Venue: arXiv.org\n# Authors: Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n## Abstract\nRetrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.\n## E. Tuning Pattern\nRAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
            "reference_string": "[271571401 | Gao et al. | 2024 | Citations: 20]"
        },
        {
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.14510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326445025",
                    "name": "Jiayi Wu"
                },
                {
                    "authorId": "2274199835",
                    "name": "Hengyi Cai"
                },
                {
                    "authorId": "1387839383",
                    "name": "Lingyong Yan"
                },
                {
                    "authorId": "2322418772",
                    "name": "Hao Sun"
                },
                {
                    "authorId": "2320925202",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2237948548",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2243455567",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "2238182172",
                    "name": "Ming Gao"
                }
            ],
            "abstract": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
            "corpus_id": 274859726,
            "sentences": [
                {
                    "corpus_id": "274859726",
                    "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
                    "text": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
                    "score": 0.633802846349254,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8388671875
                }
            ],
            "relevance_judgement": 0.8388671875,
            "relevance_judgment_input_expanded": "# Title: PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao\n## Abstract\nThe emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.\n",
            "reference_string": "[274859726 | Wu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 120,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.09476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2127522115",
                    "name": "Jon Saad-Falcon"
                },
                {
                    "authorId": "144112155",
                    "name": "O. Khattab"
                },
                {
                    "authorId": "2254255092",
                    "name": "Christopher Potts"
                },
                {
                    "authorId": "2253469012",
                    "name": "Matei Zaharia"
                }
            ],
            "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
            "corpus_id": 265221210,
            "sentences": [
                {
                    "corpus_id": "265221210",
                    "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
                    "text": "Retrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer support (Petroni et al., 2021;Wang et al., 2019). Typically, a RAG system consists of a retriever and a downstream language model (LM). Given a user question, the retriever finds relevant passages from a corpus and the LM uses these passages to generate a response. This formulation admits a multitude of choices: what retrieval model to use, how to divide the documents into retrieval chunks, and how to prompt or finetune the LM to use the retrieved information, to name only a few of the simplest design decisions. \n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes, and cost/latency budgets. To tune their own RAG systems, practitioners traditionally need hand annotations for test questions, passages to retrieve (to assess the retriever), and responses to generate, labeled specifically for their target domain. Alternatively, they may evaluate different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs. \n\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al., 2023). For instance, the open-source RAGAS framework (James and Es, 2023) prompts an LM for evaluating the relevance of retrieved information and the faithfulness and accuracy of generated responses. Unfortunately, such strategies currently rely for evaluation on a fixed set of heuristically hand-written prompts, offering little adaptability to various evaluation contexts and no guarantees about quality. \n\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the Automated RAG Evaluation System. ARES is the first automated RAG evaluation system to generate tailored LLM judges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and accuracy compared to existing approaches like RA-GAS. Furthermore, unlike existing RAG evaluation systems, ARES provides confidence intervals for its scoring by leveraging prediction-powered inference (PPI;Angelopoulos et al. 2023).",
                    "score": 0.6415990111099272,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 701
                        },
                        {
                            "start": 704,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1797
                        },
                        {
                            "start": 1800,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2133
                        },
                        {
                            "start": 2134,
                            "end": 2312
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 192,
                            "end": 214,
                            "matchedPaperCorpusId": "221507798"
                        },
                        {
                            "start": 214,
                            "end": 232,
                            "matchedPaperCorpusId": "143424870"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.837890625
                }
            ],
            "relevance_judgement": 0.837890625,
            "relevance_judgment_input_expanded": "# Title: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Jon Saad-Falcon, O. Khattab, Christopher Potts, Matei Zaharia\n## Abstract\nEvaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.\n## Introduction\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer support (Petroni et al., 2021;Wang et al., 2019). Typically, a RAG system consists of a retriever and a downstream language model (LM). Given a user question, the retriever finds relevant passages from a corpus and the LM uses these passages to generate a response. This formulation admits a multitude of choices: what retrieval model to use, how to divide the documents into retrieval chunks, and how to prompt or finetune the LM to use the retrieved information, to name only a few of the simplest design decisions. \n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes, and cost/latency budgets. To tune their own RAG systems, practitioners traditionally need hand annotations for test questions, passages to retrieve (to assess the retriever), and responses to generate, labeled specifically for their target domain. Alternatively, they may evaluate different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs. \n\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al., 2023). For instance, the open-source RAGAS framework (James and Es, 2023) prompts an LM for evaluating the relevance of retrieved information and the faithfulness and accuracy of generated responses. Unfortunately, such strategies currently rely for evaluation on a fixed set of heuristically hand-written prompts, offering little adaptability to various evaluation contexts and no guarantees about quality. \n\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the Automated RAG Evaluation System. ARES is the first automated RAG evaluation system to generate tailored LLM judges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and accuracy compared to existing approaches like RA-GAS. Furthermore, unlike existing RAG evaluation systems, ARES provides confidence intervals for its scoring by leveraging prediction-powered inference (PPI;Angelopoulos et al. 2023).",
            "reference_string": "[265221210 | Saad-Falcon et al. | 2023 | Citations: 120]"
        },
        {
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296597690",
                    "name": "Patrice B'echard"
                },
                {
                    "authorId": "2296597772",
                    "name": "Orlando Marquez Ayala"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
            "corpus_id": 275357908,
            "sentences": [
                {
                    "corpus_id": "275357908",
                    "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
                    "text": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
                    "score": 0.7354331668851211,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83740234375
                }
            ],
            "relevance_judgement": 0.83740234375,
            "relevance_judgment_input_expanded": "# Title: Multi-task retriever fine-tuning for domain-specific and efficient RAG\n# Venue: arXiv.org\n# Authors: Patrice B'echard, Orlando Marquez Ayala\n## Abstract\nRetrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.\n",
            "reference_string": "[275357908 | B'echard et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270181751",
                    "name": "Zilun Zhang"
                },
                {
                    "authorId": "2174678931",
                    "name": "Haozhan Shen"
                },
                {
                    "authorId": "8200875",
                    "name": "Tiancheng Zhao"
                },
                {
                    "authorId": "2330774884",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2330612748",
                    "name": "Bin Chen"
                },
                {
                    "authorId": "2149196373",
                    "name": "Yuxiang Cai"
                },
                {
                    "authorId": "2093090552",
                    "name": "Yongheng Shang"
                },
                {
                    "authorId": "2111612160",
                    "name": "Jianwei Yin"
                }
            ],
            "abstract": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG",
            "corpus_id": 273969615,
            "sentences": [
                {
                    "corpus_id": "273969615",
                    "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                    "text": "Task-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks. Demonstrate-Search-Predict [116] introduces a modular approach that breaks down complex problems into manageable tasks, enhancing performance in multi-hop reasoning and open-domain question answering. Similarly, RA-DIT [117] uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks. These methods highlight the importance of tailoring RAG systems to specific tasks, enabling more effective and accurate solutions across diverse domains.",
                    "score": 0.6251108748755779,
                    "section_title": "C. Retrieval-Augmented Generation",
                    "char_start_offset": 69936,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 701
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 398,
                            "end": 403,
                            "matchedPaperCorpusId": "263605962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                }
            ],
            "relevance_judgement": 0.8359375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG\n# Venue: arXiv.org\n# Authors: Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin\n## Abstract\nUltra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG\n## C. Retrieval-Augmented Generation\nTask-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks. Demonstrate-Search-Predict [116] introduces a modular approach that breaks down complex problems into manageable tasks, enhancing performance in multi-hop reasoning and open-domain question answering. Similarly, RA-DIT [117] uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks. These methods highlight the importance of tailoring RAG systems to specific tasks, enabling more effective and accurate solutions across diverse domains.",
            "reference_string": "[273969615 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
            "venue": "Natural Language Processing",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/div-class-title-maximizing-rag-efficiency-a-comparative-analysis-of-rag-methods-div.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1017/nlp.2024.53?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/nlp.2024.53, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328607527",
                    "name": "Tolga \u015eakar"
                },
                {
                    "authorId": "2328604444",
                    "name": "Hakan Emekci"
                }
            ],
            "abstract": "\n This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
            "corpus_id": 273749074,
            "sentences": [
                {
                    "corpus_id": "273749074",
                    "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
                    "text": "This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
                    "score": 0.6548520479698565,
                    "section_title": "abstract",
                    "char_start_offset": 2,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82958984375
                }
            ],
            "relevance_judgement": 0.82958984375,
            "relevance_judgment_input_expanded": "# Title: Maximizing RAG efficiency: A comparative analysis of RAG methods\n# Venue: Natural Language Processing\n# Authors: Tolga \u015eakar, Hakan Emekci\n## Abstract\n\n This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.\n",
            "reference_string": "[273749074 | Sakar et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 67,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.20119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2358035947",
                    "name": "Lorenz Brehme"
                },
                {
                    "authorId": "2340022307",
                    "name": "Thomas Str\u00f6hle"
                },
                {
                    "authorId": "2333425035",
                    "name": "Ruth Breu"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.",
            "corpus_id": 278170975,
            "sentences": [
                {
                    "corpus_id": "278170975",
                    "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
                    "text": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.",
                    "score": 0.7255586097953897,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82763671875
                }
            ],
            "relevance_judgement": 0.82763671875,
            "relevance_judgment_input_expanded": "# Title: Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets\n# Venue: arXiv.org\n# Authors: Lorenz Brehme, Thomas Str\u00f6hle, Ruth Breu\n## Abstract\nRetrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.\n",
            "reference_string": "[278170975 | Brehme et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279752649",
                    "name": "Scott Barnett"
                },
                {
                    "authorId": "2279020735",
                    "name": "Zach Brannelly"
                },
                {
                    "authorId": "2266469333",
                    "name": "Stefanus Kurniawan"
                },
                {
                    "authorId": "2307101480",
                    "name": "Sheng Wong"
                }
            ],
            "abstract": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "corpus_id": 270560495,
            "sentences": [
                {
                    "corpus_id": "270560495",
                    "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
                    "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
                    "score": 0.6725433618665033,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.822265625
                }
            ],
            "relevance_judgement": 0.822265625,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models\n# Venue: arXiv.org\n# Authors: Scott Barnett, Zach Brannelly, Stefanus Kurniawan, Sheng Wong\n## Abstract\nLarge Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.\n",
            "reference_string": "[270560495 | Barnett et al. | 2024 | Citations: 2]"
        },
        {
            "title": "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 21,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.08067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41017337",
                    "name": "Dongyu Ru"
                },
                {
                    "authorId": "2315986460",
                    "name": "Lin Qiu"
                },
                {
                    "authorId": "2311193493",
                    "name": "Xiangkun Hu"
                },
                {
                    "authorId": "2257868315",
                    "name": "Tianhang Zhang"
                },
                {
                    "authorId": "2321562456",
                    "name": "Peng Shi"
                },
                {
                    "authorId": "46923811",
                    "name": "Shuaichen Chang"
                },
                {
                    "authorId": "2316087242",
                    "name": "Jiayang Cheng"
                },
                {
                    "authorId": "35504092",
                    "name": "Cunxiang Wang"
                },
                {
                    "authorId": "2256995981",
                    "name": "Shichao Sun"
                },
                {
                    "authorId": "2316222890",
                    "name": "Huanyu Li"
                },
                {
                    "authorId": "2316058939",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "2307185797",
                    "name": "Binjie Wang"
                },
                {
                    "authorId": "3024518",
                    "name": "Jiarong Jiang"
                },
                {
                    "authorId": "2264160722",
                    "name": "Tong He"
                },
                {
                    "authorId": "2287451122",
                    "name": "Zhiguo Wang"
                },
                {
                    "authorId": "2217850099",
                    "name": "Pengfei Liu"
                },
                {
                    "authorId": "2260818095",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "2313339226",
                    "name": "Zheng Zhang"
                }
            ],
            "abstract": "Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems. This work has been open sourced at https://github.com/amazon-science/RAGChecker.",
            "corpus_id": 271874517,
            "sentences": [
                {
                    "corpus_id": "271874517",
                    "title": "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) systems [18,7] enhance Large Language Models (LLMs) by incorporating external knowledge bases, enabling more precise and contextually relevant responses [7,53,13]. As these systems become integral to a variety of applications [54,2,8], it's imperative to develop robust and comprehensive evaluation frameworks to assess their performance and identify areas for improvement. Evaluating RAG systems, however, presents several challenges: \n\n(1) modular complexity: The modular nature of RAG systems, comprising both a retriever and a generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics that can holistically assess the entire system as well as evaluate the individual modules and their interplay [53], allowing for fully understanding the sources of the errors and misses and how they are generated. (2) metric limitation: Existing metrics for evaluating RAG systems, which are often rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically, traditional metrics like recall@k and MRR [44] for retrievers depend on annotated chunks and a rigid chunking approach, missing out on the full semantic scope of the knowledge base. For generators, typical measures such as n-gram-based (e.g., BLEU [30], ROUGE [19]), embedding-based (e.g., BERTScore [56]), and LLM-based methods [45] perform well with concise answers but fail to detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed, semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both the retrieval and generation components in RAG systems. (3) metric reliability: the reliability of existing metrics for RAG remains under-explored. Effective evaluation metrics must not only accurately reflect system performance but also align with human judgments to ensure their utility in real-world scenarios. \n\nTo overcome these challenges, we introduce RAGCHECKER, an innovative evaluation framework designed for detailed analysis of both retrieval and generation processes. RAGCHECKER is based on claim-level entailment checking which involves operations of extracting claims from the response and ground truth answer and checking them against other texts.",
                    "score": 0.6521546197211328,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 472
                        },
                        {
                            "start": 475,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1974
                        },
                        {
                            "start": 1977,
                            "end": 2141
                        },
                        {
                            "start": 2142,
                            "end": 2324
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 45,
                            "end": 49,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 263,
                            "end": 267,
                            "matchedPaperCorpusId": "258740478"
                        },
                        {
                            "start": 267,
                            "end": 269,
                            "matchedPaperCorpusId": "263866951"
                        },
                        {
                            "start": 1114,
                            "end": 1118,
                            "matchedPaperCorpusId": "16944215"
                        },
                        {
                            "start": 1320,
                            "end": 1324,
                            "matchedPaperCorpusId": "11080756"
                        },
                        {
                            "start": 1332,
                            "end": 1336,
                            "matchedPaperCorpusId": "964287"
                        },
                        {
                            "start": 1401,
                            "end": 1405,
                            "matchedPaperCorpusId": "259951284"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.822265625
                }
            ],
            "relevance_judgement": 0.822265625,
            "relevance_judgment_input_expanded": "# Title: RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation\n# Venue: Neural Information Processing Systems\n# Authors: Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, Zheng Zhang\n## Abstract\nDespite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems. This work has been open sourced at https://github.com/amazon-science/RAGChecker.\n## Introduction\nRetrieval-Augmented Generation (RAG) systems [18,7] enhance Large Language Models (LLMs) by incorporating external knowledge bases, enabling more precise and contextually relevant responses [7,53,13]. As these systems become integral to a variety of applications [54,2,8], it's imperative to develop robust and comprehensive evaluation frameworks to assess their performance and identify areas for improvement. Evaluating RAG systems, however, presents several challenges: \n\n(1) modular complexity: The modular nature of RAG systems, comprising both a retriever and a generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics that can holistically assess the entire system as well as evaluate the individual modules and their interplay [53], allowing for fully understanding the sources of the errors and misses and how they are generated. (2) metric limitation: Existing metrics for evaluating RAG systems, which are often rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically, traditional metrics like recall@k and MRR [44] for retrievers depend on annotated chunks and a rigid chunking approach, missing out on the full semantic scope of the knowledge base. For generators, typical measures such as n-gram-based (e.g., BLEU [30], ROUGE [19]), embedding-based (e.g., BERTScore [56]), and LLM-based methods [45] perform well with concise answers but fail to detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed, semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both the retrieval and generation components in RAG systems. (3) metric reliability: the reliability of existing metrics for RAG remains under-explored. Effective evaluation metrics must not only accurately reflect system performance but also align with human judgments to ensure their utility in real-world scenarios. \n\nTo overcome these challenges, we introduce RAGCHECKER, an innovative evaluation framework designed for detailed analysis of both retrieval and generation processes. RAGCHECKER is based on claim-level entailment checking which involves operations of extracting claims from the response and ground truth answer and checking them against other texts.",
            "reference_string": "[271874517 | Ru et al. | 2024 | Citations: 21]"
        },
        {
            "title": "Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing",
            "venue": "Advances in Computer Signals and Systems",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.23977/acss.2024.080608",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23977/acss.2024.080608?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23977/acss.2024.080608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324473075",
                    "name": "Xiangcan Li"
                }
            ],
            "abstract": ": In the field of complex query processing, traditional natural language processing models are often difficult to effectively deal with the diversity and complexity of query contents. The Retrieval Augmented Generation (RAG) model demonstrates unique advantages in processing complex queries by combining the two processes of retrieval and generation. This paper provides an in-depth discussion on the working principle of the RAG model and applies it to complex query processing scenarios. By analyzing real cases and validating experimental results, we demonstrate the significant advantages of the RAG model in enhancing query processing results. Although the RAG model shows good performance in processing complex queries, its application still faces some challenges and limitations. This paper concludes with an outlook on the future development of the RAG model, exploring possible optimization directions and application prospects.",
            "corpus_id": 272955388,
            "sentences": [
                {
                    "corpus_id": "272955388",
                    "title": "Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing",
                    "text": "The RAG model, known as Retrieval-Augmented Generation (RAG), is a natural language processing framework that combines information retrieval with generative modeling [7].The core idea of the RAG model is to retrieve document fragments relevant to the query from a large-scale document repository before generating a natural language response, and these fragments are fed into the generative model as contextual information to make the generated response more relevant and accurate [8]. These snippets are fed into the generation model as contextual information to make the generated response more relevant and accurate. By combining retrieval and generation in this way, the RAG model is able to provide more accurate answers in complex query scenarios\uff0cshowed in Figure 1: Retrieval Augmented Generation (RAG) is a technique that deeply integrates traditional information retrieval methods with generative modeling. In the RAG model, the retriever is responsible for filtering out the most relevant documents or information fragments from a large-scale dataset, while the generator generates natural language responses based on these retrieved contents [9]. The advantage of this approach is that it can utilize the rich information in the external knowledge base to compensate for the knowledge blindness of the generative model due to insufficient training data when facing complex queries, thus improving the quality and accuracy of the response [10]. The Information Retrieval Step: \n\nThe workflow of a RAG model is usually divided into two main steps: retrieval and generation. In the retrieval phase, the model selects the document fragments with the highest relevance to the query from a pre-constructed document library based on the input query. These snippets are passed to the model in the generation phase as additional contextual information. The model in the generation phase then uses these information fragments to generate the final natural language response. Through this two-stage processing, the RAG model is able to better understand the deeper semantics behind complex queries and generate answers that match the user's needs. Response Generation Step: \n\nThe advantages of RAG models in complex query processing are mainly reflected in their efficient utilization of external knowledge and high relevance of generated content. Compared with traditional generative models, RAG models not only rely on pre-trained language models, but also dynamically acquire the most relevant knowledge to the query, which gives them an obvious advantage when facing diverse and high-complexity queries.",
                    "score": 0.6395670045068313,
                    "section_title": "Overview of the RAG model and its retrieval enhancement generation technique",
                    "char_start_offset": 2174,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1486
                        },
                        {
                            "start": 1489,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1753
                        },
                        {
                            "start": 1754,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1975
                        },
                        {
                            "start": 1976,
                            "end": 2147
                        },
                        {
                            "start": 2148,
                            "end": 2173
                        },
                        {
                            "start": 2176,
                            "end": 2347
                        },
                        {
                            "start": 2348,
                            "end": 2607
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 169,
                            "matchedPaperCorpusId": "3238904"
                        },
                        {
                            "start": 481,
                            "end": 484,
                            "matchedPaperCorpusId": "7883049"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8212890625
                }
            ],
            "relevance_judgement": 0.8212890625,
            "relevance_judgment_input_expanded": "# Title: Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing\n# Venue: Advances in Computer Signals and Systems\n# Authors: Xiangcan Li\n## Abstract\n: In the field of complex query processing, traditional natural language processing models are often difficult to effectively deal with the diversity and complexity of query contents. The Retrieval Augmented Generation (RAG) model demonstrates unique advantages in processing complex queries by combining the two processes of retrieval and generation. This paper provides an in-depth discussion on the working principle of the RAG model and applies it to complex query processing scenarios. By analyzing real cases and validating experimental results, we demonstrate the significant advantages of the RAG model in enhancing query processing results. Although the RAG model shows good performance in processing complex queries, its application still faces some challenges and limitations. This paper concludes with an outlook on the future development of the RAG model, exploring possible optimization directions and application prospects.\n## Overview of the RAG model and its retrieval enhancement generation technique\nThe RAG model, known as Retrieval-Augmented Generation (RAG), is a natural language processing framework that combines information retrieval with generative modeling [7].The core idea of the RAG model is to retrieve document fragments relevant to the query from a large-scale document repository before generating a natural language response, and these fragments are fed into the generative model as contextual information to make the generated response more relevant and accurate [8]. These snippets are fed into the generation model as contextual information to make the generated response more relevant and accurate. By combining retrieval and generation in this way, the RAG model is able to provide more accurate answers in complex query scenarios\uff0cshowed in Figure 1: Retrieval Augmented Generation (RAG) is a technique that deeply integrates traditional information retrieval methods with generative modeling. In the RAG model, the retriever is responsible for filtering out the most relevant documents or information fragments from a large-scale dataset, while the generator generates natural language responses based on these retrieved contents [9]. The advantage of this approach is that it can utilize the rich information in the external knowledge base to compensate for the knowledge blindness of the generative model due to insufficient training data when facing complex queries, thus improving the quality and accuracy of the response [10]. The Information Retrieval Step: \n\nThe workflow of a RAG model is usually divided into two main steps: retrieval and generation. In the retrieval phase, the model selects the document fragments with the highest relevance to the query from a pre-constructed document library based on the input query. These snippets are passed to the model in the generation phase as additional contextual information. The model in the generation phase then uses these information fragments to generate the final natural language response. Through this two-stage processing, the RAG model is able to better understand the deeper semantics behind complex queries and generate answers that match the user's needs. Response Generation Step: \n\nThe advantages of RAG models in complex query processing are mainly reflected in their efficient utilization of external knowledge and high relevance of generated content. Compared with traditional generative models, RAG models not only rely on pre-trained language models, but also dynamically acquire the most relevant knowledge to the query, which gives them an obvious advantage when facing diverse and high-complexity queries.",
            "reference_string": "[272955388 | Li | 2024 | Citations: 1]"
        },
        {
            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108795448",
                    "name": "Mingtian Zhang"
                },
                {
                    "authorId": "2284682723",
                    "name": "Shawn Lan"
                },
                {
                    "authorId": "2067492948",
                    "name": "Peter Hayes"
                },
                {
                    "authorId": "2282542157",
                    "name": "David Barber"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.",
            "corpus_id": 267750557,
            "sentences": [
                {
                    "corpus_id": "267750557",
                    "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
                    "text": "In this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
                    "score": 0.6327797363862314,
                    "section_title": "Conclusion",
                    "char_start_offset": 23095,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 197,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 952
                        },
                        {
                            "start": 955,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1346
                        },
                        {
                            "start": 1349,
                            "end": 1381
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8203125
                }
            ],
            "relevance_judgement": 0.8203125,
            "relevance_judgment_input_expanded": "# Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning\n# Venue: arXiv.org\n# Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber\n## Abstract\nRetrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.\n## Conclusion\nIn this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
            "reference_string": "[267750557 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 163,
            "citation_count": 39,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.13193, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112539433",
                    "name": "Shangyu Wu"
                },
                {
                    "authorId": "2303313918",
                    "name": "Ying Xiong"
                },
                {
                    "authorId": "2301404967",
                    "name": "Yufei Cui"
                },
                {
                    "authorId": "107747459",
                    "name": "Haolun Wu"
                },
                {
                    "authorId": "2243412535",
                    "name": "Can Chen"
                },
                {
                    "authorId": "2283264350",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "2303518782",
                    "name": "Lianming Huang"
                },
                {
                    "authorId": "2272581493",
                    "name": "Xue Liu"
                },
                {
                    "authorId": "2271790635",
                    "name": "Tei-Wei Kuo"
                },
                {
                    "authorId": "2290008872",
                    "name": "Nan Guan"
                },
                {
                    "authorId": "2302177675",
                    "name": "C. Xue"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG update, including RAG with/without knowledge update. Then, we introduce RAG evaluation and benchmarking, as well as the application of RAG in representative NLP tasks and industrial scenarios. Finally, this paper discusses RAG's future directions and challenges for promoting this field's development.",
            "corpus_id": 271270644,
            "sentences": [
                {
                    "corpus_id": "271270644",
                    "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
                    "text": "This section gives an overview of RAG for NLP. As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions. Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc. \n\nRetrieval Fusions aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance. \n\nGenerator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93]. Those generators generate responses or make predictions. \n\nThe workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs; \n\n(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.",
                    "score": 0.7227757440128956,
                    "section_title": "OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION",
                    "char_start_offset": 3632,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 46
                        },
                        {
                            "start": 47,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 914
                        },
                        {
                            "start": 917,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1514
                        },
                        {
                            "start": 1517,
                            "end": 1644
                        },
                        {
                            "start": 1645,
                            "end": 1829
                        },
                        {
                            "start": 1830,
                            "end": 1975
                        },
                        {
                            "start": 1976,
                            "end": 2032
                        },
                        {
                            "start": 2035,
                            "end": 2163
                        },
                        {
                            "start": 2166,
                            "end": 2355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1761,
                            "end": 1765,
                            "matchedPaperCorpusId": "49313245"
                        },
                        {
                            "start": 1765,
                            "end": 1769,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1953,
                            "end": 1957,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 1970,
                            "end": 1974,
                            "matchedPaperCorpusId": "252846580"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81689453125
                }
            ],
            "relevance_judgement": 0.81689453125,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation for Natural Language Processing: A Survey\n# Venue: arXiv.org\n# Authors: Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, C. Xue\n## Abstract\nLarge language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG update, including RAG with/without knowledge update. Then, we introduce RAG evaluation and benchmarking, as well as the application of RAG in representative NLP tasks and industrial scenarios. Finally, this paper discusses RAG's future directions and challenges for promoting this field's development.\n## OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION\nThis section gives an overview of RAG for NLP. As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions. Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc. \n\nRetrieval Fusions aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance. \n\nGenerator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93]. Those generators generate responses or make predictions. \n\nThe workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs; \n\n(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.",
            "reference_string": "[271270644 | Wu et al. | 2024 | Citations: 39]"
        },
        {
            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
            "venue": "",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2361597409",
                    "name": "Shaohan Wang"
                },
                {
                    "authorId": "48378753",
                    "name": "L. Zhang"
                },
                {
                    "authorId": "2106681735",
                    "name": "Zheren Fu"
                },
                {
                    "authorId": "2349977855",
                    "name": "Zhendong Mao"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods focus on optimizing the retriever or generator in the RAG system by directly utilizing the top-k retrieved documents. However, the documents effectiveness are various significantly across user queries, i.e. some documents provide valuable knowledge while others totally lack critical information. It hinders the retriever and generator's adaptation during training. Inspired by human cognitive learning, curriculum learning trains models using samples progressing from easy to difficult, thus enhancing their generalization ability, and we integrate this effective paradigm to the training of the RAG system. In this paper, we propose a multi-stage Curriculum Learning based RAG system training framework, named CL-RAG. We first construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution. Then, we train the model in stages based on the curriculum learning approach, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.",
            "corpus_id": 278635834,
            "sentences": [
                {
                    "corpus_id": "278635834",
                    "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
                    "text": "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2022;Ram et al., 2023;Zhang et al., 2024) and question answering (Izacard et al., 2023;Shi et al., 2023;Yoran et al., 2023;Lin et al., 2023;Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023;Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
                    "score": 0.6890229350951205,
                    "section_title": "Retrieval-augmented Generation",
                    "char_start_offset": 5200,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 184,
                            "end": 207,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 207,
                            "end": 224,
                            "matchedPaperCorpusId": "256459451"
                        },
                        {
                            "start": 224,
                            "end": 243,
                            "matchedPaperCorpusId": "271915498"
                        },
                        {
                            "start": 267,
                            "end": 289,
                            "matchedPaperCorpusId": "251371732"
                        },
                        {
                            "start": 306,
                            "end": 325,
                            "matchedPaperCorpusId": "263608822"
                        },
                        {
                            "start": 325,
                            "end": 342,
                            "matchedPaperCorpusId": "263605962"
                        },
                        {
                            "start": 888,
                            "end": 908,
                            "matchedPaperCorpusId": "271915498"
                        },
                        {
                            "start": 1225,
                            "end": 1245,
                            "matchedPaperCorpusId": "263608822"
                        },
                        {
                            "start": 1316,
                            "end": 1334,
                            "matchedPaperCorpusId": "263605962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81689453125
                }
            ],
            "relevance_judgement": 0.81689453125,
            "relevance_judgment_input_expanded": "# Title: CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning\n# Venue: \n# Authors: Shaohan Wang, L. Zhang, Zheren Fu, Zhendong Mao\n## Abstract\nRetrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods focus on optimizing the retriever or generator in the RAG system by directly utilizing the top-k retrieved documents. However, the documents effectiveness are various significantly across user queries, i.e. some documents provide valuable knowledge while others totally lack critical information. It hinders the retriever and generator's adaptation during training. Inspired by human cognitive learning, curriculum learning trains models using samples progressing from easy to difficult, thus enhancing their generalization ability, and we integrate this effective paradigm to the training of the RAG system. In this paper, we propose a multi-stage Curriculum Learning based RAG system training framework, named CL-RAG. We first construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution. Then, we train the model in stages based on the curriculum learning approach, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.\n## Retrieval-augmented Generation\nUsing documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2022;Ram et al., 2023;Zhang et al., 2024) and question answering (Izacard et al., 2023;Shi et al., 2023;Yoran et al., 2023;Lin et al., 2023;Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023;Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
            "reference_string": "[278635834 | Wang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction Tuning a Word-Embedding based Retrieval Augmented Large Language Model",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.00451, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328976357",
                    "name": "Subhadip Nandi"
                },
                {
                    "authorId": "2328975473",
                    "name": "Neeraj Agrawal"
                }
            ],
            "abstract": "Few-Shot Cross-Domain NER is the process of leveraging knowledge from data-rich source domains to perform entity recognition on data-scarce target domains. Most previous state-of-the-art (SOTA) approaches use pre-trained language models (PLMs) for cross-domain NER. However, these models are often domain specific. To successfully use these models for new target domains, we need to modify either the model architecture or perform model fine-tuning using data from the new domains. Both of these result in the creation of entirely new NER models for each target domain which is infeasible for practical scenarios. Recently, several works have attempted to use LLMs to solve Few-Shot Cross-Domain NER. However, most of these are either too expensive for practical purposes or struggle to follow LLM prompt instructions. In this paper, we propose IF-WRANER (Instruction Finetuned Word-embedding based Retrieval Augmented large language model for Named Entity Recognition), a retrieval augmented LLM, finetuned for the NER task. By virtue of the regularization techniques used during LLM finetuning and the adoption of word-level embedding over sentence-level embedding during the retrieval of in-prompt examples, IF-WRANER is able to outperform previous SOTA Few-Shot Cross-Domain NER approaches. We have demonstrated the effectiveness of our model by benchmarking its performance on the open source CrossNER dataset, on which it shows more than 2% F1 score improvement over the previous SOTA model. We have deployed the model for multiple customer care domains of an enterprise. Accurate entity prediction through IF-WRANER helps direct customers to automated workflows for the domains, thereby reducing escalations to human agents by almost 15% and leading to millions of dollars in yearly savings for the company.",
            "corpus_id": 273798397,
            "sentences": [
                {
                    "corpus_id": "273798397",
                    "title": "Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction Tuning a Word-Embedding based Retrieval Augmented Large Language Model",
                    "text": "With Retrieval Augmented Generation (RAG), instead of having the same hardcoded domain examples appended for every query, examples are selected dynamically based on their similarity with the input query. To achieve this, embeddings for domain examples are at first computed using a pre-trained universal embedder and the examples along with their embeddings are stored in a vector database. During inference, when a query comes in, its embedding is computed using the same embedder. Then similarity scores of the query embedding with all the embeddings stored in the vector db are computed and the top k most similar examples are selected and appended to the prompt, which is then sent to the LLM for response generation. The RAG framework is largely made up of two components \u2022 Retriever: It is responsible for generating query embedding during inference using the embedder and also for extracting top k most similar examples from vector DB using a similarity function. \n\n\u2022 Generator: The prompt, made up of the user query and the top k examples obtained by the retriever are passed along to the generator component (in our case an LLM) which is responsible for generating a response. The complete RAG architecture is shown in Figure 1. RAG with LLM yields good results for Few-Shot Cross-Domain NER when using proprietary LLMs like GPT4. However, most open source LLMs struggle to produce output in the format specified as part of prompt instruction. This becomes a challenge, because using GPT4 to perform NER for applications at scale can be extremely costly. Therefore, we need to finetune open source LLMs so that they can follow prompt instructions.",
                    "score": 0.6571181594459291,
                    "section_title": "Retrieval Augmented Generation (RAG)",
                    "char_start_offset": 10463,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 390
                        },
                        {
                            "start": 391,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1656
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81103515625
                }
            ],
            "relevance_judgement": 0.81103515625,
            "relevance_judgment_input_expanded": "# Title: Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction Tuning a Word-Embedding based Retrieval Augmented Large Language Model\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Subhadip Nandi, Neeraj Agrawal\n## Abstract\nFew-Shot Cross-Domain NER is the process of leveraging knowledge from data-rich source domains to perform entity recognition on data-scarce target domains. Most previous state-of-the-art (SOTA) approaches use pre-trained language models (PLMs) for cross-domain NER. However, these models are often domain specific. To successfully use these models for new target domains, we need to modify either the model architecture or perform model fine-tuning using data from the new domains. Both of these result in the creation of entirely new NER models for each target domain which is infeasible for practical scenarios. Recently, several works have attempted to use LLMs to solve Few-Shot Cross-Domain NER. However, most of these are either too expensive for practical purposes or struggle to follow LLM prompt instructions. In this paper, we propose IF-WRANER (Instruction Finetuned Word-embedding based Retrieval Augmented large language model for Named Entity Recognition), a retrieval augmented LLM, finetuned for the NER task. By virtue of the regularization techniques used during LLM finetuning and the adoption of word-level embedding over sentence-level embedding during the retrieval of in-prompt examples, IF-WRANER is able to outperform previous SOTA Few-Shot Cross-Domain NER approaches. We have demonstrated the effectiveness of our model by benchmarking its performance on the open source CrossNER dataset, on which it shows more than 2% F1 score improvement over the previous SOTA model. We have deployed the model for multiple customer care domains of an enterprise. Accurate entity prediction through IF-WRANER helps direct customers to automated workflows for the domains, thereby reducing escalations to human agents by almost 15% and leading to millions of dollars in yearly savings for the company.\n## Retrieval Augmented Generation (RAG)\nWith Retrieval Augmented Generation (RAG), instead of having the same hardcoded domain examples appended for every query, examples are selected dynamically based on their similarity with the input query. To achieve this, embeddings for domain examples are at first computed using a pre-trained universal embedder and the examples along with their embeddings are stored in a vector database. During inference, when a query comes in, its embedding is computed using the same embedder. Then similarity scores of the query embedding with all the embeddings stored in the vector db are computed and the top k most similar examples are selected and appended to the prompt, which is then sent to the LLM for response generation. The RAG framework is largely made up of two components \u2022 Retriever: It is responsible for generating query embedding during inference using the embedder and also for extracting top k most similar examples from vector DB using a similarity function. \n\n\u2022 Generator: The prompt, made up of the user query and the top k examples obtained by the retriever are passed along to the generator component (in our case an LLM) which is responsible for generating a response. The complete RAG architecture is shown in Figure 1. RAG with LLM yields good results for Few-Shot Cross-Domain NER when using proprietary LLMs like GPT4. However, most open source LLMs struggle to produce output in the format specified as part of prompt instruction. This becomes a challenge, because using GPT4 to perform NER for applications at scale can be extremely costly. Therefore, we need to finetune open source LLMs so that they can follow prompt instructions.",
            "reference_string": "[273798397 | Nandi et al. | 2024 | Citations: 0]"
        },
        {
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "venue": "Knowledge-Based Systems",
            "year": 2025,
            "reference_count": 137,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342276561",
                    "name": "Lilian Some"
                },
                {
                    "authorId": "2341600949",
                    "name": "Wenli Yang"
                },
                {
                    "authorId": "2342277330",
                    "name": "Michael Bain"
                },
                {
                    "authorId": "2341910700",
                    "name": "Byeong Kang"
                }
            ],
            "abstract": null,
            "corpus_id": 275906690,
            "sentences": [
                {
                    "corpus_id": "275906690",
                    "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                    "text": "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109]. \n\nQuery expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122]. Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114]. Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123]. Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105]. \n\nRetrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications. RAG capabilities are experienced in varied settings. In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114]. \n\nIn question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context. This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120]. RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124]. Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125]. Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].",
                    "score": 0.6381589617040168,
                    "section_title": "Retrieval augmented generation",
                    "char_start_offset": 50470,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 296
                        },
                        {
                            "start": 299,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 1035
                        },
                        {
                            "start": 1038,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1512
                        },
                        {
                            "start": 1515,
                            "end": 1684
                        },
                        {
                            "start": 1685,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1977
                        },
                        {
                            "start": 1978,
                            "end": 2155
                        },
                        {
                            "start": 2156,
                            "end": 2325
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 600,
                            "end": 604,
                            "matchedPaperCorpusId": "265498312"
                        },
                        {
                            "start": 1808,
                            "end": 1812,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80712890625
                }
            ],
            "relevance_judgement": 0.80712890625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods\n# Venue: Knowledge-Based Systems\n# Authors: Lilian Some, Wenli Yang, Michael Bain, Byeong Kang\n## Abstract\nNone\n## Retrieval augmented generation\nDense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109]. \n\nQuery expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122]. Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114]. Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123]. Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105]. \n\nRetrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications. RAG capabilities are experienced in varied settings. In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114]. \n\nIn question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context. This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120]. RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124]. Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125]. Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].",
            "reference_string": "[275906690 | Some et al. | 2025 | Citations: 0]"
        },
        {
            "title": "AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 29,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325115068",
                    "name": "Yixiong Fang"
                },
                {
                    "authorId": "2350599634",
                    "name": "Tianran Sun"
                },
                {
                    "authorId": "2279412470",
                    "name": "Yuling Shi"
                },
                {
                    "authorId": "2269756996",
                    "name": "Xiaodong Gu"
                }
            ],
            "abstract": "While RAG demonstrates remarkable capabilities in LLM applications, its effectiveness is hindered by the ever-increasing length of retrieved contexts, which introduces information redundancy and substantial computational overhead. Existing context pruning methods, such as LLMLingua, lack contextual awareness and offer limited flexibility in controlling compression rates, often resulting in either insufficient pruning or excessive information loss. In this paper, we propose AttentionRAG, an attention-guided context pruning method for RAG systems. The core idea of AttentionRAG lies in its attention focus mechanism, which reformulates RAG queries into a next-token prediction paradigm. This mechanism isolates the query's semantic focus to a single token, enabling precise and efficient attention calculation between queries and retrieved contexts. Extensive experiments on LongBench and Babilong benchmarks show that AttentionRAG achieves up to 6.3$\\times$ context compression while outperforming LLMLingua methods by around 10\\% in key metrics.",
            "corpus_id": 277043707,
            "sentences": [
                {
                    "corpus_id": "277043707",
                    "title": "AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation",
                    "text": "Retrieve-Augmented Generation (RAG) is a framework that enhances the capabilities of LLMs by integrating external knowledge through retrieval. \n\nA RAG system typically consists of two components: a retriever, which fetches relevant documents, called contexts, from a large corpus based on a query, and a generator, which generates an answer using both the retrieved context and the model's internal knowledge. This combination enables more accurate and contextually relevant outputs, especially for tasks requiring detailed or up-to-date information that might not be present in the model's training data.",
                    "score": 0.737609041910489,
                    "section_title": "Retrieve-Augmented Generation",
                    "char_start_offset": 5881,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 145,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 605
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                }
            ],
            "relevance_judgement": 0.80615234375,
            "relevance_judgment_input_expanded": "# Title: AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Yixiong Fang, Tianran Sun, Yuling Shi, Xiaodong Gu\n## Abstract\nWhile RAG demonstrates remarkable capabilities in LLM applications, its effectiveness is hindered by the ever-increasing length of retrieved contexts, which introduces information redundancy and substantial computational overhead. Existing context pruning methods, such as LLMLingua, lack contextual awareness and offer limited flexibility in controlling compression rates, often resulting in either insufficient pruning or excessive information loss. In this paper, we propose AttentionRAG, an attention-guided context pruning method for RAG systems. The core idea of AttentionRAG lies in its attention focus mechanism, which reformulates RAG queries into a next-token prediction paradigm. This mechanism isolates the query's semantic focus to a single token, enabling precise and efficient attention calculation between queries and retrieved contexts. Extensive experiments on LongBench and Babilong benchmarks show that AttentionRAG achieves up to 6.3$\\times$ context compression while outperforming LLMLingua methods by around 10\\% in key metrics.\n## Retrieve-Augmented Generation\nRetrieve-Augmented Generation (RAG) is a framework that enhances the capabilities of LLMs by integrating external knowledge through retrieval. \n\nA RAG system typically consists of two components: a retriever, which fetches relevant documents, called contexts, from a large corpus based on a query, and a generator, which generates an answer using both the retrieved context and the model's internal knowledge. This combination enables more accurate and contextually relevant outputs, especially for tasks requiring detailed or up-to-date information that might not be present in the model's training data.",
            "reference_string": "[277043707 | Fang et al. | 2025 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context [30][31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.\n\nSeveral surveys have extensively discussed current RAG systems, covering aspects like text generation [7,8], integration with LLMs [6,39], multimodal [40], and AI-generated content [41].While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appro-  priate algorithm for practical implementation remains challenging.In this paper, we focus on best practices for applying RAG methods, advancing the understanding and application of RAG in LLMs.",
            "score": 0.9393830879509706,
            "section_title": "Retriever and Generator Fine-tuning",
            "char_start_offset": 7670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 551
                },
                {
                    "start": 553,
                    "end": 739
                },
                {
                    "start": 739,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1037
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 185,
                    "matchedPaperCorpusId": "258865283"
                },
                {
                    "start": 658,
                    "end": 660,
                    "matchedPaperCorpusId": "250340214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9306640625
        },
        {
            "corpus_id": "270062508",
            "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
            "text": "Retrieval-Augmented Generation.We review the literature of Retrieval-Augmented Generation (RAG) in terms of (1) Naive RAG, (2) Advanced RAG, and (3) Modular RAG.For (1), Naive RAG follows a standard process including indexing, retrieval, and generation (Ma et al., 2023).However, its quality faces significant challenges such as low precision, hallucination, and redundancy during the process.For (2), Advanced RAG is further developed to overcome the shortcomings of Naive RAG.Specifically, during the indexing stage, the objective is to enhance the quality of the indexed content by optimizing data embedding (Li et al., 2023).During the retrieval stage, the focus is on identifying the appropriate context by calculating the similarity between the query and chunks, where the techniques involve fine-tuning embedding models (Xiao et al., 2023), or learning dynamic embeddings for different context (Karpukhin et al., 2020).During the generation stage, it merges the retrieved context with the query as an input into large language models (LLMs), where it addresses challenges posed by context window limits with re-ranking the most relevant content (Jiang et al., 2023b;Zhuang et al., 2023), or compressing prompts (Litman et al., 2020;Xu et al., 2023).In addition, Self-RAG (Asai et al., 2023) is proposed to identify whether retrieval is necessary, or the retrieved context is relevant, which helps language models to produce meaningful generation (Asai et al., 2023).For (3), Modular RAG diverges from the traditional Naive RAG structure by incorporating external modules to further enhance the performance, including search module (Wang et al., 2023a), memory module (Wang et al., 2022;Cheng et al., 2023b), tuning module (Lin et al., 2023), and task adapter (Cheng et al., 2023a;Dai et al., 2023).",
            "score": 0.8808888912710653,
            "section_title": "Related Work",
            "char_start_offset": 6288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 628,
                    "matchedPaperCorpusId": "258987900"
                },
                {
                    "start": 901,
                    "end": 925,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "263830701"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "264406035"
                },
                {
                    "start": 1218,
                    "end": 1239,
                    "matchedPaperCorpusId": "214641123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "276774736",
            "title": "Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models (LMs) by incorporating retrieved text passages into the input, leading to significant improvements in knowledge-intensive tasks (Guu et al., 2020;Lewis et al., 2020)  instruction-tuning LMs with a fixed number of retrieved passages or jointly pre-training a retriever and LM followed by few-shot fine-tuning (Luo et al., 2023;Izacard et al., 2022). Some approaches adaptively retrieve passages during generation (Jiang et al., 2023), while others, like Schick et al. (2023), train LMs to generate API calls for named entities. However, these improvements often come with trade-offs in runtime efficiency, robustness, and contextual relevance (Mallen et al., 2023;Shi et al., 2023). To address these challenges, recent work introduces methods like SELF-RAG, which enables on-demand retrieval and filters out irrelevant passages through self-reflection, enhancing robustness and control (Lin et al., 2024;Yoran et al., 2024). SELF-RAG (Asai et al., 2023) also evaluates the factuality and quality of the generated output without relying on external models during inference, making it more efficient and customizable. Additionally, other concurrent RAG methods, such as LATS (Zhou et al., 2023), explore ways to improve retrieval for specific tasks like question answering through tree search. \n\n3 Method",
            "score": 0.8444377736223083,
            "section_title": "Retrievel Augementated Generation",
            "char_start_offset": 6546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1363
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 474,
                    "end": 494,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 515,
                    "end": 535,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 725,
                    "end": 742,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 947,
                    "end": 965,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.701171875
        },
        {
            "corpus_id": "277781278",
            "title": "UltraRAG: A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) is an effective method for mitigating hallucination and factual inaccuracy issues of large language models (LLMs) (Jiang et al., 2023;Xu et al., 2023;Luo et al., 2023;Hu et al., 2023). It has been widely adopted in various natural language processing (NLP) tasks, such as open-domain QA (Trivedi et al., 2023), language modeling (He et al., 2021), and dialogue (Cai et al., 2019). A typical RAG system consists of two key components, a retriever and a generator (Shi et al., 2023;Yu et al., 2023). The retriever retrieves relevant documents from an external corpus based on the user's query (Karpukhin et al., 2020;Xiong et al., 2021) and the generator utilizes these documents as context of inputs to augment the generation process (Ram et al., 2023;Xu et al., 2023). \n\nWith the continuous advancement of research on RAG systems, recent studies have introduced additional modules and explored various training methods specifically tailored for RAG systems (Yan et al., 2024;Lin et al., 2023;Wang et al., 2023;Wei et al., 2024). For example, Yan et al. (2024) propose an additional retrieval evaluator to refine the quality of retrieved documents. Li et al. (2024) utilize a rollout method to obtain rewards from the entire RAG system for each module and optimize them based on the reward. The success of these approaches highlights the growing need for a general-purpose RAG toolkit, which can streamline development and evaluation across diverse RAG frameworks. \n\nRAG Toolkits. Various RAG toolkits have been developed to assist users in building customized RAG systems, such as LangChain (Chase, 2022) and LlamaIndex (Liu, 2022).",
            "score": 0.838690627859316,
            "section_title": "Related Work",
            "char_start_offset": 4812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 203,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 356,
                    "end": 378,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "237452184"
                },
                {
                    "start": 430,
                    "end": 448,
                    "matchedPaperCorpusId": "52281331"
                },
                {
                    "start": 660,
                    "end": 684,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 684,
                    "end": 703,
                    "matchedPaperCorpusId": "220302524"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "276287820",
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "score": 0.8321421183923233,
            "section_title": "Related Work",
            "char_start_offset": 5971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "score": 0.8268299276906443,
            "section_title": "Introduction",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "275993994",
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "score": 0.8173813341354541,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.955078125
        },
        {
            "corpus_id": "273185619",
            "title": "Contrastive Learning to Improve Retrieval for Real-World Fact Checking",
            "text": "Retrieval-augmented generation (RAG) relies on two key modules: a retriever and a reader/generation model. For many RAG systems, noisy retrieval hurts downstream performance by providing irrelevant or misleading documents (Yoran et al., 2024). Sauchuk et al. (2022) found that adding distractors can cause a 27% drop on veracity classification accuracy on FEVER. Therefore, it's important for retrievers to find relevant documents and simultaneously avoid damaging ones. Shi et al. (2023) attempts to solve this problem by finetuning the retrieval component while fixing the reader LM, similar to our work. Other approaches like Ke et al. (2024) create a more complex system with a \"bridging\" model between the retriever and reader. Nevertheless, noisy retrieval remains a failure point in RAG systems (Barnett et al., 2024), and tangible downstream gains can be realized by further finetuning.",
            "score": 0.8164915442655989,
            "section_title": "Retrieval Augmented Generation Systems",
            "char_start_offset": 5329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 265,
                    "matchedPaperCorpusId": "250340232"
                },
                {
                    "start": 802,
                    "end": 824,
                    "matchedPaperCorpusId": "266933076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "RAG-Memory-Finetuning is a technique to optimize the performance of large language models (LLMs) by combining Retrieval Augmented Generation (RAG) with fine-tuning.This approach aims to connect LLMs to external knowledge sources 56 AgentLM-70B:https://huggingface.co/THUDM/agentlm-70B 57 AgentLM-7B:https://huggingface.co/THUDM/agentlm-7b 58 AgentLM-7B:https://huggingface.co/THUDM/agentlm-13B 59 AgentTuning Github:https://github.com/THUDM/AgentTuningNavigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies 2023-12-20 through a retrieval mechanism and combine it with generative capabilities to search and integrate relevant information from knowledge bases.The goal of RAG-Memory-Finetuning is to improve the consistency and reliability of the output and reduce hallucination issues.Specifically,the RAG technique is implemented through the following steps:\n\n1) Chunking an external domain-specific knowledge base into small documents,each about 150 words.2) Creating embeddings using a pre-trained model and storing document vectors in a vector database.3) When an input query is passed to the LLM,the most relevant information is retrieved from the external database using metrics such as cosine similarity and combined with the LLM as additional context.4) This external context and the input prompt are passed together to the text generator to produce the output response.Responses generated by RAG are more factual,specific,and diverse.The parameter knowledge provided by traditional finetuning techniques is static,while RAG allows us to bypass retraining and obtain up-to-date information through retrievalbased generation to produce reliable outputs.M gen predicts IsUse given x, y t 12: end if through self-reflection and also addresses the hallucination issues present in large models.\n\nAlgorithm 2 present an overview of SELF-RAG [4] at inference.For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.",
            "score": 0.8155934496217639,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 144027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1868
                },
                {
                    "start": 1870,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "271744960",
            "title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature",
            "text": "Retrieval Augmented Generation -RAG (Lewis et al., 2020) represents a transformative approach to enriching the quality and pertinence of generated content by integrating external insights derived from extensive datasets or repositories of knowledge. By embedding pertinent external knowledge sources into the generation process, RAG is designed to elevate the coherence, factual precision, and overall utility of generated text. This methodology proves advantageous in domains necessitating precise and contextually nuanced content generation, such as questionanswering, summarizing, and advanced dialogue systems. By controlling retrieval mechanisms, RAG ensures that the resultant outputs are provided with high accuracy and contextual relevance, thereby advancing the frontiers of natural language processing applications. \n\nIn pursuit of enhancing the precision of model responses and mitigating the phenomenon of LLM hallucinations, a novel approach has emerged: Retrieval Augmented Fine-Tuning (RAFT) (Zhang et al., 2024). This methodology integrates the RAG framework with fine-tuning techniques, empowering models not only to acquire domain-specific knowledge but also to adeptly retrieve and comprehend external contexts crucial for task execution. RAFT introduces the idea of chain-of-thought prompting for building the fine-tuning data set. This prompting technique enables the model's answers to show its reason line with a sequence of arguments, enhancing its explicability. \n\nRAG and RAFT were designed to confront the complexity of tailoring LLMs to specialized domains. Within these realms, the emphasis pivots from general knowledge reasoning to optimizing accuracy vis-\u00e0-vis a meticulously defined array of domain-specific documents.",
            "score": 0.809485592733832,
            "section_title": "Retrieval-Augmented Approach",
            "char_start_offset": 11378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 56,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7587890625
        },
        {
            "corpus_id": "266998785",
            "title": "Bridging the Preference Gap between Retrievers and LLMs",
            "text": "Retrieval-augmented Generation (RAG). Augmenting LLMs with relevant information retrieved from various knowledge sources is proven effective across numerous NLP tasks, including language modeling (Borgeaud et al., 2022;Khandelwal et al., 2020;Shi et al., 2023b), question answering (Lewis et al., 2020;Izacard et al., 2022;de Jong et al., 2023;De Jong et al., 2023;Shi et al., 2023b;Guu et al., 2020;Izacard and Grave, 2020;Xu et al., 2023), fact versification (Lewis et al., 2020) and text generation (Lewis et al., 2020). Specifically, RAG utilizes input as a query and comprises two main components: (1) a retriever retrieves a set of items from a side corpus. Particular items may vary across different tasks, including documents, passages, or even tokens. In this study, we focus on retrieving passages; and (2) a LLM incorporates the retrieved items, as additional information in the input context, and makes final predictions. \n\nA fundamental question in this process arises regarding the disparate preferences between LLMs and retrievers, as LLMs performing optimally only when their preferences are satisfied. Bridging the preference gap is crucial. Depending on which components are subject to updates, this challenge can be categorized into three families. \n\nFinetuning retrievers and LLMs jointly. This is the most widely used setting of RAG (Izacard et al., 2022;Khandelwal et al., 2020;Wu et al., 2022;Guu et al., 2020;Lewis et al., 2020). However, most prior work is based on relative small LMs (< 1B). For example, Altas (Izacard et al., 2022) finetunes LLM (T5 (Raffel et al., 2020a)) and retriever (Contriever (Izacard et al., 2021)) jointly by leveraging the LLM to provide supervisory signal to train the retriever.",
            "score": 0.8079930974037443,
            "section_title": "Related Work",
            "char_start_offset": 7083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1735
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 219,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 219,
                    "end": 243,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 282,
                    "end": 302,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 323,
                    "end": 344,
                    "matchedPaperCorpusId": "256231037"
                },
                {
                    "start": 344,
                    "end": 365,
                    "matchedPaperCorpusId": "256231037"
                },
                {
                    "start": 383,
                    "end": 400,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 461,
                    "end": 480,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 502,
                    "end": 522,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1376,
                    "end": 1400,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1416,
                    "end": 1433,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1433,
                    "end": 1452,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "score": 0.805642954216085,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 145958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 979
                },
                {
                    "start": 981,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1721
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "270764659",
            "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
            "text": "To address the misalignment between different components of retrieval-augmented generation (RAG) and improve overall generation performance, we propose the DPA-RAG framework, which is illustrated in Figure 2. In general, DPA-RAG improves traditional RAG architecture in two main aspects: (1) we fine-tune a preference-aligned reranker between the retriever and the LLM to selectively filter out knowledge that aligns with LLMs' knowledge preferences ( \u00a73.3); and (2) we design a self-alignment mechanism that fine-tunes LLMs to better recognize and utilize knowledge consistent with their reasoning preferences ( \u00a73.4).To acquire the LLM's preference knowledge, we devise a three-step construction method, motivated by our preliminary analysis of how different types of retrieved documents affect RAG performance ( \u00a73.2).Below, we will first introduce the task definition ( \u00a73.1) and then we delve into the specifics of our approach.",
            "score": 0.8048916133052813,
            "section_title": "Methodology",
            "char_start_offset": 8100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "269757277",
            "title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) leverages a retriever that provides substantial external information for improving the generated output of LLMs.This strategy utilizes knowledge in a parameter-free manner, circumvents the high training costs of LLMs' parameterized knowledge.Furthermore, it alleviates the hallucination issues in LLMs, significantly enhancing the factual accuracy and relevance of the generated content.The concept of RAG is rooted in the DrQA framework (Chen et al., 2017), which marked the initial phase of integrating retrieval mechanisms with Language Models (LMs) through heuristic retrievers like TF-IDF for sourcing evidence.Subsequently, RAG underwent evolution with the introduction of Dense Passage Retrieval (Karpukhin et al., 2020), and further advancements in RAG (Lewis et al., 2020) and REALM (Ram et al., 2023).These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation components.Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs).Exemplary developments such as RE-PLUG (Shi et al., 2023) and IC-RALM (Ram et al., 2023) demonstrate the potent generalization abilities of LLMs in zero-shot or few-shot scenarios.These models are capable of following complex instructions, understanding retrieved information, and utilizing limited demonstrations to generate high-quality responses.",
            "score": 0.8025279268341984,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 6872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1449
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.650390625
        },
        {
            "corpus_id": "271956620",
            "title": "Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models",
            "text": "By integrating external information, RAG methods enhance reasoning and generation process (Gao et al. 2024;Zhao et al. 2024). Early work primarily focuses on improving retrieval model performance to obtain relevant documents for subsequent generation (Qu et al. 2021;Wang et al. 2023;Zheng et al. 2024). Recent research has expanded RAG framework to real-world noisy scenarios, aiming to build robust RAG systems by enhancing the generator (Fang et al. 2024;Xiang et al. 2024). For instance, Self-RAG (Asai et al. 2024) employs four specialized tokens and GPT-4-generated instruction-tuning data to fine-tune the Llama2 model. RetRobust (Yoran et al. 2024) introduces an automated data generation method to fine-tune the generator to utilize retrieved passages against noise effectively. Ro-bustRAG (Xiang et al. 2024) proposes a defense framework that enhances RAG model robustness against retrieval corruption attacks through an isolate-then-aggregate strategy, achieving certifiable robustness via secure text aggregation techniques. However, these investigations are constrained by their narrow focus on specific noise types and the inherent assumption that noise is harmful, potentially hindering method generalization. This paper aims to present a systematic analysis of RAG noise and reveal its role.",
            "score": 0.8015983633906012,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "269330213"
                },
                {
                    "start": 440,
                    "end": 458,
                    "matchedPaperCorpusId": "32339717"
                },
                {
                    "start": 501,
                    "end": 518,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428466796875
        },
        {
            "corpus_id": "276107364",
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) combines retrieval and generation to enhance language models in knowledge-intensive tasks. Early RAG systems [57] retrieve context documents relevant to a query and use these as input for generation, enabling models to answer queries beyond their training data. More recent approaches explore the integration of retrieval with both encoder-decoder [41] and decoder-only [87] architectures, optimizing performance for a variety of downstream tasks. \n\nOne key area of focus in RAG is mitigating hallucinations and improving the factuality of generated responses [69]. Techniques such as self-consistency [113] and noisy context filtering [28] have been proposed to enhance the reliability of RAG outputs. However, the quality of retrieved documents remains a critical factor in the overall effectiveness of RAG pipelines. Recent works emphasize the importance of combining retrieval with robust re-ranking techniques to ensure that the retrieved context is both relevant and precise [45].",
            "score": 0.8000176868188537,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 8442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1023
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "273812370",
            "title": "Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors",
            "text": "Retrieval-augmented generation (RAG) integrates traditional language modeling with dynamic external data retrieval to address limitations such as data recency and relevance in response generation (Lewis et al., 2020;Izacard & Grave, 2021;Cheng et al., 2024;Zhang et al., 2024a). A typical RAG system consists of three main components: a retriever, a language model generator, and an extensive knowledge database. \n\nRetriever: This component employs an encoder to encode input queries and retrieve the top-k relevant documents from the knowledge base. The retriever function f r maps a query Q to a subset of documents D k \u2282 D, where D denotes the entire knowledge database. More formally, this can be expressed as: \n\nLLM generator: Post retrieval, the generator, often a pre-trained model, synthesizes the final text output based on the contextual information extracted from the retrieved documents (Lewis et al., 2020). The generator's function f g integrates the context from the retrieved documents to enhance the relevance and accuracy of the generated content, given by: \n\nKnowledge database: Serving as the repository of information, the knowledge base contains diverse sources such as Wikipedia, news articles, and domain-specific literature, providing the factual backbone for retrieval operations. The integration of these components allows RAG systems to produce contextually enriched responses, effectively minimizing issues like hallucinations often seen in standalone language models. Moreover, the ability to dynamically pull information from updated sources ensures that the responses generated are not only relevant but also accurate (Guu et al., 2020).",
            "score": 0.7986703512044926,
            "section_title": "Retrieval-augmented generation",
            "char_start_offset": 7939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 216,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 238,
                    "end": 257,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 899,
                    "end": 919,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1650,
                    "end": 1668,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Interestingly, larger LMs generally do not benefit from finetuning, while smaller ones do. Therefore, a small fine-tuned LM with RAG can perform on par or better than a large LM; e.g., StableLM2 (1.6B) vs. Llama3 (8B) (Table 3). \u2022 Retrieval model: Comparing retrievers with varying performance in the RAG system, we observe that as the popularity of factual knowledge increases, the performance of the retriever decreases (Figure 7). Moreover, the performance of the RAG system increases by using higher performance retriever (Figures 1 and 8). \u2022 Fine-tuning vs. RAG: Comparing these two knowledge injection methods, RAG substantially outperforms fine-tuning. Fine-tuned LMs combined with RAG either outperform or perform on par with vanilla LMs with RAG in all but one case (Figure 1). \n\nWhile fine-tuning improves accuracy in answering factual questions, both with and without RAG, it demands a considerable amount of effort and resources. This leads us to our second research question: (RQ2): Can we avoid the cost of fine-tuning by developing an advanced RAG approach that surpass the performance of a fine-tuned LM with RAG? To answer this question, we develop Stimulus RAG (SRAG), a new RAG approach that stimulates an LM to generate the correct response based on the provided hint in the prompt. The hint is extracted from the top retrieved documents by the retrieval model. Our results demonstrate that Stimulus RAG outperforms all other combinations of fine-tuning, both with and without retrievethen-generate RAG. \n\nTo summarize, this paper makes the following contributions: \n\n\u2022 We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods.",
            "score": 0.7967312042104989,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1878
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "In this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in Retrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on retrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while ignoring misleading information, with no architectural changes required. \n\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages GPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts robustness, with less structured formats sometimes aiding discrimination. \n\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully improve hallucination resistance in noisy retrieval environments. We release our code, dataset, and checkpoints to support further research in this direction, and highlight future extensions including multi-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.",
            "score": 0.7957693704376294,
            "section_title": "Conclusion",
            "char_start_offset": 23325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "271270644",
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "text": "As introduced in Section 6, RAG training includes two branch of works, RAG with/without datastore update. For RAG without datastore update, the main challenge is how to jointly optimize all parameters in RAG. This may involves new loss functions with multiple objectives, new optimizations for efficient tuning parameters in retriever and generator, or other training strategies. \n\nFor RAG with datastore update, one challenge is how to align the retrieval representations with the generator's representations. Although the time cost of the update operation in datastore cannot be ignored, some works [14] reduce the update frequency by asychronously updating, thus achieving the alignment of knowledge representation and model's representation. Another challenge is when to retrain/fine-tune the generator in RAG when new corpus is added. Due to the in-context learning capability of exisitng LLM-based generators and high training overhead, retraining/finetuning the generator or directly inferring the generator becomes a challenging choice for different scenarios. Recently, some efficient training strategies [28,57] have been proposed to accelerate the fine-tuning process, which can be taken into considerations.",
            "score": 0.7860524377702166,
            "section_title": "RAG Training",
            "char_start_offset": 59907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1219
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "249191271"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "271097348",
            "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
            "text": "Retrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents from external databases and incorporating them into the generation process (Gao et al., 2023b;Lewis et al., 2020;Khandelwal et al., 2020;Izacard & Grave, 2021;Luo et al., 2023a;Xia et al., 2024b;Wang et al., 2024). Recent work has primarily focused on enabling LLMs to understand when and what to retrieve (Ma et al., 2023;Chen et al., 2023b;Jiang et al., 2023b;Schick et al., 2024), or designing approaches to better utilize contexts (Yu et al., 2023;Yoran et al., 2023;Wang et al., 2023b;Sarthi et al., 2024;Baek et al., 2023;Xu et al., 2023;Kim et al., 2024). Among them, SAIL (Luo et al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents. Self-Reflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in instruction-tuning datasets. However, both approaches require additional instruction-tuning of generic LLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b). Furthermore, long context with retrieved documents can suffer from computational inefficiency and position bias (Liu et al., 2024). Corrective RAG (Yan et al., 2024) on the other hand proposes a lightweight retrieval evaluator, but it lacks the capability for high-level reasoning. In contrast, our proposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model to efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist LMs to verify and integrate.",
            "score": 0.7828181340502995,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 228,
                    "end": 252,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "264306280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "271329405",
            "title": "MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) combines traditional language models with external databases to improve natural language processing (NLP) tasks [33], [34], [35].RAG models use a retriever to retrieve relevant information and a generator to generate answers based on the retrieved information.This improves accuracy and relevance, especially for domainspecific queries [36], [16].RAG's strengths lie in its ability to update knowledge bases without retraining and customise components for specific tasks such as cybersecurity [33], [34].However, RAG struggles with latency and scalability issues, especially when processing concurrent queries [33].Despite these limitations, RAG remains a versatile tool for a range of NLP applications, from chatbots to content creation.Ongoing research focuses on optimizing retrieval mechanisms and computational power [36], [16].",
            "score": 0.7781024660091518,
            "section_title": "B. Retrieval Augmented Generation",
            "char_start_offset": 7134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 870
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 373,
                    "end": 377,
                    "matchedPaperCorpusId": "3541996"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 859,
                    "end": 863,
                    "matchedPaperCorpusId": "3541996"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "278033562",
            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
            "text": "Recent advancements in Retrieval-Augmented Generation (RAG) have spurred the development of various evaluation tools and benchmarks (Gao et al., 2023). However, existing solutions often suffer from limitations, either lacking comprehensive datasets or failing to sufficiently assess retriever performance. Several tools have emerged to evaluate RAG systems, focusing on metrics such as context relevance, answer faithfulness, and answer relevance. For example, RAGAS (Es et al., 2023) provides a framework for evaluating these dimensions of RAG performance. Similarly, ARES (Saad-Falcon et al., 2023) offers an automated evaluation system, utilizing lightweight language model judges fine-tuned on synthetic data to assess both retrieval and generation components. Additionally, RAGCHECKER (Ru et al., 2024) enables detailed analysis of retrieval and generation within RAG systems. While these tools offer valuable insights through diverse evaluation metrics, they often lack dedicated datasets tailored for benchmarking RAG performance comprehensively.",
            "score": 0.7750803310279007,
            "section_title": "RAG Framework",
            "char_start_offset": 5911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "273345967",
            "title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering",
            "text": "The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval. \n\nAddressing similar issues, Sawarkar et al. [16] proposed Blended RAG, which incorporates semantic search and hybrid query-based retrievers to enhance the accuracy and relevancy of retrieved information. By using a combination of keyword-based, vector-based, and semantic-based searches, Blended RAG aims to mitigate the retrieval limitations observed in ITER-RETGEN by ensuring that more relevant documents are retrieved, thus improving the overall effectiveness of the RAG system. However, the complexity and computational intensity of implementing multiple retrieval strategies, as well as scalability concerns in real-world applications, remain significant challenges for Blended RAG. \n\nAnother approach by Ma et al. [9] focused on Query Rewriting within retrieval-augmented settings. Their \"Rewrite-Retrieve-Read\" framework aims to adapt the search query itself to better align with the information needs, thereby enhancing the relevancy and effectiveness of retrieval. This method reduces the dependency on extensive, domain-specific datasets by refining queries to work with existing open-domain data. Nonetheless, the system's reliance on high-quality training data and its limited ability to generalize across diverse datasets highlight persistent issues in the field. \n\nCheng et al. [3] introduced Lift Yourself Up, a retrieval-augmented text generation model using self-memory and external metrics like ROUGE/BLEU for more reliable and contextually relevant generation. \n\nThese studies highlight the need to refine retrieval and generation integration, addressing challenges like dynamic retrieval contexts, iterative process optimization, and scalability.",
            "score": 0.7732134888732309,
            "section_title": "Related Work",
            "char_start_offset": 4291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 1963,
                    "end": 1966,
                    "matchedPaperCorpusId": "258479968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "272988123",
            "title": "Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis",
            "text": "Retrieval-Augmented Generation (RAG [6]) provides LLMs with relevant external information from document databases, enabling them to generate outputs that are more contextually accurate, detailed, and grounded for ODQA tasks. This approach helps overcome the limitations of static, pre-trained knowledge in LLMs. In traditional RAG, documents are parsed and processed to extract text, then divided into smaller chunks using fixed-size chunking strategies to facilitate more precise retrieval of relevant content. Each chunk is embedded into a low-dimensional dense vector space that captures its semantic content, allowing for efficient indexing and retrieval of relevant chunks in response to queries. This method enhances generation by conditioning the language model on retrieved, contextually relevant chunks, leading to more accurate and grounded outputs. However, traditional RAG methods face several challenges that limit their effectiveness. These methods primarily rely on small text chunks, requiring the retriever to search through a large database to find relevant information. This can be inefficient, as the retriever often needs to recall numerous text chunks, sometimes necessitating re-ranking to optimize performance. Moreover, small text chunks can lead to semantic incompleteness and the loss of critical details due to document truncation. Dividing crucial context or concepts into multiple segments can impair coherence. Choosing an optimal chunk size is challenging: if chunks are too small, context is lost; if they are too large, retrieval becomes less precise-clearly, one size does not fit all. General-purpose LLMs are typically pre-trained on large text corpora using self-supervised learning techniques, such as predicting the next word in a sentence (autoregressive models) or filling in masked tokens (masked language models). To adapt LLMs for specific tasks, they undergo fine-tuning on task-specific datasets, enhancing their ability to follow instructions, improve contextual understanding, and solve complex problems. Despite these advancements, LLMs are generally not pre-trained or fine-tuned to inherently incorporate external retrieved context from databases, which is crucial for generating more accurate answers in ODQA. To address these limitations, the Retrieval-Augmented Fine-Tuning (RAFT [15]) methodology optimizes LLMs to integrate retrieved content from external databases during fine-tuning.",
            "score": 0.7709030155377665,
            "section_title": "A.1.1 Synthetic Data Generation",
            "char_start_offset": 31288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2442
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 39,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "RAG is continuously integrating with more LLM-related technologies. In Modular RAG, many components are composed of trainable language models. Through fine-tuning, the performance of the components and the compatibility with the overall flow can be further optimized. This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
            "score": 0.7684576637459273,
            "section_title": "E. Tuning Pattern",
            "char_start_offset": 55399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "269983737",
            "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020;Borgeaud et al., 2022;Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings.Finetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the LLM or retriever in the RAG framework.For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning framework to fine-tune both the LLM and the retriever simultaneously.InstructRetro (Wang et al., 2023) pre-trains a larger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on it.ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to enhance the model's ability to perform context awareness in conversational QA.RAFT (Zhang et al., 2024) proposes a kind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune LLM and improve LLM's ability to understand the retrieved documents under the RAG framework.",
            "score": 0.7653723926770121,
            "section_title": "Related Work",
            "char_start_offset": 2461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1656
                },
                {
                    "start": 1656,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 260,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 260,
                    "end": 282,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 723,
                    "end": 746,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including PubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO (Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models in tasks like open-domain question answering and fact verification, addresses issues of over-or under-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated outputs. The method improves context quality by identifying useful context through lexical and information-theoretic approaches and training context filtering models to refine retrieved contexts during test time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG) (Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models (LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate a fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens to evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific needs and has shown superior performance in open-domain question-answering, reasoning, fact verification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on the quality of retrieval and more meta-data understanding of the repository would enhance the effectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow advances beyond the traditional retrieve-then-read mode and employs a prepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually relevant, time-critical, or domain-specific information. Key innovations include generating metadata, synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary) for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems.",
            "score": 0.7653593308625986,
            "section_title": "Recent Advancement in the field:",
            "char_start_offset": 25127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "276580741",
            "title": "RankCoT: Refining Knowledge for Retrieval-Augmented Generation through Ranking Chain-of-Thoughts",
            "text": "Modular RAG systems (Gao et al., 2024;Xu et al., 2024c) focus on refining external knowledge through different modules implemented by LLMs, which have become a key trend in the RAG area. For instance, Self-RAG (Asai et al., 2024a) uses different tags for adaptive retrieval (Jiang et al., 2023) and self-reflection to refine knowledge. Some approaches also focus on reformulating queries to identify more useful documents for answering ques-tions (Yan et al., 2024;Trivedi et al., 2023). Yan et al. (2024) introduce a retrieval evaluator that acts as a judge to trigger query reformulation, search, and knowledge refinement actions to supply more accurate evidence for generation. \n\nTo further improve the performance of modular RAG systems, these models focus on fine-tuning various components of the RAG framework. Some efforts aim to align the information needs between the retriever and the generator by optimizing the retrievers based on feedback from the generation models (Yu et al., 2023;Shi et al., 2024;Izacard and Grave, 2021). Lin et al. (2024) adapt LLMs within the RAG setting by constructing instructiontuning data for Supervised Fine-Tuning (SFT), enabling the models to better leverage the retrieved documents. Additionally, Li et al. (2024) use Direct Preference Optimization (DPO) (Rafailov et al., 2024) to jointly optimize the modules in a RAG system, aligning their data preferences.",
            "score": 0.7634998823378095,
            "section_title": "Related Work",
            "char_start_offset": 5892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1405
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 230,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 274,
                    "end": 294,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 465,
                    "end": 486,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 979,
                    "end": 996,
                    "matchedPaperCorpusId": "258960666"
                },
                {
                    "start": 996,
                    "end": 1013,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1013,
                    "end": 1037,
                    "matchedPaperCorpusId": "227746078"
                },
                {
                    "start": 1300,
                    "end": 1323,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "278367823",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "text": "Retrieval-augmented generation (RAG) enhances generation performance by integrating relevant external knowledge into the generation pipeline. Early research primarily adopted prompt-based approaches, guiding LLMs through processes such as query generation, query decomposition, and multi-turn information retrieval [44,28,43,15,33,22]. Despite their effectiveness, these methods often require intricate prompt engineering and impose substantial demands on the model's reasoning capabilities. To improve efficiency and reduce dependency on strong black-box LLMs, subsequent work has proposed supervised fine-tuning strategies for smaller LLMs. For instance, Self-RAG [1] employs a self-reflection mechanism, iteratively refining model outputs through predicted reflection tokens. RetroLLM [24] integrates retrieval and generation by enabling the model to directly generate fine-grained evidence from the corpus via constrained decoding. Recent advances also include test-time scaling techniques [25,14,47,13], notably Monte Carlo Tree Search (MCTS), which dynamically expands the search space during inference. For example, RAG-star [13] integrates retrieved information into a tree-based reasoning process, while AirRAG [5] employs MCTS to activate intrinsic reasoning capabilities and expand the solution space. Despite promising results, these approaches introduce significant computational overhead, limiting their practical applicability.",
            "score": 0.7616711320457774,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 6269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
            "score": 0.7601636356612899,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 65622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1424,
                    "end": 1429,
                    "matchedPaperCorpusId": "258841283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "278171164",
            "title": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of Complex Verilog Code",
            "text": "To overcome the limitations of LLMs in domain-specific tasks, retrieval-augmented generation (RAG) has emerged as a widely adopted technique in LLM-based generation frameworks. RAG enhances the accuracy and domain knowledge of LLMs by incorporating external knowledge sources into the generation process. Instead of relying solely on the internal parameters of the model, RAG enables the LLM to look up relevant information from a designated knowledge base before generating outputs. The key step of the RAG is the retrieval step, where a retriever module queries a database to fetch domain-specific documents or code snippets. The retrieved contents are then integrated into the model's prompt during generation. For instance, ChipNeMo [2] incorporates RAG by retrieving relevant textual passages to enrich prompts for Verilog code generation, supplemented with further parameter fine-tuning to improve output quality. AutoVCoder [17] enhances the retrieval process by fine-tuning the retriever itself using contrastive learning to boost relevance and diversity in the retrieved content. \n\nHowever, current RAG approaches often place significant emphasis on optimizing the retriever, which introduces additional complexity through separate data preparation and model fine-tuning. This underscores the need for a more lightweight yet effective RAG strategy, particularly in simplifying retriever construction without compromising performance.",
            "score": 0.7587496827474108,
            "section_title": "C. Retrieval-Augmented Generation in LLM for EDA",
            "char_start_offset": 12580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "273963552",
            "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs",
            "text": "Retrieval Augmented Generation (RAG) [9], a method first introduced in 2020, gained popularity after the emergence of LLM. RAG offers benefits such as associating answers with professional knowledge without undergoing training, thereby enhancing the answer quality and reducing hallucinations. This is achieved by first retrieving relevant information from external knowledge, mainly through embedding models, and subsequently combining the retrieved information into the input prompt of LLMs to generate answers. There are a number of works aiming at improving RAG pipeline from various aspects such as query rewrite, document rerank, and post-retrieval processing [10]. The RAG framework proved to be highly suitable for text classification tasks. In this regard, [11] utilized retrieval-enhanced LLMs to generate text classification datasetes under zero-shot condition. Moreover, [12] proposed a Retrieval-Augmented framework to alleviate poor generalization issues existed in text classification taskes. \n\nHowever, for methods like RAG or CoT that do not entail fine-tuning, the outputs might not always comply with the instructions in the prompt, while fine-tuning enables adjustments of the behaviors of LLMs, such as adhering to specific writing styles [10]. At the same time, adding content to the prompt and generating reasoning prior to answering leads to increased computation costs. For instance, as of July 2024, for GPT-4-o, it is $5 per 1M input and $15 per 1M output, and the cost for text-embedding-3-large is $0.13 .This can be depleted rapidly when we have a large volume of data and if we aim to apply CoT and RAG to every data point. In production scenarios where there is a massive amount of data and the answer is required as soon as possible, a smaller yet quicker specialized model that directly outputs the answer is much preferred. While [13] reveals that LLMs can improve themselves through the utilization of CoT prompting and self-consistency to generate high-quality answers from unlabeled data and fine-tuning based on these generated answers. Studies such as [14] have further demonstrated that fine-tuning a model with fewer parameters through knowledge distillation from the CoT outputs of larger LLMs results in improved task performance on datasets like GSM8K.",
            "score": 0.7583921438641594,
            "section_title": "Introduction",
            "char_start_offset": 2349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1865,
                    "end": 1869,
                    "matchedPaperCorpusId": "253080328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.429931640625
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging paradigm that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].In a typical RAG architecture, a retriever component first selects the most relevant documents or passages based on the input query, and then a generator component conditions on both the query and the retrieved content to produce a final output [ Izacard and Grave, 2021 ].RAG has shown significant promise in improving the factual accuracy, consistency, and contextual awareness of generative models across a wide range of applications, such as question answering, dialogue systems, and content creation [ Zhao et al., 2024 ].However, implementing RAG effectively in real-world, enterprise settings poses several challenges, which this paper aims to address.",
            "score": 0.7557317876941765,
            "section_title": "Background",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 920
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 533,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 766,
                    "end": 787,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.",
            "score": 0.7511701234507469,
            "section_title": "Related Work",
            "char_start_offset": 4844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "266164171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "F.1. Retrieval-Augmented Generation (RAG) \n\nAdvances in Retrieval-Augmented Generation (RAG) continue to extend the capabilities of Large Language Models (LLMs) in domain adaptation, efficiency, and longcontext reasoning. RAFT (Zhang et al., 2024c) improves factual accuracy by fine-tuning models to ignore irrelevant retrievals and cite only the most pertinent sources. CoRAG (Wang et al., 2025) enhances multi-hop reasoning through iterative retrieval, refining queries based on intermediate results rather than relying on a single retrieval step. DRAGIN (Su et al.) introduces dynamic retrieval by detecting real-time information needs using model uncertainty and self-attention cues, enabling context-sensitive query formulation during generation. RAPID (Chen et al.,  (Qian et al., 2024) integrates external retrieval with a cognitive memory system, recording episodic interactions and distilling them into semantic memory to improve retrieval relevance and consistency. Speculative RAG (Wang et al., 2024c) reduces latency and enhances comprehension by generating draft responses using a small model and verifying them with a larger model. CAG (Chan et al., 2024) addresses retrieval latency by preloading cached documents into extended context windows, bypassing real-time retrieval altogether. Parametric RAG (Su et al., 2025) replaces input-context retrieval with document parameterization, temporarily updating LLM weights during inference to embed external knowledge directly, thereby streamlining the retrieve-update-generate process.",
            "score": 0.7506214691472735,
            "section_title": "F. Related Work",
            "char_start_offset": 138273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1546
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 248,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "276249310",
            "title": "Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection",
            "text": "In this section, we elaborate on the proposed Self-Selection framework for enhanced Retrieval-Augmented Generation (RAG). Before explaining our method, we first revisit two preliminary concepts, i.e. Large Language Model (LLM) and Retrieval-Augmented Generation (RAG). Then, we present the formulation of our Self-Selection framework with detailed notations. In order to strengthen the capabilities of the LLM in accurately generating and selecting responses, we develop a novel Self-Selection-RGP method, which is essentially fining tuning the LLM over a newly built Retrieval-Generation Preference (RGP) dataset.",
            "score": 0.747969535489106,
            "section_title": "SELF-SELECTION FRAMEWORK",
            "char_start_offset": 6140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 614
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "271744947",
            "title": "MaxMind: A Memory Loop Network to Enhance Software Productivity based on Large Language Models",
            "text": "RAG empowers LLMs to extract info from diverse sources, boosting precision and reducing hallucinations. Recent focus include: (1) Retrieval Enhancement: Borgeaud et al. [16] integrates retrieval-augmented training, optimizing parameters and reducing complexity. ANCE [17] constructs realistic negative samples via ANN index updates. Karpukhin et al. [18] uses contrastive learning and similarity matrix for efficient answer extraction. However, challenges remain in capturing complex semantics and overlooking info, necessitating further RAG innovations. (2) Boosting Generator: cheng et al. [19] finetunes generator via memory pool, minimizing KL loss for precise memory selection. Asai et al. [20] improves text quality and factuality through retrieval and self-reflection. Current methods face memory capacity limits, hindering wide info coverage and adaptation to changing needs. (3) Integrating & Fine-tuning: Shi et al. [21] enhance LM with adjustable retrievers. Cheng et al. [22] create versatile retrievers for zeroshot tasks. lin et al. [23] and jiang et al. [9] optimize generator and retriever for better recall. However, black-box LLMs limit interpretability and flexibility, necessitating long-embedding models. In summary, RAG confronts key challenges: (1) Index accuracy. Content chunking and size determination may result in omissions or noise in answers. (2) Retrieval limitations. The query process occasionally missing relevant documents, extracting irrelevant context, or providing incomplete answers, hindering the full utilization of potential information. These issues impose stringent constraints on SOTG tasks.",
            "score": 0.7459220050566686,
            "section_title": "B. Retrieval-Augmented Generation",
            "char_start_offset": 5197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1636
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "258479968"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Despite significant advancements in generative models, AIGC still grapples with challenges like outdated knowledge, lack of long-tail knowledge [27], and risks of leaking private training data [28]. Retrieval-Augmented Generation (RAG) aims to mitigate these issues with its flexible data repository [29]. The retrievable knowledge acts as non-parametric memory, which is easily updatable, accommodates extensive long-tail knowledge, and can encode confidential data. Moreover, retrieval can lower generation costs. RAG can reduce the size of large models [30], support long contexts [31], and eliminate certain generation steps [32]. \n\nA typical RAG process is depicted in Fig. 1. Given an input query, the retriever identifies relevant data sources, and the retrieved information interacts with the generator to improve the generation process. There are several foundational paradigms (foundations in short) according to how the retrieved results augment the generation: they can serve as augmented input to the generator [33], [34]; they can join at the middle stage of generation as latent representations [35], [36]; they can contribute to the final generation results in the form of logits [37], [38]; they can even influence or omit certain generation steps [32], [39]. Additionally, researchers have proposed various enhancements to improve the foundational RAG process. These methods encompass specific optimizations for individual components as well as holistic enhancements aimed Fig. 1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and the generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results and ultimately produces outcomes of various modalities. at the entire pipeline. \n\nIn addition, while the concept of RAG initially emerged in text-to-text generation [34], this technique has also found applications across various domains, including codes [40]- [42], audios [43], [44], images [45]- [47], videos [48], [49], 3D [50], [51], knowledge [52]- [54], and AI for science [55], [56].",
            "score": 0.7443327630506013,
            "section_title": "I. INTRODUCTION A. Background",
            "char_start_offset": 1995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "237452184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
            "score": 0.7431968047566778,
            "section_title": "Methodology",
            "char_start_offset": 8999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "272689561",
            "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
            "text": "RAG is proposed to enhance generation quality by leveraging external knowledge bases. As research progresses, RAG technology has undergone three major developmental stages: Naive RAG, Advanced RAG, and Modular RAG. \n\nNaive RAG. Typically, naive RAG follows a \"Retrievalthen-Read\" process [21,26,27], consisting of a simple retriever and a pre-trained language model as the generator. Its workflow involves two simple steps: (1) retrieving relevant passages from a pre-constructed knowledge base based on the user query, and (2) combining the retrieved information with the input query to generate a response. \n\nEarly works primarily focused on optimizing the integration of retrievers and generators, including end-to-end joint training of retrievers and generators [21,26], separately training generators to better utilize retrieved documents with frozen retrievers [3,28], and modifying the model's decoding methods [10,29]. With the emergence of LLMs, the capabilities of generative models have significantly advanced. To further enhance the quality of generated context, prompt engineering have been proposed to optimize model outputs without additional training. To enhance the model's reasoning capabilities and the robustness of responses, various prompting techniques such as Chainof-Thought (CoT) [30], Tree-of-Thought (ToT) [31], and Self-Consistency [32] have been proposed. These methods extend the number of LLM's reasoning paths, thereby improving the likelihood of arriving at the correct result during the decoding process. However, Naive RAG also faces certain limitations. Firstly, the retrieved documents may contain noise or irrelevant information, which can interfere with the model's responses [5,33]. Secondly, the high reasoning cost inherent to large models is further exacerbated in the RAG process; the inclusion of lengthy retrieved documents can slow down the generation process and consume more computational resources. \n\nAdvanced RAG. To tackle the issues discussed earlier, additional components have been added to the RAG process, making it more complex. These enhanced systems, known as Advanced RAG, introduce specialized modules at different stages of the retrieval and generation pipeline, which can be categorized as pre-retrieval and post-retrieval components.",
            "score": 0.7400751439889719,
            "section_title": "Retrieval-augmented Generation System",
            "char_start_offset": 6549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 295,
                    "end": 298,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1716,
                    "end": 1719,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66845703125
        },
        {
            "corpus_id": "274965531",
            "title": "XRAG: eXamining the Core - Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) Jiang et al. [2023], Guu et al. [2020], Gao et al. [2023b], Borgeaud et al. [2022], Asai et al. [2024], Singh et al. [2025], Han et al. [2025], Mao et al. [2025] represents a pivotal strategy in Q&A tasks, demonstrating enhanced performance by delivering more informative and accurate answers compared to relying solely on large language models (LLMs). The efficacy of basic RAG systems Du et al. [2024], Lu et al. [2024] is contingent upon the seamless operation of four core components: pre-retrieval, retrieval, post-retrieval, and generation. The preretrieval stage indexes the corpus and reforms queries for efficient retrieval. The retrieval stage focuses on identifying and extracting documents relevant to a given query. The post-retrieval stage refines, summarizes, or compacts information to ensure contextual clarity. Finally, the generation stage employs the LLM to produce responses. These sequential stages critically influence output quality, highlighting the RAG framework's interdependence. Advanced RAG modules (e.g., reranker, refiner) offer sophisticated algorithms for tailored search solutions, surpassing standardized methodologies. \n\nToolkits like LangChain Chase [2022] and LlamaIndex Liu [2022], modularize the RAG process, increasing adaptability and broadening its applications. However, they are typically cumbersome, making adaptation to new data challenging and validating or optimizing innovative methods inconve-  Zhang et al. [2024] (Fair.Comp) indicates evaluation by aligning key components like seeds, generators, retrievers, and instructions. Unified Datasets (Unif.Data) ensures unified dataset formats for retrieval and generation. Modular Evaluation (Mod.Eva) assesses RAG modular differences. Failure Management (Fail.Mgmt) systematically implements strategies for identifying and mitigating RAG failure points. ConR uses token-matching for evaluating retrieval, ConG uses token-matching for evaluating generation, and CogL is based on LLM-based instructions for retrieval and generation evaluation.",
            "score": 0.7378032325613082,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 56,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 97,
                    "end": 119,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 121,
                    "end": 139,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "277043707",
            "title": "AttentionRAG: Attention-Guided Context Pruning in Retrieval-Augmented Generation",
            "text": "Retrieve-Augmented Generation (RAG) is a framework that enhances the capabilities of LLMs by integrating external knowledge through retrieval. \n\nA RAG system typically consists of two components: a retriever, which fetches relevant documents, called contexts, from a large corpus based on a query, and a generator, which generates an answer using both the retrieved context and the model's internal knowledge. This combination enables more accurate and contextually relevant outputs, especially for tasks requiring detailed or up-to-date information that might not be present in the model's training data.",
            "score": 0.737609041910489,
            "section_title": "Retrieve-Augmented Generation",
            "char_start_offset": 5881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
            "score": 0.7354331668851211,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "269484332",
            "title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models",
            "text": "The vast majority of machine learning systems, including large generative models, are designed as self-contained systems, with both knowledge and reasoning encoded in model parameters.However, these models cannot work effectively for tasks that require knowledge grounding [1,48], especially in case of non-stationary Figure 1: A high-level overview of the uRAG ecosystem.The ecosystem consists of a shared search engine that serves multiple RAG models, each performing its own task.\n\ndata where new information is actively being produced [49,57].As suggested by Zamani et al. [57], this issue can be addressed when machine learning systems are being enhanced with the capability of retrieving stored content.For example, in retrieval-augmented generation (RAG), as a special case of retrieval-enhanced machine learning (REML) [57], systems consume the responses provided by a retrieval model for the purpose of text generation [25,26].RAG models demonstrate substantial promise across various applications, including open-domain question answering [6,20,25,44,60], fact verification [46], dialogue systems [8,45,53], machine translation [4], and personalized generation [37,38].\n\nIn the RAG literature, the retrieval component is often implemented using either of the following two approaches:\n\n(1) Employing an off-the-shelf retrieval model that does not require training for the downstream RAG system: in this category, RAG systems either use APIs from commercial web search engines [29], term matching retrieval models [11], such as TF-IDF and BM25, or neural ranking models trained on relevance annotations provided as an external resource [25]; (2) Training a retrieval model given the feedback from the downstream RAG system through knowledge distillation [15] or endto-end optimization [35].\n\nAs expected, the latter category offers the current state-of-theart performance for various tasks [17,52].From an IR perspective, in this category, the downstream RAG model is the only \"user\" of the search engine.",
            "score": 0.7328004852810677,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 483
                },
                {
                    "start": 485,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1294
                },
                {
                    "start": 1296,
                    "end": 1799
                },
                {
                    "start": 1801,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "230437698"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1058,
                    "end": 1061,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 1110,
                    "end": 1113,
                    "matchedPaperCorpusId": "51879945"
                },
                {
                    "start": 1113,
                    "end": 1116,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 1138,
                    "end": 1141,
                    "matchedPaperCorpusId": "235166182"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "Retrieval-augmented generation (RAG) (Lewis et al., 2020;Gao et al., 2023;Guti\u00e9rrez et al., 2024;Asai et al., 2024) is a powerful technique that enhances large language models (LLMs) for various knowledge-intensive tasks such as question answering (QA) by incorporating external knowledge sources. This method not only customizes responses to handle long-tail knowledge but also avoids the need for costly model retraining (Ovadia et al., 2023). Additionally, RAG helps reduce the issue of LLM hallucination by ensuring responses are grounded in relevant evidence (Shuster et al., 2021), thereby improving the overall accuracy and reliability of LLM outputs. \n\n* Work done during an internship at Amazon. \n\nWhile extensive research has focused on developing effective (Asai et al., 2024;Lin et al., 2024;Liu et al., 2024) and efficient (Xu et al., 2024a) RAG systems for general-domain QA tasks, adapting RAG to specialized domains for LLMs poses significant challenges. These models often struggle with distribution shifts and fail to accurately extract information from domain-specific contexts (Miller et al., 2020;Liu et al., 2022). Moreover, directly using black-box LLMs (OpenAI, 2023;Anthropic, 2023;Wang et al., 2023b) in specialized domains raises concerns about privacy when dealing with sensitive proprietary data. It is essential to finetune LLMs on domain-relevant QA tasks to unlock the full potential of LLM-based RAG systems in specialized domains. \n\nDespite the critical need for domain-specific finetuning, the primary challenge lies in the acquisition of high-quality fine-tuning data towards RAG applications. Prior works rely on continuous pretraining (Chen et al., 2023;Zhang et al., 2024a) on specialized corpora or fine-tuning on domain-specific instruction-tuning data (Wu et al., 2024;Wadden et al., 2024). However, the mismatch between these general-purpose tasks and domain-specific QA hinders their effectiveness.",
            "score": 0.7310214833949973,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 564,
                    "end": 586,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 768,
                    "end": 787,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1097,
                    "end": 1118,
                    "matchedPaperCorpusId": "216867120"
                },
                {
                    "start": 1118,
                    "end": 1135,
                    "matchedPaperCorpusId": "237417170"
                },
                {
                    "start": 1794,
                    "end": 1811,
                    "matchedPaperCorpusId": "269136910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Retrieval-augmented generation (RAG) is a hybrid approach that integrates LLMs with a retrieval mechanism to fetch relevant information from external documents or knowledge in real-time during the text generation process. This technique allows LLMs to dynamically access vast external corpora (like knowledge graphs, databases or search engines) to retrieve the most relevant information [41,90,105,107]. RAG enables language models to retrieve factual information and generate more accurate and contextually aware outputs, especially in cases where a language model's training data may be outdated or incomplete [99]. This approach provides the LLM with an ''external memory'' to supplement its internal knowledge base, thus enhancing LLM quality and accuracy. \n\nRAG comprise of retrievers, generators, and knowledge bases [41]. The retriever dynamically fetches relevant information from external corpora, the generator uses this retrieved information to generate a response, and the knowledge base is a collection of text such as scientific articles, news articles, or Knowledge Graphs [95,119,120]. This architecture allows the LLM to access up-to-date knowledge beyond its static training data, alleviating hallucination issues by grounding the generated response in factual data. \n\nRAG enhances retrieval and generation through several key techniques which may include document chunking, embedding models, retrieval techniques, querying, knowledge graph integration, iterative retrieval and generation and self-reflection. Chunking breaks down large text into manageable sizes using a mix of static and semantic methods to maintain context [119,121]. Helps embedding models to represent information and queries in a way that maintains semantic meaning, improving retrieval accuracy [25]. RAG employs retrieval methods such as dense retrieval, which uses vector representations (cosine similarity) for semantic matching, and sparse encoding for keyword matches between queries and documents; with hybrid approaches combining the strengths of both [95,119]. Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109].",
            "score": 0.7310097314434585,
            "section_title": "Retrieval augmented generation",
            "char_start_offset": 48408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1097,
                    "end": 1101,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "273993423",
            "title": "Towards Optimizing a Retrieval Augmented Generation using Large Language Model on Academic Data",
            "text": "Given the growing trend of many organizations integrating Retrieval Augmented Generation (RAG) into their operations, we assess RAG on domain-specific data and test state-of-the-art models across various optimization techniques. We incorporate four optimizations; Multi-Query, Child-Parent-Retriever, Ensemble Retriever, and In-Context-Learning, to enhance the functionality and performance in the academic domain. We focus on data retrieval, specifically targeting various study programs at a large technical university. We additionally introduce a novel evaluation approach, the RAG Confusion Matrix designed to assess the effectiveness of various configurations within the RAG framework. By exploring the integration of both open-source (e.g., Llama2, Mistral) and closed-source (GPT-3.5 and GPT-4) Large Language Models, we offer valuable insights into the application and optimization of RAG frameworks in domain-specific contexts. Our experiments show a significant performance increase when including multi-query in the retrieval phase.",
            "score": 0.7291665112432764,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "272524830",
            "title": "LLMs Will Always Hallucinate, and We Need to Live With This",
            "text": "Fine-tuning improves how large language models (LLMs) perform on specific tasks, but it does not always ensure that the information they produce is factually accurate. Retrieval-Augmented Generation (RAG) addresses this by combining the strengths of language models with information retrieval systems, allowing the model to generate content based on accurate, up-to-date information. \n\nIn simple terms, RAG works like this: \n\nHere, y is the output, G is the language model, x is the input, and R(x) is the relevant information retrieved from an external knowledge base.",
            "score": 0.7283141771593568,
            "section_title": "Retrieval-Augmented Generation (RAG): Bridging Knowledge Gaps",
            "char_start_offset": 12455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "264288947",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020;Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a;Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.",
            "score": 0.7276490486695028,
            "section_title": "RELATED WORK",
            "char_start_offset": 4683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 148,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 148,
                    "end": 167,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 937,
                    "end": 955,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "276408221",
            "title": "Post-training an LLM for RAG? Train on Self-Generated Demonstrations",
            "text": "As retrieval-augmented generation (RAG) has garnered interest as an effective way to improve LLM performance (Lewis et al., 2020). We focus on the setting where we prepend retrievals to the prompt, which is a common, simple method for incorporating retrievals into the context, though alternative RAG architectures exist (e.g., Shi et al., 2023). Our work builds upon previous studies which investigate how to train LLMs to better handle retrieved context. Some of these propose pre-training methods that boost RAG capabilities (Guu et al., 2020;Izacard et al., 2023;Shi et al., 2024), while others finetune a LLM for better RAG capabilities Luo et al. (2023). Our method targets the latter fine-tuning setting, where we hypothesize that out-of-distribution training examples cause performance degradation. Our key insight is to self-generate demonstrations, rather than using a teacher LLM (Luo et al., 2023) or human-written responses (Lin et al., 2024b).",
            "score": 0.7267934803028488,
            "section_title": "Training RAG-enabled LLMs",
            "char_start_offset": 5208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 957
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 129,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 328,
                    "end": 345,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 528,
                    "end": 546,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 546,
                    "end": 567,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 567,
                    "end": 584,
                    "matchedPaperCorpusId": "264172290"
                },
                {
                    "start": 937,
                    "end": 956,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60302734375
        },
        {
            "corpus_id": "271744975",
            "title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case",
            "text": "Furthermore, it incorporates paged optimizers to handle memory spikes. The second method implemented was a novel method called Retrieval Augmented Fine Tuning (RAFT), a training procedure for domain-specific Retrieval Augmented Generation (RAG), which can adapt pretrained LLMs like LLaMa 2 and Mistral for RAG in specific domains, such as ours in travel. [9] RAG is a text generation method for outsourcing relevant information, from a knowledge base, or a large corpus of relevant, factual and quality information, to supply an LLM with contextual clues for producing factual responses. Then, an RLHF training pipeline will be done for domain-specific LLM curation, aligned with human preferences, with reward model training. [10] II. RELATED WORK Through a comprehensive review of literature and existing research papers, we build an understanding for state-or-theart techniques and approaches aimed to achieve competitive performances. New innovations are expressed in different variations for enhancing large language models. Table II summarizes the selection of models, objective addressed, unique approach, performance results and ultimate findings. A survey of existing solutions show that initiatives have been developed to overcome shortcoming identified with public general-purpose and pretrained LLM. Some of the feature performance issues garnered from literature review are potential risks of hallucination, underdeveloped retrieval approaches, and inefficiencies in the use of computational resources. \n\nZhang et. al introduced Retrieval Augmented fine-tuning (RAFT) as a novel training strategy for fine-tuning LLMs to better perform on RAG tasks. The key concept is data augmentation to generate \"question, answer, document\" triplets before fine-tuning. This is done by generating realistic questions paired with elaborate chain of thought answering scheme and purposefully including relevant and irrelevant context documents. Through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. RAFT operates by training the model to disregard any retrieved documents that do not contribute to answering a given question, thereby eliminating distractions. The optimal ratio of oracle to distractor documents during training varies across datasets, but including some distractors improves generalization. Finally, during RAG, RAFT retrieves the top-k documents from the database. With RAFT, Zhang et.",
            "score": 0.7265230448254425,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2440
                },
                {
                    "start": 2441,
                    "end": 2515
                },
                {
                    "start": 2516,
                    "end": 2536
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "273654010",
            "title": "Mask-based Membership Inference Attacks for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances response accuracy and relevance by incorporating external data retrieval into the generation process [11]. A common RAG paradigm involves using the user query to retrieve a set of documents, which are then concatenated with the original query and used as context [20]. \n\nRecent research has focused on various retrieval methods, including token-based retrieval [19], data chunk retrieval [30], and graph-based retrieval [9,18]. Additionally, studies have explored adaptive retrieval [16] and multiple retrieval [14]. More advanced techniques such as query rewriting [10,22] and alignment between retriever and LLM [5,42] are beyond the scope of this paper.",
            "score": 0.7260839214248839,
            "section_title": "Related Work 2.1 Retrieval-Augmented Generation",
            "char_start_offset": 5244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 702
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 313,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "2.1 Retrieval-Augmented Generation (RAG) Retrieval-augmented generation (RAG), first introduced by Lewis et al. (2020), has emerged as one of the most popular approaches to enhance the generation ability of LLMs (Liu, 2022;Chase, 2022;Van Veen et al., 2023;Ram et al., 2023;Shi et al., 2023). This synergy markedly boosts the output's accuracy and relevance (Gao et al., 2023), mitigating essential issues commonly referred to as \"hal-lucinations\" of LLMs (Shuster et al., 2021). One of RAG's distinctive features is its flexible architecture, allowing for the seamless interchange or update of its three core components: the dataset, the retriever, and the LLM. This flexibility means that adjustments to any of these elements can be made without necessitating re-training or fine-tuning of the entire system (Shao et al., 2023;Cheng et al., 2023). These unique advantages have positioned RAG as a favored approach for a range of practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants (Panagoulias et al., 2024).",
            "score": 0.7256207206506915,
            "section_title": "Related Work",
            "char_start_offset": 4018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1075,
                    "matchedPaperCorpusId": "267112617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "278170975",
            "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
            "text": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.",
            "score": 0.7255586097953897,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82763671875
        },
        {
            "corpus_id": "269758033",
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "text": "Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of generative language models through integrating information retrieval techniques.It addresses a critical challenge faced by standalone generative language models: the tendency to produce responses that, while plausible, may not be grounded in facts.By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [23] or factually incorrect outputs, thereby improving the content's reliability and richness.[73] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [73,64].\n\nNumerous studies of RAG systems have emerged from various perspectives since the advent of Large Language Models (LLMs) [55,45,59,42,41,69,16].The RAG system comprises two primary components: Retrieval and Generation.The retrieval component aims to extract relevant information from various external knowledge sources.It involves two main phases, indexing and searching.Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28].The searching component utilizes these indexes to fetch relevant documents on the user's query, often incorporating the optional rerankers [4,39,6,52] to refine the ranking of the retrieved documents.The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases.As the \"Emerging\" ability [59] of LLMs and the breakthrough in aligning human commands [42], LLMs are the best performance choices model for the generation stage.Prompting methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and Respond (RaR) [8] guide better generation results.In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21].",
            "score": 0.7246158758190803,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 847
                },
                {
                    "start": 849,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1219
                },
                {
                    "start": 1219,
                    "end": 1390
                },
                {
                    "start": 1390,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 2064
                },
                {
                    "start": 2064,
                    "end": 2328
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "264426669"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "264426669"
                },
                {
                    "start": 973,
                    "end": 976,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.537109375
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
            "score": 0.7237727205065172,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "273501949",
            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
            "text": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
            "score": 0.7235959696138269,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "271270644",
            "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
            "text": "This section gives an overview of RAG for NLP. As shown in Figure 1, RAG typically consists of three modules, the retriever, the generator, and retrieval fusions. Retriever module usually comprises three components: an encoder for encoding inputs into embeddings, an efficient indexing that supports approximate nearest neighbor search, and the datastore for storing external knowledge in the form of key-value pairs. The main challenge in the retriever module is finding the optimal trade-off between retrieval efficiency and retrieval quality. The retrieval efficiency refers to how fast the relevant information can be obtained, which involves accelerating encoding, efficient indexing, batch querying in the datastore, etc. The retrieval quality refers to how relevant the information can be retrieved, which involves chunk representation learning, advanced approximate nearest neighbor search algorithms, etc. \n\nRetrieval Fusions aims to leverage the retrieved information to augment the generation. These fusion techniques can be categorized into three major types: query-based fusion, latent fusion, and logits-based fusion. The query-based fusion augments inputs with retrievals before feeding them into the generators. The logits-based fusion focuses on the output logits of generators and fuses the retrievals logits for more robust logits. The latent fusion refers to introducing retrieval representations into the latent representations of generators, thus implicitly improving the models' performance. \n\nGenerator module can be classified into two branches of generators: default generators and retrieval-augmented (RA) generators. The default generators include most pre-trained/fine-tuned large language models, such as GPT-series models [11,114,118,119], Mistral models [71], and Gemini-series models [4,108,124]. The RA generators refer to the pre-trained/fine-tuned generators that consist of modules for fusing retrievals, such RETRO [10] and Enc-Dec [93]. Those generators generate responses or make predictions. \n\nThe workflow of RAG involves three steps: (1) retrieving the relevant information from external databases based on given inputs; \n\n(2) fusing the retrieved information with inputs or intermediate states based on the fusion techniques; (3) making predictions by generators based on the input and corresponding retrievals.",
            "score": 0.7227757440128956,
            "section_title": "OVERVIEW OF RETRIEVAL-AUGMENTED GENERATION",
            "char_start_offset": 3632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2163
                },
                {
                    "start": 2166,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 1761,
                    "end": 1765,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1953,
                    "end": 1957,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "252846580"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Lastly, we wish to outline several potential directions for future RAG research and applications. \n\n1) Novel Design of Augmentation Methodologies: Existing research has explored various interaction patterns between retrievers and generators. However, due to distinct objectives in these two components, the practical augmentation process has a significant impact on the final generation results. Investigation of more advanced foundations for augmentation holds promise for fully unleashing the potential of RAG. \n\n2) Flexible RAG Pipelines: RAG systems are progressively embracing flexible pipelines, such as recursive, adaptive, and iterative RAG. With precise tuning and meticulous engineering, the unique blend of retrieval sources, retrievers, generators, and RAG subsystems promises to tackle complex tasks and boost overall performance. We eagerly anticipate pioneering exploration that will drive the evolution of even more innovative RAG systems. \n\n3) Broader Applications: RAG is a general technique applied in various applications. However, some generative tasks have not yet explored RAG, and in many domains, RAG is applied naively without considering the domain's unique characteristics. We believe designing domain-specific RAG techniques will significantly benefit broader applications. \n\n4) Efficient Deployment and Processing: There exist several deployment solutions for query-based RAG with LLMs, such as LangChain [350], LLAMA-Index [135], and PipeRAG [351]. However, for other RAG foundations and/or generation tasks, there lacks a plug-and-play solution. Besides, due to retrieval overhead and increasing complexities in retrievers and generators, achieving efficient RAG is still challenging and necessitates further system-level optimizations. \n\n5) Incorporating Long-tail and Real-time Knowledge: While a key motivation of RAG is to harness real-time and long-tail knowledge, few studies have explored the pipeline for knowledge updating and expansion. Many existing works use merely the generators' training data as retrieval sources, neglecting the dynamic and flexible information that retrieval could offer. As a consequence, there is a growing research on designing RAG systems with continuously updated knowledge and flexible sources. We also expect RAG to step further, adapting to personalized information in today's web service.",
            "score": 0.7215287039282999,
            "section_title": "B. Potential Future Directions",
            "char_start_offset": 79468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2363
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "267312134",
            "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
            "text": "The retrieval-augmented generation (RAG) significantly enhances performance in knowledge-intensive tasks and open-domain question-answering by providing context as input to the language model (Lewis et al. 2020, Mao et al. 2021, Kang et al. 2023). The retriever also plays a crucial role in language models by providing evidence for pretraining and few-shot fine-tuning (Guu et al. 2020, Izacard et al. 2022a). With the recent advancements in instruction language models, the combination of retriever and language models involves either using the retriever in advance to fetch evidence or iteratively retrieving it when needed (Jiang et al. 2023, Shao et al. 2023). Our base framework, Self-RAG (Asai et al. 2023), deviates from these approaches by being designed to perform retrieval on-demand, resulting in better cost efficiency compared to scenarios where retrieval is always active. However, in domain-specific fields like biomedical or clinical domains, the general method of retrieving context may not be applicable. Therefore, Self-BioRAG utilizes retrieval methods and documents tailored to specific domains, retrieving meaningful context that aligns with the intended field.",
            "score": 0.7212084300363957,
            "section_title": "Retrieval-augmented generation",
            "char_start_offset": 9147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1184
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 210,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 210,
                    "end": 227,
                    "matchedPaperCorpusId": "221802772"
                },
                {
                    "start": 370,
                    "end": 386,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "268819582",
            "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
            "text": "In contrast to efforts aimed at enhancing the retrieval component of RAG systems, another research focuses on optimizing the generator part.SAIL (Luo et al., 2023) trains the LLM to differentiate irrelevant contexts during the generation process.Self-RAG (Asai et al., 2024) trains the LLM to self-reflective on the retrieved contexts.Rewrite-Retrieve-Read (Ma et al., 2023) trains a small model to rewrite queries for a black-box reader while we train the LLM to refine queries by itself in this paper.",
            "score": 0.7198338827954058,
            "section_title": "Generator in Retrieval Augmented Generation",
            "char_start_offset": 21392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 503
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 274,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "278339057",
            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
            "text": "Retrieval Fig. 1. Overview of DRO objective. The selection model directly estimate a document permutation for the generator to predict an answer, with both components trained jointly. \n\n1 Introduction Large language models (LLMs) have shown remarkable text generation abilities; however, they often provide factually incorrect content [4,53,73] due to the hallucination [16] or out-of-date information [9]. To mitigate these limitations, retrieval-augmented generation (RAG) is proposed to integrate external retrievers with LLMs, which enables the model to access extensive corpora and retrieve relevant documents for references, thereby enhancing factuality. By integrating the retriever with LLMs, RAG has shown superior performance in knowledge-intensive tasks such as question answering [49,61] and conversational information seeking [5,24,68]. \n\nFollowing the most widely used architecture [9,11,23], RAG typically includes two components to answer an input query: (i) knowledge selection, where retrieval and re-ranking models select target documents, (ii) answer generation, where an LLM generator generates correct answers conditioned on the selected documents. To enhance coverage and improve answer quality, RAG models often provide multiple retrieved documents as input to the generator. The interrelationships among these documents are crucial for final performance [15,28,32,72]. We refer to a specific selection of retrieved documents as a document permutation. \n\nImproving RAG performance. To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45]. \n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25,51].",
            "score": 0.7196434701907461,
            "section_title": "Generative selector",
            "char_start_offset": 138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "271162228"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 796,
                    "end": 799,
                    "matchedPaperCorpusId": "270370889"
                },
                {
                    "start": 842,
                    "end": 845,
                    "matchedPaperCorpusId": "267406766"
                },
                {
                    "start": 845,
                    "end": 848,
                    "matchedPaperCorpusId": "269983269"
                },
                {
                    "start": 896,
                    "end": 899,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 902,
                    "end": 905,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "10986612"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "270199429"
                },
                {
                    "start": 2040,
                    "end": 2043,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.904296875
        },
        {
            "corpus_id": "277994112",
            "title": "Synergizing RAG and Reasoning: A Systematic Review",
            "text": "Tuning-Based. The tuning-based approach improves the integration of RAG and reasoning by optimizing model parameters to internalize the retrieval-augmented chain-ofthought mechanism within LLMs. Current research mainly targets three goals: retrieval pathway optimization, structured generation enhancement, and collaborative training with external modules. \n\nFor retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy. \n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning. \n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
            "score": 0.7179822655630566,
            "section_title": "5.2.2",
            "char_start_offset": 63935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 1183,
                    "end": 1186,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1947,
                    "end": 1951,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "269293007",
            "title": "A Survey on Efficient Inference for Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) [29] aims to improve the quality of LLMs' responses by incorporating external knowledge sources. RAG can be also viewed as a technique to improve the inference efficiency when handling a large amount of data. Instead of merging all information into an excessively long prompt, RAG only adds relevant retrieved information to the original prompt, ensuring that the model receives necessary information while reducing prompt length significantly. FLARE [30] uses predictions of upcoming sentences to proactively decide when and what information to retrieve. REPLUG [31] treats the LLM as a black box and augments it with a tuneable retrieval model. It prepends retrieved documents to the input for the frozen black-box LLM, and further utilizes the LLM to supervise the retrieval model. Self-RAG [32] enhances LLM's quality and factuality through retrieval and self-reflection. It introduces reflection tokens to make the LLM controllable during the inference phase.",
            "score": 0.7155634316248867,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 20663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1001
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "LLMs excel in text generation but also confront challenges such as outdated knowledge and the generation of hallucinatory content [6,19,43]. In response to these challenges, RAG, also referred to as RALM (Retrieval-Augmented Language Models), incorporates external knowledge to generate responses characterized by enhanced accuracy and realism [47]. This is particularly critical in domains that heavily depend on precision and reliability, including but not limited to the legal, medical, and financial sectors. Retrieval models have been promoting the development of language models [15,33,59]. \n\nConventional RAG systems adhere to a standardized workflow encompassing indexing, retrieval, and generation phases [28,36]. The indexing phase encompasses data cleansing, extraction, transformation into plain text, segmentation, and indexing, utilizing embedding models to transform text fragments into vector representations [2,18]. In the retrieval phase, the system computes similarity scores based on the user's query to select the most pertinent text fragments. In the generation phase, the query and selected documents are amalgamated into prompts, facilitating the LLMs in generating a response. While this method is straightforward, it encounters challenges related to retrieval quality, generation quality, and enhancement processes [21,23]. \n\nIn response to these challenges, researchers concentrate on the enhancement of the retriever, a task that can be categorized into three key aspects: pre-retrieval processing, retrieval model optimization, and post-retrieval processing [20]. Pre-retrieval processing encompasses data transformer to enhance text standardization, ensuring factual accuracy, optimizing index structures, adjusting block sizes, and rewriting query [4,16,50,52]. Retrieval model optimization entails the fine-tuning of domain-specific embedding models and the application of dynamic embedding techniques [11,60]. Post-retrieval processing minimizes context length through reranking and compression operations, aiming to emphasize critical information, diminish noise interference, and enhance integration and utilization by the generator [37,53,55]. Furthermore, to enhance the precision and efficiency of the generator when handling retrieval content, scholars have undertaken a series of optimization measures. As an illustration, researchers have devised methods such as Chain-of-Note (CON) for the generator [58].",
            "score": 0.7145376565659608,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2447
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "233231373"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "18739283"
                },
                {
                    "start": 589,
                    "end": 592,
                    "matchedPaperCorpusId": "267377589"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "207670589"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 925,
                    "end": 928,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 1345,
                    "end": 1348,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1782,
                    "end": 1785,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1785,
                    "end": 1788,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 1934,
                    "end": 1938,
                    "matchedPaperCorpusId": "252519173"
                },
                {
                    "start": 2168,
                    "end": 2172,
                    "matchedPaperCorpusId": "257532405"
                },
                {
                    "start": 2175,
                    "end": 2178,
                    "matchedPaperCorpusId": "264590451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "270123776",
            "title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts",
            "text": "Retrieval-Augmented Generation (RAG) approaches (Gao et al., 2023;Asai et al., 2023a;Ram et al., 2023) tend to enhance LLMs following a retrieve-then-read pipeline when given the input query, which first retrieve a set of documents from the external corpus, and then utilize the retrieved documents as side information for an LLM to make the final prediction.This pipeline has demonstrated their superior performance, especially in knowledge-intensive tasks by fine-tuning or directly prepending to LLMs (Hu et al., 2022;Izacard et al., 2023).However, the essence of RAG is the quality of retrieved passages, yet plenty of existing approaches rely on similarity-based retrieval (Xu et al., 2023;Mallen et al., 2023) that tend to ignore the underlying utility of associated passages.Though Self-RAG, REPLUG (Asai et al., 2023b;Shi et al., 2023) have made progress in introducing the power of LLMs in augmenting retrieval ability and even achieve adaptive retrieval, purely depending on the capability of LLMs can be dangerous since the innate \"uncontrollable\" characteristics of LLMs would sometimes degrade the performance.Furthermore, they tend to ignore the potential inner relationships among retrieved passages and utilize them in isolation, which deteriorate the performance.",
            "score": 0.7139538080610449,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 5191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1280
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 85,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 521,
                    "end": 542,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 695,
                    "end": 715,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42578125
        },
        {
            "corpus_id": "270199806",
            "title": "Phantom: General Trigger Attacks on Retrieval Augmented Language Generation",
            "text": "Retrieval Augmented Generation (RAG). RAG is a technique used to ground the responses of an LLM generator to a textual corpus which may help minimize hallucinations [46] and help ensure response freshness, without requiring expensive fine-tuning or re-training operations. RAG systems use two main components: a retriever and a generator. RAG Retriever. A knowledge base is a set of documents collected either from the user's local file system or from external sources such as Wikipedia and news articles. The retriever is a separately trained embedding model that produces document embeddings in a vector space [12; 21; 24]. The retriever model operates over passages, which are contiguous, fixed-size sequences of tokens in a document. Given a user's query , the retriever generates encodings of the query E  and encodings E  of all documents passages  in the knowledge base. The top- most similar passages, as identified by the similar score sim(E  , E  ), are selected. These document passages are then aggregated in a prompt that is forwarded, together with the user query, to the generator. LLM Generator. This is an LLM typically trained with the autoregressive next-token prediction objective. We will consider instruction trained models that are subsequently fine-tuned with safety alignment objectives -Harmlessness, Helpfulness, and Honesty (HHH) -, such as GPT-4 [1] or Llama 3 [49]. The LLM is given as input the system prompt (examples in Figure 1), a user's query  and the top- retrieved passages-this enables personalization and grounding. The main advantage of RAG over other personalization methods (e.g, fine-tuning the LLM on users' data) is the relatively low computation cost. This is because several pre-trained retriever models are publicly available and computing similarity scores with the knowledge database is in general inexpensive. Attacks on RAG. Here we summarize the emergent research thread concerning attacks against RAG systems. We direct the reader to Appendix A.1 for a broader discussion of related work. [58] introduce corpus poisoning attacks on RAG systems, although these are focused towards the retriever, and have no adversarial objective on the generator.",
            "score": 0.7111794933058945,
            "section_title": "Background and Related Work",
            "char_start_offset": 4882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499755859375
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging architecture that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].A typical RAG system consists of three main components: a retriever, a generator, and a knowledge base.\n\nThe retriever is responsible for finding the most relevant documents or passages from the knowledge base given an input query.It uses techniques from information retrieval and semantic search to efficiently search through large collections of text and rank the results based on their similarity to the query [ Karpukhin et al., 2020 ].Advanced retrieval methods may employ dense vector representations, sparse encodings, or a combination of both to capture semantic meaning beyond simple keyword matching [ Zhang et al., 2023 ].\n\nThe generator is a large pre-trained language model, such as GPT-4, Claude Opus or T5, that takes the input query and the retrieved documents as context to generate a final output.The generator is trained to condition its output on both the query and the retrieved knowledge, allowing it to incorporate relevant information and produce more accurate, consistent, and contextually appropriate responses [ Lewis et al., 2020 ].The generator may use techniques like attention, copying, or content selection to effectively fuse the retrieved knowledge with its own learned patterns.\n\nThe knowledge base is a structured or unstructured collection of documents that the RAG system can retrieve from.It can include a wide range of sources, such as web pages, books, articles, databases, or proprietary enterprise data [ Guu et al. 2020, Khandelwal et al., 2020 ].The knowledge base is typically pre-processed and indexed in a way that enables efficient retrieval based on semantic similarity.The quality, coverage, and freshness of the knowledge base are critical factors in the overall performance of the RAG system.\n\nDuring inference, a RAG system works as follows [ Lewis et al., 2020, Lewis et al., 2020, Karpukhin et al., 2020, Izacard and Grave, 2021 ]:\n\n1.The input query is passed to the retriever, which searches the knowledge base and returns a ranked list of relevant documents.",
            "score": 0.7108371319515505,
            "section_title": "Detailed explanation of RAG architecture and components",
            "char_start_offset": 5982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 898
                },
                {
                    "start": 900,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1478
                },
                {
                    "start": 1480,
                    "end": 1593
                },
                {
                    "start": 1593,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1885
                },
                {
                    "start": 1885,
                    "end": 2010
                },
                {
                    "start": 2012,
                    "end": 2152
                },
                {
                    "start": 2154,
                    "end": 2156
                },
                {
                    "start": 2156,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 1728,
                    "end": 1755,
                    "matchedPaperCorpusId": "253428554"
                },
                {
                    "start": 2124,
                    "end": 2151,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "271710111",
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "text": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
            "score": 0.7092603199637795,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "271039137",
            "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) is a technique that enhances generation models by incorporating relevant content retrieved from knowledge sources, which has been proven to be effective for mitigating hallucinations in LLMs (Guu et al., 2020;Lewis et al., 2020). RAG system generally adheres to a workflow encompassing three components: indexing, retrieval, and generation. Recent advancements have focused on enhancing the retrieval component. RETRO (Borgeaud et al., 2022) amalgamates large-scale corpora with pre-trained frozen BERT embeddings. Atlas (Izacard et al., 2023) conducts joint training of a retriever and a sequence-to-sequence model to attain a language model with robust few-shot learning capabilities. Self-RAG (Asai et al., 2024) selectively retrieves knowledge and generates critique tokens to criticize its own output. \n\nApart from prior works, our proposed CCU-RAG specifically targets knowledge-intensive tasks within classical Chinese question-answering. It centers on endowing TongGu to discern both the timing and content of retrieval, while enhancing the synergistic efficiency between the model and the retrieval system it relies upon.",
            "score": 0.708525858944842,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1167
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 455,
                    "end": 478,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 558,
                    "end": 580,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 733,
                    "end": 752,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72998046875
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Training RAG models requires balancing the optimization of both retrieval and generation components to achieve optimal performance. Effective training strategies ensure the retriever fetches relevant information while the generator produces coherent and accurate outputs. This section reviews various methods in RAG training, including static training, unidirectional guided training, and collaborative training, shown in Figure 8. Each approach offers distinct benefits and challenges, affecting the effectiveness and adaptability of RAG models across applications. By exploring these paradigms, we can enhance the integration of retrieval and generation processes, ultimately improving RAG performance. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption. \n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance. \n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information. to better integrate and utilize external information. For example, RETRO [23] employs a pre-trained BERT model as the retriever to provide relevant context that enhances the generator's output. Similarly, RALMs [264] use a pre-trained COLBERTV2 retriever to fine-tune large language models (LLMs), thereby improving the generator's capacity to incorporate retrieved data effectively. ITER-RTGEN [217] utilizes S-BERT to guide the fine-tuning of a T5 generator, ensuring that the generated text aligns closely with the relevant retrieved information. Moreover, SMALLCAP [201] integrates CLIP as a retriever to direct a GPT-2 model in generating accurate and contextually appropriate captions. \n\nThis guided approach ensures that the generator benefits from a steady stream of relevant information, enhancing the overall quality and relevance of the generated content. \n\nGenerator-Guided Retriever Training.",
            "score": 0.7080869647052384,
            "section_title": "RAG Training",
            "char_start_offset": 74282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2366
                },
                {
                    "start": 2369,
                    "end": 2541
                },
                {
                    "start": 2544,
                    "end": 2580
                }
            ],
            "ref_mentions": [
                {
                    "start": 994,
                    "end": 999,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1748,
                    "end": 1752,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 2244,
                    "end": 2249,
                    "matchedPaperCorpusId": "252668790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "272120100",
            "title": "IDAS: Intelligent Driving Assistance System Using RAG",
            "text": "Retrieval Augmented Generation (RAG) is a technique that combines the strengths of information retrieval and natural language generation to improve the quality and relevance of generated text. It was introduced in 2020 by Lewis et al. at Facebook Artificial Intelligence Research (FAIR) [25]. The main idea is to enhance the generation process by retrieving relevant documents or pieces of information from a large corpus that cannot fit inside the working memory of a large language model. These relevant documents can then be used as context information for the LLM to inform and guide the generation of the final output. \n\nRAG models can produce more accurate, informative, and contextually relevant responses compared to traditional generation models that rely solely on the input query without additional context. \n\nThe RAG process typically involves three main components: \n\n1) Vector database generation: In this component, we create a database of the documents that later can be used to search for relevant information efficiently. 2) Retriever: This component is responsible for searching and retrieving relevant documents or passages from a vector database based on the input query. 3) Generator: Once the relevant information is retrieved, the generator component uses this information to produce a coherent and contextually appropriate response. \n\nThe generator is a language model that can leverage the retrieved documents to generate high-quality text.",
            "score": 0.7056698164525703,
            "section_title": "B. RAG PIPELINE FOR SPECIFIC CAR KNOWLEDGE",
            "char_start_offset": 12565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68505859375
        },
        {
            "corpus_id": "252735056",
            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
            "text": "Although there are several similar retrieval augmented architectures, such as REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2021), we used Re-trieval Augmented Generation (RAG) in our experiments due to its excellent open-source documentation and availability. \n\nWhen the RAG model is finetuned for downstream QA tasks, the original implementation keeps the encoding of passages and the external knowledge base fixed. This is because re-encoding the external knowledge base is computationally expensive and relies on a sophisticated implementation. Despite not finetuning the passage encodings, the RAG model performs well for datasets with Wikipedia-like knowledge bases because the DPR retriever components have already been trained on Wikipedia-based datasets (Kwiatkowski et al., 2019;Joshi et al., 2017). However, the feasibility of adapting RAG to specific ODQA domains such as research papers and news is not well understood. This is a critical research gap to address, as improved domain adaptation can further improve the ODQA performance of RAG. This paper explores the feasibility of using RAG in specialized domains for ODQA. In particular, we propose two modifications to the original RAG to improve its domain adaptability. Motivated by recent end2end retrieval augmented mechanisms (Guu et al., 2020;Sachan et al., 2021;Singh et al., 2021), we first propose a method to finetune the RAG model with its neural retriever and update its knowledge encodings asynchronously during training. We refer to this as RAG-end2end since it allows us to update all RAG components during training, including the external knowledge base, the DPR model, and the BART model. Secondly, we propose an auxiliary training signal to help our model learn more domain-specific knowledge. This took the form of generating a concise and factual statement about a document using a self-retrieved set of passages from the provided domain-specific knowledge base. These two modifications offer a unique feature to RAG-end2end over RAG: joint training of the retriever and generator for the end QA task and domain adaptation.",
            "score": 0.7055534383428012,
            "section_title": "Introduction",
            "char_start_offset": 1686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 1342,
                    "end": 1361,
                    "matchedPaperCorpusId": "235390519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "276885355",
            "title": "Optimizing Multi-Hop Document Retrieval Through Intermediate Representations",
            "text": "To address factual inaccuracies in language models, Retrieval-Augmented Generation (RAG) integrates external knowledge by providing relevant passages as contextual input, thereby enhancing factual reasoning and enabling real-time utilization of information. However, RAG systems often struggle with complex, multi-hop queries (Yang et al., 2018;Trivedi et al., 2022;Ho et al., 2020), which require iterative synthesis of information. \n\nRecent advancements in RAG have introduced multi-round retrieval strategies, including query rewriting (Ma et al., 2023;Shao et al., 2023b;Jiang et al., 2023) and self-questioning (Press et al., 2023b), aimed at improving performance on complex tasks. Nevertheless, these methods face a significant limitation: they incur increased computational latency and cost due to the repeated invocation of large language models (LLMs) for query generation. To address these challenges, several methodological variants have been proposed. Adaptive-RAG (Jeong et al., 2024) reduces inefficiencies by dynamically assessing query complexity and selectively activating retrieval processes, thus lowering inference overhead for simpler queries while retaining multi-round retrieval for more complex cases. Alternatively, Efficient RAG (Zhuang et al., 2024) uses a smaller model to iteratively generate queries and filter irrelevant information, bypassing the need for repeated LLM calls, thereby improving computational efficiency.",
            "score": 0.7035768422741796,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4266,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1452
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 345,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 345,
                    "end": 366,
                    "matchedPaperCorpusId": "236771976"
                },
                {
                    "start": 366,
                    "end": 382,
                    "matchedPaperCorpusId": "226236740"
                },
                {
                    "start": 556,
                    "end": 575,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 575,
                    "end": 594,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 616,
                    "end": 637,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 978,
                    "end": 998,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 1256,
                    "end": 1276,
                    "matchedPaperCorpusId": "271769059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "273798525",
            "title": "LLM-Ref: Enhancing Reference Handling in Technical Writing with Large Language Models",
            "text": "A.1 Basic Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) is an advanced technique that combines information retrieval with text generation, making it particularly effective when generating responses that require specific contextual information from an external knowledge base. The process is typically divided into three main stages: Ingestion, retrieval, and response generation. Ingestion: Once an input file is read, the first stage in RAG involves chunking and embedding, where source texts are segmented into smaller, manageable units, which are then converted into embedding vectors for retrieval. Smaller chunks generally enhance query precision and relevance, while larger chunks may introduce noise, reducing accuracy. Effective chunk size management is crucial for balancing comprehensiveness and precision. Embedding transforms both the user's query and knowledge base documents into comparable formats, enabling the retrieval of the most relevant information. \n\nRetrieval: In the next stage, the relevant information is retrieved from a vector knowledge base such as FAISS. The retriever searches this vector store to find the most relevant chunks of information based on the user's query. This stage is crucial for ensuring that the model has access to the necessary context for generating accurate and contextually relevant responses. \n\nResponse Generation: In the final stage, the retrieved context is combined with the user's query and fed into the LLM, such as GPT-4, to generate a coherent and relevant response. The model uses the context provided by the retrieved documents to produce answers that are informed by the most pertinent information available. This step highlights the synergy between retrieval and generation, ensuring that the output is not only accurate but also contextually grounded. \n\nEach stage of the RAG process is designed to leverage the strengths of both retrieval and generation, enabling the creation of responses that are informed by specific and relevant external knowledge. By combining these components, RAG systems can significantly enhance the quality and relevance of generated content, making them a powerful tool for applications requiring precise and contextually aware responses.",
            "score": 0.7027236403551236,
            "section_title": "A Appendix",
            "char_start_offset": 31515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "273346467",
            "title": "FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG",
            "text": "Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It mainly consists of retrieval and generation. The retrieval modules (a.k.a. retrievers) aim to find useful information used to facilitate the generation modules (a.k.a. generators). As such, generators' performance largely depends on the effectiveness and efficiency of retrievers. However, the widely used retrieval paradigm remains flat. It treats retrieval procedures as a one-off deal with constant granularity. Despite effectiveness, we argue that they suffer from two limitations: (1) flat retrieval exerts a significant burden on one retriever; (2) constant granularity limits the ceiling of retrieval performance. In this work, we propose a progressive retrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG, so as to balance effectiveness and efficiency. Specifically, FunnelRAG establishes a progressive retrieval pipeline by collaborating coarse-to-fine granularity, large-to-small quantity, and low-to-high capacity, which can relieve the burden on one retriever and also promote the ceiling of retrieval performance. Extensive experiments manifest that FunnelRAG achieves comparable retrieval performance while the time overhead is reduced by nearly 40 percent.",
            "score": 0.7025788258081749,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "270688152",
            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; where plan and answer tasks use the same example data, different loss functions, and train two task-specific prompts simultaneously. The right shows the inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrieving relevant passages, thereby improving both the quality and accuracy of generated content, particularly in knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorporating a fixed number of retrieved passages with a query into LLMs to generate a response. Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts. However, existing approaches may not take full advantage of the planning capabilities of LLMs. Additionally, these methods may struggle to extract relevant content from retrieved passages and are easily influenced by irrelevant information. \n\nParameter-Efficient Fine-Tuning. Despite the powerful generative capabilities of LLMs, fine-tuning them requires substantial computational resources (Lester et al., 2021;Ding et al., 2022;Liu et al., 2023). To achieve more efficient fine-tuning, parameter-efficient tuning methods have emerged. These methods either fine-tune a small portion of the model parameters or introduce additional learnable parameters without fine-tuning the model itself (Hu et al., 2021;Liu et al., 2021;Ding et al., 2022;Wang et al., 2023).",
            "score": 0.7024938559194522,
            "section_title": "Related Work",
            "char_start_offset": 7176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 334,
                    "end": 351,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 530,
                    "end": 548,
                    "matchedPaperCorpusId": "264350686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "273969566",
            "title": "Toward Optimal Search and Retrieval for RAG",
            "text": "Retrieval-augmented generation (RAG) is a promising method for addressing some of the memory-related challenges associated with Large Language Models (LLMs). Two separate systems form the RAG pipeline, the retriever and the reader, and the impact of each on downstream task performance is not well-understood. Here, we work towards the goal of understanding how retrievers can be optimized for RAG pipelines for common tasks such as Question Answering (QA). We conduct experiments focused on the relationship between retrieval and RAG performance on QA and attributed QA and unveil a number of insights useful to practitioners developing high-performance RAG pipelines. For example, lowering search accuracy has minor implications for RAG performance while potentially increasing retrieval speed and memory efficiency.",
            "score": 0.7017168120876025,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "Retrieval-augmented generation. RAG has emerged as a powerful tool in knowledge-intensive NLP tasks such as language modeling (Borgeaud et al., 2022) and question answering (Lewis et al., 2020;Shi et al., 2024a). The typical approach involves integrating a retriever with the LLM generator and designing a fine-tuning process to align the retriever with LLM capabilities. To further refine RAG, recent research explored various enhancements. These include developing dynamical retrieval processes to refine the relevance of fetched content (Jiang et al., 2023;Jeong et al., 2024;Su et al., 2024), and filtering out irrelevant contexts to robustify RAG (Yoran et al., 2024;Yu et al., 2024Yu et al., , 2023;;Wang et al., 2024). Additionally, several studies have developed instruction-tuning methods aimed specifically at improving search and RAG capabilities of LLMs (Liu et al., 2024;Lin et al., 2024;Dong et al., 2024;Wei et al., 2024). \n\nSelf-training. Self-training (or Pseudo-Labeling) is one of the earliest approaches to semi-supervised learning (Rosenberg et al., 2005). The method uses a teacher model to generate new labels on which a student model is fitted. Self-training has been widely adopted for various NLP tasks including text classification (Du et al., 2021), natural language understanding (Vu et al., 2021) and ranking (Wang et al., 2022). Recently, the idea of selftraining has also been applied to LLM instruction fine-tuning (Yuan et al., 2024;Li et al., 2024), reasoning (Pang et al., 2024), and alignment (Gulcehre et al., 2023), yet to the best of our knowledge, this pipeline has not been widely explored for RAG applications. The major drawback of self-training is that it is vulnerable to label noise (Arazo et al., 2020).",
            "score": 0.7014415435778859,
            "section_title": "Related Work",
            "char_start_offset": 5393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 149,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 173,
                    "end": 193,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 193,
                    "end": 211,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 540,
                    "end": 560,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 560,
                    "end": 579,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 579,
                    "end": 595,
                    "matchedPaperCorpusId": "268509926"
                },
                {
                    "start": 652,
                    "end": 672,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 672,
                    "end": 687,
                    "matchedPaperCorpusId": "270878612"
                },
                {
                    "start": 687,
                    "end": 706,
                    "matchedPaperCorpusId": "265212816"
                },
                {
                    "start": 706,
                    "end": 724,
                    "matchedPaperCorpusId": "267751221"
                },
                {
                    "start": 1052,
                    "end": 1076,
                    "matchedPaperCorpusId": "7648360"
                },
                {
                    "start": 1259,
                    "end": 1276,
                    "matchedPaperCorpusId": "222132916"
                },
                {
                    "start": 1309,
                    "end": 1326,
                    "matchedPaperCorpusId": "237502871"
                },
                {
                    "start": 1339,
                    "end": 1358,
                    "matchedPaperCorpusId": "245131402"
                },
                {
                    "start": 1448,
                    "end": 1467,
                    "matchedPaperCorpusId": "267035293"
                },
                {
                    "start": 1467,
                    "end": 1482,
                    "matchedPaperCorpusId": "260866107"
                },
                {
                    "start": 1530,
                    "end": 1552,
                    "matchedPaperCorpusId": "261031028"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7353515625
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "score": 0.7013790842233214,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "276928032",
            "title": "OpenRAG: Optimizing RAG End-to-End via In-Context Retrieval Learning",
            "text": "Retrieval-augmented Generation (RAG). The RAG system combines LLMs, retrievers, and datastores, each contributing to performance improvement. Significant research has focused on improving RAG by tuning LLMs to address challenges such as enhancing on-demand retrieval (Asai et al., 2023;Jeong et al., 2024), optimizing response efficiency (Wang et al., 2024d), and enabling self-reasoning capabilities (Li et al., 2024). Additional efforts have explored building domain-specific (Wang et al., 2024e) or large datastores (Shao et al., 2024). While some studies focus on retrieval, exploring adaptive retrieval strategies (Wang et al., 2024a;c) and leveraging LLMs to develop stronger retrievers (Guu et al., 2020;Shi et al., 2023), research on end-to-end relevance learning for RAG scenarios remains limited. Our work addresses this gap, paving the way for new advancements in RAG systems. \n\nRelevance Learning. Relevance learning is an important and long-established area of research. Traditionally, text relevance has been measured by heuristic rules based on term overlap, as seen in the widely-used BM25 (Robertson et al., 2009). With advances in deep learning, neural retrievers have emerged (Karpukhin et al., 2020), learning relevance from human-annotated datasets (Kwiatkowski et al., 2019). Further research has explored pre-training retrievers using weakly supervised text pairs, such as cropped text spans within documents (Izacard et al., 2021) and relational text pairs extracted from web data (Zhou et al., 2022;Wang et al., 2022), to enable retrievers to learn general relevance. This general relevance can then be refined to task-specific and domain-specific relevance through downstream fine-tuning, resulting in improved performance. Our method falls within these advancements, where the LLM acts as a container of general relevance, providing on-the-fly supervision of specific in-context relevance for relevance learning.",
            "score": 0.699993529324235,
            "section_title": "Related Works",
            "char_start_offset": 29817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1939
                }
            ],
            "ref_mentions": [
                {
                    "start": 693,
                    "end": 711,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1106,
                    "end": 1130,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 1195,
                    "end": 1219,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1270,
                    "end": 1296,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "265594594",
            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
            "text": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
            "score": 0.6989411777358807,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2257
                },
                {
                    "start": 2260,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "273186155",
            "title": "Enhancing Retrieval in QA Systems with Derived Feature Association",
            "text": "First introduced by [8], retrieval augmented generation (RAG) allows LLMs to pull relevant information into context from a cache of documents. The system allows these models to access up-to-date information, rely less on their parameterized-memory, and leverage a large corpus of documents during generation, despite their limited context window [17]. RAG extends LLMs with a retrieval mechanism that takes a query, selects the most relevant texts from a given corpus, and hands them to the generator to inform its answer. Early approaches, were optimized end-to-end, using a jointly learned retriever and generator that communicated through a shared embedding space [8]. However, the requirement that such a system must be trained from scratch for each choice of generator architecture makes this approach expensive and cumbersome given the rapid pace with which new LLMs are developed. However, this paradigm was subverted by [14], which assumes the generator to be black-box, training a generator-agnostic retriever that simply prepends the retrieved text to the generator's input. In practice, the retriever is often further simplified to score documents based on their cosine similarity in a pretrained embedding space (dense retrieval); also popular is the use of BM25, a simple term-frequency based similarity metric (sparse retrieval) [17].",
            "score": 0.6971465279275642,
            "section_title": "Introduction 1.Preliminaries",
            "char_start_offset": 31,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "273502289",
            "title": "RAC: Efficient LLM Factuality Correction with Retrieval Augmentation",
            "text": "For the factual enhancement perspective, the second sub-type is Retrieval Augmented Generation (RAG) (Chen et al., 2017;Lewis et al., 2020;Izacard et al., 2022). Self-RAG (Asai et al., 2023) is proposed to selectively retrieve knowledge and introduce a model to decide whether to retrieve it. Yoran et al. (2024) designed an NLI model to identify and remove irrelevant context in RAG and improve robustness. SAIL (Luo et al., 2023) is tuned on instructions to insert retrieved documents before instructions. Jiang et al. (2023b) actively anticipate future content and decide when and what to retrieve in long-form generation. Yan et al. (2024) designed a lightweight retrieval evaluator to assess the overall quality of retrieved documents for a query to filter out or correct incorrect or irrelevant retrieval content for RAG. \n\nFrom the self-correction or self-alignment perspective, Zhang et al. (2024) proposed a selfevaluation component. They prompt an LLM to validate the factuality of its own generated responses based on its internal knowledge and utilize Self-Knowledge Tuning to augment the LLM's self-# of generation API calls # of search queries for each sentence or iteration # of correction iterations total # of retrieval calls RARR (Gao et al., 2023) 1 n q 1 n s * n q CRITIC (Gou et al., 2024) 1 1 3 3 EVER (Kang et al., 2024) n s 3 2 3*n s RAC (ours) \n\n1 1 1 1 Recently, some work has investigated postcorrection using retrieved content, the area we focus on. Gao et al. (2023) introduces RARR, which generates several questions for each output, retrieves Bing Search for each question one by one, and repeatedly revises the output based on the retrieved content by iterating questions. Two major drawbacks to this method are that the retrieval process is expensive due to many calls to the retriever, and the correction for this method is cascaded, which can introduce errors.",
            "score": 0.6970239215711209,
            "section_title": "Related Work",
            "char_start_offset": 6114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 120,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 120,
                    "end": 139,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 293,
                    "end": 312,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 508,
                    "end": 528,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1248,
                    "end": 1266,
                    "matchedPaperCorpusId": "254247260"
                },
                {
                    "start": 1292,
                    "end": 1310,
                    "matchedPaperCorpusId": "258823123"
                },
                {
                    "start": 1478,
                    "end": 1495,
                    "matchedPaperCorpusId": "254247260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4052734375
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "Retrieval-Augmented Generation (RAG, (Lewis et al., 2020;Su et al.;Wang et al., 2025)) has gained significant interest in Natural Language Processing for enhancing large language models (LLMs) on knowledge-intensive tasks through external information retrieval, with applications across search engines, conversational agents, chatbots, and many other applications. RAG addresses key LLM limitations, including hallucinations, outdated information, and insufficient domain-specific knowledge, particularly in open-domain question answering. Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering). Building on advancements in LLM training methodologies, DeepSeek has enhanced its AI models, notably DeepSeek-R1 (Liu et al., 2024;Guo et al., 2025;Shao et al., 2024), by implementing Group Relative Policy Optimization (GRPO), an advanced reinforcement learning algorithm that improves training efficiency and model performance beyond traditional supervised fine-tuning. GRPO reduces computational overhead by eliminating the value function, using group-based advantage estimation for simplified reward computation, lowering memory usage, and integrating Kullback-Leibler (KL) divergence regularization for stable, efficient training. It outperforms standard Rejection Sampling Fine-Tuning (RFT), which relies on offline sampling, and Online RFT, which dynamically samples from an evolving policy. GRPO also supports process supervision (GRPO+PS), providing step-by-step feedback for improved reasoning, surpassing outcome supervision (GRPO+OS), which evaluates only final answers. Addressing the limitations of static retrieval in traditional RAG, DRAGIN (Dynamic Retrieval-Augmented Generation based on Information Needs, (Su et al.)) is an advanced framework that dynamically determines when and what to retrieve during text generation.",
            "score": 0.6962971368264554,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 578,
                    "end": 599,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "276576000",
            "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
            "text": "For instance, [54][55][56][57] introduces an eviction policy that significantly lowers the memory footprint during generation, while [58,42,39,41,[59][60][61] investigate quantizing KV caches to minimize storage requirements with only minor performance loss. Notably, our approach can be seamlessly integrated with these state-of-the-art eviction and quantization strategies, thereby addressing both speed and memory efficiency. \n\nRetrieve-augmented generation. Retrieval-augmented generation (RAG) is widely used to enhance the generation capabilities of language models by retrieving supporting documents. Retrievers such as dense retrievers or BM25 [22,62,63] are typically employed to find the most relevant documents for a given user query or context. In addition to simply concatenating the retrieved text with the input [64], there are also other methods for integrating the retrieved knowledge into the models. For instance, LongMem [65] encodes documents into caches and uses a trained side net to incorporate these knowledge caches into the context for long-context tasks. With the advances of LLMs across various NLP tasks, certain studies have focused on improving RAG with LLM [66,67]. Some research also explores how to enhance the robustness of LLMs when processing retrieved knowledge. For example, RAFT [68] provides a fine-tuning strategy that strengthens LLMs against noisy contexts in domain-specific RAG.",
            "score": 0.6946699443121254,
            "section_title": "Related Work",
            "char_start_offset": 26797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "259263947"
                },
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 140,
                    "end": 143,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "264828796"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "257495837"
                },
                {
                    "start": 659,
                    "end": 662,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "259137816"
                },
                {
                    "start": 1190,
                    "end": 1194,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 1194,
                    "end": 1197,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1320,
                    "end": 1324,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "273589563",
            "title": "ANSPRE: Improving Question-Answering in Large Language Models with Answer-Prefix Generation",
            "text": "Retrieval-Augmented Generation (RAG). RAG augments the input with retrieved documents from the knowledge base to improve knowledge-intensive tasks [15]. Other variants of RAG instructiontune an LLM with input augmented by a fixed number of retrieved documents [16], or few-shot fine-tuning after jointly pre-training the retriever and LLM [8]. Instead of statically retrieving once, another line of work actively retrieves documents across the course of the LLM generation [11] or trains an LLM to generate API calls for information [20]. The inspiring work of SELF-RAG trains an arbitrary LLM that can decide whether retrieved documents are necessary and reflects on retrieved documents and the responses using reflection tokens. SELF-RAG achieves state-of-the-art results on various ODQA benchmarks. However, these approaches do not consider the importance of locating the answer phrase in the LLM's verbose responses. The confidence of the answers has yet to be fully studied in these approaches. \n\nRe-ranking and Aggregating. Re-ranking, referring to reranking samplings or results from language models, is a common approach to enhance generation quality. With the advancements in LLM, many re-ranking and aggregating techniques have been proposed. Self-Consistency [23] samples a set of Chain-of-thought (CoT) [25] reasoning paths and answers, represented by sequences, and aggregates the answers. This technique requires marginalizing the answer in the sequence and is only studied in mathematics tasks, where the reasoning paths and answers follow a template. This requirement limits the application of Self-Consistency in other question-answering tasks. Our two aggregating techniques are inspired by Self-Consistency with some improvements. Thanks to ANSPRE ability to output the answer phrase, our aggregating techniques can be applied to ODQA tasks or any tasks where ANSPRE is applicable. Instead of aggregating over the samplings like Self-Consistency, our aggregating technique also specializes in the context of RAG, where we aggregate the answers in two directions: samplings in best documents and best sampling across documents.",
            "score": 0.692231335399647,
            "section_title": "Related Work",
            "char_start_offset": 25051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 339,
                    "end": 342,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 533,
                    "end": 537,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "271571143",
            "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
            "text": "Retrieval-Augmented Generation (RAG) enhances the performance of large language models by leveraging external knowledge bases to retrieve relevant document segments through semantic similarity computations [23], [29]. This approach significantly reduces the incidence of hallucinations-defined as instances where generated content deviates from factual accuracy [38], [42]. Early research on RAG primarily focused on developing sparse or dense retrievers [19], [20], whereas contemporary studies have emphasized optimizing the inte-gration of RAG with Large Language Models (LLMs). These optimizations encompass the timing of retrieval, methodological enhancements, and refined utilization of contextual information to mitigate noise within retrieved documents [10], [12]. \n\nWith respect to adaptive retrieval strategies, the FLARE project has proposed two novel methods: proactive retrieval based on retrieval instructions and confidence-based proactive retrieval, both designed to mitigate unnecessary retrievals [18]. Furthermore, process optimization has evolved from the \"Rewrite-Retrieve-Read\" paradigm [24] to \"ITER-RETGEN\" implementing iterative retrieval to incrementally access more granular and comprehensive knowledge [30]. Concerning the management of noise within retrieved documents, studies conducted demonstrated that irrelevant noise documents do not necessarily deteriorate system performance; conversely, they can enhance accuracy by up to 35%. Conversely, documents incorrectly classified as relevant to the query introduce significant interference, substantially impacting the model's generative performance [9]. \n\nIn the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG [16] project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
            "score": 0.691941097050354,
            "section_title": "B. Retrieval-augmented Generation",
            "char_start_offset": 9292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "259316759"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "267312134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "273798183",
            "title": "CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation",
            "text": "2.3.1 Retrieval-augmented Generation. RAG [14,20] is widely used for handling knowledge-intensive NLP tasks. In a typical RAG pipeline, a dense-embedding-based retriever searches for relevant information from an external database, which is then used by the LLM during the generation process. To improve this pipeline, some studies [5,18,22,35] have focused on adjusting retrievers to suit the generation needs of LLMs better, developing multi-step retrieval methods, and filtering out irrelevant information. Although there are many advanced retrievers [8,9,15,16,27,34], it's more promising to optimize the retriever and LLM together in an end-toend process [25,31]. For example, the research [30] has focused on training retrievers and LLMs together, either simultaneously or in stages. However, this requires surrogate loss for optimization and complicates the training pipeline, especially when the embedding database needs to be re-indexed frequently which will bring high compute costs. Therefore, methods such as [5] decompose complex, multi-step queries into smaller sub-intents to improve response comprehensiveness without frequently re-indexing. However, these approaches often overlook the critical role of chunk combination order, which can significantly impact the overall response quality of LLMs. To the best of our knowledge, this paper is the first approach to consider chunk combination order within the RAG task.",
            "score": 0.690802801298584,
            "section_title": "Related Work",
            "char_start_offset": 12537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "274023917",
            "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering",
            "text": "The retrieval-augmented generation paradigm, originating from the DrQA framework by Chen et al., initially used heuristic retrievers like TF-IDF to source evidence from Wikipedia, followed by a neural model to extract answers. This methodology was advanced by DPR (Karpukhin et al., 2020), using pre-trained transformers like BERT for retrieval and reading. Retrieval Augmented Generation (RAG) (Lewis et al., 2020) further evolved the approach by shifting from answer extraction to generation, enabling free-form text creation. Advances in RAG have explored retrieval as a critical tool for augmentation, with Schick et al., Luo et al., and Asai et et al., 2020) and RETRO (Borgeaud et al., 2022) integrated retrieval during the pre-training phase. Recently, Large Language Models (LLMs) have been incorporated into this framework, as seen in REPLUG (Shi et al., 2023b) and IC-RALM (Ram et al., 2023). While prior work on RAG primarily addressed general knowledge, this study introduces the first application of RAG to medical literature, harnessing a vast collection of medical textbooks. Our innovative knowledge self-refinement strategies enhance the fidelity of retrieved information, marking the first refinement of RAG's retrieval component for elevated performance in the medical domain.",
            "score": 0.6903698393212938,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 22486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1295
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 415,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 674,
                    "end": 697,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "269502216",
            "title": "On the Evaluation of Machine-Generated Reports",
            "text": "Retrieval-Augmented Generation.Early retrieval augmented generation systems have been evaluated using task-specific metrics on end-to-end tasks.For example, in the context of question answering, exact match and  1 metrics have been used [30,41].For summarization, ROUGE and BERTScore on reference summaries are common [26].These approaches have two limitations: they only measure ability to complete end tasks, and thus cannot assess intermediate stages or evaluate generation across multiple dimensions; and they are not well-suited to capture failures that can be introduced by current generative models [27].\n\nMore recently, techniques have proposed to more holistically evaluate RAG systems.Gienapp et al. [25] introduce a theoretical framework for evaluating ad hoc generative retrieval.Chen et al. [11] focus on robustness of RAG systems against various perturbations.Thakur et al. [82] benchmark hallucinations and the ability of RAG systems to identify relevant information for 18 languages.Others have introduced benchmarks to measure the ability of RAG systems to provide citations [6,23,53,90].While not specifically  designed for RAG applications, metrics designed to evaluate factuality (e.g., FactScore [58]) or faithful manipulation of long inputs (e.g., BooookScore [10]) can complement application-specific evaluation frameworks.\n\nMost approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance).Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73].Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
            "score": 0.6902493961216065,
            "section_title": "3.2.4",
            "char_start_offset": 19611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1527
                },
                {
                    "start": 1527,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "258865156"
                },
                {
                    "start": 1101,
                    "end": 1104,
                    "matchedPaperCorpusId": "258587884"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "238207962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "278635834",
            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
            "text": "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2022;Ram et al., 2023;Zhang et al., 2024) and question answering (Izacard et al., 2023;Shi et al., 2023;Yoran et al., 2023;Lin et al., 2023;Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023;Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
            "score": 0.6890229350951205,
            "section_title": "Retrieval-augmented Generation",
            "char_start_offset": 5200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1444
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 207,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 207,
                    "end": 224,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 224,
                    "end": 243,
                    "matchedPaperCorpusId": "271915498"
                },
                {
                    "start": 267,
                    "end": 289,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 306,
                    "end": 325,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 325,
                    "end": 342,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 888,
                    "end": 908,
                    "matchedPaperCorpusId": "271915498"
                },
                {
                    "start": 1225,
                    "end": 1245,
                    "matchedPaperCorpusId": "263608822"
                },
                {
                    "start": 1316,
                    "end": 1334,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81689453125
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Generator-Guided Retriever Training. Conversely, generator-guided retriever training focuses on optimizing the retriever based on the generator's performance and requirements. In this paradigm, the generator's ability to produce coherent and accurate text influences the retriever's selection process. DKRR [95] leverages the generator's attention scores to fine-tune the retriever, enhancing its capability to select the most pertinent information. AAR [272] employs smaller language models to generate supervision signals that guide the retriever's training, ensuring that the retrieved documents are optimally aligned with the generator's needs. RA-DIT [140] fine-tunes large language models before training the retriever, fostering better alignment and synergy between the two components. Additionally, UPRISE [33] uses a frozen LLM to guide the fine-tuning of a prompt retriever, thereby improving its effectiveness in retrieving data that the generator can utilize more effectively. This bidirectional influence ensures that the retriever evolves in tandem with the generator, fostering a more integrated and efficient RAG system.",
            "score": 0.6874143116102047,
            "section_title": "RAG Training",
            "char_start_offset": 76826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1136
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "score": 0.6871429220980978,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.857421875
        },
        {
            "corpus_id": "269293655",
            "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has emerged as a prominent approach in natural language processing, combining the strengths of retrieval and generation models [35], with use cases in decreasing hallucination [1,29], knowledge-grounding [9,16,34], and personalization [25,26].Evaluating RAG systems is important as it ensures the effectiveness of integrating retrieval-based methods with generative models [10,23].Traditionally, RAG evaluation has primarily relied on end-to-end assessment, which entails comparing the generated output with one or more ground truth references [20].While this is crucial, it presents several limitations, especially, for evaluating retrieval models in RAG systems.\n\nFirst, end-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system's behavior.Secondly, it is resourceintensive, consuming significant time and computational power, particularly when dealing with a large set of retrieval results consumed by the LLM.To process long input sequences resulting from the utilization of all retrieved documents by the LLM, GPUs with substantial memory capacities are essential for end-to-end evaluation.Moreover, many ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline.Finally, optimizing ranking models often requires document-level feedback, such as user clicks [3,6].However, endto-end evaluation only provides list-level feedback for the retrieval results.That said, this paper studies retrieval evaluation in RAG.\n\nHuman annotations can be a potential solution for evaluating retrieval models in RAG, however, accurate annotations are often challenging and costly to obtain.More recently, with the emergence of large language models (LLMs) and their advanced capabilities in reasoning and text comprehension, they have been utilized to annotate documents for retrieval evaluation [10,23].Nevertheless, these approaches predominantly evaluate the retriever in RAG systems based on human preferences, whereas the primary objective of the retrieval model in RAG is to serve the LLM that leverages the retrieved results [35].",
            "score": 0.687087743492723,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 701
                },
                {
                    "start": 703,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1752
                },
                {
                    "start": 1754,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2127
                },
                {
                    "start": 2127,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1598,
                    "end": 1601,
                    "matchedPaperCorpusId": "258212955"
                },
                {
                    "start": 1601,
                    "end": 1603,
                    "matchedPaperCorpusId": "2979013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "273227829",
            "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) integrates user queries with a collection of pertinent documents sourced from an external knowledge database, incorporating two essential elements: the Retrieval Component and the Generation Component. 1) The retrieval component is responsible for fetching relevant documents or information from the external knowledge database. It identifies and retrieves the most pertinent data based on the input query. 2) After the retrieval process, the generation component takes the retrieved information and generates coherent, contextually relevant responses. It leverages the capabilities of the language model to produce meaningful outputs. Formally, this RAG framework, denoted as M, can be defined as follows: \n\nIn this framework, G and R represent the generation module and the retrieval module, respectively, while q denotes the input query and D refers to the external database. The retrieval module R In this work, we target several key points essential for an efficient and effective Retrieval-Augmented Generation (RAG) system which are elaborated below: \n\n\u2022 Comprehensive Information Retrieval: The indexing function \u03c6(\u2022) must be adept at extracting global information, as this is crucial for enhancing the model's ability to answer queries effectively. \n\n\u2022 Efficient and Low-Cost Retrieval: The indexed data structure D must enable rapid and costefficient retrieval to effectively handle a high volume of queries. \u2022 Fast Adaptation to Data Changes: The ability to swiftly and efficiently adjust the data structure to incorporate new information from the external knowledge base, is crucial for ensuring that the system remains current and relevant in an ever-changing information landscape.",
            "score": 0.6869935874955568,
            "section_title": "RETRIEVAL-AUGMENTED GENERATION",
            "char_start_offset": 5793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "There are several promising extensions to Finetune-RAG that could further improve its robustness and applicability: \n\n\u2022 Training with more in-context RAG: Real-world retrieval often returns more than two documents, and the context window of LLMs are increasing rapidly. At the time of our work, we focused on relatively low context window of 8k, which would realistically be used for two to three RAG documents using up to 3k context window. With increasing context window, future work can explore training with more RAG chunks to optimize LLMs RAG performance even at high level of stresses caused by more retrieved chunks. To support this, we future-proofed our dataset by including two additional relevant chunks per example to support generating more complex multi-document training scenarios. \u2022 Joint retrieval-generation optimization: While Finetune-RAG focuses on improving the generation component, combining it with learned retrieval mechanisms such as rerankeraware retrievers or contrastively trained retrievers could lead to further improvements in factual accuracy and context filtering. \u2022 Multimodal extensions: Hallucination is not limited to text-based models. Extending Finetune-RAG to multimodal settings, such as image-caption retrieval or code+documentation generation, may help build more robust grounded systems in other domains. \n\n\u2022 Evaluation on downstream tasks: While our benchmarking focuses on controlled hallucination settings, future work should assess Finetune-RAG's impact on end-to-end performance in downstream RAG applications such as open-domain question answering, legal document summarization, and domain-specific information retrieval.",
            "score": 0.6869525821075564,
            "section_title": "Future Work",
            "char_start_offset": 21636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "272832445",
            "title": "Block-Attention for Efficient Prefilling",
            "text": "RAG is a widely used technique to improve generations of language models by using retrieved nearest-neighbor documents or passages as references, which typically involves two stages: retrieval and generation. Before generation, retrieval finds most similar passages with the user query or the context, by using BM25 or dense retrieval model (Lee et al., 2021;Lan et al., 2023;2024;Ma et al., 2025;Che et al., 2024). After collecting retrieved passages, there are numerous techniques to incorporate the knowledge during generation. Earlier works includes concatenation (Izacard & Grave, 2021), cross-attention (Borgeaud et al., 2022) and distribution interpolation Khandelwal et al. (2020). Some studies have also begun to attempt to directly use the retriever as text generator, that is, text generation is performed by selecting context-aware phrases from a collection of supporting documents (Lan et al., 2023;Cao et al., 2024). \n\nRecently, LLMs becomes the most powerful paradigm for most NLP tasks, and simply concatenating all retrieved documents into the context of LLMs becomes the most simple and effective way for retrieval-augmented generation (RAG). For example, Self-RAG (Asai et al., 2023) leverage a critic model to decide which content in retrieved passages should be used during generation. As a specific application of RAG, tool learning is widely used to call external APIs to retrieve related passages from external database or tools to solve knowledge-intensive tasks (Schick et al., 2023).",
            "score": 0.6869124442575225,
            "section_title": "RETRIEVAL-AUGMENTED GENERATION (RAG)",
            "char_start_offset": 26192,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 930
                },
                {
                    "start": 933,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1510
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 414,
                    "matchedPaperCorpusId": "271961833"
                },
                {
                    "start": 568,
                    "end": 591,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 912,
                    "end": 929,
                    "matchedPaperCorpusId": "268031947"
                },
                {
                    "start": 1488,
                    "end": 1509,
                    "matchedPaperCorpusId": "256697342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "277452045",
            "title": "RARE: Retrieval-Augmented Reasoning Modeling",
            "text": "Retrieval-Augmented Generation Retrieval-augmented generation (RAG) systems enhance LLMs by incorporating external knowledge during inference [85]. Modern RAG systems advance in knowledge indexing [56,36,73,79], query rewriting [42,71], document compression [76], retrieval denoising [39,13], iterative retrieval [3,26] and so on, achieving increasingly high retrieval accuracy. E.g., Memory 3 [78] introduces explicit memory as a third form of memory. Compared with parameters and text-based retrieval, it externalizes knowledge into a cheaper, retrievable format. However, RAG methods focus on knowledge supplementation rather than reasoning capacity acquisition-retrieved information serves as input augmentation rather than scaffolding for cognitive process development. RARE redefines RAG's role in the training stage: by injecting retrieved knowledge into training prompts, it transforms retrieval contexts into reasoning skill incubators, enabling models to directly learn cognitive patterns from knowledge-anchored examples. This shifts RAG from a post-hoc patch to an integral component of reasoning capability formation. \n\nNotably, our most relevant work is Retrieval-Augmented Fine-Tuning (RAFT) [83]. For the imperfect retrieval issue [69,38], RAFT solely focuses on identifying helpful information from retrieved documents. It learns to mimic the structured output format of teacher models that extract and directly quote sentences, rather than fostering domain thinking-unleashing reasoning capabilities involving higher-order cognitive processes. Moreover, RAFT introduces two hyperparameters related to the proportion of golden and distract documents during training, which is reported to cause training unstability issues [83,17]. Finally, RAFT relies on teacher model supervision rather than reward signals, and thus cannot be directly applied in reinforcement learning. \n\nDomain LLMs Domain-specific LLMs have emerged as critical tools for addressing specialized tasks. Previous work focuses primarily on knowledge internalization through specialized pretraining [49] or fine-tuning [51] to incorporate relevant expertise.",
            "score": 0.6868533065430451,
            "section_title": "Related Work",
            "char_start_offset": 6281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This paper focuses on Retrieval-Augmented Generation (RAG) system for black-box Large Language Models (LLMs), namely black-box RAG.In this section, we first give the definition of RAG and subsequently introduce the black-box RAG.\n\nRetrieval-Augmented Generation (RAG).Given a natural language question , an external knowledge corpus W and a generative language model M, a RAG system aims to help M generate more accurate and informative responses to  using a retrieval model R, which effectively retrieves relevant documents D = ( 1 ,  2 ,  3 , ...) from W. The form of introducing external knowledge to the language model varies, including modifying attention weights during generation, incorporating it into input prompts, or using it in post-calibration of the model output.Moreover, existing RAG methods typically require joint fine-tuning of the retriever and the language model (e.g.Atlas [15], REALM [9]).However, joint fine-tuning is unaffordable in amount of practical scenarios due to the extremely large parameter scale of LLMs.In these scenarios, we can alternatively treat an LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a RAG system, namely black-box RAG.Next, we introduce the definition of the black-box RAG.\n\nRetrieval-Augmented Generation System for Black-box LLM (Black-box RAG).A RAG system for blackbox LLM aims to enhance the generation capability of the black-box LLM M  by retrieving external knowledge without updating the LLM parameters.While the parameters of the black-box LLM M  are frozen, the parameters of the retrieval model R are learnable.Thus, the RAG system for black-box LLM only optimizes R to improve overall system performance, without modifying M  .Moreover, existing black-box RAG systems typically inject the retrieved documents D into M  by constructing an input prompt that concatenates the question  and documents D, which leverages the powerful in-context learning capabilities of the LLM.",
            "score": 0.6838457999713647,
            "section_title": "Problem Formulation",
            "char_start_offset": 14673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 231,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 907,
                    "end": 910,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "276937897",
            "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System",
            "text": "Retrieval-augmented generation (RAG), as a cutting-edge technological paradigm, aims to address challenges faced by large language models (LLMs), such as data freshness (He et al., 2022), hallucinations (B\u00e9n\u00e9dict et al., 2023;Chen et al., 2023;Zuccon et al., 2023;Liang et al., 2024), and the lack of domain-specific knowledge (Li et al., 2023;Shen et al., 2023). This is particularly relevant in knowledge-intensive tasks like opendomain question answering (QA) (Lazaridou et al., 2022). By integrating two key components: the retriever and the generator, this technology enables more precise responses to input queries (Singh et al., 2021;Lin et al., 2023). While the feasibility of the retrieval-augmentation strategy has been widely demonstrated through practice, its effectiveness heavily relies on the relevance and accuracy of the retrieved documents (Li et al., 2022;Tan et al., 2022). The introduction of excessive redundant or incomplete information through retrieval not only fails to enhance the performance of the generation model but may also lead to a decline in answer quality (Shi et al., 2023;Yan et al., 2024). \n\nIn response to the aforementioned challenges, current research efforts mainly focus on two aspects: improving retrieval accuracy (Zhuang et al., 2024;Sidiropoulos and Kanoulas, 2022;Guo et al., 2023) and enhancing the robustness of LLMs against toxic information (Longpre et al.;Kim et al., 2024). However, in RAG systems, a commonly overlooked aspect is the chunked processing of textual content, which directly impacts the quality of dense retrieval for QA (Xu et al., 2023). This is due to the significant \"weakest link\" effect in the performance of RAG systems, where the quality of text chunking constrains the retrieved content, thereby influencing the accuracy of generated answers (Ru et al., 2024). Despite advancements in other algorithmic components, incremental flaws in the chunking strategy can still detract from the overall system performance to some extent.",
            "score": 0.6837152625807805,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 244,
                    "matchedPaperCorpusId": "264350686"
                },
                {
                    "start": 244,
                    "end": 264,
                    "matchedPaperCorpusId": "261891399"
                },
                {
                    "start": 621,
                    "end": 641,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 641,
                    "end": 658,
                    "matchedPaperCorpusId": "259370874"
                },
                {
                    "start": 1093,
                    "end": 1111,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 1282,
                    "end": 1314,
                    "matchedPaperCorpusId": "248524694"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "278534606",
            "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models by fetching relevant documents at inference time and conditioning a seq2seq generator on both the user's query and the retrieved text. Retrieval can be performed via classical sparse methods (e.g., BM25) or dense vector search with approximate nearest neighbor (ANN) techniques. By injecting explicit evidence, RAG mitigates the tendency of pure generative models to hallucinate and has yielded 5-15-point gains in Exact Match and F1 on benchmarks like NaturalQuestions and TriviaQA. It also excels in multihop reasoning tasks such as HotPotQA by chaining together evidence from multiple sources. \n\nDespite these strengths, RAG introduces key operational hurdles: \n\n1) Latency Overhead: Each query incurs a retrieval step-spanning inverted-index lookups or large-scale vector searches-that can dominate end-to-end response time. 2) Noise Sensitivity: The generator's output quality depends critically on retrieval precision; irrelevant or lowquality passages can mislead the model. 3) Context Window Limits: Even state-of-the-art LLMs have bounded input lengths (4K-32K tokens), forcing truncation of long or multiple concatenated passages and risking information loss. 4) Index Maintenance: Building and updating retrieval indices for expansive or rapidly changing corpora requires substantial storage, compute resources, and engineering effort. \n\nTo address these challenges, recent research has explored improvements in retrieval, generation, and hybrid architectures: a) Self-Reflective RAG Asai et al. introduce an iterative scheme where the model emits a special token when it detects missing evidence, triggering a focused second retrieval step to fill in information f) Index and ANN Optimizations Advances in ANN libraries (e.g., FAISS, Annoy) and hardware acceleration have lowered retrieval times to single-digit milliseconds for large embedding collections. Techniques like cache-aware prefetching and adaptive index sharding further enhance throughput.",
            "score": 0.6835348804755398,
            "section_title": "A. Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 5154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2027
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "276558046",
            "title": "Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG in Edge Device",
            "text": "The integration of retrieval-augmented generation (RAG) with large language models (LLMs) (Lewis et al., 2020) has emerged as a pivotal advancement in mitigating the issue of factual hallucination (Ji et al., 2023)-an inherent limitation of LLMs when generating knowledge-intensive responses. By leveraging external knowledge sources, RAG enables LLMs to utilize relevant knowledge dynamically, enhancing both the accuracy and reliability of their outputs. RAG is especially crucial in the context of specialized domains, where precision is paramount and errors can be costly. Also, in RAG, LLMs must not only incorporate the relevant external information as the input, but also contextualize the information within the nuances of the target domain. To optimize the RAG-LLM for a specific domain, recently domain-specific RAG (Tianjun Zhang, 2024) has been developed where LLMs can early access the target domain through finetuning. The practicality of the domain-specific RAG is more noteworthy when computational resources are limited such as edge devices since with only a small-scaled LLM some tasks should be performed reliably. \n\nDespite the promise of the domain-specific RAG, the input external knowledge (generally retrieved information dubbed contexts) may consist of both irrelevant and relevant contexts. Hence, reasoning process such as chain-of-thought (CoT) (Wei et al., 2022) is useful for understanding and focusing on the relevant context. To this end, in RAFT (Tianjun Zhang, 2024), LLM learns the reasoning as well as answering in finetuning. Also, in (Yu et al., 2023), the reasoning is prompted to generate a summary of all the contexts. Elaborated reasoning is beneficial, and yet obtaining this kind of reasoning dataset for domain-specific learning is time-consuming and costful, and also it incurs a large testing cost. \n\nUnreliability of reasoning is also a critical issue, especially when the parameter-efficient fine-tuning (PEFT) like LoRA adapters (Hu et al., 2021;Huang et al., 2023;Bang et al., 2024) are used to reduce the computational burden in training resource constraint environment.",
            "score": 0.682202583813008,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 110,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 197,
                    "end": 214,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1373,
                    "end": 1391,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the limitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism, which retrieves relevant documents or information from an external knowledge source, and (ii) a generation module, which processes this information to generate human-like text (Lewis et al. 2020). This combination allows RAG models to not only generate fluent text but also ground their outputs in real-world, up-to-date data. \n\nThe retrieval module in RAG typically leverages dense vector representations to identify relevant documents from large datasets, such as Wikipedia or proprietary databases. Once retrieved, these documents are passed to the generative module, often built using transformer-based architectures, to generate responses grounded in the retrieved knowledge. This methodology helps mitigate the hallucination problem and ensures that the generated text is more factual and contextually appropriate (Thakur et al. 2021). Over the period, RAG models have seen applications in various domains, including open-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), and personalized recommendations. \n\nFigure 2: A basic flow of the RAG system along with its component",
            "score": 0.6821715402840021,
            "section_title": "Overview of Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 2068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "274234908",
            "title": "From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive Grammars",
            "text": "The basic pipeline for retrieval augmented generation (Naive RAG) (Gao et al., 2024) consists of a database of documents, a retriever, a process of combining the retrieved documents with the prompt, and the LLM generating an answer based on the prompt. Advanced RAG pipelines described in (Gao et al., 2024) are modifications of different parts of the Naive RAG pipeline. In the context of retrieval augmented generation from a descriptive grammar, the first component of the pipeline-the database of documents-is the grammar itself, hence it is fixed and not as modifiable as for RAG tasks that utilize the Internet or a large database for answering one question. We chose the chunking method that is simple but still context-aware-splitting grammars into paragraphs. We avoid model-based adaptive chunking methods, which would interfere with interpreting the subsequent RAG components due to grammars being a relatively underexplored domain. \n\nThe second component of the RAG pipeline is the retrieval method. We evaluate BM25 (described in (Trotman et al., 2012) and taken as a baseline in (Trotman et al., 2014)), a language-agnostic retriever based on term frequency, and state-ofthe-art retrievers/rerankers based on embeddings featured in the Massive Text Embedding Benchmark (MTEB) leaderboard (Muennighoff et al., 2023). Ideally, the chosen embedding-based retrieval methods should respond well to linguistic diversity, since descriptive grammars contain examples in the described language, which may contain diacritic signs and subwords or segments that are rare or unused in English. The weights assigned by the tokenizer to embeddings of such symbols due to their absence in the vocabulary would be random noise, and the resulting embeddings of the paragraphs would have high variance. Therefore, the tokenizer of the chosen retriever should ideally contain byte-level byte pair encoding (BBPE) (Wang et al., 2020). \n\nThe third component of the RAG pipeline is the prompt.",
            "score": 0.6821094285064965,
            "section_title": "Method and Overall Architecture",
            "char_start_offset": 8319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 1984
                }
            ],
            "ref_mentions": [
                {
                    "start": 1043,
                    "end": 1065,
                    "matchedPaperCorpusId": "9444322"
                },
                {
                    "start": 1093,
                    "end": 1115,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 1907,
                    "end": 1926,
                    "matchedPaperCorpusId": "202539075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "271534349",
            "title": "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
            "text": "Retrieval-augmented generation (RAG) [23] is an approach that enhances the quality of generation by retrieving from external knowledge bases. This method includes three key components [24]: retriever, generator, and augmentation techniques. The retriever is used to find relevant information from a large-scale dataset or knowledge base, including pertinent documents, facts, or text snippets that are relevant to the input query or prompt. The retrieved information is fed into the generator, which integrates this external knowledge into the generation stage. Augmentation techniques focus on how retrieved information is integrated into the generation process. To formalize the RAG process, consider a scenario where we want to generate code based on a query q and a set of retrieved candidates {c 1 , c 2 , ..., c n }. The process can be described by the following formula: \n\nwhere the Retrieve(\u2022) function selects the most relevant candidates based on the query q from the candidate set {c 1 , c 2 , ..., c n }, and the Generate(\u2022) function then takes the query and the retrieved candidates to generate the target code. \n\nIn recent years, researchers have conducted a substantial amount of research related to RAG, highlighting its promising potential for future applications [23], [25]- [28]. Many studies have utilized RAG for code-related research [29]- [42]. In repository-level code completion, due to the massive amount of code in the repository and limited context of generator [12], it is impractical to use the entire repository as the context for generation. Therefore, most current methods employ the RAG method to retrieve suitable candidates from the repository for generation [12], [13], [16], [17].",
            "score": 0.6795804643155101,
            "section_title": "A. Retrieval-Augmented Generation",
            "char_start_offset": 6053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 1362,
                    "end": 1366,
                    "matchedPaperCorpusId": "258170012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "273798073",
            "title": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation",
            "text": "In traditional RAG, finetuning methods are widely employed to enhance the retriever and generator (Lin et al., 2024;Ke et al., 2024). Beyond this, modular RAG systems integrate a series of LLM-based components (Gao et al., 2023). Fine-tuning helps models better follow complex instructions within these components (He et al., 2024), improving RAG systems' performance and task adaptability (Asai et al., 2024;Zhang et al., 2024;Jeong et al., 2024). Classic supervised fine-tuning strategy (SFT) trains only on positive samples. While DPO as a more direct reinforcement learning fine-tuning (RLFT) method, leverages positive-negative sample pairs to effectively and efficiently strengthen LLMs' ability to follow complex instructions. Under the multisource setting, our work thus employs DPO to enhance the model's ability to follow the retrieval selection instruction to select the optimal retrieval source during adaptive retrieval.",
            "score": 0.6795454303003744,
            "section_title": "Related Work",
            "char_start_offset": 6869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 447,
                    "matchedPaperCorpusId": "268553748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "264288947",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "text": "A few concurrent works2 on RAG propose new training or prompting strategies to improve widely-adopted RAG approaches. Lin et al. ( 2023) fine-tune both the retriever and LM on instruction-tuning datasets in two steps. While we also train our model on diverse instruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best possible model output via fine-grained self-reflection, making it widely applicable and more robust and controllable. Yoran et al. ( 2023) use a natural language inference model and Xu et al. (2023) use a summarization model to filter out or compress retrieved passages before using them to prompt the LM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. Moreover, our self-reflection mechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou et al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks and to generate with tree search, guided by LM-generated value scores. While their value function simply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to generate fine-grained self-reflection and customizable inference. \n\nTraining and generating with critics. Training LLMs with reinforcement learning (e.g., Proximal Policy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven effective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce fine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique on retrieval and generation, we train our target LM on task examples augmented with reflection tokens from a critic model offline, with a far lower training cost compared to RLHF. In addition, reflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on human preference alignment during training.",
            "score": 0.6794437194472001,
            "section_title": "Concurrent RAG work.",
            "char_start_offset": 5973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 1563,
                    "end": 1584,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62841796875
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "RAG represents an approach to text generation that is based not only on patterns learned during training but also on dynamically retrieved external knowledge [1]. This method combines the creative flair of generative models with the encyclopedic recall of a search engine. The efficacy of the RAG system relies fundamentally on two components: the Retriever (R) and the Generator (G), the latter representing the size and type of LLM. \n\nThe language model can easily craft sentences, but it might not always have all the facts. This is where the Retriever (R) steps in, quickly sifting through vast amounts of documents to find relevant information that can be used to inform and enrich the language model's output. Think of the retriever as a researcher part of the AI, which feeds the contextually grounded text to generate knowledgeable answers to Generator (G). Without the retriever, RAG would be like a well-spoken individual who delivers irrelevant information.",
            "score": 0.6790958811904939,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "273375021",
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "text": "Retrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.",
            "score": 0.6770586978311228,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "273798073",
            "title": "PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation",
            "text": "Knowledge Source Exploration for RAG. In the era of LLM, RAG (Lewis et al., 2020;Guu et al., 2020) builds on the versatile LLM as a foundation and serves as a bridge between external knowledge and the model's internal parameterized knowledge by following the \"Retriever-and-Reader\" paradigm (Chen et al., 2017;Das et al., 2019). For various downstream tasks (Zhu et al., 2021;Zhou et al., 2023;Cai et al., 2019), RAG systems retrieve accessible sources as comprehensively as possible to enhance generation, especially for knowledge-intensive question answering task (Khattab et al., 2022). In terms of the manner of retrieval sources, recent advanced RAG research can be divided into two categories. One line of study conducts in-depth exploration within a single retrieval source, referred to as Single-Source RAG (SS-RAG). It primarily includes multi-step RAG methods (Trivedi et al., 2023;Ram et al., 2023;Borgeaud et al., 2022) that use subqueries for iterative retrieval and ARAG methods (Yao et al., 2023;Asai et al., 2024;Dhole, 2025) that flexibly determine \"when and what to retrieve\" for a more adaptive and in-depth retrieval process. For Single-Source ARAG (SS-ARAG), the limitation of a single retrieval source imposes an upper bound on the capability of the RAG system. Another line of research focuses on Multi-Source RAG (MS-RAG). CRAG (Yan et al., 2024) uses the web as a backup retrieval source, while ReAct, an agent framework, can be instantiated to achieve basic MS-ARAG. However, it cannot foresee the features of different retrieval sources and heavily relies on their descriptions for selection, leading to low-quality and unstable multi-source retrieval. Therefore, PrefRAG aims to achieve adaptive retrieval while ensuring a stable selection of the most suitable retrieval source during iteration. Fine-Tuning for RAG. In traditional RAG, finetuning methods are widely employed to enhance the retriever and generator (Lin et al., 2024;Ke et al., 2024).",
            "score": 0.676921599738979,
            "section_title": "Related Work",
            "char_start_offset": 5024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 376,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 376,
                    "end": 394,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 870,
                    "end": 892,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 892,
                    "end": 909,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 993,
                    "end": 1011,
                    "matchedPaperCorpusId": "252762395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "269292881",
            "title": "Retrieval-Augmented Generation-based Relation Extraction",
            "text": "Retrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
            "score": 0.6767104194678113,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "271088607",
            "title": "FACTS About Building Retrieval Augmented Generation-based Chatbots",
            "text": "Active Retrieval augmented generation (FLARE) [7] iteratively synthesizes a hypothetical next sentence.If the generated sentence contains low-probability tokens, FLARE would use the sentence as the new query for retrieval and regenerate the sentence.Mialon et al. [12] reviews works for advanced augmented generation methods in language model.Self-refine [11] builds an agent to improve the initial answer of RAG through iterative feedback and refinement.ReAct [16] Agent is widely used for handling the complex queries in a recursive manner.On the RAG evaluation front, RAGAS [4] and ARES [14] utilize LLMs as judges and build automatic RAG benchmark to evaluate the RAG system.Zhu et al. [17] overview the intensive usages of LLM in a RAG pipeline including retriever, data generation, rewriter, and reader.We believe that our work provides a unique perspective on building secure enterprise-grade chatbots via our FACTS framework.",
            "score": 0.6765993075887304,
            "section_title": "RELATED WORK",
            "char_start_offset": 21721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 103,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 933
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "257900871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "271270246",
            "title": "Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks",
            "text": "Retrieval augmented generation (RAG) is a technique [2] that combines information retrieval with language models to enhance their ability to generate relevant and factual text. In RAG, the language model is augmented with an external knowledge base or other information source, such as a collection of documents or web pages. When generating text, the model first retrieves relevant information, based on the input query, and then uses that information to guide the generation process. This process is applied in the BioASQ challenge, where the relevant information source is the annual baseline of PubMed. \n\nRAG has been shown to improve the factual accuracy of generated text compared to standalone language models [21]. It allows the model to access a vast amount of external knowledge and incorporate it into the generated output. RAG is particularly useful for tasks that require domain-specific knowledge or up-to-date information [3].",
            "score": 0.6759689854358462,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 941
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490966796875
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
            "score": 0.6756885509871785,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "269982120",
            "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
            "text": "Retrieval-augmented Generation Equipping a parametric language model with a non-parametric datastore has proven effective for various NLP tasks, such as language modeling [36,54,81], opendomain question answering [24,41,66,49], machine translation [35,11] and others.Given the vast  design space of this generation paradigm, numerous approaches with different focuses have been proposed.For instance, RETRO [8] and PlugLM [12] feature architectural innovations for better integration with the non-parametric datastore.REALM [21] introduces an end-to-end methodology for joint optimization of the language model and the retriever.REPLUG [66] and RA-DIT [48] enhance the retriever using LLM feedback to better align these two components.DSP [37] and InteR [17] explore sophisticated interactions between the retriever and the language model.Selfmem [13] employs a reward model to iteratively refine both retrieval and generation processes.Self-RAG [4] introduces a self-reflection mechanism that significantly enhances the quality and factuality of language models.For a more comprehensive review, refer to [18,5,3].Our work, xRAG, distinguishes itself from previous studies by adopting a modality fusion approach to the problem of retrieval augmentation, resulting in a both effective and efficient RAG system.",
            "score": 0.6745697507741931,
            "section_title": "Related Work",
            "char_start_offset": 3884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 1111,
                    "end": 1113,
                    "matchedPaperCorpusId": "263866951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "268513072",
            "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
            "text": "This method modifies only a small portion of the model's weights, reducing the number of parameters that need to be updated during fine-tuning.By focusing on these adaptable components, LoRA efficiently updates the model, maintaining performance while significantly lowering computational demands and memory usage.\n\nImproving the performance of Retrieval Augmented Generation (RAG) involves several strategic enhancements across data preparation, indexing, and query handling.To reduce computational time, we can explore various index types for better context retrieval.Additionally, we can also transform queries to better match the retrieval context.Each of these tactics aims at refining the interaction between the LLM and the data, ensuring more accurate, relevant, and efficient generation outcomes [20].",
            "score": 0.6739141440358241,
            "section_title": "B. Challenges in the Generative Models",
            "char_start_offset": 7772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "270062557",
            "title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
            "text": "Retrieval-Augmented Generation RAG is a family of techniques for generating output structures while using retrieved nearest-neighbor structures as a reference.It typically involves two stages: retrieval and generation.Retrieval finds most similar contexts based on BM25 or learned embedding, where the context could be represented as token embedding [16,37], dense embedding [5] or raw text [9,12,19].Once those contexts are retrieved, different architectures are leveraged to incorporate them into the model.Popular approaches includes concatenation [12,19] and cross-attention [3,18].In recent years, the LLM architecture evolves towards a single decoder only model with significant larger sizes.To this end, concatenation of raw text [19] is becoming popular for being simple and practical.Many advanced approaches have been developed on top of it in recent years.Self-RAG [2] is proposed to selectively retrieve knowledge and introduce a critic model to decide whether to retrieve.Yoran et al. [38] designed an NLI model to identify the irrelevant context and improve robustness.SAIL [23] is tuned on instructions to insert retrieved documents before instructions.Toolformer [30] is pre-trained for calling APIs such as Wikipedia.Jiang et al. [14] actively anticipate future content and decide when and what to retrieve in long-form generation.CRAG [36] makes the attempt to explore and design corrective strategies for RAG to improve its robustness of generation.Specifically, an external T5 model is trained and used to determine the usefulness of the retrieved context.Generally, these approaches target on exploiting retrieval as a useful tool to augment generation or whether retrieval is necessary.\n\nEfficiency in RAG The efficiency of LLM inference is a widely explored research area, where different categories of approaches have been studied.Some works are towards architecture level acceleration such as efficient attention [31], Mixture of Expert [6], Transformer-alternative architectures [7], etc.While some other works are algorithm-level acceleration like quantization [22] or speculative decoding [17].Nevertheless, most previous works are not RAG specific.Recently, RAG Cache [15] was proposed as a multilevel dynamic caching system tailored for RAG from the system perspective.",
            "score": 0.6736922151349107,
            "section_title": "Related Work",
            "char_start_offset": 4186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 509
                },
                {
                    "start": 509,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1083
                },
                {
                    "start": 1083,
                    "end": 1168
                },
                {
                    "start": 1168,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1708
                },
                {
                    "start": 1710,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 2014
                },
                {
                    "start": 2014,
                    "end": 2122
                },
                {
                    "start": 2122,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "231717977"
                },
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 582,
                    "end": 585,
                    "matchedPaperCorpusId": "220128068"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1962,
                    "end": 1965,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 2117,
                    "end": 2121,
                    "matchedPaperCorpusId": "254096365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.509765625
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "score": 0.6725433618665033,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "271212697",
            "title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
            "text": "The generation module serves as the primary functional component within this system, receiving outputs from the data retrieval module and invoking the LLM module to summarize and condense information, ultimately generating conclusions.\n\nRetrieval-augmented generation (RAG) represents an innovative approach amalgamating retrieval-based methods with generative models, typically Language Models (LMs), to enrich the quality and relevance of generated text (Ciuc\u0103 et al., 2023;Gao et al., 2024).This article's expert question and answer system embodies the concept of RAG.In this paradigm, LLMs retrieve input questions (segmented into subqueries via CoT) from the data retrieval module to acquire information for enhancing generation.Subsequently, LLMs reconsider both the acquired information and original questions.During the reconsideration phase, prompt engineering, leveraging task description, input data, contextual information, and prompt style, enriches and standardizes LLM output (Zhao et al., 2023), enabling LLM to produce answers meeting predefined standards, akin to those of domain experts or proficient students.\n\nIn the realm of RAG research, advanced and modular RAG methodologies are evolving from the original or naive RAG approach (Gao et al., 2024).Advanced RAG incorporates additional processing stages pre-and post-retrieval.Pre-retrieval processing concentrates on optimizing data indexing via various methods, including data granularity refinement, index structure optimization, and metadata incorporation to enhance retrieval content quality.Postretrieval processing involves reranking and prompt compression, with embedding playing a crucial role.Optimization strategies encompass fine-tuning embedding or employing dynamic embedding methods.Embracing the design principles of Advanced RAG, we enhance the pre-retrieval stage with query planning and expansion, while bolstering post-retrieval stages with reranking and summarization techniques.",
            "score": 0.6713790903172299,
            "section_title": "Generation module",
            "char_start_offset": 18381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 237,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1129
                },
                {
                    "start": 1131,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.677734375
        },
        {
            "corpus_id": "270620574",
            "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by integrating external, non-parametric knowledge provided by retrievers. In RAG framework, the retriever locates and looks up useful documents based on a given query, and then the LLM interacts with these retrieved results to generate a response. The coordination of retrieval and generation achieves impressive performance without additional training. Especially in domain-specific and knowledge-intensive Figure 1: A comparison between RAG and R 2 AG. R 2 AG employs a trainable R 2 -Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval information further. \n\ntasks, RAG offers real-time knowledge with high interpretability to LLMs, effectively mitigating the hallucination problem (Mallen et al., 2023). \n\nHowever, there exists a semantic gap between retrievers and LLMs due to their vastly different training objectives and architectures (BehnamGhader et al., 2022). Specifically, retrievers, typically encoder architecture, are designed to retrieve the most relevant documents for a query (Zhu et al., 2023b). Conversely, LLMs, generally decoder architecture, are expected to answer questions based on their inherent knowledge or given documents. However, the interaction between retrievers and LLMs in RAG primarily relies on simple text concatenation (BehnamGhader et al., 2022). This poor communication strategy will lead to several challenges for LLMs. Externally, it is hard for LLMs to utilize more information from retrievers in separate processes. In RAG, the retrieved documents that only preserve sequential relationships are unidirectionally delivered to LLMs, and LLMs do not fully understand why retrievers provide the documents. \n\nParticularly, low-quality documents inevitably appear in retrieved results (Barnett et al., 2024), but LLMs have to accept this noise passively. Internally, it is hard for LLMs to handle all of the retrieved documents with their inherent knowledge. LLMs must process all the results and assess which documents are important, impacting their ability to generate accurate answers (Wu et al., 2024).",
            "score": 0.6712116644515567,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54541015625
        },
        {
            "corpus_id": "276449952",
            "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
            "text": "Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. [22] introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval [10,36] and enhancing generation capabilities [2,17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself. Despite these significant advancements, the challenge of seamlessly integrating dynamic historical context in RAG models for multi-turn dialogues remains an elusive goal. Though achieve remarkable progress, most existing approaches continue to depend predominantly on static knowledge bases and do not adequately address the need to capture the evolving contextual nuances within conversations. This gap propels the development of DH-RAG in this paper, aimed at more effectively incorporating both static external knowledge and the transient context prevalent in ongoing dialogues, thereby enhancing the quality and coherence of multi-turn dialogue interactions.",
            "score": 0.6709731578402862,
            "section_title": "Related Work",
            "char_start_offset": 3838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1568
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77978515625
        },
        {
            "corpus_id": "273374817",
            "title": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language Models",
            "text": "Retrieval Augmented Generation. Given a question q and a set of top-k retrieved documents D ={d 1 , d 2 , . . . , d k } from a collection C, the goal of retrieval augmented generation (RAG) is to generate an answer for q using D as context. LLMs are currently an important component of RAG pipelines, acting as the generator. The generator is given q, D, and an instruction prompt on how to generate the answer (Jeong et al., 2024;Lee et al., 2024;Li et al., 2024a). Using top-k retrieved documents helps LLMs to be exposed to information that it might not have been trained/fine-tuned with during development. These documents are commonly retrieved using an effective sparse and/or dense retriever (Lewis et al., 2020). \n\nAttributive RAG. LLMs are prone to generate hallucinated (and even factually incorrect) answers (Ji et al., 2023;Rawte et al., 2023;Yue et al., 2024). \n\nAttributing answers in RAG with LLMs is an approach taken as a step towards ensuring the veracity of the output of these models (Bohnet et al., 2022;Hu et al., 2024;Kamalloo et al., 2023;Khalifa et al.;Li et al., 2024b). Menick et al. (2022) teach language models to support answers with verified quotes. Ye et al. (2024) propose a learning-based framework in which they fine-tune LLMs to generate citations, as opposed to prompting or relying on post-hoc attribution. Stolfo (2024) analyzes whether every generated sentence in the output of LLMs is grounded in the retrieved documents or the LLM's pre-training data.",
            "score": 0.6708746263409016,
            "section_title": "Background",
            "char_start_offset": 4962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 431,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 431,
                    "end": 448,
                    "matchedPaperCorpusId": "270514562"
                },
                {
                    "start": 448,
                    "end": 465,
                    "matchedPaperCorpusId": "259501744"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "263831293"
                },
                {
                    "start": 855,
                    "end": 872,
                    "matchedPaperCorpusId": "268667523"
                },
                {
                    "start": 1063,
                    "end": 1078,
                    "matchedPaperCorpusId": "268819100"
                },
                {
                    "start": 1078,
                    "end": 1095,
                    "matchedPaperCorpusId": "263830219"
                },
                {
                    "start": 1181,
                    "end": 1197,
                    "matchedPaperCorpusId": "265220884"
                },
                {
                    "start": 1345,
                    "end": 1358,
                    "matchedPaperCorpusId": "269033410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60009765625
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning. A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length. Interesting future directions include exploring (automated) position optimization with more advanced retrieval ordering methods, and fine-tuning the LLMs for RAG with more fine-grained and multi-step reasoning chains. (2) Contriever is more similar to BM25, while bge is more similar to e5 (since their curves are closer respectively).",
            "score": 0.6704764249596916,
            "section_title": "Conclusions",
            "char_start_offset": 29954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "273375021",
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "text": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
            "score": 0.6697642054979545,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2392
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "263835211",
            "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
            "text": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to address certain limitations inherent to standalone LLMs, such as outdated information and the inability to memorize (Chase, 2022;Liu, 2022). These challenges are elaborated upon in Sec 4.2.1. Yet, while RAG offers solutions to some issues, it introduces its own set of challenges, including the potential for insufficient information and the misinterpretation of related data, as detailed in Sec 4.2.2. This subsection delves into various strategies devised to mitigate these challenges. Within the realm of retrieval-augmented generation, enhancement techniques can be broadly categorized into several pivotal areas: \n\n(1) The Normal Setting of Utilizing Retrieved Text for Generations (Sec 5.2.1): \n\n(2) Interactive Retrieval and Generation (Sec 5.2.2): Examples here include the integration of Chain-of-Thoughts steps into query retrieval (He et al., 2022) and the use of an LLMbased agent framework that taps into external knowledge APIs (Yao et al., 2023a). \n\n(3) Adapting LLMs to the RAG Setting (Sec 5.2.3): This involves methods like the one proposed by Peng et al. (2023), which combines a fixed LLM with a plug-and-play retrieval module. Another notable approach is REPLUG (Shi et al., 2023), a retrieval-augmented framework that treats the LLM as a black box and fine-tunes retrieval models using language modeling scores. \n\n(4) Retrieving from Additional Knowledge Bases (Sec 5.2.5 and Sec 5.2.4): This category includes methods that retrieve from external parametric memories (Chen et al., 2023a) or knowledge graphs (Zhang et al., 2023d) to enhance the model's knowledge base.",
            "score": 0.668875832104444,
            "section_title": "On Retrieval-Augmented Generation",
            "char_start_offset": 107043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 210,
                    "matchedPaperCorpusId": "253107178"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6044921875
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) Compared to standard text generation, RAG incorporates a retrieval module that accesses external knowledge to enhance generation quality (Lewis et al., 2020b;Guu et al., 2020;Zhu et al., 2023).The mainstream RAG follows a \"retrieve-then-read\" paradigm, where the retrieval module provides external knowledge as additional context, which is then read by generation models to produce the final output (Izacard et al., 2023;Shi et al., 2023;Ram et al., 2023;Borgeaud et al., 2022;Lin et al., 2023;Zhu et al., 2024).To optimize the use of external knowledge, some methods focus on crafting effective prompts that guide the utilization of retrieved information (Shi et al., 2023;Ram et al., 2023).These prompt-based methods are applicable to any LLM without tuning its parameters.However, they depend heavily on skillful prompt writing and the LLMs' ability to understand instructions.In contrast, other studies attempts to directly train the model to better use the retrieved knowledge.For example, REALM (Guu et al., 2020) and RETRO (Borgeaud et al., 2022) incorporate retrieval in end-to-end retrieval-augmented pretraining.RA-DIT (Lin et al., 2023) employs finetuning to enhance LLMs' retrieval understanding.These tuning-based methods often yield better performance than prompt-based methods by optimizing model parameters for RAG.However, they may compromise the LLMs' general capabilities, particularly in non-retrieval scenarios.Different from existing methods, we design a new lightweight tuning method for RAG.It is a plug-and-play module that enhances RAG performance using trainable virtual tokens, which can be removed in non-RAG scenarios to preserve the LLMs' general generation abilities.",
            "score": 0.6687863612203153,
            "section_title": "Related Work",
            "char_start_offset": 5432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 812
                },
                {
                    "start": 812,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1736
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 195,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 436,
                    "end": 458,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 492,
                    "end": 514,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1067,
                    "end": 1090,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "267750726",
            "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",
            "text": "In a Retrieval-Augmented Generation (RAG) system, a critical challenge is determining when to initiate the retrieval process. Several approaches have been proposed to address this issue: \n\n(1) Fine-tuning Large Language Models (LLMs) has proven effective but comes with substantial computational costs (Qin et al., 2023a;Lin et al., 2022). Some studies have focused on fine-tuning the LLM to mimic human-like web browsing behavior (Schick et al., 2023;Nakano et al., 2021). Self-RAG (Asai et al., 2023) introduces special tokens known as reflection tokens to regulate retrieval behavior. \n\n(2) Another intuitive approach involves evaluating the LLM's confidence based on the logits generated by the model (Jiang et al., 2021;Guo et al., 2017). FLARE (Jiang et al., 2023) dynamically activates RAG if the logits fall below a predefined threshold. \n\n(3) Other research has employed iterative prompting to determine if additional information is required (Wei et al., 2022;Liu et al., 2022;Rubin et al., 2022), or has combined Chain-of-Thought prompting (Wei et al., 2022) with RAG (Press et al., 2023;Khattab et al., 2022). For instance, ReAct (Yao et al., 2023) alternates between generating thoughts and actions, creating a sequence of thought-action-observation steps. \n\n(4) Evaluating the complexity or popularity of user questions to assess the need for retrieval is also a feasible approach (Mallen et al., 2023). SKR (Wang et al., 2023b) refers to similar questions it has previously encountered to determine the necessity of retrieval. \n\nDistinct from these existing methods, SlimPLM evaluates the necessity of retrieval by analyzing the answer generated by a smaller LLM. This approach does not increase LLM inference times while enhancing judgment accuracy.",
            "score": 0.6680872422114834,
            "section_title": "Retrieval Necessity Judgment",
            "char_start_offset": 7455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1540
                },
                {
                    "start": 1543,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1764
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 321,
                    "matchedPaperCorpusId": "258615343"
                },
                {
                    "start": 321,
                    "end": 338,
                    "matchedPaperCorpusId": "249191391"
                },
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 705,
                    "end": 725,
                    "matchedPaperCorpusId": "235078802"
                },
                {
                    "start": 725,
                    "end": 742,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 750,
                    "end": 770,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 969,
                    "end": 986,
                    "matchedPaperCorpusId": "231632658"
                },
                {
                    "start": 986,
                    "end": 1005,
                    "matchedPaperCorpusId": "245218561"
                },
                {
                    "start": 1050,
                    "end": 1068,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1078,
                    "end": 1098,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 1141,
                    "end": 1159,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1394,
                    "end": 1415,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1421,
                    "end": 1441,
                    "matchedPaperCorpusId": "263828724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "270845515",
            "title": "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
            "text": "Retrival Augmented Generation (RAG).As shown in Figure 2, RAG systems (Chen et al., 2024;Gao et al., 2023;Lewis et al., 2020;Li et al., 2022Li et al., , 2024) ) are comprised of three fundamental components: knowledge base, retriever, and LLM generator.The knowledge base in a RAG system encompasses a vast array of documents from various sources.For simplicity, we denote the knowledge base as K, comprising n documents, i.e., K = {D 1 , D 2 , . . ., D n }, where D i denotes the ith document.This knowledge base can be significantly large, often containing millions of documents from sources like Wikipedia (Thakur et al., 2021b).When a user submits a query, the retriever R identifies the top-m documents from the knowledge base that are most relevant to the query.This selection serves as the external knowledge to assist the LLM Generator G in providing an accurate response.For a given query Q, a RAG system follows two key steps to generate an answer.\n\n\u2776 Step 1-Knowledge Retrieval: The retriever employs two encoders: a query encoder h Q and a document encoder h D .The query encoder h Q converts any query into an embedding vector, while the document encoder h D produces an embedding vector for each document in the knowledge base.Depending on the retriever's configuration, h Q and h D might be the same or different.For a given query Q, the RAG system retrieves m documents (termed as retrieved documents) from the knowledge base K that exhibit the highest semantic similarity with Q.Specifically, for each document D j \u2208 K, the similarity score between D j and the query Q is computed by their inner product as\n\nFor simplicity, we omit h Q and h D and denote the set of m retrieved documents as R(Q; K), representing the documents from the knowledge base K with the highest similarity scores to the query Q.",
            "score": 0.6667906891007543,
            "section_title": "Background",
            "char_start_offset": 4403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 36,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 958
                },
                {
                    "start": 960,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1623
                },
                {
                    "start": 1625,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 89,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 106,
                    "end": 125,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "278170975",
            "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
            "text": "In recent years, Large Language Models (LLMs) have made significant progress in research and have grown increasingly popular [1]. However, LLMs face several challenges, including issues with hallucinations caused by insufficient context [2], as well as limitations in their learned content, which prevent them from addressing questions requiring specific or proprietary information [1]. To address these issues, [3] introduced Retrieval-Augmented Generation (RAG), which extends LLMs by integrating external knowledge sources for knowledge-intensive natural language processing (NLP) tasks. By incorporating domain-specific information, RAG systems enable tailored responses for specialized topics, improving accuracy, relevance, and contextual understanding. Since 2022, numerous RAG systems have demonstrated the effectiveness of this approach in overcoming some limitations of LLMs [1]. \n\nThese systems are applied across a wide range of NLP tasks and outperform in domain-specific scenarios by leveraging specialized knowledge to enhance performance and relevance [1]. RAG systems operate through three interconnected components: (1) indexing, which structures and organizes external knowledge bases; (2) retrieval, which identifies and extracts relevant documents from these sources; (3) and generation, which combines retrieved information with the input to produce a coherent, contextually relevant response using an LLM and prompt engineering techniques. These systems offer numerous customization options, including fine-tuning retrieval mechanisms, optimizing prompting techniques, refining generation models, and customizing knowledge base design [4], [5]. This flexibility raises the question of what settings should be used to configure RAG systems, and how these systems can be compared and evaluated to determine the most effective system for specific domains. This particular task of identifying optimal parameters in RAG systems is commonly referred to in the literature as RAG evaluation [6], [7]. During the RAG evaluation, each component of the RAG system is systematically evaluated within a predefined workflow to ensure that the system meets the overall quality standards [6]. The process begins with an input query from a prepared QA evaluation dataset, which is used to compute metrics such as retrieval accuracy and response relevance, providing statistical insight into the performance of the RAG components and thus a comprehensive understanding of their effectiveness [6].",
            "score": 0.6657036468409143,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2199
                },
                {
                    "start": 2200,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "261530162"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "271646356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "266933076",
            "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
            "text": "Retrieval augmented generation encompasses using documents to augment large language models through pre-training and at inference time [7,9,12]. Due to the compute cost, data preparation time and required resources using RAG without training or finetuning is an attractive proposition. However, challenges arise when using large language models for information extraction such as performance with long text [8]. \n\nA recent survey [19] showed that large language models are used across the RAG pipeline including retriever, data generation, rewriter, and reader. Our work complements this survey by taking a software engineering perspective to shine a light on what issues engineers will face and what software engineering research is necessary to realise solutions with the current state-of-the-art RAG systems. \n\nEmerging work has looked at benchmarking RAG systems [3] but not at the failures occurring during implementation. Software engineering research has investigated the use of RAG systems for code-related tasks [15]. However, the application of RAG systems is broader than software engineering tasks. This paper complements existing work by presenting challenges faced during the implementation of a RAG system with a focus on practitioners. \n\nErrors and failures that arise from RAG systems overlap with other information retrieval systems including 1) no metrics for query rewriting, 2) document re-ranking, and 3) effective content summarisation [19]. Our results confirm this The unique aspects are related to the semantic and generative nature of the use of large language models including evaluating factual accuracy [16].",
            "score": 0.6654578380237284,
            "section_title": "RELATED WORK",
            "char_start_offset": 4000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 140,
                    "end": 143,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "259860357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This joint training helps the generation model better utilize the retrieved information, and improves model synergy and generalization performance.However, this approach becomes impractical when the generation module is a large language model, which can have billions of parameters.On one hand, fine-tuning the full LLM is often infeasible due to the massive computational resources required; on the other hand, many existing LLMs are only accessible via APIs [30,31] and cannot be fine-tuned.\n\nTo overcome the infeasibility of fine-tuning LLMs in RAG, black-box RAG, which alternatively regards a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it without fine-tuning, has achieved success in knowledgeintensive tasks and gained much attention.Existing black-box RAG methods [36,44,53,54] typically fine-tune the retriever only based on LLMs' preferences (e.g., whether LLMs can give correct answer with the retrieved documents) and concatenate all the retrieved documents as the input, which suffers both effectiveness and efficiency issues.Only considering LLMs' preferences in retrieval causes ignorance of factual information, which can degenerate the effectiveness of RAG for it may mislead the retriever.As demonstrated in Figure 1, the LLM can answer correctly with the retrieved documents, but the documents themselves do not actually contain relevant factual information for the given question.For example, Q1 asks the location of the State Hermitage Museum; however, the retrieved document Fig. 2. The overview of FIT-RAG provides information about the Museum of Moscow.Although the LLM can give the correct answer, the retrieved document is actually unnecessary.If these unnecessary documents is used to reward the retriever, they can mislead the retriever.Besides, concatenating all the retrieved documents as the input causes waste of tokens, which can introduce excessive unnecessary tokens and hurts the efficiency of RAG.",
            "score": 0.6648629106112277,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 493
                },
                {
                    "start": 495,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "258960666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "271924151",
            "title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering",
            "text": "Retrieval-Augmented Generation (RAG) is a current research hotspot for Multi-hop QA tasks [10]. It can integrate the internal knowledge of the model with the external knowledge retrieved. LLM can retrieve external content through RAG to expand their knowledge base, thereby improving their ability to generate accurate and contextually relevant responses. Historically, various studies attempt to adapt the use of generative models to improve their performance. For instance, REPLUG [17] uses different retrieved content to generate corresponding answers and then combine them. Self-Rag [2] fine-tunes a generation model to simultaneously produce answers along with relevance, support, and usefulness scores. Concurrently, several methods for multi-hop QA emphasize the content and timing of retrieval. Self-Ask [15] lets the model generate sub-questions and queries, and continuously alternate between retrieval and generation. PROMPTAGATOR [4], Take a step back [32] focus on abstracting high-level concepts and utilizing LLMs for prompt-based query generation. Additionally, the confidence-based method, FLARE [8], generates queries using low-confidence tokens. However, most studies directly feed the retrieval content into the generation model, ignoring the evaluation and processing of the retrieval content. Unlike them, HiRAG highlights the importance of verifying retrieved content and adjusts the retriever to enhance the relevance of results when the quality of the retrieved information is subpar.",
            "score": 0.6646979859349712,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1509
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 94,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "272753572",
            "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines",
            "text": "RAG (Retrieval-Augmented Generation) is an effective strategy for enhancing model knowledge by retrieving relevant information from external sources (Fan et al., 2024). RAG has been leveraged in various scenarios including knowledge-intensive question answering (Borgeaud et al., 2022;Guu et al., 2020), machine translation (He et al., 2021), and hallucination elimination (B\u00e9chard & Ayala, 2024). Current works has focused on improving specific aspects of RAG. RG-RAG (Chan et al., 2024) proposes to refine the query for retrieval by decomposition and disambiguation. Self-RAG (Asai et al., 2023) incorporates the self-reflection of LLM to enhance the generation quality. The AI search engine could be viewed as a form of RAG with the Internet serving as the external knowledge source. Recently, MindSearch (Chen et al., 2024c) proposes an AI search engine framework to simulate the human minds in web information seeking. Meanwhile, multiple benchmarks of RAG (Yang et al., 2024b;Chen et al., 2024b) have been introduced to comprehensively evaluate a RAG system. However, both the current AI search engine and RAG benchmark are limited to the text-only setting, leaving the multimodal search engine and evaluation largely unexplored. To bridge this gap, we introduce MMSEARCH-ENGINE and MMSEARCH, a multimodal AI search engine pipeline and dataset designed to evaluate various multimodal scenarios.",
            "score": 0.664521980729584,
            "section_title": "A RELATED WORK",
            "char_start_offset": 36563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 262,
                    "end": 285,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 285,
                    "end": 302,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "236460076"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "269362546"
                },
                {
                    "start": 982,
                    "end": 1001,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "269214364",
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "text": "Our experimental results demonstrate several important findings:\n\nGains in accuracy from RAG with fine-tuning component models 1. RAG with fine-tuned retriever, fine-tuned generator, or full fine-tuning outperforms the generic RAG.Specifically, using the FinanceBench dataset, we achieved accuracy improvements of up to 20 percentage points over baseline RAG.\n\n2. Fine-tuning the retriever model results in higher accuracy gains compared to fine-tuned generators.This advantage is significant as fine-tuning embedding models for retrieval is less costly and less laborintensive than fine-tuning LLMs for generation.\n\nGains in accuracy from RAG with OODA Reasoning 1. Integrating iterative reasoning capabilities, such as OODA, with the RAG engine substantially enhances performance.Specifically, the Generic RAG with OODA Reasoning configuration achieves an accuracy increase of up to 50 percentage points across the FinanceBench dataset compared to the generic RAG baseline.\n\nFinanceBench dataset, the generic OODA configuration outperformed the fully fine-tuned RAG by a considerable margin of 20-25 percentage points.",
            "score": 0.6643777614294094,
            "section_title": "Key Findings",
            "char_start_offset": 22603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 66,
                    "end": 231
                },
                {
                    "start": 231,
                    "end": 359
                },
                {
                    "start": 361,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 615
                },
                {
                    "start": 617,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 975
                },
                {
                    "start": 977,
                    "end": 1120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "273025786",
            "title": "Generative AI Application for Building Industry",
            "text": "Text generation models have advanced significantly, with prominent examples including OpenAI's GPT-4 [33], Google's Gemini [34], and Meta's LLaMA [35]. These models leverage transformer architectures to produce coherent and contextually relevant text, making them invaluable for various applications such as chatbots, content creation, and more. Notably, models like LLaMA and Google's Gemini are available as open-source projects, allowing users to download and run them locally, providing flexibility for a wide range of use cases. \n\nOne of the most significant advancements in text generation is the adoption of Retrieval-Augmented Generation (RAG), a technique that greatly enhances the capabilities of language models by integrating external knowledge sources during the generation process [36]. Unlike traditional text Generation models, which rely solely on the pre-trained knowledge encoded within the model, RAG retrieves relevant information from indexed content to supplement the model's responses [37]. This approach allows for the generation of text that is not only more accurate but also more contextually relevant, as it incorporates upto-date or domain-specific information that the model may not have encountered during its initial training. \n\nRAG is versatile and can be applied to both SLMs and LLMs. By enabling these models to dynamically retrieve and utilize external data, RAG significantly improves their performance, making them particularly valuable in scenarios where the context, accuracy, and timeliness of information are critical [37]. \n\nThe implementation of RAG can vary in complexity, as illustrated by different pipeline configurations in Table 2 [38]: \n\n\u2022 Naive RAG involves basic indexing, standard retrieval methods based on query-document similarity, and straightforward text generation from the retrieved chunks. \u2022 Advanced RAG introduces more sophisticated techniques, such as hierarchical indexing for more efficient data organization, query rewriting to improve retrieval precision, and the use of dense retrievers for better semantic understanding. It also includes post-retrieval processes like reranking and filtering to prioritize key information, ultimately leading to higher-quality text generation. \u2022 Modular RAG represents a highly customizable approach, incorporating advanced indexing techniques like chunk optimization and hierarchical document organization. This configuration also supports pre-retrieval processes such as query expansion and transformation, and post-retrieval processes like compression and selection to refine the retrieved content.",
            "score": 0.6641844053393363,
            "section_title": "Text Generation Models",
            "char_start_offset": 9594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2413
                },
                {
                    "start": 2414,
                    "end": 2607
                }
            ],
            "ref_mentions": [
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72216796875
        },
        {
            "corpus_id": "268510182",
            "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
            "text": "We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.",
            "score": 0.6637037363719895,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "277622120",
            "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG",
            "text": "Retrieval-augmented generation (RAG), amalgamating an information retrieval component with a text generator model, is commonly used to mitigate the issues of hallucination and knowledge obsolescence in LLMs [24,35,45]. However, the goals of the retriever (retrieving more relevant information) and generator (extracting useful information to produce precise and coherent responses) in RAG are different and can be mismatched. To address this issue, current research focuses mainly on two approaches: (1) Utility judgments, which directly entails utilizing LLMs to identify useful retrieved information based on its utility for downstream tasks [79,80,82]. Utility judgments typically serve as post-processing steps for retrieval results and do not directly influence the retriever. (2) Utility-optimized retriever, which involves transferring the capability of LLMs to evaluate the utility of retrieved information to the retriever. Specifically, two primary optimization functions are commonly employed: (a) calculating the likelihood of the ground truth answers given the query and retrieval information [2,16,22,27,30,35,52,57,74]; (b) directly using evaluation metrics of the downstream generation tasks [17,66,76], such as exact match (EM), and ROUGE [36], and computing the performance difference between the generated answer and the ground truth answer. However, this approach relies on ground truth answers for specific downstream tasks and limits generalization.",
            "score": 0.6636117693084785,
            "section_title": "Utility-Focused RAG",
            "char_start_offset": 8345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1471
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 211,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 648,
                    "end": 651,
                    "matchedPaperCorpusId": "268733288"
                },
                {
                    "start": 651,
                    "end": 654,
                    "matchedPaperCorpusId": "273532096"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "258547128"
                },
                {
                    "start": 1118,
                    "end": 1121,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1127,
                    "end": 1130,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1212,
                    "end": 1215,
                    "matchedPaperCorpusId": "259281167"
                },
                {
                    "start": 1215,
                    "end": 1218,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "266977237",
            "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge",
            "text": "Retrieval-Augmented Generation (RAG) enhances Language Models by incorporating retrieved text, significantly improving performance in knowledgebased tasks, applicable in both fine-tuned and offthe-shelf scenarios (Gao et al., 2023;Gupta et al., 2024). Traditional RAG (Lewis et al., 2020), also known as Naive RAG, incorporates retrieval content to aid generation but faces key challenges: 1) varying retrieval quality, 2) generation of responses prone to inaccuracies, and 3) difficulties in coherently integrating retrieved-context with current tasks. To overcome the limitations of Naive RAG, advanced methods introduce more contextually rich information during inference. The DSP frame-  work (Khattab et al., 2022) facilitates an intricate exchange between frozen LMs and retrieval models, improving context richness, while PKG (Luo et al., 2023) allows LLMs to retrieve relevant information for complex tasks without altering their parameters. The working mechanism of WisdoM is similar to RAG, but different at the following aspects: \u2460 WisdoM utilizes LVLM to generate world knowledge to provide coherent and accurate context rather than retrieval, \u2461 WisdoM incorporates a contextual fusion mechanism to diminish noise within the context. For additional experimental analysis and discussion, please refer to \u00a7 5.3.",
            "score": 0.6627878018735993,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "268510197",
            "title": "RAFT: Adapting Language Model to Domain Specific RAG",
            "text": "While these approaches leverage in-domain learning they fail to prepare for the open-book nature of the test setting.\n\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval augmented generation (RAG).We propose a novel adaptation strategy -Retrieval-Augmented Fine Tuning (RAFT).RAFT specifically addresses the challenge of fine-tuning LLMs to both incorporate domain knowledge while also improving in-domain RAG performance.RAFT aims to not only enable models to learn domain-specific knowledge through fine-tuning, but also to ensure robustness against distracting retrieved information.This is achieved by training the models to understand the dynamics between the question (prompt), the domain-specific documents retrieved, and the right answer.Going back to our analogy to the open book exam, our approach is analogous to studying for an open-book exam by recognizing relevant, and irrelevant retrieved documents.\n\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate answer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic (2023), and in the presence of distractor documents (D k ).We explain the methodology in Section 3 and analyze the sensitivity to the number of distractor documents (k) at train-and test-time in Section 5. RAFT consistently outperforms Supervised-finetuning both withand without-RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018), andHuggingFace Hub, Torch Hub, andTensorflow Hub Gorilla datasets Patil et al. (2023), presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.Our code is available at https://github.com/ShishirPatil/gorilla.",
            "score": 0.6619671608967928,
            "section_title": "Introduction",
            "char_start_offset": 2208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 119,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 948
                },
                {
                    "start": 950,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 1097,
                    "end": 1114,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "266933076",
            "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
            "text": "The new advancements of Large Language Models (LLMs), including ChatGPT, have given software engineers new capabilities to build new HCI solutions, complete complex tasks, summarise documents, answer questions in a given artefact(s), and generate new content. However, LLMs suffer from limitations when it comes to up-to-date knowledge or domain-specific knowledge currently captured in company's repositories. Two options to address this problem are: a) Finetuning LLMs (continue training an LLM using domain specific artifacts) which requires managing or serving a fine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG) Systems that rely on LLMs for generation of answers using existing (extensible) knowledge artifacts. Both options have pros and cons related to privacy/security of data, scalability, cost, skills required, etc. In this paper, we focus on the RAG option. \n\nRetrieval-Augmented Generation (RAG) systems offer a compelling solution to this challenge. By integrating retrieval mechanisms with the generative capabilities of LLMs, RAG systems can synthesise contextually relevant, accurate, and up-to-date information. A Retrieval-Augmented Generation (RAG) system combines information retrieval capabilities, and generative prowess of LLMs. The retrieval component focuses on retrieving relevant information for a user query from a data store. The generation component focuses on using the retrieved information as a context to generate an answer for the user query. RAG systems are an important use case as all unstructured information can now be indexed and available to query reducing development time no knowledge graph creation and limited data curation and cleaning. \n\nSoftware engineers building RAG systems are expected to preprocess domain knowledge captured as artifacts in different formats, store processed information in appropriate data store (vector database), implement or integrate the right query-artifact matching strategy, rank matched artifacts, and call the LLMs API passing in user queries and context documents. New advances for building RAG systems are constantly emerging [8,12] but how they relate and perform for a specific application context has to be discovered. \n\nIn this work we present the lessons learned and 7 failure points arising from 3 case studies.",
            "score": 0.6610176327041437,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2219
                },
                {
                    "start": 2222,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 2124,
                    "end": 2127,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 2127,
                    "end": 2130,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "276422305",
            "title": "Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.",
            "score": 0.6607895377857065,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "273638614",
            "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
            "text": "It trains the model to adaptively retrieve passages, generate text, and reflect on its own outputs using special tokens called reflection tokens. This method allows the model to critique and refine its responses, reducing redundancy by discouraging the inclusion of irrelevant or repetitive information. \n\nWe introduce a new model, ChunkRAG that emphasizes an innovative chunking strategy aimed at further reducing redundancy and improving the effectiveness of RAG models. Our approach involves segmenting documents into semantically coherent and non-overlapping chunks that are more aligned with the specific information needs of the query. \n\nDespite advancements in Retrieval-Augmented Generation systems like RAG, CRAG, Self-RAG, and Self-CRAG, limitations persist in effectively retrieving and utilizing relevant information due to fixed size chunking, static relevance thresholds, and single-pass relevance scoring. Our approach addresses these issues through several key innovations: implementing semantic chunking for topically coherent chunks, employing LLM-based query rewriting to refine user queries, introducing advanced LLM-based relevance scoring with self-reflection and a critic LLM for more accurate assessments, utilizing dynamic threshold determination to adaptively filter relevant information, and incorporating initial filtering methods to improve efficiency by reducing redundancy and prioritizing chunks. These enhancements collectively surpass traditional models by providing more precise retrieval and contextually appropriate answers, leading to significant improvements in performance as demonstrated by standard evaluation metrics.",
            "score": 0.6605871966069692,
            "section_title": "Literature Review",
            "char_start_offset": 6006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1660
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68505859375
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model. \u2022 RL from LLM/human feedback. Reinforcement learning based on feedback from the final generated answers. In addition to using human evaluations, powerful LLMs can also serve as an evaluative judge. 3) Dual FT: In the RAG system, fine-tuning both the retriever and the generator simultaneously is a unique feature of the RAG system. It is important to note that the emphasis of system fine-tuning is on the coordination between the retriever and the generator. An exemplary implementation is RA-DIT [27], which fine-tunes both the LLM and the retriever. The LM-ft component updates the LLM to maximize the Fig. 16. Generator fine-tuning pattern, The main methods include SFT, distillation and RL from LLM/human feedback. Fig. 17. Dual fine-tuning pattern. In this mode, both the retriever and generator participate in fine-tuning, and their preferences will be aligned. likelihood of the correct answer given the retrieval-augmented instructions while the R-ft component updates the retriever to minimize the KL-Divergence between the retriever score distribution and the LLM preference.",
            "score": 0.6603601198799904,
            "section_title": "E. Tuning Pattern",
            "char_start_offset": 57340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "275922066",
            "title": "Provence: efficient and robust context pruning for retrieval-augmented generation",
            "text": "Retrieval-Augmented Generation (RAG) has become a widely-used paradigm for improving factuality, attribution, and adaptability of Large Language Models (LLMs) (Das et al., 2019;Asai et al., 2024;Seo et al., 2019;Lewis et al., 2020;Mallen et al., 2023a;Min et al., 2023). Augmenting a given user's query with retrieved relevant contexts helps to avoid the generation of untruthful information and enables the provision of references used to generate the answer. Furthermore, using a domain-specific datastore may enable access and reasoning over a previously unknown knowledge -without fine-tuning the LLM. One additional advantage of the RAG approach is the easy plug-and-play architecture (LangChain): practitioners may choose components (retrievers, generator LLMs, context granularity etc.) which best suit their particular cases to maximize the final performance. At the same time, the use of RAG adds computational overhead due to both retrieval latency and the increased input length for the LLMs. It may also propagate irrelevant information present in retrieved contexts into generated responses. These issues can be solved by developing more efficient and robust LLMs -either by making architectural changes to process long contexts more efficiently (Nawrot et al., 2024;Dao, 2024;Chevalier et al., 2023;Louis et al., 2025) or increasing the diversity of the tuning data to improve processing of irrelevant contexts (Lin et al., 2024). However, tuning the LLM can be highly resource-consuming, or even impossible to apply for proprietary (closed) LLMs. An alternative solution consists in pruning retrieved contexts by removing context parts irrelevant to the user's query -which reduces context lengths and therefore speeds up generation. Such context pruning module can be used in a plug-and-play manner with any generator LLM, featuring both easy use and better transparency in the RAG pipeline. \n\nDespite initial efforts on developing context pruners for RAG, none of the existing solutions provide a model ready to be used out-of-the-box in practice.",
            "score": 0.660251336343685,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "85449634"
                },
                {
                    "start": 231,
                    "end": 252,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1280,
                    "end": 1290,
                    "matchedPaperCorpusId": "259936734"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61865234375
        },
        {
            "corpus_id": "278020653",
            "title": "ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE",
            "text": "The Retrieval-Augmented Generation (RAG) system consists of two primary components: a Retriever and a Generator, which work together to enhance response accuracy and relevance. In the context of a mental health support system, RAG enables AI-powered assistance by retrieving evidence-based mental health information and generating personalized, context-aware responses.",
            "score": 0.6601709041954602,
            "section_title": "System Architecture and Workflow",
            "char_start_offset": 4445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 369
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences. \n\nMethods like RECITE [104] and ITER-RETGEN [105] focus on improving factual accuracy and integrating retrieved knowledge, ensuring grounded, accurate responses through knowledge recitation and iterative retrieval-generation. Frameworks such as Selfmem [106] and Self-RAG [107] enhance factual consistency via self-reflection. Techniques like Step-Back Prompting [108] improve reasoning by guiding LLMs to abstract concepts. The GENREAD approach [109] replaces traditional retrieval with LLM-generated contextual documents, demonstrating superior performance in knowledgeintensive tasks like open-domain QA and fact-checking. \n\nIterative and active retrieval-generation strategies aim to dynamically enhance both retrieval and generation processes. ITER-RETGEN [105] alternates between retrieval and generation to improve response quality, while FLARE [110] predicts and adapts to future information needs during generation. These methods boost relevance and accuracy in knowledge-intensive tasks. The Adaptive-RAG framework [111] selects retrieval strategies based on query complexity, outperforming static models in efficiency and accuracy. FunnelRAG [112] proposes a progressive retrieval paradigm with coarse-to-fine granularity for RAG, to enable load balancing and improve retrieval performance. \n\nReducing hallucinations in response is a critical challenge in RAG systems. To address this, various strategies focus on improving the trustworthiness of generated outputs by leveraging more reliable retrieval mechanisms and robust post-generation processes. Methods such as RAGTruth [113] introduce large-scale datasets designed to detect and mitigate hallucinations, enabling models to generate more trustworthy responses. SEER [114] proposes a novel evidence extraction learning paradigm, which utilizes the model to calibrate its extraction preference via self-alignment. Ayala et al. [115] use external knowledge sources to reduce errors in structured output generation, enhancing the reliability of RAG systems in practical applications. \n\nTask-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks.",
            "score": 0.6600334859993202,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 67680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2253
                },
                {
                    "start": 2256,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 464,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 478,
                    "end": 483,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1058,
                    "end": 1063,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1359,
                    "end": 1364,
                    "matchedPaperCorpusId": "273346467"
                },
                {
                    "start": 1940,
                    "end": 1945,
                    "matchedPaperCorpusId": "273351413"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64697265625
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "Baseline Mistral-7B-Instruct Establish the degree of pretraining and instruct-tuning knowledge within the model LoRA Finetuning\n\nFine-tuning with Low-Rank Adaptation NEFTune Finetuning Fine-tuning with Noise-Enhanced Fine-Tuning Instruct LLM + RAG (Raw)\n\nRetrieval-Augmented Generation with raw articles Instruct LLM + RAG (Extracted) RAG with auto-extracted data Best Combination\n\nBest fine-tuning + RAG approach",
            "score": 0.6597918437197137,
            "section_title": "Method Description",
            "char_start_offset": 45534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 129,
                    "end": 253
                },
                {
                    "start": 255,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "267312595",
            "title": "Corrective Retrieval Augmented Generation",
            "text": "But a considerable portion of the text within these retrieved documents is often nonessential for generation, which should not have been equally referred to and involved in RAG. \n\nOn account of the above issues, this paper particularly studies the scenarios where the retriever returns inaccurate results. \n\nA method named Corrective Retrieval-Augmented Generation (CRAG) is proposed to self-correct the results of retriever and improve the utilization of documents for augmenting generation. A lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query. This serves as a crucial component in RAG, contributing to informative generation by reviewing and evaluating the relevance and reliability of the retrieved documents. A confidence degree is quantified based on which different knowledge retrieval actions of {Correct, Incorrect, Ambiguous} can be triggered. For the latter two actions, large-scale web searches (Piktus et al., 2021;Komeili et al., 2022) are integrated as a strategic extension, since retrieval from static and limited corpora can only return sub-optimal documents in terms of scope and diversity. This augmentation is implemented to broaden the spectrum of retrieved information, harnessing the expansive and dynamic nature of the web to complement and enrich the initially obtained documents. Furthermore, to eliminate redundant contexts contained in retrieved documents that are unhelpful for RAG, a decompose-then-recompose algorithm is meticulously crafted throughout the retrieval and utilization process. This algorithm ensures the refinement of retrieved information, optimizing the extraction of key insights and minimizing the inclusion of non-essential elements, thereby enhancing the utilization of retrieved data. \n\nCRAG is plug-and-play and experimentally implemented into RAG (Lewis et al., 2020) and Self-RAG (Asai et al., 2024) for demonstrating its adaptability to RAG-based approaches.",
            "score": 0.6587569072336636,
            "section_title": "Introduction",
            "char_start_offset": 2100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 987,
                    "end": 1008,
                    "matchedPaperCorpusId": "236034557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72021484375
        },
        {
            "corpus_id": "273696073",
            "title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval",
            "text": "Retrieval-Augmented Generation (RAG). Over the past few years, interest has been growing in conditioning LLM outputs on retrieved information (Chen et al., 2017;Lee et al., 2019;Guu et al., 2020;Lewis et al., 2020;Lazaridou et al., 2022;Asai et al., 2024). This strategy seeks to make LLM systems more efficient, updatable, and transparent by decoupling the system's knowledge from the model parameters. This makes it easy to update the knowledge corpus and also makes it possible to inspect the sources relied upon when LLMs produce factual statements. \n\nMulti-Hop Retrieval. The standard RAG formulation is best suited for \"simple\" questions, where a direct search can find all the information required for producing responses. Beyond these, benchmarks such as HotPotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), and HoVer (Jiang et al., 2020) assess systems on gathering and synthesizing information from several independent documents within a massive corpus like Wikipedia. To tackle retrieval in this setting, early systems like MDR (Xiong et al.) and Baleen (Khattab et al., 2021) introduced bespoke strategies for finetuning the retrieval models that produce representations of queries and documents, adapting them directly for compositional search queries. Unfortunately, fine-tuning the retriever representations is a data hungry approach that is also challenging to scale to massive corpora like the Web, as re-training the retriever often requires re-indexing the corpus. Increasingly, research in this space (Trivedi et al., 2023), Figure 2: An overview of the standard multi-hop retrieval pipeline we study in this work. A user asks a question to the system. In each hop, the LLM generates search queries for the retriever and receives a collection of documents. The overall set of retrieved documents and the user question are then given to a downstream LLM for grounded answer generation.",
            "score": 0.6586988813608425,
            "section_title": "RELATED WORK",
            "char_start_offset": 3883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1913
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 161,
                    "end": 178,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 178,
                    "end": 195,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 195,
                    "end": 214,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 237,
                    "end": 255,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 772,
                    "end": 791,
                    "matchedPaperCorpusId": "52822214"
                },
                {
                    "start": 835,
                    "end": 855,
                    "matchedPaperCorpusId": "226278099"
                },
                {
                    "start": 1074,
                    "end": 1096,
                    "matchedPaperCorpusId": "230437663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.724609375
        },
        {
            "corpus_id": "276249796",
            "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
            "text": "Recent advances in retrieval-augmented generation (RAG) for large language models (LLMs) have demonstrated remarkable capabilities in various tasks (Anthropic, 2024; Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). Hurst et al., 2024;Dubey et al., 2024;Yang et al., 2024a;Mesnard et al., 2024;Asai et al., 2024;Chen et al., 2024a;b;Wei et al., 2025;Sun et al., 2025b;a), empowering LLMs to acquire up-to-date or domain-specific knowledge while mitigating hallucinations (Gao et al., 2023;Fan et al., 2024;Qiao et al., 2024). The effectiveness of RAG systems, however, hinges on the alignment1 between the retriever and the LLM-an inherently challenging goal as these components are typically developed independently without co-training. This lack of co-training can result in semantic mismatch and suboptimal interactions: retrievers may fail to provide information tailored to the LLM's needs, while LLMs may struggle to generate effective queries or seamlessly incorporate retrieved content. Existing approaches address this misalignment through three main strategies: (1) fine-tuning retrievers to align with LLM preferences, (2) optimizing LLMs to adapt to retriever behavior, and (3) introducing intermediate modules to bridge the gap between them (Ma et al., 2023;Shi et al., 2024;Asai et al., 2024;Wei et al., 2025;Yu et al., 2024a;b). Despite progress, these methods face notable challenges: fine-tuning retrievers often requires carefully curated data and may not be feasible for commercial search engines (Schmidt, 2014;Nakano et al., 2021), while optimizing LLMs is resource-intensive and risks compromising their original capabilities (Zhou et al., 2024).",
            "score": 0.6573345541390989,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 401,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "270559546"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "271909559"
                },
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "270225999"
                },
                {
                    "start": 578,
                    "end": 595,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1360,
                    "end": 1377,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1377,
                    "end": 1395,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1395,
                    "end": 1412,
                    "matchedPaperCorpusId": "271909559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "278033562",
            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
            "text": "Large Language Models (LLMs) have continuously advanced, demonstrating performance levels that increasingly surpass human capabilities (Achiam et al., 2023;Dubey et al., 2024). Despite their expanding knowledge base, the capacity of parametric knowledge within LLMs is inherently limited (Yu et al., 2023a;Lewis et al., 2020). As a result, LLMs face challenges when responding to information that emerges after their training period or when encountering data that is underrepresented within their training corpus (Mallen et al., 2023;Kasai et al., 2023). \n\nTo address these limitations, Retrieval-Augmented Generation (RAG) systems have been proposed as a practical solution (Lu et al., 2023;Gao et al., 2023;Fan et al., 2024;Hofst\u00e4tter et al., 2023). RAG enhances LLM performance by integrating external, non-parametric knowledge retrieved by a retrieval system, thereby extending the model's ability to respond accurately beyond its parametric knowledge (Vu et al., 2023). Research has demonstrated that RAG techniques improve domain adaptability (Hsieh et al., 2023) and mitigate hallucination issues (Ji et al., 2023). \n\nHowever, while RAG systems have advanced rapidly, research on robust and comprehensive evaluation methods lags behind. We identify several key challenges in the evaluation of RAG systems. First, the retrieval pools used for evaluation are often excessively large, making the process resourceintensive and inefficient (Mallen et al., 2023;Zhao et al., 2024). For instance, many studies rely on the Wikipedia snapshot2 , containing over five million entries, for evaluating retrievers and RAG systems (Lu et al., 2023;Izacard and Grave, 2021). Indexing such large datasets incurs significant computational costs and introduces substantial delays. \n\nSecond, current evaluation methods for RAG systems tend to focus disproportionately on performance improvements, often overlooking the complex dynamics between retrieval and generation (Xie et al., 2024;Ru et al., 2024).",
            "score": 0.6573235605473284,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 306,
                    "matchedPaperCorpusId": "252408513"
                },
                {
                    "start": 306,
                    "end": 325,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 513,
                    "end": 534,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 534,
                    "end": 553,
                    "matchedPaperCorpusId": "251105205"
                },
                {
                    "start": 675,
                    "end": 692,
                    "matchedPaperCorpusId": "258212542"
                },
                {
                    "start": 709,
                    "end": 726,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 726,
                    "end": 750,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1104,
                    "end": 1121,
                    "matchedPaperCorpusId": "266176951"
                },
                {
                    "start": 1442,
                    "end": 1463,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 1624,
                    "end": 1641,
                    "matchedPaperCorpusId": "258212542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
            "score": 0.6571550227143843,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "273798397",
            "title": "Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction Tuning a Word-Embedding based Retrieval Augmented Large Language Model",
            "text": "With Retrieval Augmented Generation (RAG), instead of having the same hardcoded domain examples appended for every query, examples are selected dynamically based on their similarity with the input query. To achieve this, embeddings for domain examples are at first computed using a pre-trained universal embedder and the examples along with their embeddings are stored in a vector database. During inference, when a query comes in, its embedding is computed using the same embedder. Then similarity scores of the query embedding with all the embeddings stored in the vector db are computed and the top k most similar examples are selected and appended to the prompt, which is then sent to the LLM for response generation. The RAG framework is largely made up of two components \u2022 Retriever: It is responsible for generating query embedding during inference using the embedder and also for extracting top k most similar examples from vector DB using a similarity function. \n\n\u2022 Generator: The prompt, made up of the user query and the top k examples obtained by the retriever are passed along to the generator component (in our case an LLM) which is responsible for generating a response. The complete RAG architecture is shown in Figure 1. RAG with LLM yields good results for Few-Shot Cross-Domain NER when using proprietary LLMs like GPT4. However, most open source LLMs struggle to produce output in the format specified as part of prompt instruction. This becomes a challenge, because using GPT4 to perform NER for applications at scale can be extremely costly. Therefore, we need to finetune open source LLMs so that they can follow prompt instructions.",
            "score": 0.6571181594459291,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 10463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "270215715",
            "title": "Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) [30] has emerged as a standard practice to enhance the performance of LLMs, aiming to mitigate their problems of \"hallucinations\" and knowledge cut-off.An RAG system typically includes a Retriever that extracts relevant information from an external knowledge database, and a backbone LLM to generate grounded responses by considering the given relevant information (in-context learning [10]).\n\nTechnological advancements in RAG have addressed fundamental questions such as \"what should be retrieved\", \"when should retrieval occur\", and \"how should the retrieved information be utilized\".In terms of \"what to retrieve\", research has evolved from simple token retrieval [28] and entity retrieval [34] to more complex structures like chunks [37] and knowledge graphs [25].Granularity matters a lot in retrieval, coarse-granularity-retrieval yields more information but with lower precision, while fine-granularity-retrieval offers comprehensive information at the cost of efficiency.The question of \"when to retrieve\" has led to strategies ranging from single [49,42] to adaptive [20,17] and multiple retrieval [19] methods.Regarding \"how to use\" the retrieved data, integration techniques have been developed for various levels of the model architecture, including the input [29], intermediate [3], and output layers [31].Although integration at the intermediate and output layers is more effective, challenges remain concerning training requirements and efficiency limitations.",
            "score": 0.6567389030144739,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 6462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 429
                },
                {
                    "start": 431,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 806
                },
                {
                    "start": 806,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1513
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1352,
                    "end": 1356,
                    "matchedPaperCorpusId": "258078950"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "269758033",
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "text": "In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21].Appendix A details the complete RAG structure.Figure 1 illustrates the structure of the RAG systems as mentioned.\n\nFig. 1: The structure of the RAG system with retrieval and generation components and corresponding four phrases: indexing, search, prompting and inferencing.The pairs of \"Evaluable Outputs\" (EOs) and \"Ground Truths\" (GTs) are highlighted in read frame and green frame, with brown dashed arrows.\n\nThe importance of evaluating RAG is increasing in parallel with the advancement of RAG-specific methodologies.On the one hand, RAG is a complex system intricately tied to specific requirements and language models, resulting in various evaluation methods, indicators, and tools, particularly given the black-box LLM generation.Evaluating RAG systems involves specific components and the complexity of the overall system assessment.On the other hand, the complexity of RAG systems is further compounded by the external dynamic database and the various downstream tasks, such as content creation or open domain question answering [16,70].These challenges necessitate the development of comprehensive evaluation metrics that can effectively capture the interplay between retrieval accuracy and generative quality [2,7].To clarify the elements further, we try to address the current gaps in the area, which differs from the prior RAG surveys [74,16,24] that predominantly collected specific RAG methods or data.We have compiled 12 distinct evaluation frameworks, encompassing a range of aspects of the RAG system.Following the procedure of making benchmarks, we analyze through targets, datasets and metrics mentioned in these benchmarks and summarize them into A Unified Evaluation Process of RAG (Auepora) as three corresponding phases.\n\nFor this paper, we contribute in the following aspects:\n\n1. Challenge of Evaluation: This is the first work that summarizes and classifies the challenges in evaluating RAG systems through the structure of RAG systems, including three parts retrieval, generation, and the whole system.",
            "score": 0.6560314304173129,
            "section_title": "Introduction",
            "char_start_offset": 2079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 377
                },
                {
                    "start": 379,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 673
                },
                {
                    "start": 675,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1681
                },
                {
                    "start": 1681,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 2008
                },
                {
                    "start": 2010,
                    "end": 2065
                },
                {
                    "start": 2067,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 184,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "235544863"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "13822774"
                },
                {
                    "start": 1306,
                    "end": 1309,
                    "matchedPaperCorpusId": "253523033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "271865600",
            "title": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs",
            "text": "We proposed a approach named WeKnow-RAG for End-to-End Retrieval-Augmented Generation, as illustrated in Fig. 1. The pipeline comprises a KG workflow and a web search workflow to address the End-to-End Retrieval-Augmented Generation challenge, ultimately integrating both effectively. We will detail each component of the pipeline in the following sections.",
            "score": 0.6556867384187381,
            "section_title": "Methods",
            "char_start_offset": 11246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 357
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68701171875
        },
        {
            "corpus_id": "273323460",
            "title": "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation",
            "text": "Retrieval Augmented Generation (RAG). RAG systems (Lewis et al., 2020) leverage external knowledge bases to assist language models (LMs) in responding to user queries. A RAG system is composed of a retriever \u03d5 and a generator \u03b8. The retriever is connected to a corpus of knowledge document chunks D = {d i } n i=1 , where d i \u2208 V and V denotes the space of natural texts. Given a user query q \u2208 V, the retriever retrieves from D the k-most relevant documents D r = {d r i } k i=1 \u2282 D. The generator is an LM that generates an answer a according to the query q and the retrieved documents D r . The overall framework can be described as P(a|q, D) = P \u03d5 (D r |q)P \u03b8 (a|q, D r ), where P \u03d5 (D r |q) and P \u03b8 (a|q, D r ) denotes the retrieval and generation process respectively. Embedding Model. Most modern RAG systems utilize vector databases to construct retriever (Gao et al., 2023). In this setting, an embedding model E \u03d5 : V \u2192 R m is employed to encode the document chunks as a set of vector representations in a m-dimensional latent space that captures semantic similarity1 . The retriever retrieves a document according to its measured similarity to the query, i.e., P \u03d5 (d|q) \u221d exp(S(E \u03d5 (d), E \u03d5 (q))), where S : R m \u00d7 R m \u2192 R is a similarity metric. Common choices of S include cosine similarity and dot product. \n\nGoodness-of-Fit (GoF) Test. A GoF test is a statistical procedure for comparing an observation x to an expected distribution P. Formally, it decides between a null hypothesis H 0 : x \u223c P indicative of fitness and an alternative hypothesis H 1 : x \u0338 \u223c P. In a one-sided right-tailed test, the test statistic t(x) is compared to the critical value c, and H 0 is rejected if t(x) \u2265 c.",
            "score": 0.6554361435209408,
            "section_title": "Background",
            "char_start_offset": 4496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 70,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "Due to the extraordinarily large number of parameters, fine-tuning large language models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a retrieval-augmented generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs\u2019 preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this article proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer which takes the factual information and LLMs\u2019 preferences as labels respectively. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer, which enables FIT-RAG to avoid unnecessary augmentation and reduce augmentation tokens as much as possible. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ, and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.",
            "score": 0.6551573241459369,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "273749074",
            "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
            "text": "This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
            "score": 0.6548520479698565,
            "section_title": "abstract",
            "char_start_offset": 2,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "The non-parametric knowledge injection is performed using RAG, which consists of two components: the Retriever and the Generator [6,15]. Retriever. The first key component in a RAG system is a retriever , which builds an index for a document corpus . During inference, given an input sequence , the retriever identifies and ranks relevant documents   = (, ). In our retrieval process, we employ both sparse and dense retrievers. We utilize BM25 [44] as a sparse retriever due to its popularity and effectiveness. For dense retrievers, we employ DPR [28] and Contriever [22] methods. Both models convert textual data into vector representations using a transformer network. The similarity between the query  and document  is defined as  (, ) = \u00ec  \u2022 \u00ec , which computes the dot product between embedding vectors \u00ec  and \u00ec . DPR employs two independent BERT models, trained discriminatively using querydocuments pairs with negative samples from BM25. Contriever, on the other hand, is trained using a shared BERT model for query and document encoding, optimized using a contrastive loss. We also employ a two-step retrieval pipeline, which includes first-stage retrieval using BM25 and reranking using DPR [1, 7,34]. Generator. The second step involves a generator component responsible for synthesizing an answer, typically implemented via LMs. Generative LMs operate by predicting the probability distribution of the next token, given the previous tokens. In RAG, the generative LM takes a query  and top- ranked documents from   , denoted as    = [ 1 , ...,   ], and generates a response by sequentially predicting the next token. Our RAG prompt prepends the documents before the query, following [15,39]. \n\nIn this paper, we define and assess four distinct configurations of injecting knowledge with fine tuning and RAG: (1) -FT-RAG: the vanilla LM without retrieved documents, (2) -FT+RAG: the vanilla LM with retrieved documents, (3) +FT-RAG: the fine-tuned LM without retrieved documents, (4) +FT+RAG: the fine-tuned LM with retrieved documents.",
            "score": 0.6543231839473775,
            "section_title": "Knowledge Injection with RAG",
            "char_start_offset": 16008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "2218552"
                },
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "249097975"
                },
                {
                    "start": 1205,
                    "end": 1207,
                    "matchedPaperCorpusId": "264350276"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 1699,
                    "end": 1702,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "273695367",
            "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
            "text": "The Standard RAG framework has better performance than the above non-retrieval or self-verifying methods, highlighting the importance of retrieved documents for knowledge-intensive queries. However, their low performance is still unsatisfactory, suggesting that their principles of retrieval and generation are weak and leave much to be desired. In the experiments, we illustrate that the performance can be further improved under the guidance of rules from two perspectives: through in-context learning (ICL) in RuleRAG-ICL and through RGFT in RuleRAG-FT. \n\nFor RuleRAG-ICL (RG-DPR + LLAMA2_7B), introducing rules in the retrieval stage alone enhances the recall performance of the retriever and further improves the answer accuracy of the original LLAMA2_7B. RuleRAG-ICL (RG-DPR + RG-LLAMA2_7B) consistently surpasses Standard RAG across various metrics (+9.3 in R@10, +5.9 in EM and +3.2 in T-F1 on average absolute performance over all five benchmarks), achieving the improved performance. This confirms the sub-optimal ability of the current RAG and the effectiveness of our proposed dual rule-guided retriever and generator. \n\nFor RuleRAG-FT, our proposed RGFT can amazingly improve performance by a significant margin (+45.7 in R@10, +24.2 in EM and +15.3 in T-F1 compared to the best performance of RuleRAG-ICL). To further corroborate that these gains are due to the introduced rules, we first isolate the key component, rules, from fine-tuning data F R for RGFT, to form the standard supervised fine-tuning (SSFT) (Rule Ablation in Table 2) and then isolate the impact of the fine-tuned generator from the fine-tuned retriever in RuleRAG-FT (RGFT Ablation in Table 2). RGFT Ablation shows both RGFT-DPR and RGFT-LLAMA2_7B are beneficial when used individually, implicitly suggesting that the two phases do not depend on each other.",
            "score": 0.6542406008633959,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 21869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1841
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "270869499",
            "title": "First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1",
            "text": "Retrieval Augmented Generation (RAG) technology [2,6,7,9,15] in the field of natural language processing represents an innovative breakthrough.Traditional NLP techniques primarily rely on large language models, but their accuracy and depth may be limited when dealing with complex queries that require extensive background knowledge.To overcome this limitation, RAG combines conventional information retrieval methods with modern generative language models, aiming to enhance the model's text generation capabilities by incorporating external knowledge sources.The core principle is to integrate retrieval and generation techniques, allowing the model to access and utilize a vast amount of external information before generating text.RAG excels in addressing knowledge-intensive NLP tasks such as question answering, fact verification, and more.In recent years, RAG systems have evolved from a primary stage to an advanced stage, and then to a modular stage, to improve performance, cost-effectiveness, and   crease the difficulty of the pre-training task, gradually increasing the proportion of masking as the number of epochs increased.Specifically, we set an initial mask proportion of 0.3, and after every 10 epochs of pre-training, we perform fine-tuning of the downstream task.If the performance of the fine-tuning is lower than the previous one, we increase the mask proportion by 0.05 and continue with pre-training.Ultimately, we increase the number of pre-training epochs to 140, which significantly improves the text generation performance of the downstream task.For the construction of the training set with retrieval knowledge, we use D(Description) as the query and calculate the similarity with the key of each key-value pair in the knowledge base (e.g., vector inner product, L2 distance, or cosine similarity).If the similarity is larger than the threshold k, we call it an effective retrieval.We retrieve this key-value pair and concatenate the value to the end of the query as the new training sample corresponding to the query.For the val set and test set, we use the same retrieval method to construct the val set and test set with retrieval knowledge.Retrieval Iterations.For the first retrieval augmentation, the embeddings of key-value pairs are computed using a model trained on a training set without a knowledge base.",
            "score": 0.6542197543014553,
            "section_title": "Retrieval Augmentation in NLP",
            "char_start_offset": 3769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 1912
                },
                {
                    "start": 1912,
                    "end": 2048
                },
                {
                    "start": 2048,
                    "end": 2174
                },
                {
                    "start": 2174,
                    "end": 2195
                },
                {
                    "start": 2195,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "252735160"
                },
                {
                    "start": 51,
                    "end": 53,
                    "matchedPaperCorpusId": "264426178"
                },
                {
                    "start": 53,
                    "end": 55,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 55,
                    "end": 57,
                    "matchedPaperCorpusId": "267053546"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "247058346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55517578125
        },
        {
            "corpus_id": "271915646",
            "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
            "text": "The domain of Natural Language Processing (NLP) has witnessed substantial progress [14,21,30,31,42], especially through the advent of Large Language Models (LLMs) [2,17,18,27,35]. These models show exceptional text generation proficiency, yielding high fluency and readability outputs [32,39]. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG). \n\nSupervised Fine-Tuning (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model's parameters to align with the data distribution and task requirements, ensuring the model's behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research. Ouyang et al. [18] pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent. Zhou et al. [41] compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data. \n\nRetrieval Augmented Generation (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation [9,15]. RAG has been further enhanced by a range of innovative techniques: fine-tuning retrieval models to obtain precise semantic representations [11,28,33], reformulating queries to align with the semantic space of queries and documents [8,20,29], fine-tuning LLMs to harmonize the output of the retriever with the LLM's preference [10,22,34]. \n\nIn our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.",
            "score": 0.6540379131757683,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "259950027"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "259858754"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "254998782"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "273026102",
            "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
            "text": "Baselines without retrievals. We compare ours with several strong, publicly available pre-trained LLMs, including Llama2-7B,13B (Touvron et al., 2023), SAIL-7B (Luo et al., 2023) as well as instruction-tuned models, Alpaca-7B,13B (Dubois et al., 2023). Additionally, we consider models trained and reinforced with private data such as ChatGPT (Ouyang et al., 2022). For instructiontuned LMs, we utilize the official system prompt or instruction format of the corresponding model. \n\nBaselines with retrievals. We evaluate models incorporating retrieval during both testing and training phases, focusing on standard Retrieval-Augmented Generation (RAG) baselines with open-source Large Language Models (LLMs) like Llama2, Alpaca and LongChat (Li et al., 2023). These models generate outputs based on queries alongside top retrieved documents using our retriever. We also present results for RAG baselines utilizing private data, including RAG-ChatGPT, RAG2.0 (Contextual.AI, 2024), and RAG-Command R+ (Cohere Team, 2024), which prepend top-retrieved documents to the query. Ad-ditionally, we assess RQ-RAG (Chan et al., 2024), which employs proprietary retriever models. Finally, our comparisons extend to Perplexity.ai, Self-RAG (Asai et al., 2024), and SAIL (Luo et al., 2023), which are also finetuned with retrieved texts.",
            "score": 0.6539075389492368,
            "section_title": "Baselines",
            "char_start_offset": 13866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1324
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 364,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 740,
                    "end": 757,
                    "matchedPaperCorpusId": "272768248"
                },
                {
                    "start": 1228,
                    "end": 1247,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48779296875
        },
        {
            "corpus_id": "269005050",
            "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
            "text": "DPO obviates the need to train a reward model, the sample from the LLM during fine-tuning or conduct an extensive hyperparameter search.Figure 2 illustrates the pivotal procedures involved in RLHF and DPO.Retrieval Augmented Generation (RAG) has emerged as a favored paradigm for enabling Large Language Models (LLMs) to access external data, serving as a grounding mechanism to counter hallucinations.RAG models amalgamate pre-trained parametric and non-parametric memory for language generation.The parametric memory is a pre-trained seq2seq model, while the non-parametric memory is a dense vector index of Wikipedia accessed with a pre-trained neural retriever.With RAG, LLMs retrieve contextual documents from a database to enhance the accuracy of their responses.Frameworks such as LangChain, LlamaIndex, FastRAG, and others serve as orchestrators, connecting LLMs with tools, databases, memories, etc., thereby augmenting their capabilities.User instructions are not inherently optimized for retrieval.Various techniques, including multi-query retriever, HyDE, etc., can be employed to rephrase/expand them and enhance performance.Figure 3 elucidates the operational mechanism of the RAG.To recall previous instructions and responses, LLMs and chatbots like ChatGPT incorporate this history into their context window.This buffer can be enhanced with summarization (e.g., using a smaller LLM), a vector store + RAG, etc [71].Both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy) need to be evaluated.Tools like Ragas and DeepEval can simplify this process [72].\n\nAdvanced RAG techniques bolster retrieval techniques for RAG models and evaluate their performance using industry-standard metrics.Advanced RAG techniques systematize various approaches and provide a comprehensive examination of the progression of RAG paradigms.They encompass the Naive RAG, the Advanced RAG, and the Modular RAG.Pinecone's LLM Agent exemplifies an agent that can utilize tools like calculators, search, or executing code.Using agents, an LLM can write and execute Python code.It can search for information and even query a SQL database.",
            "score": 0.6536451808148847,
            "section_title": "The Art of Reasoning and Acting",
            "char_start_offset": 34328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1630
                },
                {
                    "start": 1632,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 1962
                },
                {
                    "start": 1962,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.",
            "score": 0.6532832550897627,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "276107364",
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models by integrating retrieval mechanisms, allowing them to generate responses based on dynamically retrieved documents rather than relying solely on pre-trained knowledge. This approach is particularly effective for knowledge-intensive tasks such as open-domain question answering, fact verification, and knowledge-based text generation. Rankify provides a modular and extensible interface for applying multiple RAG methods, including zero-shot generation, Fusion-in-Decoder (FiD) [41], and in-context learning [89]. \n\nIn Rankify, the Generator module enables seamless integration of RAG techniques, allowing users to experiment different generative approaches. Users can specify the desired RAG method and model, applying generation strategies across retrieved documents. \n\nUsers can apply these methods to generate responses based on retrieved documents. Listing 7 demonstrates how to use Rankify's RAG module with an in-context learning approach. \n\nListing 7: Applying Retrieval-Augmented Generation (RAG) in Rankify. \n\nRankify allows users to leverage large-scale language models such as LLaMA [108], GPT-4 [8], and T5-based models [87] for retrieval-augmented generation. By supporting both encoderdecoder architectures (FiD [41]) and decoder-only models (e.g., GPT, LLaMA), Rankify provides flexibility for optimizing generation quality based on task-specific requirements.",
            "score": 0.6530683929258192,
            "section_title": "Retrieval-Augmented Generation (RAG) Models",
            "char_start_offset": 21549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1436
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6337890625
        },
        {
            "corpus_id": "276885086",
            "title": "Personalized Text Generation with Contrastive Activation Steering",
            "text": "Retrieval-Augmented Generation (RAG) RAG-based approaches achieve personalization through context-aware retrieval. Given input x, the system retrieves k most relevant historical responses from P u using retriever R, then generates personalized responses by combining retrieved documents R(x, P u , k) with the input prompt: \n\nParameter-Efficient Fine-Tuning (PEFT) PEFT methods customize LLMs by training lightweight adapters (e.g., LoRA (Hu et al., 2021)) on user-specific data while keeping base model parameters frozen (Tan et al., 2024b). For each user u, a distinct adapter \u03b8 u is trained via: \n\nwhere L(\u2022) denotes the sequence-to-sequence cross-entropy loss. During inference:",
            "score": 0.6528942375311936,
            "section_title": "Base Solutions",
            "char_start_offset": 5184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 682
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "267523232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "270214689",
            "title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search",
            "text": "In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data. \n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area. \n\nEarlier efforts, such as BioBERT (Lee et al., 2019 [3]), SciBERT (Beltagy et al., 2019 [4]), and LEGAL-BERT (Chalkidis et al., 2020 [5]) have effectively demonstrated the efficacy of domain-specific embeddings in information retrieval tasks. These endeavors primarily investigated two methodologies: (a) extending the pre-training of BERT and (b) pre-training BERT from scratch, both employing domain-specific corpora. Despite yielding commendable results, these methodologies necessitated substantial domainspecific corpora, with figures as staggering as 21.3B words for BioBERT, 3.17B tokens for SciBERT, and 11.5GB of text data for LEGAL-BERT, thereby posing significant challenges, particularly in lowresource domains like enterprises.",
            "score": 0.6526072624935932,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1940
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "The following three sections explore core components-\"Retrieval\", \"Generation\" and \"Augmentation\", respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG's downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII.",
            "score": 0.6523434194955634,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "271874517",
            "title": "RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) systems [18,7] enhance Large Language Models (LLMs) by incorporating external knowledge bases, enabling more precise and contextually relevant responses [7,53,13]. As these systems become integral to a variety of applications [54,2,8], it's imperative to develop robust and comprehensive evaluation frameworks to assess their performance and identify areas for improvement. Evaluating RAG systems, however, presents several challenges: \n\n(1) modular complexity: The modular nature of RAG systems, comprising both a retriever and a generator, complicates the design of effective evaluation metrics. It is crucial to establish metrics that can holistically assess the entire system as well as evaluate the individual modules and their interplay [53], allowing for fully understanding the sources of the errors and misses and how they are generated. (2) metric limitation: Existing metrics for evaluating RAG systems, which are often rule-based or coarse-grained, fall short in providing accurate and interpretable results. Specifically, traditional metrics like recall@k and MRR [44] for retrievers depend on annotated chunks and a rigid chunking approach, missing out on the full semantic scope of the knowledge base. For generators, typical measures such as n-gram-based (e.g., BLEU [30], ROUGE [19]), embedding-based (e.g., BERTScore [56]), and LLM-based methods [45] perform well with concise answers but fail to detect finer distinctions in longer responses. To bridge these gaps, it is essential to develop detailed, semantic-based evaluation metrics that effectively capture the intricacies and overall quality of both the retrieval and generation components in RAG systems. (3) metric reliability: the reliability of existing metrics for RAG remains under-explored. Effective evaluation metrics must not only accurately reflect system performance but also align with human judgments to ensure their utility in real-world scenarios. \n\nTo overcome these challenges, we introduce RAGCHECKER, an innovative evaluation framework designed for detailed analysis of both retrieval and generation processes. RAGCHECKER is based on claim-level entailment checking which involves operations of extracting claims from the response and ground truth answer and checking them against other texts.",
            "score": 0.6521546197211328,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "258740478"
                },
                {
                    "start": 267,
                    "end": 269,
                    "matchedPaperCorpusId": "263866951"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "16944215"
                },
                {
                    "start": 1320,
                    "end": 1324,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1332,
                    "end": 1336,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1401,
                    "end": 1405,
                    "matchedPaperCorpusId": "259951284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.822265625
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "From the retriever evaluation experiments, we know the best retriever, i.e., the best combination of indices + query. In this section, we extend this knowledge to evaluate the RAG pipeline. To avoid the effect of LLM size or type, we perform all experiments using FLAN-T5-XXL. SqUAD is a commonly bench-marked dataset for RAG systems or Generative Q&A using LLMs. Our study juxtaposes three variations of the RAG pipeline from prior work using the evaluation metrics of Exact Match (EM) and F1 scores to gauge the accuracy of answer generation, as well as Top-5 and Top-10 for retrieval accuracy. \n\n\u2022 RAG-original [12]: This variant, a model fine-tuned on the Natural Questions dataset, has been appraised without domain-specific adaptation. \u2022 RAG-end2end [12]: As an extension of RAG-original, this model undergoes additional fine-tuning, tailored for domain adaptation to the SQuAD. \u2022 Blended RAG: Distinctively, our Blended RAG variant has not undergone training on the SQuAD dataset or any related corpora. It harnesses an optimized amalgamation of field selections and hybrid query formulations with semantic indices to feed LLMs to render the most precise responses possible. \n\nConsequently, as shown in Table IV, our Blended RAG showcases enhanced performance for Generative Q&A with F1 scores higher by 50%, even without dataset-specific finetuning. This characteristic is particularly advantageous for large enterprise datasets, where fine-tuning may be impractical or unfeasible, underscoring this research's principal application.",
            "score": 0.6520545117268138,
            "section_title": "V. RAG EXPERIMENTATION",
            "char_start_offset": 13629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1541
                }
            ],
            "ref_mentions": [
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "267069204",
            "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
            "text": "Retrieval-Augmented Generation (RAG) models combine a generative model with an information retrieval function, designed to overcome the inherent constraints of generative models.(1) They integrate the robustness of a large language model (LLM) with the relevance and up-to-dateness of external information sources, resulting in responses that are not only natural and human-like but also the latest, accurate, and contextually relevant to the query. (1)(2)(3)(4) The interaction of the two modules (retrieval and generation) enables responses that would not be achievable with either module alone, making RAG more than just the sum of its components. This approach represents a significant milestone in the field of generative models by enabling the induction of high-quality responses in less-explored domains at a low expense. (5,6) In the conventional RAG operation, the initial step involves converting input queries into vector embeddings, which are then used to retrieve relevant data from the vectorized database. Following this, the generative part of RAG utilizes the retrieved external data for producing contextually rich responses. (7) Thus, both the embedding and generative models are considered crucial factors in the performance of RAG, directly affecting the retrieval process.(8) However, in niche domains, the performance of generic LLM-based embedding models appears suboptimal compared to their effectiveness in more general fields. The lack of specialized training data in these domains results in embeddings that do not adequately capture the nuances and specificity of the domain (9), leading to less accurate and contextually relevant information retrieval. Despite the evident presence of these functional limitations, they have not been much identified through experiments, therefore the optimality of the conventional LLM-based vector embedding RAG methods for niche domains has remained in obscurity. Researchers have been aware of these shortcomings of LLMs and have explored supplementary processes such as fine-tuning to improve the performance. (8,(10)(11)(12) However, the cost of fine-tuning, especially when it involves adjusting the entire or majority of parameters in LLM, has rapidly become expensive, thereby increasing the demand for alternative solutions.",
            "score": 0.6512187329435144,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1604,
                    "end": 1607,
                    "matchedPaperCorpusId": "227277273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "278129706",
            "title": "DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering",
            "text": "The earliest Retrieval-Augmented Generation (RAG) methods adopted the retrieve-then-read paradigm. Initially, a retriever fetches relevant documents from a corpus, and then a generative model produces an answer based on these documents. Common retrieval methods include sparse retrieval (e.g., BM25 (Robertson and Zaragoza, 2009)), dense retrieval (e.g., E5 (Wang et al., 2022), DPR (Karpukhin et al., 2020)), and search engines like Bing and Google. \n\nTo enhance retrieval accuracy, researchers have proposed various optimization strategies. D2LLM (Liao et al., 2024) transfers the computationally expensive cross-encoder capabilities to a more efficient bi-encoder model. MRAG (Besta et al., 2024) introduces multi-head attention mech- anisms to encode documents into multiple vectors, capturing semantic information more comprehensively. Additionally, some studies utilize reranking techniques to filter retrieval results, ensuring that only the most relevant knowledge is retained (Chen et al., 2024a;Yu et al., 2024b). LongLLM-Lingua (Jiang et al., 2024) further optimizes the document ranking order. \n\nMeanwhile, some research has explored leveraging the inherent knowledge capabilities of large models to enhance the adaptability of retrieval strategies, thereby reducing unnecessary external queries. For instance, SKR (Wang et al., 2023b) assesses the complexity of the the given question by comparing it with similar past questions. FLARE (Jiang et al., 2023b) and DRAGIN (Su et al., 2024) trigger external retrieval when the model's output logits indicate uncertainty. \n\nGiven that the effectiveness of RAG systems heavily depends on query quality, many studies focus on optimizing query formulation to enhance retrieval recall. Methods like HyDE (Gao et al., 2023) and Query2doc (Wang et al., 2023a) generate a pseudo-document based on the question, which is then used for retrieval. RRR (Ma et al., 2023) introduces a rewrite-retrieve-read paradigm and fine-tunes the rewrite model using PPO.",
            "score": 0.6508134089189092,
            "section_title": "RAG",
            "char_start_offset": 5683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 329,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 383,
                    "end": 407,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 549,
                    "end": 568,
                    "matchedPaperCorpusId": "270710849"
                },
                {
                    "start": 1039,
                    "end": 1059,
                    "matchedPaperCorpusId": "270619595"
                },
                {
                    "start": 1327,
                    "end": 1347,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 1449,
                    "end": 1470,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1482,
                    "end": 1499,
                    "matchedPaperCorpusId": "268509926"
                },
                {
                    "start": 1758,
                    "end": 1776,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1791,
                    "end": 1811,
                    "matchedPaperCorpusId": "257505063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "275906660",
            "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models",
            "text": "Retrieval-Augmented generation (RAG) offers a promising solution to customize LLMs for specific domains [12]. Rather than retraining LLMs to incorporate updates, RAG enhances these models by leveraging external knowledge from text corpora without modifying their architecture or parameters. This approach enables LLMs to generate responses by leveraging not only their pre-trained knowledge but also real-time retrieved domain-specific information, thereby providing more accurate and reliable answers. The naive RAG systems operate through three key steps: knowledge preparation, retrieval, and integration. During knowledge preparation, external sources such as documents, databases, or webpages are divided into manageable textual chunks and converted into vector representations for efficient indexing. In the retrieval stage, when a user submits a Fig. 1: The development trends in the field of GraphRAG with representative works. query, the system searches for relevant chunks using keyword matching or vector similarity measures. The integration stage then combines these retrieved chunks with the original query to create an informed prompt for the LLM's response. Recently, some advanced RAG systems have evolved beyond simple text chunk retrieval to offer more sophisticated knowledge augmentation approaches. These include hierarchical RAG that preserves document structure through multi-level retrieval [13], [14], re-ranking systems that implement two-stage retrieval for higher recall and precision [15], [16], self-querying RAG that automatically decomposes complex queries [17], and adaptive RAG that dynamically adjusts retrieval strategies based on query types [18], [19]. These advanced RAG systems aim to address the limitations of naive RAG approaches by improving context awareness, retrieval accuracy, and handling complex queries more effectively. \n\nThe emergence of RAG has offered a promising approach for customizing LLMs with domain-specific knowledge. However, despite its potential, RAG faces several critical limitations that impact its effectiveness in practical applications. These limitations can be broadly categorized into four main challenges that significantly affect the performance and utility of RAGenhanced LLMs. The primary challenge lies in complex query understanding. Specialized domains often involve intricate terminology and industry-specific jargon that requires precise interpretation [20]. User queries in these areas typically contain numerous technical terms and industry-specific expressions, with solutions often requiring reasoning across multiple related concepts.",
            "score": 0.6505108566902712,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2313
                },
                {
                    "start": 2314,
                    "end": 2441
                },
                {
                    "start": 2442,
                    "end": 2622
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "271909426",
            "title": "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain",
            "text": "A Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) system is an intelligent generative system that utilizes a knowledge base, denoted as D, which contains a set of documents. In this case, each document, represented as d i \u2208 D, is a string of legal text. A traditional implementation will segment each document d i into a set of chunks c j \u2208 C i . These chunks are then transformed into vector embeddings using a specialized embedding model. After the ingestion of the documents, a user can submit a query q, which will be vectorized using the same embedding model. The system then retrieves the top-k chunks R q = {r 1 , r 2 , . . . , r K } most relevant to the query, using similarity metrics such as cosine similarity. The retrieved chunks, together with the query and a system prompt P , are processed by a large language model (LLM) to generate the final response. This overall process is formalized as: Contextual Retriever(q, D) \u2192 R q LLM P (q, R q ) \u2192 answer Contextual Retriever: The contextual retriever module locates relevant information from an external knowledge repository, returning a corresponding context set R q . Typically, RAG architectures incorporate a bi-encoder retriever such as DPR (Karpukhin et al., 2020), known for its efficiency and accuracy in information retrieval. The Contextual Retriever module often includes a reranking step. First, the top-k \u2032 items are retrieved using a bi-encoder retriever. Then, all k \u2032 items will be reranked using a cross-encoder model that outputs a similarity score between q and each item. The top k < k \u2032 results are returned. Answer Generator: The generator component, which often leverages a sequence-to-sequence model, receives both the question and the context as inputs. It then generates an answer y j,q with the likelihood P G (y j,q | q, r j,q ).",
            "score": 0.6504623675798427,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 2373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1829
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "269982691",
            "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
            "text": "To validate the effectiveness of FlashRAG, we conduct a series of experiments for providing reproducible benchmarking results and facilitating further exploration. In our main experiment, by default, we employ the latest LLaMA-3-8B-instruct [109] model as the generator and E5-base-v2 as the retriever. Some methods may fine-tune their own models in different RAG components, and we provide the details of them in Appendix A. \n\nMethods. We evaluate the performance of all supported RAG methods. These methods are categorized based on the RAG component they primarily aim to optimize: AAR [101] aims at optimizing the retriever; LongLLMLingua [49], RECOMP [52], Selective-Context [50] and Trace [102] focus on refining and compressing the input; Ret-Robust [104], Spring [103] and REPLUG [97] aim to enhance the generator and its decoding approaches; SKR [37], Adaptive-RAG [38] introduce the judger to decide the necessity of retrieval for a query; SuRe [98], Self-RAG [11], FLARE [12], Iter-RetGen [99], RQRAG [106] and ITRG [100] optimize the entire RAG flow, including using multi-round retrieval and generation processes.",
            "score": 0.65045484355499,
            "section_title": "Textual Experimental Result and Discussion",
            "char_start_offset": 14257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 1125
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 593,
                    "matchedPaperCorpusId": "258960666"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 873,
                    "end": 877,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "269293435"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 999,
                    "end": 1003,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Based on how the retriever augments the generator, we categorize RAG foundations into 4 classes, as shown in Fig. 3. \n\n1) Query-based RAG: Stemming from the idea of prompt augmentation, query-based RAG seamlessly integrates the user's query with insights from retrieved information, feeding it directly into the initial stage of the generator's input. This method is prevalent in RAG applications. Post-retrieval, the obtained content is merged with the user's original query to form a composite input, which is then processed by the generator to create a response. Query-based RAG is widely employed across various modalities. \n\nFor text generation, REALM [33] employs a dual-BERT framework to streamline knowledge retrieval and integration, marrying pre-trained models with knowledge extractors. Lewis et al. [34] leveraged DPR for information retrieval and employs BART as the generator to effectively enhance the generation. SELF-RAG [85] utilizes a critique module to determine whether the retrieval is required. In addition to being compatible with local generators, query-based RAG is also applicable to scenarios that use LLM through API calls. RE-PLUG [86] follows this methodology by treating the language Fig. 3: Taxonomy of RAG foundations. model as a \"black box\", and effectively integrates relevant external documents into the query. In-Context RALM [87] uses BM25 for document retrieval and trains a predictive reranker to reorder and integrate the top-ranked documents. \n\nIn the field of code, several works [42], [88]- [91] have utilized the query-based paradigm to incorporate contextual information from text or code into the prompt, resulting in improved effectiveness of downstream tasks. \n\nRecent researches in Knowledge Base Question Answering (KBQA) has also shown significant effects of combining retrieval and language models. For instance, Uni-Parser [92], RNG-KBQA [82], and ECBRF [93] effectively improve the performance and accuracy of QA systems by merging queries and retrieved information into prompts.",
            "score": 0.6498673215655904,
            "section_title": "A. RAG Foundations",
            "char_start_offset": 15878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "253237115"
                },
                {
                    "start": 1536,
                    "end": 1540,
                    "matchedPaperCorpusId": "247450969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.701171875
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "As more and more Generative AI (GenAI) applications are integrated into real-world production systems, Retrieval-Augmented Generation (RAG) has been adopted in industry as a common technique to improve the output of Large Language Models (LLMs). RAG alleviates inherent LLM pitfalls such as propensity to hallucinate, generating outdated knowledge, and lack of traceability to data sources (Fan et al., 2024;Gao et al., 2024). \n\nIntroducing a retrieval step into the generation process introduces, however, several practical challenges. While an LLM with a large number of parameters, such as GPT-4 (OpenAI et al., 2024), can be prompted to work with any kind of input and generate any kind of textual output, the retriever needs to be small, fast, and perform well with data sources that tend to be domain-specific. : Given an ecosystem of RAG applications, how do we build a retriever that can adapt to a specific domain and to a variety of retrieval tasks? \n\nOff-the-shelf retrievers of different sizes are available to AI practitioners. Embedding services such as Voyage1 perform well on open-source benchmarks but they do not necessarily generalize to the kind of data seen in real-world settings, especially when this data is structured and comes from existing databases. \n\nAnother practical challenge is achieving scalability and generalization across different GenAI use cases that depend on retrieval. A crucial advantage of LLMs compared to traditional machine learning models is that they can generalize to a myriad of tasks due to vast amounts of pretraining data and instruction fine-tuning (Wei et al., 2022;Zhang et al., 2024a;Ouyang et al., 2022). But if the retriever does not perform well and fast across many retrieval tasks, the downstream generation will be negatively affected. \n\nThe problem we are trying to solve is then: how to adapt the retrieval step to a specific domain and to a variety of retrieval tasks? In this work, we are not interested in the choice of LLM, assuming that improvements in the retrieved results translate into improvements in the downstream generation task.",
            "score": 0.6482334560368266,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 408,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1604,
                    "end": 1622,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1642,
                    "end": 1662,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "252992929",
            "title": "Enhanced vectors for top-k document retrieval in Question Answering",
            "text": "In this architecture, the model that in the fine-tuning phase, uses the training to recover passages for each instance using the model that is being fine-tuned in inner-loop retrieval.\n\nMoving towards retriever training which is \"end-to-end\", this is carried out by ORQA, REALM, and RAG. However, for each training batch , as it's impossible to apply forward and backward passes on the complete set, such inner loop retrieval necessitates significant approximations. Here, while fine-tuning for OpenQA, RAG, REALM, ORQA, and freeze the document encoder along with the vectors that are indexed, are freezed, which limits the model's capacity to adapt to this job and/or to new corpora.\n\nAt the time of the document encoder fine-tuning and re-indexing a just few more times, RGS provides efficient and scalable approach in which the trainng samples are self-gathered by the retriever. By using outer-loop retrieval, RGS eliminates these restrictions.",
            "score": 0.647842362350437,
            "section_title": "Inner-loop retrieval",
            "char_start_offset": 28099,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "274859726",
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "text": "To satisfy the requirements of RAG system, existing retrieval-augmented generators are primarily categorized into end-to-end and pipeline frameworks. In end-to-end frameworks, models such as RetRobust (Yoran et al., 2024a) and RAAT (Fang et al., 2024) enhance the robustness of the generator by performing SFT on high-quality data. In pipeline frameworks, models like DPA-RAG (Dong et al., 2024a), REAR (Wang et al., 2024), Self-RAG (Asai et al., 2024), andRankRAG (Yu et al., 2024) enhance the robustness of the generator by explicitly re-ranking the retrieved documents. Furthermore, VTG (Sun et al., 2023) improves citation quality by introducing explicit citation verification and modification. However, these methods still struggle to align the generator with the RAG requirements fully.",
            "score": 0.6477009662361313,
            "section_title": "Retrieval-Augmented Generator",
            "char_start_offset": 24550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 792
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 457,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "270764613",
            "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) system typically comprises a search engine for knowledge retrieval and a Large Language Model (LLM) for answer generation (Khandelwal et al., 2019;Guu et al., 2020;Lewis et al., 2020;Borgeaud et al., 2022;Ram et al., 2023;Shi et al., 2023).Given a user-posed question, RAG first searches for relevant knowledge snippets using the search engine and then generates the answer via machine reading comprehension (Chen et al., 2017).\n\nAdaptive retrieval augmented generation dynamically determines whether LLMs require retrieved knowledge, thereby reducing the adverse effect of inaccurately retrieved information.FLARE (Jiang et al., 2023) and DRAGIN (Su et al., 2024) activate the search engine when LLMs output tokens with low probability.Self-RAG (Asai et al., 2023) and Wang et al. (2024) prompt LLMs to decide on retrieval.Self-knowledge guided generation (Wang et al., 2023) trains a classification model to judge the factuality of model generation.\n\nExisting adaptive RAG methods mainly face two challenges.1) To decide when to retrieve, it is superficial to have the decision of retrieval solely on the output of LLM.However, the retrieval decision made by LLMs is still at risk of hallucination, which potentially does not reliably indicate the actual knowledge sufficiency (Yona et al., 2024).Furthermore, LLMs have the tendency to confidently produce incorrect contents even when correct knowledge is missing from their parameters (Huang et al., 2023;Xu et al., 2024).2) To integrate retrieved knowledge, these attempts rely on the correctness of search engine returned knowledge, neglecting to re-rank multiple retrieved knowledge and optimize the reasoning paths.\n\nRetrieval augmented reasoning integrates the reasoning capabilities of LLMs into the RAG framework to solve complex questions.IRCoT (Trivedi et al., 2022) implements retrieval augmentation within multi-step chain-of-thought (CoT, Wei et al., 2022) reasoning processes, which is adopted by many following works (Su et al., 2024;Jeong et al., 2024).",
            "score": 0.6475496538057656,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 4109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 465
                },
                {
                    "start": 467,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 988
                },
                {
                    "start": 990,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1336
                },
                {
                    "start": 1336,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1709
                },
                {
                    "start": 1711,
                    "end": 1837
                },
                {
                    "start": 1837,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 184,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 184,
                    "end": 201,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 201,
                    "end": 220,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 220,
                    "end": 242,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 445,
                    "end": 464,
                    "matchedPaperCorpusId": "3618568"
                },
                {
                    "start": 652,
                    "end": 672,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 684,
                    "end": 701,
                    "matchedPaperCorpusId": "268509926"
                },
                {
                    "start": 783,
                    "end": 802,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 894,
                    "end": 912,
                    "matchedPaperCorpusId": "263828724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "270560978",
            "title": "Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG",
            "text": "Retrieval-Augmented Generation (RAG) is a general paradigm which enhances LLMs by including relevant information retrieved from external databases into the input [24].RAG typically consists of three phases: indexing, retrieval, and generation.First, the indexing phase constructs external databases and their retrieval index from external data sources; Second, given a user query, the retrieval system then utilizes these index to fetch the relevant document chunks as context.Third, the retrieved context is then integrated into the input prompt for LLMs, and LLMs then generate the final output based on the augmented inputs.RAG has been widely used in various domains [25][26][27][28].For example, RAG has been specialized to software engineering tasks such as code generation [27,28], which retrieves the similar code from the code base and augments the prompt with the retrieved code for model inference.",
            "score": 0.6471241610045231,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 10204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 909
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "253097741"
                },
                {
                    "start": 679,
                    "end": 683,
                    "matchedPaperCorpusId": "257663528"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "247450969"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "257663528"
                },
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "247450969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5986328125
        },
        {
            "corpus_id": "266551228",
            "title": "HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses",
            "text": "Retrieval-Augmented Generation. RAG incorporates the external knowledge retrieval component via prompt engineering to achieve more factual consistency, enhancing the reliability and interpretability of LLMs' responses [39]. Classic RAG methods leverage retriever models to source relevant documents from large knowledge corpora [84], followed by reranker models that distill contents and reader models for further processing [57,62]. Despite advancements in retriever [33,51,60] and reranker efficiency [14,91], they still encounter difficulty in acquiring highquality datasets for training query-document pair retrievers or limited information in user queries which weakens their generalization capability [19]. Moreover, some researches focus on fine-tuning reader LLMs, applying instruction-tuning with retrieved knowledge or RAG API calls [4,24,46,49,79,90,97]. However, creating such datasets is also challenging due to the need for manual label correction, which in turn, may erode LLMs generalization capabilities and cause catastrophic forgetting in routine Q&A tasks. \n\nBeyond optimizing submodels, HyDE [19] introduces an innovative method where instruction-following LLMs generate hypothesis documents based on user queries to enhance retriever performance, particularly in zero-shot scenarios. Other methods like CoN [93] and CoK [44] involve LLMs in note-making and step-wise reasoning verification through customized prompts, and greatly rely on frequent interactions with LLMs. However, such an approach is excessively inefficient for deployment in real-world Q&A scenarios. \n\nOur HyKGE, uses LLM hypothesis output for exploratory directions in KGs and corrects model errors using graph reasoning chains during pre-retrieval, and applies fine-grained alignment in  post-retrieval to maintain effective, diverse knowledge, enhancing retrieval efficiently without fine-tuning or excessive interactions. \n\nKnowledge Graph Query-Answer. Compared to knowledge stored in document repositories [23], the knowledge contained within KGs has the advantages of being structured and inferable, rendering it a more suitable source for supplementing LLMs [28,30,48,50,63,72].",
            "score": 0.6461029122338584,
            "section_title": "RELATED WORK",
            "char_start_offset": 7992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1589
                },
                {
                    "start": 1592,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "263620134"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "259165563"
                },
                {
                    "start": 429,
                    "end": 432,
                    "matchedPaperCorpusId": "267334785"
                },
                {
                    "start": 843,
                    "end": 846,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 849,
                    "end": 852,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "271854868",
            "title": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama ModelsOptimizing RAG Techniques Based on Locally Deployed Ollama ModelsA Case Study with Locally Deployed Ollama Models",
            "text": "Self-Reflective Retrieval-Augmented Generation (SELF-RAG) is a novel framework designed to enhance the quality and factual accuracy of LLMs through on-demand retrieval and a self-reflection mechanism [42]. \n\nUnlike traditional RAG methods, SELF-RAG endows LLMs with the following capabilities: \n\n1. On-demand Retrieval: The LLMs autonomously determines whether to retrieve relevant information from an external knowledge base based on the input it receives. 2. Self-Reflection: The LLMs evaluates and reflects upon both the retrieved information and its own generated content, thereby improving the quality and reliability of its output. \n\nThe training process of SELF-RAG consists of two stages: \n\n1. Offline Critic Model Training: An independent critic model is trained to generate \"reflection tokens\". These tokens are inserted into the LLMs' output to guide its self-reflection process.",
            "score": 0.6443006668699777,
            "section_title": "Overview of SELF-RAG",
            "char_start_offset": 20609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 890
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52783203125
        },
        {
            "corpus_id": "270559941",
            "title": "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities",
            "text": "Retrieval Augmented Generation The Retrieval Augmented Generation (RAG) (Lewis et al., 2021) (Gao et al., 2024) fundamentally enhances the capabilities of LLMs by retrieving pertinent documents from an external knowledge base, which is made possible through the calculation of semantic similarity between user's query and documents.Notably, the de facto approach is to segment documents into chunks and construct text representation embedding upon these chunks.By leveraging external knowledge sources like Wikipedia, RAG effectively mitigates the issue of generating factually inaccurate content, thereby greatly enhancing the reliability of generated text.RAG typically involves a retrieval system for query-relevant knowledge indexing from a knowledge base, and a LM that serves as question answering interface.\n\nAdvanced RAG Advanced RAG model incorporates multiple optimization strategies that span both pre-retrieval and post-retrieval processes.For example, Self-RAG (Asai et al., 2023) introduces a selective knowledge retrieval mechanism, which utilizes a retrieve token to determine the necessity of retrieval and employs multiple critic tokens to assess answer quality.Building upon this, RQ-RAG (Chan et al., 2024) is trained to dynamically improve query by integrating rewriting, decomposition, and disambiguation functionalities.Furthermore, chain-of-thought (COT) (Wang et al., 2024) and Tree-of-thought (TOT) (Yao et al., 2023) are incorporated to RAG to tackle certain logicaldemanding tasks, which proved to have significantly enhanced the performance of LLMs.\n\nConcurrent Post-Retrieval Work Previous research has explored the application of language model-based approaches to further distill or extract query-relevant key information from retrieved document segments (Lewis et al., 2021).Some studies have developed summarization models (Lennox et al., 2023) (Jung et al., 2024) to derive crucial query-relevant information from retrieved document blocks, RECOMP (Xu et al., 2023) has designed a dual-encoder extractive compressor and a encoder-decoder abstractive model to expunge irrelevant context and summarize upon the remainder.",
            "score": 0.6441703805056628,
            "section_title": "Related Work",
            "char_start_offset": 5409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 814
                },
                {
                    "start": 816,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1578
                },
                {
                    "start": 1580,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 2154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [21,14].In response, Retriever-Augmented Generation (RAG) systems [21,20] are becoming increasingly popular in user-facing dialogue applications [35].Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response.The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [21].\n\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1,31,7].In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks.However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs.With numerous commercial and open-source generative LLMs readily available 1 and many variable parameters in the RAG system design (Figure 1), tuning an optimal system for a particular RAG application involves iterative evaluation of multiple configurations.This motivates the need for automated RAG evaluation solutions.\n\nIn response, automated RAG evaluation systems like RAGAS [9] and TruLens [37] have emerged.These systems adopt a zero-shot LLM prompt-based approach to predict a set of curated RAG evaluation metrics.However, the lack of unified RAG benchmarks makes it difficult to compare approaches against each other.Each new study designs a new dataset, often employing LLMs as generators and labelers [9,33,4], which renders them irreproducible.A few benchmarks like RGB [4], AttributionBench [22] and RAGTruth [41] have been proposed recently, but they are small in size and target a disjoint set of labels.The exact RAG evaluation criteria also vary from study to study.",
            "score": 0.6441157137289597,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 730
                },
                {
                    "start": 732,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1521
                },
                {
                    "start": 1523,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "235898896"
                },
                {
                    "start": 902,
                    "end": 904,
                    "matchedPaperCorpusId": "258947803"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "263152733"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "263152733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "276580244",
            "title": "LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers",
            "text": "The introduction of scaling laws (Kaplan et al. 2020;Hoffmann et al. 2022) has led to significant advancements in Large Language Models (LLMs) (Brown et al. 2020;Touvron et al. 2023a,b;Dubey et al. 2024;Abdin et al. 2024), enabling them to achieve remarkable successes across a wide range of tasks (Zhang et al. 2023;Jiang et al. 2024a;Yang et al. 2023;Yao et al. 2023;Lee et al. 2024). Despite their extensive training and vast knowledge base, LLMs continue to encounter challenges in accurately handling certain * Corresponding author: Yang Feng. types of information, including less commonly known facts, specialized domain knowledge, and events that occurred after their pre-training phase (Mallen et al. 2023a;Huang et al. 2023). In these cases, LLMs often fail to provide accurate responses and may even produce hallucinated answers. Hence Retrieval-Augmented Generation (RAG) is proposed to enhance LLM-generated responses by appending real-time retrieval of relevant external knowledge to user input of LLMs, offering a promising solution to address these challenges (Gao et al. 2024;Ding et al. 2024). However, due to the constraints of the search techniques and the coverage of databases, the retrieval results are often inaccurate and incomplete, affecting the effectiveness of RAG systems. \n\nMany researchers have made efforts to optimize RAG systems by improving the accuracy and completeness of retrieval results. To improve accuracy, researchers have developed numerous query rewriting techniques (Ma et al. 2023;Asai et al. 2023;Chan et al. 2024) which try to refine the query fed to a retriever to make the query better match with the retriever and better align to user intents. On the other hand, to improve completeness, hybrid retrieval is employed to broaden the scope of retrieval, which integrates multiple retrievers and databases to maximize the advantages of diverse retrievers (Lu et al. 2022;Wang et al. 2023;Yu et al. 2022;Zhang et al. 2020;Li et al. 2024).",
            "score": 0.6437755570696537,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1301
                },
                {
                    "start": 1304,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 53,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55908203125
        },
        {
            "corpus_id": "278129909",
            "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark",
            "text": "Retrieval-Augmented Generation (RAG) systems have demonstrated potential in enhancing the factual grounding of large language models (LLMs) by conditioning generation on retrieved evidence. However, several systemic challenges persist. These include persistent hallucinations, particularly when retrieval returns incomplete or noisy passages; fragile pipeline behavior when faced with retrieval uncertainty; and lack of calibration or interpretability in retriever scores. \n\nAs highlighted by [3], RAG systems face fundamental limitations in addressing several critical issues. Their research demonstrates that LLMs struggle with \"hallucinations when asked to discuss content that goes beyond training data,\" which RAG aims to mitigate but cannot fully eliminate, especially when guard rails are easily circumvented by slight prompt modifications. The timeliness problem remains particularly challenging, as \"it is prohibitively expensive for [providers] to continuously update their models,\" making just-in-time knowledge updating crucial but imperfect in implementation. \n\nFurthermore, Church et al. identify document processing limitations where \"RAG tends to process documents in very simple ways, typically as a sequence of chunks,\" failing to properly handle complex document structures containing tables, figures, and other elements. This simplistic approach, combined with OCR errors in older documents, significantly impacts retrieval quality. Their evaluation also reveals that RAG summaries, while impressive at first glance, are often \"long-winded\" and \"sometimes seem to miss the point,\" indicating fundamental limitations in how these systems integrate retrieved information. \n\nWhile open-source frameworks such as LangChain and LlamaIndex have introduced modules for evaluating retrieval quality and response faithfulness, these tools often require additional configuration and may not fully address the stringent requirements of high-stakes domains like finance. Moreover, few systems support real-time indexing or dynamic corpus updates, limiting their effectiveness in fast-changing environments. SMARTFinRAG addresses these issues by providing an interactive platform that allows users to experiment with different RAG components in real-time, facilitating a deeper understanding of system behavior.",
            "score": 0.6437701541639941,
            "section_title": "Challenges in General RAG Systems.",
            "char_start_offset": 6465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "272765802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
            "score": 0.6435781957712307,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Over the past decade, large language models (LLMs) have demonstrated promising capability in processing natural language [1]. Owing to the vast amount of knowledge encoded in their internal parameters, LLMs such as GPT [3] and LLaMa [9] have demonstrated remarkable performance on various downstream tasks, including Open-domain Question Answering (ODQA) [38], Reading Comprehension [7]. However, the fixed parametric knowledge of LLMs has hindered the further applications of LLMs and made them prone to errors (hallucination [7] and factual errors [2]). To overcome the limitations of parametric knowledge, one promising approach is Retrieval-Augmented Generation (RAG) [2,15]. Compared to relying solely on parametric knowledge, RAG enables LLMs to use retrievers to access relevant information from external knowledge sources, enhancing their question-answering abilities. Among the two components of RAG, current methods primarily focus on optimizing the retriever to return more relevant documents due to the high cost of finetuning and black-box LLM APIs. Previous retrievers leveraged deep learning technology (e.g., dense retrieval [37]) to encode the text representations from the lexical space into the high-dimensional latent space, allowing them to model more complex semantic relationships between queries and corpora. However, the separation between the retriever and generation model has hindered their full integration, limiting their compatibility in downstream applications []. Some advanced RAG systems, such as RA-DIT [18], have adopted joint training mechanisms that fine-tune both the retriever and the generation model for better alignment. However, this approach is impractical due to the need for frequent fine-tuning and fails to utilize the LLMs' semantic understanding capabilities during the retrieval stage. Consequently, generative retrieval (GR), also known as LLM-based retrieval, leverages the parametric memory of generative models to directly generate document identifiers (DocIDs) [16], which has aroused much attention. By memorizing the documents as the parametric knowledge of LLM, this kind of method breaks the limitations of traditional IR in terms of document granularity and simple relevance matching [23], offering more flexibility and creativity, thus better meeting practical needs.",
            "score": 0.6428624760265298,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 2019,
                    "end": 2023,
                    "matchedPaperCorpusId": "269303210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70947265625
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "For simple questions, it reduces unnecessary overhead, enabling faster responses and minimizing hallucinations. For complex or multi-faceted tasks, it increases reasoning depth to improve accuracy and better integrate retrieved context, enabling LLMs to effectively process and reason with augmented context. This adaptive approach mimics human-like deliberative reasoning for knowledgeintensive tasks without costly retraining, enhancing efficiency and performance while maintaining accuracy and reducing hallucinations. Together, RAFT enhances RAG by integrating retrieval with supervised fine-tuning, enabling models to dynamically leverage external knowledge and prioritize relevant content while ignoring distractors. DRA-GIN dynamically determines when and what to retrieve during text generation, minimizing redundant retrievals and improving efficiency. KV Caching optimizes memory usage by storing intermediate hidden states, reducing computational overhead in RAG, while Test-Time Scaling dynamically allocates resources based on task complexity. These advancements enable RAG systems to integrate external knowledge more accurately, efficiently, and at scale, ensuring faster and more effective utilization of retrieved data within the LLM framework. While these recent advancements have enhanced retrieval integration in LLMs, significant challenges remain in balancing retrieval fidelity, response quality, and computational efficiency. Current methods often struggle to dynamically determine when and how much external information to incorporate, sometimes overwhelming the model or sacrificing the coherence of its responses. Motivated by these persistent challenges, our work seeks to refine the synergy between retrieval and generation through a dual approach. First, we fine-tune language models via policy optimization, enabling them to more effectively integrate and utilize retrieved content. This refinement not only improves factual alignment but also enhances overall response quality. Second, we introduce a mechanism that selectively triggers external retrieval based on the model's internal state, ensuring that additional information is incorporated only when necessary. This targeted strategy optimizes computational resources while preserving the language model's coherence. In the following sections, we outline our contributions that extend state-of-the-art methods by addressing both the optimization of retrieval-augmented generation and the efficient management of computational overhead. Our contributions are as follows:",
            "score": 0.6421374390812464,
            "section_title": "Introduction",
            "char_start_offset": 4240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2304
                },
                {
                    "start": 2305,
                    "end": 2523
                },
                {
                    "start": 2524,
                    "end": 2557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "265221210",
            "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
            "text": "Retrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer support (Petroni et al., 2021;Wang et al., 2019). Typically, a RAG system consists of a retriever and a downstream language model (LM). Given a user question, the retriever finds relevant passages from a corpus and the LM uses these passages to generate a response. This formulation admits a multitude of choices: what retrieval model to use, how to divide the documents into retrieval chunks, and how to prompt or finetune the LM to use the retrieved information, to name only a few of the simplest design decisions. \n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes, and cost/latency budgets. To tune their own RAG systems, practitioners traditionally need hand annotations for test questions, passages to retrieve (to assess the retriever), and responses to generate, labeled specifically for their target domain. Alternatively, they may evaluate different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs. \n\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al., 2023). For instance, the open-source RAGAS framework (James and Es, 2023) prompts an LM for evaluating the relevance of retrieved information and the faithfulness and accuracy of generated responses. Unfortunately, such strategies currently rely for evaluation on a fixed set of heuristically hand-written prompts, offering little adaptability to various evaluation contexts and no guarantees about quality. \n\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the Automated RAG Evaluation System. ARES is the first automated RAG evaluation system to generate tailored LLM judges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and accuracy compared to existing approaches like RA-GAS. Furthermore, unlike existing RAG evaluation systems, ARES provides confidence intervals for its scoring by leveraging prediction-powered inference (PPI;Angelopoulos et al. 2023).",
            "score": 0.6415990111099272,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2312
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 214,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 214,
                    "end": 232,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "A recent paper introduces CommunityKG-RAG (Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge Graphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances the accuracy and contextual relevance of fact-checking by utilizing multi-hop connections within KGs, outperforming traditional methods without requiring additional domain-specific training. The RAPTOR model (Sarthi et. al. 2024) introduces a hierarchical approach to retrieval-augmented language models, addressing limitations in traditional methods that retrieve only short, contiguous text chunks. RAPTOR forms a summary tree to retrieve information at varying abstraction levels by recursively embedding, clustering, and summarizing text. Experiments demonstrate RAPTOR's superior performance, especially in question-answering tasks requiring complex reasoning. When paired with GPT-4, RAPTOR improves accuracy on the QuALITY benchmark by 20%. This advancement in RAG further proves the utility of the RAG system however recent LLM launches that support long-term context have significantly shown improved performance. A recent study (Li et. al. 2024) compared the efficiency of Retrieval Augmented Generation (RAG) and long-context (LC) Large Language Models (LLMs), such as Gemini-1.5 and GPT-4. While LC models outperform RAG when adequately resourced, RAG's cost-efficiency remains advantageous. To balance performance and cost, the paper introduces Self-Route. This method dynamically directs queries to either RAG or LC based on model self-reflection, optimizing both computation cost and performance. This study offers valuable insights into the optimal application of RAG and LC in handling long-context tasks. Nguyen et. al., 2024 introduce SFR-RAG , a small but highly efficient Retrieval Augmented Generation (RAG) model, which is designed to enhance the integration of external contextual information into Large Language Models (LLMs) while minimizing hallucinations. LA-RAG (Li et. al., 2024), a novel Retrieval-Augmented Generation (RAG) paradigm designed to enhance Automatic Speech Recognition (ASR) in large language models (LLMs).",
            "score": 0.641187718870325,
            "section_title": "Recent Advancement in the field:",
            "char_start_offset": 27165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "267320876",
            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
            "text": "In this paper, we have introduced an innovative framework (CRUD-RAG) for evaluating retrievalaugmented generation (RAG) systems that is both comprehensive and scenario-specific. Our unique categorization of text generation tasks into the CRUD-Create, Read, Update, and Delete-types provides a structured approach to assess the capabilities and limitations of RAG systems in handling a variety of textual contexts. To facilitate this evaluation, we have meticulously constructed largescale datasets for each CRUD category, which are tailored to challenge and reflect the performance of RAG systems under different operational conditions. Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources. \n\nOur study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results. The insights provided by our findings offer a valuable roadmap for researchers and practitioners in the field, guiding them in the development and refinement of RAG systems. We believe that the methodologies and results presented in this paper will spur further exploration and innovation in the realm of RAG technologies. Our work aims to catalyze advancements in text generation applications, pushing the envelope of what is possible with the integration of retrieval mechanisms and language models. We hope that this contribution will serve as a cornerstone for future research efforts, fostering the creation of more intelligent, adaptive, and context-aware generative systems.",
            "score": 0.6406703796399257,
            "section_title": "CONCLUSION",
            "char_start_offset": 67227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9140625
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "In our research, we concentrated primarily on the application of retrieval augmentation during the inference stage, without delving into its integration during pre-training or fine-tuning phases. Future work will aim to explore these compelling areas. Moreover, while our study has highlighted the privacy risks associated with commonly employed retrieval-augmented generation (RAG) systems, other retrieval-based language models (LMs) feature distinct components and architectures (Huang et al., 2023;Borgeaud et al., 2022) that warrant further investigation. In addition, developing effective strategies to protect retrieval data and leveraging RAG systems for the safeguarding of training data represent open research questions that we intend to pursue.",
            "score": 0.640259175730761,
            "section_title": "Limitations",
            "char_start_offset": 29021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 756
                }
            ],
            "ref_mentions": [
                {
                    "start": 502,
                    "end": 524,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "We experimented with five different combinations of finetuning and RAG in order to determine what the most effective approaches could be for the SLR-automation process.Our experimental setup investigated the following methodologies:\n\n1. Baseline: Evaluate the Mistral-7B-Instruct on its knowledge and ability to answer SLR-related test dataset questions.\n\n2. Fine-tuning LLMs using LoRA: Leveraging Low-Rank Adaptation for fast and efficient parameter adjustment.\n\n3. Fine-tuning LLMs using NEFTune: Introducing noise into embedding vectors to investigate effects on generalization improvements.\n\n4. Instruct LLM + RAG with Raw Articles: Combining LLMs with Retrieval-Augmented Generation, using unprocessed article text as the retrieval corpus.\n\n5. Instruct LLM + RAG with Auto-Extracted Data: Employing RAG with a knowledge base of automatically extracted data comprising the finetuning dataset for focused information retrieval.\n\n6. Best Finetuned LLMs + Best RAG Solution: Integrating the top-performing fine-tuning and RAG methods to optimize SLR automation.\n\nEach method also summarised in Table 3, was evaluated for factually correct answers with respect to the SLR dataset.",
            "score": 0.6401612440705309,
            "section_title": "Experimental Design",
            "char_start_offset": 44331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 354
                },
                {
                    "start": 356,
                    "end": 463
                },
                {
                    "start": 465,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 747,
                    "end": 931
                },
                {
                    "start": 933,
                    "end": 1063
                },
                {
                    "start": 1065,
                    "end": 1181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "score": 0.6399868251610288,
            "section_title": "Introduction",
            "char_start_offset": 2330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "272955388",
            "title": "Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing",
            "text": "The RAG model, known as Retrieval-Augmented Generation (RAG), is a natural language processing framework that combines information retrieval with generative modeling [7].The core idea of the RAG model is to retrieve document fragments relevant to the query from a large-scale document repository before generating a natural language response, and these fragments are fed into the generative model as contextual information to make the generated response more relevant and accurate [8]. These snippets are fed into the generation model as contextual information to make the generated response more relevant and accurate. By combining retrieval and generation in this way, the RAG model is able to provide more accurate answers in complex query scenarios\uff0cshowed in Figure 1: Retrieval Augmented Generation (RAG) is a technique that deeply integrates traditional information retrieval methods with generative modeling. In the RAG model, the retriever is responsible for filtering out the most relevant documents or information fragments from a large-scale dataset, while the generator generates natural language responses based on these retrieved contents [9]. The advantage of this approach is that it can utilize the rich information in the external knowledge base to compensate for the knowledge blindness of the generative model due to insufficient training data when facing complex queries, thus improving the quality and accuracy of the response [10]. The Information Retrieval Step: \n\nThe workflow of a RAG model is usually divided into two main steps: retrieval and generation. In the retrieval phase, the model selects the document fragments with the highest relevance to the query from a pre-constructed document library based on the input query. These snippets are passed to the model in the generation phase as additional contextual information. The model in the generation phase then uses these information fragments to generate the final natural language response. Through this two-stage processing, the RAG model is able to better understand the deeper semantics behind complex queries and generate answers that match the user's needs. Response Generation Step: \n\nThe advantages of RAG models in complex query processing are mainly reflected in their efficient utilization of external knowledge and high relevance of generated content. Compared with traditional generative models, RAG models not only rely on pre-trained language models, but also dynamically acquire the most relevant knowledge to the query, which gives them an obvious advantage when facing diverse and high-complexity queries.",
            "score": 0.6395670045068313,
            "section_title": "Overview of the RAG model and its retrieval enhancement generation technique",
            "char_start_offset": 2174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2173
                },
                {
                    "start": 2176,
                    "end": 2347
                },
                {
                    "start": 2348,
                    "end": 2607
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "3238904"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "7883049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "274131235",
            "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation",
            "text": "Retrieval-Augmented Generation (RAG) typically involves three key stages: Indexing, Retrieval, and Generation [33,51]. Indexing aims to transform raw data into a vectorized database. This stage begins by converting external data-ranging from structured to unstructured formats-into a standardized format. An embedding model then encodes the processed data into smaller chunks, which are stored in the vectorized database. Indexing establishes a robust foundation for efficient and precise retrieval. Retrieval involves applying RAG algorithms to search for and expand the prompt. In this stage, the user's initial query is encoded into an input vector using the same embedding model utilized during Indexing. The system then computes the similarity between the input vector and the stored chunks, selecting the top K most relevant chunks based on their proximity. Generation utilizes a synthetic prompt formed by combining the user's query with the retrieved documents. This enriched prompt provides the LLM with additional contextual information, which helps mitigate hallucinations or constrain the generated responses. Consequently, the Generation stage enhances the accuracy and reliability of the model's outputs. \n\nAdvanced RAG methods have been developed to address the limitations of naive RAG by optimizing various components, including query prompts [16,51,57], indexing structures, similarity calculations, and prompt integration. Additionally, some advanced approaches leverage retrieval data to enhance training datasets, particularly in low-resource domains. \n\nFor instance, [62] retrieves relevant instances and uses large language models (LLMs) to generate new samples that integrate both original and retrieved data, thereby addressing data scarcity in specialized domains. [56] involves utilizing retriever models to identify relevant samples and expand the set of positive examples within privacy policy question-answering datasets, enhancing the training process. However, these methods often rely on external data, which poses challenges when the data is sensitive and cannot be accessed due to privacy concerns, or when computational resources are limited.",
            "score": 0.6394623576459274,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 37833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1367,
                    "end": 1370,
                    "matchedPaperCorpusId": "265042961"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "270379921",
            "title": "TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
            "text": "The DPR retriever follows a bi-encoder architecture, one for the queries and the other for the documents in the knowledge corpus.This method can be finetuned on any seq2seq task, whereby both the generator and retriever are jointly learned.In the fine-tuning phase, only the query encoder is trained.This reduces considerably the computational burden compared to REALM, which periodically updates the document index during pre-training [10].The results in [11] show that RAG responses are more factual, specific, and diverse than other baselines based on language models.For these reasons and given the nature of the task involved in telecommunication standards, we rely on RAG to design TelecomRAG as detailed in the next section.",
            "score": 0.6393829476188748,
            "section_title": "C. Retrieval-Augmented Methods",
            "char_start_offset": 9263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 731
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5732421875
        },
        {
            "corpus_id": "273345235",
            "title": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers in RAG for Coding Tasks",
            "text": "Large language models (LLMs) have achieved remarkable success in natural language processing, but they still encounter significant limitations in the domain of knowledge-intensive tasks. In particular, LLMs are susceptible to \"hallucinations\" when confronted with queries that exceed the scope of their training data or necessitate the utilization of current information [1]. Retrieval-Augmented Generation (RAG) is the leading technique for improving LLMs by providing demonstrations from knowledge bases. By referencing external knowledge sources, RAG effectively mitigates the problem of generating factual inconsistency content and facilitates the continuous updating of knowledge [2]. \n\nRecently, the application of RAG has achieved promising results for various code-specific tasks such as Code Search [3], Program Synthesis [4], [5], and Assertion Generation [6]. In these tasks, LLM learns from contextual prompts consisting of task descriptions, queries, and additional demonstration examples without the need to fine-tune the model parameters. The retrieved demonstrations are typically used as the context to assist the pre-trained LLM in comprehending the task and regulating the generation behavior, which usually has a significant impact on the quality of the generated output. Therefore, it is important to retrieve appropriate demonstrations from a vast knowledge base. \n\nThe retriever plays a core role in retrieving relevant demonstrations from the external knowledge base and significantly affects the performance of RAG [7], [8]. Retrievers are typically classified into sparse and dense retrievers based on representation methods. Sparse retrievers operate at the token level, while dense retrievers operate at the level of latent semantics. The most widely-used sparse retriever, BM25 [9], ranks demonstrations based on term frequency (TF) and inverse document frequency (IDF) of the query. Dense retrievers perform retrieval by encoding the query and demonstrations into dense embedding representations and scoring each demonstration by its similarity with the query embedding. For instance, the popular RAG system Llamaindex [10] supports both BM25 and custom embedding encoders in RAG workflow.",
            "score": 0.6392589376897773,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "269130676"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "258180059"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "266359151",
            "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "2) New Patterns: Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple \"Retrieve\" and \"Read\" mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. \n\nInnovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM's capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while Recite-Read [22] emphasizes retrieval from model weights, enhancing the model's ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. \n\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN [14], showcase the dynamic use of module outputs to bolster another module's functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].",
            "score": 0.6387537824638848,
            "section_title": "C. Modular RAG",
            "char_start_offset": 13909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "274982797",
            "title": "Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under Adversarial Poisoning Attacks",
            "text": "Retrieval-Augmented Generation (RAG). RAG is a widely adopted paradigm that effectively combines the strengths of information retrieval and text generation (Lewis et al., 2020). A typical RAG system operates in two phases: (1) Retrieval Phase: \n\nIn this phase, the system retrieves the k most relevant passages from the corpus C based on the input query q. The corpus C can be easily updated with new information, ensuring the system remains adaptable to changing knowledge and specific domains. \n\n(2) Generation Phase: In this phase, the retrieved passages are combined with the original query to form the input to a large language model (LLM). By integrating its pre-trained knowledge with the retrieved passages, the LLM generates a more grounded and accurate response. This approach reduces hallucinations commonly observed in standalone LLMs and allows LLM response to adapt more effectively to evolving knowledge. 2024) focused only on the retrieval phase and investigated using gradient information to create adversarial contexts that are likely to be retrieved by many queries. (Long et al., 2024) conducted a targeted query attack, generating adversarial passage only for the target queries while not considering the influence of retrieved adversarial passages on the LLM's outputs, while Zou et al. (2024) focused on both the retrieval phase and the generation phase by crafting adversarial passages for specific queries and directly influencing the LLM's generation by inducing the model to output a target answer for specific questions. \n\nRAG with Irrelevant or Incorrect Contexts. Beyond intentionally injected adversarial contexts, RAG systems inevitably introduce irrelevant or incorrect information due to the limitations of their retrievers (Yin et al., 2023). Many works (Asai et al., 2023;Yan et al., 2024;Chen et al., 2024;Wang et al., 2024) showed that irrelevant or incorrect contexts could detrimentally affect model performance.",
            "score": 0.6386776480563735,
            "section_title": "Related Work",
            "char_start_offset": 3054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 1825,
                    "end": 1843,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "In Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role in producing the final output by integrating retrieved information with the input query. After the retrieval component pulls relevant knowledge from external sources, the generator synthesizes this information into coherent, contextually appropriate responses. The Large Language Model (LLM) serves as the backbone of the generator, which ensures the generated text is fluent, accurate, and aligned with the original query.",
            "score": 0.6386118488806979,
            "section_title": "Generator Mechanisms in RAG Systems",
            "char_start_offset": 17627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 517
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
            "score": 0.6385772840635362,
            "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
            "char_start_offset": 18167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2073
                },
                {
                    "start": 2075,
                    "end": 2150
                },
                {
                    "start": 2152,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "277113433",
            "title": "When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation Just as It Helps Developers?",
            "text": "Retrieval-Augmented Generation (RAG) [29] is proposed to enable LLMs to handle tasks by leveraging external knowledge without fine-tuning LLMs. Specifically, RAG maintains a database of relevant domain-specific or latest information unfamiliar to LLMs. Given a query, RAG first runs retrieval phase to dynamically identify multiple pieces of information (also known as entities) relevant to the task from the database. Then, the Top-k entities are included as the context in the prompt to augment the generation phase. \n\nIn the context of documentation-based API usage recommendation, RAG works for a coding requirement query  on a database of API documents D = { 1 ,  2 , ...,   }, where   represents the document of an API unfamiliar to the LLM. It executes two phases: \n\nRetrieval Phase. Given the query  as the input to a RAG system, the retriever computes similarity scores between  and each document   \u2208 D to rank all documents. The RAG system adopts a parameter  that determines the number of top-ranked documents to use as relevant contextual information. The retrieval process can be formally represented as: \n\nAugmented Generation Phase. A prompt is constructed based on top- relevant documents as the contextual information and  as the query, with an instruction to clarify the logical relationship between them. The prompt is input to the generator LLM to get the recommended API usage, which can be formally represented as: \n\nResult \u2190 GeneratorLLM (\u27e8Retriever (D, , ) , \u27e9)",
            "score": 0.6385491931967809,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 5609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1485
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "277667583",
            "title": "FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG",
            "text": "Traditional RAG systems [Ram et al., 2023] improve the accuracy and specificity of LLM generation by retrieving query-relevant content from an external knowledge base and adding the retrieved content to the context window of the LLM, and easily enable rapid updating of knowledge without retraining or fine-tuning the model. Naive RAG [Gao et al., 2023] typically divides the source document into chunks and embeds these chunks into vector space. For retrieving chunks relevant to a query, simply embed the query into vector space and select the first k nearest chunks. Many RAG systems are further enhanced based on Naive RAG, including RQ-RAG [Chan et al., 2024], Self-RAG [Asai et al., 2023b], andothers [Qian et al., 2024;Lin et al., 2023;Luo et al., 2023]. RQ-RAG is an advanced RAG system designed to refine search queries dynamically. It equips the model with the ability to rewrite, decompose, and disambiguate queries. Self-RAG introduces a mechanism for self-reflection into the RAG framework. It allows the model to assess whether it needs to retrieve additional information before generating an answer, evaluate the relevance of retrieved documents, and critique the generated responses. Although these advanced RAG methods further improve the accuracy of LLM generation, their effectiveness is limited by the fact that they can only retrieve a subset of documents. This limitation hinders the comprehensive acquisition of global information, posing challenges in solving the QFS task.",
            "score": 0.6383997224533065,
            "section_title": "Chunk-based Retrieval-Augmented Generation",
            "char_start_offset": 5998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1497
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 353,
                    "matchedPaperCorpusId": "218539531"
                },
                {
                    "start": 675,
                    "end": 700,
                    "matchedPaperCorpusId": "8599292"
                },
                {
                    "start": 726,
                    "end": 743,
                    "matchedPaperCorpusId": "268510182"
                },
                {
                    "start": 743,
                    "end": 760,
                    "matchedPaperCorpusId": "268510182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109]. \n\nQuery expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122]. Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114]. Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123]. Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105]. \n\nRetrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications. RAG capabilities are experienced in varied settings. In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114]. \n\nIn question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context. This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120]. RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124]. Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125]. Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].",
            "score": 0.6381589617040168,
            "section_title": "Retrieval augmented generation",
            "char_start_offset": 50470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "265498312"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "270702738",
            "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
            "text": "Retrieval Augmented Generation (RAG) [10,21,22,29] has emerged as a popular technique to augment large language model (LLM) generation for knowledge-intensive tasks such as open-domain question answering or fact verification [44].Using the top- retrieved segments from a suitable retrieval system, RAG systems output an answer summary grounded on the relevant context.RAG systems mitigate factual inconsistencies in LLM outputs [19,26,29,34], and enhance interpretability [21] and generalization [20], thus facilitating a wider adoption across several domains like Medicine [55] and Finance [23].\n\nSeveral companies provide end-to-end RAG frameworks such as Bing Search [39], or Google Gemini [5].Most of these systems are either proprietary or offer limited user customization.Likewise, the absence of a standardized RAG framework makes implementing RAG at a large scale challenging.Implementing atop existing frameworks requires custom code for multiple steps including retrieval, reranking, and generation.To promote wider adoption of RAG in academia, we develop Ragnar\u00f6k, a user-friendly, reusable, endto-end RAG framework offering code for customizable retrievers, rerankers, and generation models.\n\nRagnar\u00f6k comprises two key modules: (R) Retrieval and (AG) Augmented Generation.The retrieval module incorporates both the retrieval and re-ranking stages to yield the top- retrieved segments for an input user topic.Next, the augmented generation module uses the user-provided topic and retrieved segments to produce an RAG answer, formatted into individual sentences, citing the relevant information from the top- retrieved segments.Ragnar\u00f6k is deeply integrated with existing Python frameworks, such as Pyserini [31] and rank_llm [46,47] and can be easily installed via PyPI using \"pip install pyragnarok\".The framework offers easy-to-use REST APIs and an integrated WebUI to enhance user-friendliness and improve the human evaluation experience.\n\nThe Ragnar\u00f6k framework will be used for providing baselines in the upcoming TREC 2024 Retrieval Augmented Generation (RAG) Track. 1 An ideal framework should include a sufficiently large document collection covering diverse information and non-factoid, decompositional topics requiring long-form answers.",
            "score": 0.6372425052452517,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 596
                },
                {
                    "start": 598,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1203
                },
                {
                    "start": 1205,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1813
                },
                {
                    "start": 1813,
                    "end": 1953
                },
                {
                    "start": 1955,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 41,
                    "end": 44,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 44,
                    "end": 47,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 47,
                    "end": 50,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 428,
                    "end": 432,
                    "matchedPaperCorpusId": "258865710"
                },
                {
                    "start": 432,
                    "end": 435,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "235366815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "269982691",
            "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
            "text": "In RAG, the choice of retrievers and generators plays a crucial role in determining the final performance. Therefore, we conduct an experiment to explore their impact. Note that we do not include methods requiring specific retrievers or generators (e.g., Self-RAG requires trained models). \n\nAs shown in the left part of Figure 2, most methods are sensitive to retrieval quality. The performance gap between using the BM25 and E5 retriever can approach nearly 10%. This gap is likely due to the presence of more noise in the retrieved passages of BM25, thereby disturbing the generation process with irrelevant information. In contrast, compression methods such as RECOMP show better robustness across various retrievers, suggesting that compression effectively mitigates the noise. Moreover, this robustness can be further enhanced by fine-tuning the generator. For example, Ret-Robust introduces a generator-specific training strategy that effectively minimizes the impact of irrelevant passages. \n\nThe influence of generators is also explored by using two models in different sizes (Qwen-1.5-14B and LLaMA-3-8B). Intriguingly, the larger model cannot consistently outperform the smaller one. For example, in methods such as FLARE and RECOMP, the smaller model yields better performance. Given that LLaMA-3-8B outperforms Qwen-1.5-14B in many public benchmarks, it suggests that the LLMs' RAG performance may be highly relevant to their general generation capabilities rather than their size. This observation highlights the complexity of LLMs' performance evaluation, suggesting that factors other than size, such as model architecture or training data quality, can also play significant roles.",
            "score": 0.6366293931813476,
            "section_title": "Impact of Retrievers and Generators",
            "char_start_offset": 17088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1697
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "272689561",
            "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
            "text": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
            "score": 0.636267223686726,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6181640625
        },
        {
            "corpus_id": "272987023",
            "title": "Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems",
            "text": "We introduce the RAG methods from the FlashRAG toolkit that were evaluated in our study. The selection was based on two key criteria. First, we aimed to avoid RAG methods that were finetuned using specific benchmark datasets or embedding models, to minimize the negative effects of overfitting and ensure the fairness of the experiments. Second, we selected models that covered all components of the RAG pipeline, allowing us to evaluate whether different components contribute to unfairness. Based on these criteria, we selected two baseline models and four RAG methods as follows: Zero-Shot, the baseline model generates answers solely based on the language model itself, without incorporating any external knowledge. This allows us to understand the inherent biases present in the language model alone. Naive, directly utilizes retrieved documents to generate answers without any additional optimization or processing, highlighting how unprocessed external knowledge affects the outcomes. Selective-Context (Li et al., 2023), focuses on the refinement process by compressing the input prompt to select the most relevant context from the retrieved documents. It tests how refining the context affects the balance between fairness and accuracy. SKR (Wang et al., 2023b), enhances the decisionmaking component (the \"judger\"), which determines whether to retrieve documents for a query. This model allows us to analyze the impact of selective retrieval on fairness, especially when determining the necessity of external knowledge for a given query. FLARE (Jiang et al., 2023) and Iter-RetGen (Shao et al., 2023), both models optimize the entire RAG flow, including multiple retrievals and generation processes. The difference is that FLARE optimizes performance by actively deciding when and what to retrieve throughout the generation process, while Iter-RetGen improves performance by leveraging both retrieval-augmented generation and generation-augmented retrieval processes.",
            "score": 0.6342828052645535,
            "section_title": "RAG Pipeline",
            "char_start_offset": 10994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 1010,
                    "end": 1027,
                    "matchedPaperCorpusId": "263830231"
                },
                {
                    "start": 1250,
                    "end": 1270,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 1554,
                    "end": 1574,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1591,
                    "end": 1610,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6171875
        },
        {
            "corpus_id": "274859726",
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "text": "The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at https://github.com/wujwyi/PA-RAG.",
            "score": 0.633802846349254,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "273185794",
            "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) improves language model performance by incorporating relevant knowledge from external sources (Guu et al., 2020;Karpukhin et al., 2020;Lewis et al., 2020). In contrast to na\u00efve RAG, optimizing the retrieval stage can effectively enhance context relevance and improve generation performance (Jiang et al., 2023;Lin et al., 2024;Ma et al., 2023;Sarthi et al., 2024;Shi et al., 2024;Trivedi et al., 2023). An example is REPLUG, in which Shi et al. (2024) leverage LLM as supervision to learn a dense retriever model. In addition, encoding documents can increase knowledge retrieval and improve generation capabilities (Borgeaud et al., 2022;Izacard and Grave, 2021;Izacard et al., 2023;Khandelwal et al., 2019). For instance, Izacard and Grave (2021) leverages fusion-in-decoder architecture to encode multiple question-passage pairs while maintaining the model efficiency. Alternatively, selectively utilizing knowledge from the documents improves the robustness of LLMs against irrelevant context (Yan et al., 2024;Yoran et al., 2024;Yu et al., 2023;Yue et al., 2024;Zhang et al., 2024). For example, RAFT proposes to train language models with negative documents to improve generation quality and relevance (Zhang et al., 2024). Concurrent to our work, long-document retrieval and datastore scaling are proposed to optimize RAG performance (Jiang et al., 2024;Shao et al., 2024). Despite such progress, inference scaling remains under-explored for long-context RAG methods. As such, we investigate how variations in inference computation impact RAG performance, with the goal of optimizing test-time compute allocation.",
            "score": 0.6336342432687974,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 9246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1656
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 149,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 149,
                    "end": 172,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 327,
                    "end": 347,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 347,
                    "end": 364,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 364,
                    "end": 380,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "267334785"
                },
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 417,
                    "end": 438,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 471,
                    "end": 488,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 652,
                    "end": 675,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 675,
                    "end": 699,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 699,
                    "end": 720,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 720,
                    "end": 744,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1051,
                    "end": 1070,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "276408682",
            "title": "Improving Retrieval-Augmented Deep Assertion Generation via Joint Training",
            "text": "Experimental Design. In this section, we investigate how the proposed joint training strategy module affects performance in the retrieval-augmented generation setting. We first consider a \"No retriever\" baseline that directly finetunes the generator with input focal-tests and their assertions in the training datasets. We then compare our jointtraining retriever in AG-RAG with different retrievers: (1) a \"Random Retriever\" baseline that utilizes random sampling as the retriever; (2) an \"IR Retriever\" baseline that utilizes IR as the retriever, following Yu et al. [16] and Sun et al. [17]; \n\n(3) a \"Pre-trained Retriever\" baseline that utilizes a pretrained Codet5 without any fine-tuning as the retriever; (4) a \"Fine-tuned Retriever\" baseline that first fine-tunes a  pre-trained CodeT5 using the training set, and utilizes the trained CodeT5 as the retriever. \n\nResults. Table 4 presents the comparison results of our default retriever and baselines. Overall, we find that the default joint retriever achieves the best performance on all metrics and datasets. Particularly, IR Retriever achieves 57.26% for accuracy, 72.54% for CodeBLEU, and 73.57% for BLEU on average, outperforming No Retriever by 7.48%, 5.66%, and 5.30%, demonstrating the benefits of retrieving similar TAPs in guiding the assertion generation process and motivating AG-RAG to explore more powerful PLMbased retriever. Meanwhile, the downgraded performance of Random Retriever implies that randomly retrieved TAPs cannot provide helpful guiding signals due to the inherent noise in the randomly retrieved data, which lacks relevance. Besides, fine-tuning CodeT5 is able to retrieve more useful TAPs for the generator than the default CodeT5 model, with a prediction accuracy of 64.19% and 53.40% on both datasets. The possible reason lies in that, compared with the default CodeT5, fine-tuning CodeT5 incorporates knowledge of assertion generation, which improves its ability to generate more effective embeddings for retrieval.",
            "score": 0.633054542953593,
            "section_title": "RQ2: Analysis of Joint Training",
            "char_start_offset": 35524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "248347700"
                },
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "261945147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "267750557",
            "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
            "text": "In this work, we have introduced Mafin, a novel methodology for fine-tuning black-box embedding models, thereby addressing a significant gap in the field of Retrieval Augmented Generation (RAG). \n\nRecognizing the need for enhanced performance in black-box embedding models, especially when applied to new documents or within specific domains, Mafin effectively meets this challenge by augmenting a black-box model with a small tunable embedding model thus significantly boosting its performance while only requiring a minimal fine-tuning cost. This method leverages both the powerful language representation provided by large pre-trained models and the benefits of fine-tuning with a small embedding model. The low fine-tuning cost of the small models enables its use for large-scale, customized online fine-tuning tailored to each company and individual, promising to be a performance-effective and cost-efficient framework for the RAG infrastructure. \n\nWe have demonstrated Mafin's excellence in fine-tuning embedding models for text retrieval tasks within the RAG framework. Future work will explore Mafin's potential across a broader range of fields. We aim to test and validate our fine-tuning methodology's effectiveness in tasks such as classification and clustering, thereby further expanding the applicability and impact of our approach. \n\nhttp://arxiv.org/ps/2402.12177v4",
            "score": 0.6327797363862314,
            "section_title": "Conclusion",
            "char_start_offset": 23095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Large language models (LLMs) have achieved remarkable performance across various natural language processing tasks (Brown et al., 2020;Ope-nAI, 2023;Touvron et al., 2023).Despite their extensive parameters enabling them to learn rich knowledge during pre-training, LLMs may still generate hallucinated, outdated, or inaccurate content, especially in scenarios requiring long-tail knowledge (Ji et al., 2023;Zhang et al., 2023b).\n\n* Correpsonding author.\n\nTo address this problem, retrieval-augmented generation (RAG) has emerged as a pivotal strategy.By explicitly decoupling knowledge retrieval from the backbone LLMs, such architectures have achieved more accurate and reliable content generation and shown particularly enhanced performance on knowledge-intensive tasks such as open-domain question answering (Lewis et al., 2020b;Petroni et al., 2021;Izacard et al., 2023;Tan et al., 2024;Jin et al., 2024).\n\nExisting efforts in RAG development can be roughly categorized into two groups (as illustrated in Figure 1).The first group leverages the incontext learning capabilities of LLMs by incorporating retrieved information into the input along with appropriate prompts (Shi et al., 2023;Ram et al., 2023).This allows for straightforward application to any off-the-shelf LLM without tuning its parameters.However, its effectiveness largely depends on the human experience in crafting effective prompts and the LLM's ability to interpret these prompts.The second group focuses on training LLMs to enhance their performance in RAG scenarios.This training might involve either endto-end pre-training (Guu et al., 2020;Borgeaud et al., 2022) or fine-tuning (Lin et al., 2023;Wang et al., 2023) for specific tasks.These approaches can often lead to better performance, but they require significant computational resources.Recently, parameter-efficient fine-tuning techniques, such as LoRA (Hu et al., 2022), have been widely studied, significantly reducing training costs.These methods can optimize the LLMs' parameters for RAG, but unfortunately compromise the model's general generation abilities in scenarios without retrieval.",
            "score": 0.6324491162131766,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 428
                },
                {
                    "start": 430,
                    "end": 453
                },
                {
                    "start": 455,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 909
                },
                {
                    "start": 911,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1210
                },
                {
                    "start": 1210,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1455
                },
                {
                    "start": 1455,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1971
                },
                {
                    "start": 1971,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 811,
                    "end": 832,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 832,
                    "end": 853,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 853,
                    "end": 874,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1619,
                    "end": 1641,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1888,
                    "end": 1905,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488037109375
        },
        {
            "corpus_id": "270620574",
            "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "text": "R 2 AG offers the flexibility to fine-tune R 2 -Former alone or both with LLMs. Thus, in R 2 AG framework, both retrievers and LLMs can be frozen to save computational costs, making R 2 AG suitable for scenarios with limited resources. Overall, our contributions are summarized as follows: \n\n\u2022 We propose R 2 AG, an enhanced RAG framework, to incorporate retrieval information into retrieval augmented generation. Notably, R 2 AG is compatible with low-source scenarios where retrievers and LLMs are frozen. \n\n\u2022 We design a lightweight model, R 2 -Former, to bridge the semantic gap between retrievers and LLMs. R 2 -Former can be seamlessly integrated into existing RAG frameworks using open-source LLMs. \n\n\u2022 We introduce a retrieval-aware prompting strategy to inject retrieval information into the input embeddings, enhancing LLMs' ability to understand relationships among documents without much increase in complexity. \n\nExperimental results demonstrate the superior performance and robustness of R 2 AG in various scenarios. Our analysis shows that R 2 AG increases latency by only 0.8% during inference. Furthermore, it demonstrates that retrieval information anchors LLMs to understand retrieved documents and enhances their generation capabilities. \n\n2 Related Works",
            "score": 0.6322910346595345,
            "section_title": "Introduction",
            "char_start_offset": 4368,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7548828125
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "To enhance the accuracy of question and answer (Q&A) tasks, our approach included supervised finetuning on the zero-shot model for GPT-3.5 Turbo. This is needed because zero shot Retrieval-Augmented Generation (RAG) oftentimes retrieves irrelevant information or irrelevant embeddings. Subsequently, we utilized reprompting on GPT4All and Llama2 for further refinement. This process included feeding in example prompts with given questions, evidence text, and answers to guide the model in learning specific response patterns. Additionally, we benchmarked the fine-tuning process by incorporating the Retrieval-Augmented Generation (RAG) technique for Q&A tasks, thereby enhancing the evaluation and overall performance of the models. \n\nThere is an absence in standardized methodologies for evaluating model performance. We assessed the zero-shot model (LLM without fine-tuning) and subsequently evaluated the model with fine-tuning and reprompting. The approach involved iterative accuracy evaluation, incorporating fine-tuning or reprompting into models such as GPT-3.5 Turbo, GPT4All, Llama2, and Claude. The evaluation metrics used were cosine similarity and Rouge-L, measuring accuracy in each iteration",
            "score": 0.6321870131351649,
            "section_title": "Process and Work",
            "char_start_offset": 9452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "266999736",
            "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
            "text": "With recent advancements in Generative AI and LLMs, the Retrieval-Augmented Generation (RAG) (Lewis et al. 2021) approach has emerged as the preferred strategy for contex-tual question answering. RAG pipeline consists of a retrieval model followed by an LLM to generate the answer. Different approaches have been experimented with to improve components of the RAG in terms of accuracy and minimize hallucinations in answer generation. Khatry et al. (kha 2023) proposed a low-rank residual adaptation approach with the pretrained embedding model to improve the retrieval model. It was shown to lead to improved task-specific retrieval as compared to a general-purpose embeddings-based baseline. Instead of using an interleaved retrieval and generation, Shao et al. (Shao et al. 2023) proposed an iterative retrieval and generation approach where the current model output acts as an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Li et al. (Li et al. 2022) extensively surveys recent RAG-based approaches. \n\nRL has been experimented with to improve RAG. Bacciu et al. (Bacciu et al. 2023) propose an RL-based approach to train an efficient retriever model to search for relevant information in an arbitrarily large database. Once this set of relevant data has been retrieved, it is forwarded to the API-based LLM to generate the answer. In particular, the authors show that RL helps reduce hallucinations by minimizing the number of damaging documents returned by the retriever. Self-RAG (Asai et al. 2023) trains a single LLM that adaptively retrieves passages and generates and reflects on retrieved passages and their generations using reflection tokens. In our work, we assume we do not have access to the gradients of the retrieval model and LLM. We only train a policy model that resides external to RAG. \n\nGPT-4 is observed to provide human-level accuracy for automated evaluation tasks. Hack at al. (Hackl et al. 2023) investigated the consistency of feedback ratings generated by GPT-4 across multiple iterations, time spans, and stylistic variations.",
            "score": 0.6319270540049948,
            "section_title": "Related works",
            "char_start_offset": 5694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 764,
                    "end": 782,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "265221210",
            "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
            "text": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
            "score": 0.6316992916930932,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "277468629",
            "title": "ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations",
            "text": "In the era of LLM, the Retrieval Augmented Generation (RAG) paradigm integrates a retrieval model for the generation model, allowing the generation model to have access to external knowledge, improving the generation's correctness and factuality for downstream tasks, such as question answering or fact verification (Petroni et al., 2021). Traditional RAG methods typically follow a retrieve-then-generate pipeline (Lewis et al., 2020;Gao et al., 2024), where retrieval is conducted independently based on an initial query, and the retrieved documents are concatenated as context for the generation model. While effective for short-form generation tasks, this static pipeline struggles in scenarios requiring long- Figure 3: The pipeline for creating the ScholarCopilot dataset. Our final dataset includes 10M citations matched from arXiv and 6.8M citations matched from Semantic Scholar (one paper may be cited by multiple articles). However, at inference time, to ensure reference quality, we only use the 670K articles from arXiv as the corpus. \n\nform generation with evolving information needs. To address this limitation, recent methods such as FLARE (Jiang et al., 2023) and SelfRAG (Asai et al., 2023) propose iterative RAG strategies, where retrieval and generation are interleaved, allowing retrieval decisions to adapt dynamically based on the generation trajectory. These systems demonstrate improved factual accuracy for long-form content by leveraging the generation context to refine retrieval queries. Recent work, OpenScholar (Asai et al., 2024) aims to improve long-form scientific question answering with self-feedback inference in RAG. However, they still decouple the retrieval and generation models, which can lead to representational misalignment for implicit query intent and increased inference overhead. More unified approaches, such as GritLM (Muennighoff et al., 2025) and OneGen (Zhang et al., 2024), train a unified model to serve both as the generator and retriever. These models share representations and can cache hidden states during generation, improving the efficiency of the system.",
            "score": 0.6312788991072761,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 5737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 338,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 415,
                    "end": 435,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1156,
                    "end": 1176,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1189,
                    "end": 1208,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1869,
                    "end": 1895,
                    "matchedPaperCorpusId": "267681873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "273653854",
            "title": "EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update",
            "text": "Retrieval-Augmented Generation Enhancements. Retrieval-Augmented Generation (RAG) improves language models (LMs) by integrating relevant text from knowledge bases [19]. Extensions like Adaptive RAG [16], Corrective RAG [20], and Self-RAG [21] address retrieval strategy limitations and query complexity. These methods adjust retrieval based on query difficulty to enhance performance. Mallen et al. [22] classify query complexity using entity frequency, applying binary decisions on retrieval sufficiency. Qi et al. [23] use fixed operations (retrieval, reading, reranking), requiring specialized LM training. Self-RAG [21] retrieves, critiques, and generates dynamically, though uniform query handling remains suboptimal. RAG systems are also being integrated with 6G edge networks [24]. Traditional methods like Adaptive-RAG and Corrective-RAG refine retrieval based on query complexity but often rely on centralized frameworks. EACO-RAG, by contrast, uses edge computing to distribute knowledge across multiple nodes, reducing delays and communication overhead while dynamically updating local databases. \n\nCost-Effective Large Language Model Usage. Reducing LLM deployment costs is an active research focus. Techniques like model quantization [25,26] and pruning [27] cut costs but may reduce performance and require specialized hardware. LLM distillation, where smaller models mimic larger ones, offers a solution. Caching LLM responses for routine queries [28][29][30][31] or reusing key-value states during inference [32,33] are other strategies. FrugalGPT [34] uses prompt engineering and model multiplexing to select model size based on query complexity, while model multiplexing dynamically chooses the appropriate model size [35,36]. Techniques like model quantization and LLM distillation reduce computational costs but require specialized hardware. EACO-RAG offers a holistic solution, combining edge and cloud resources to optimize retrieval and generation, minimizing resource use and operational costs through a multi-armed bandit framework. \n\nResource Allocation Strategies in Edge Computing. Optimizing resource allocation in edge computing focuses on balancing delay, energy, and processing power [37]. Techniques like task offloading and resource scheduling aim to minimize latency and energy consumption [38].",
            "score": 0.6308497585551215,
            "section_title": "RELATED WORK",
            "char_start_offset": 32104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2221
                },
                {
                    "start": 2222,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "240288953"
                },
                {
                    "start": 783,
                    "end": 787,
                    "matchedPaperCorpusId": "271662512"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "258180013"
                },
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "258823276"
                },
                {
                    "start": 1524,
                    "end": 1528,
                    "matchedPaperCorpusId": "271515809"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "265607979"
                },
                {
                    "start": 2216,
                    "end": 2220,
                    "matchedPaperCorpusId": "70031557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
            "score": 0.6307701953852428,
            "section_title": "Overview of RAG Models",
            "char_start_offset": 10564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 773,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "277451521",
            "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs",
            "text": "Databricks Mosaic Research [75] presents a comprehensive evaluation of long-context Retrieval Augmented Generation (RAG) capabilities across several state-of-the-art language models. The authors compared models including Ope-nAI o1, Anthropic Claude, Google DeepMind Gemini, and Meta Llama on three diverse datasets-Databricks DocsQA, FinanceBench, and Natural Questions-by varying context lengths from 2,000 up to 2,000,000 tokens retrieved from a vector database. Using GPT-4o as the judge to assess answer correctness, the study found that Google DeepMind's Gemini 1.5 maintains strong RAG performance even with highly extended contexts (up to 2 million tokens). In contrast, open models like Llama-3.1-405B show significant performance degradation beyond 32k tokens. OpenAI's o1-preview model achieved the highest average performance score of 0.763 across the tested context lengths, although standard OpenAI o1 models underperformed at shorter contexts on specific datasets. Most models demonstrated peak performance in the 32k-64k token range before declining, highlighting critical insights into the optimal context lengths and trade-offs in deploying long-context RAG systems. \n\nWang [76] offers a comprehensive investigation into Retrieval-Augmented Generation (RAG) techniques, which are widely recognized for their ability to integrate current information, reduce hallucinations, and enhance output quality in domain-specific applications. By dissecting the multi-step RAG workflow-ranging from query classification, advanced chunking, and hybrid retrieval using methods like HyDE, to effective reranking with models such as MonoT5-the study systematically evaluates various components to establish best practices that strike a balance between performance and efficiency. Additionally, the work explores multimodal retrieval strategies that improve the handling of visual inputs and accelerate the generation of multimodal content via a \"retrieval as generation\" approach. Although the paper is more of a synthesis of existing methodologies rather than a breakthrough in novel techniques, its extensive experimentation and clear recommendations, including the use of Milvus for vector storage, reverse repacking strategies, and mixed-context finetuning for robust generation, provide valuable insights for optimizing RAG systems.",
            "score": 0.6306598988059424,
            "section_title": "K. Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 128503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "270870251"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "272832474",
            "title": "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA",
            "text": "RAG. Retrieval-Augmented Generation (RAG) augments the LM input with retrieved text passages (Lewis et al., 2020;Guu et al., 2020), leading to large improvements in knowledge-intensive tasks (Ram et al., 2023). However, the improved task performance of such approaches have been shown to come at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Oh and Thorne, 2023;Shi et al., 2023), and lack of attributions (Liu et al., 2023;Gao et al., 2023). Yoran et al. ( 2023) use a natural language inference model and Xu et al. ( 2023) employ a summarization model to filter out or compress retrieved passages before using them to prompt the LM to generate the output. In comparison, SELF-RAG (Asai et al., 2023) processes passages in parallel and filters out irrelevant ones through self-reflection, without relying on external models at inference. The self-reflection mechanism of SELF-RAG also evaluates other aspects of the model output quality, including factuality and attribution. However, SELF-RAG is not trained to comprehend conversational context, which we specifically equip SELF-multi-RAG to do. Kulkarni et al. (2024) propose a reinforcement learning (RL) based approach where the policy model can perform two actions: fetch conversation context or skip retrieval. Their approach was shown to save costs by reducing tokens when the model decides retrieval is not needed, while also slightly improving response generation. In contrast, the goal of SELF-multi-RAG is not only to decrease retrieval redundancy but also increase retrieval effectiveness. \n\nLLMs and Multi-turn Conversations. In order to enable LLMs to interact with humans in a dialogue-based settings, the standard approach is to collect multi-turn instructions (Chiang et al., 2023;Ji et al., 2023), often synthetically generated using strong LLMs, and used to fine tune the LLMs for the task of response generation. This process is known as instruction fine-tuning, which enables LLMs to generate responses in a multi-turn dialogue setting.",
            "score": 0.6303454160725239,
            "section_title": "Related Work",
            "char_start_offset": 7165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 113,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 113,
                    "end": 130,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 330,
                    "end": 351,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.485107421875
        },
        {
            "corpus_id": "274776545",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "text": "Retrieval-augmented Generation Retrievalaugmented generation (RAG) improves generation quality by incorporating relevant context from external knowledge bases, which typically employ a separate dense retriever (Gao et al., 2024;Tan et al., 2024b;Jin et al., 2024b;Tan et al., 2024a;Zhou et al., 2024). Based on training approaches, current RAG systems fall into three categories: \n\n(1) Directly prompt of generative models with retrieved context (Press et al., 2023;Trivedi et al., et al., 2020;Singh et al., 2021). However, joint training faces challenges due to the architectural differences between retrieval and generation, as well as the need for updating document indices during training. Some approaches aim to unify dense retrieval and generation within a single model, including GritLM (Muennighoff et al., 2024) and OneGen (Zhang et al., 2024a). However, GritLM operates as two distinct models with separate attention mechanisms that share parameters, while OneGen still relies on retrieving passage chunks as input for subsequent generation. \n\nGenerative Retrieval Generative retrieval (GR) retrieves by directly generating document identifiers (DocIDs) without the need for traditional document indices (Metzler et al., 2021) (Li et al., 2023a;Tang et al., 2024), and learnable DocIDs (Sun et al., 2023;Wang et al., 2023;Yang et al., 2023). However, these methods mainly focus on optimizing retrieval tasks, without considering its connections with downstream tasks. Even though UniGen (Li et al., 2024c) and CorpusLM (Li et al., 2024a) address downstream tasks, they still require mapping the generated Do-cIDs to the corresponding documents before feeding them into the generator. While RICHES (Jain et al., 2024) attempts to streamline this process but fails to solve the false pruning issue, which leads to suboptimal downstream task performance.",
            "score": 0.6302971236587627,
            "section_title": "Related Work",
            "char_start_offset": 22502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "267750726"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "267751485"
                },
                {
                    "start": 446,
                    "end": 466,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 495,
                    "end": 514,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1297,
                    "end": 1315,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1315,
                    "end": 1333,
                    "matchedPaperCorpusId": "264350310"
                },
                {
                    "start": 1333,
                    "end": 1351,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "266359654"
                },
                {
                    "start": 1530,
                    "end": 1548,
                    "matchedPaperCorpusId": "267406766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "273323692",
            "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
            "text": "2.1 RETRIEVAL-AUGMENTED GENERATION RAG technology achieves good performance in the era of LLMs by providing external knowledge to assist answering questions and reducing hallucinations (Jiang et al., 2023;Asai et al., 2023;Gao et al., 2024;Chen et al., 2024). The initial strategy of RAG involves using a retriever to search for and retain highly relevant chunks from a knowledge base based on a query, these chunks are then fed into the generation module as external knowledge, enhancing its performance (Qi et al., 2019;Lewis et al., 2020;Gur et al., 2021;Yu et al., 2022). To improve RAG effectiveness, some approaches have introduced iterative RAG, proposing various enhancements such as query expansion and rewriting (Ma et al., 2023;Li et al., 2024a;Chan et al., 2024;Shi et al., 2024), and others try to improve the corporation between retrieval and generation (Qian et al., 2024;Su et al., 2024;Luo et al., 2024;Zhang et al., 2024). Although existing methods achieve strong performance on multi-hop tasks like HotpotQA, chunk-based RAG struggles with knowledge-intensive tasks (Wang et al., 2024a). This is because chunks must contain excessive text noise and do not capture the interconnections among information , thus LLMs cannot effectively use augmented knowledge.",
            "score": 0.6298567740084897,
            "section_title": "RELATED WORK",
            "char_start_offset": 6708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1277
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "268819582"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 505,
                    "end": 522,
                    "matchedPaperCorpusId": "202773198"
                },
                {
                    "start": 522,
                    "end": 541,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 541,
                    "end": 558,
                    "matchedPaperCorpusId": "233289920"
                },
                {
                    "start": 558,
                    "end": 574,
                    "matchedPaperCorpusId": "222272210"
                },
                {
                    "start": 739,
                    "end": 756,
                    "matchedPaperCorpusId": "267406766"
                },
                {
                    "start": 756,
                    "end": 774,
                    "matchedPaperCorpusId": "268819582"
                },
                {
                    "start": 774,
                    "end": 791,
                    "matchedPaperCorpusId": "270688739"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "267682194"
                },
                {
                    "start": 887,
                    "end": 903,
                    "matchedPaperCorpusId": "268509926"
                },
                {
                    "start": 903,
                    "end": 920,
                    "matchedPaperCorpusId": "271915473"
                },
                {
                    "start": 920,
                    "end": 939,
                    "matchedPaperCorpusId": "271915498"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468017578125
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "In addition, since retrieval results are fed as a part of the LLM inputs, whether the LLMs can generate the response based on the retrieved documents instead of their internal knowledge could be seen as a special type of instruction-following ability. Studies have been conducted to teach LLMs to utilize retrieval results faithfully and constantly in RAG systems [125] On the other hand, factors such as irrelevant results and ranking perturbations are well acknowledged to be harmful for the performance of generators in RAG, so there are also studies that try to improve the robustness of LLMs from the perspective of RAG. For example, Zhang et al. [126] proposes to fine tune LLMs with the presence of retrieval results (i.e., retrieval augmented fine tuning) so that LLMs can learn the domain-specific knowledge introduced by the retriever and improve their robustness against potential distracting information from retrieval. \n\nFrom the perspective of augmentation methods, existing research mostly focuses on the joint optimization of RAG system as a whole. In other words, the loss functions of RAG optimization should be built from the performance metrics of downstream tasks directly. While this paradigm is appealing, it often has strict requirements on the design of RAG systems. Particularly, it's difficult to apply such joint optimization algorithms on a RAG system in which retrievers and generators are loosely connected through prompts constructed from discrete retrieval results. While reinforcement learning could solve the problem in theory, its empirical performance when being used as the solo optimization algorithms for ranking systems is still not satisfying at this point [127]. If you already have a good retriever and only conduct fine-tuning with a fixed LLM, then it may work [128], but this still doesn't look like a perfect solution because reinforcement learning usually subject to large variance in practice. To the best of our knowledge, how to directly connect the training of retrievers with the auto-regressive loss of the generators in RAG is still an open question. Answering this question requires us to go deep into the structure of generative AI models and retrieval models, and develop new model structures that can take advantages from studies on both sides.",
            "score": 0.6289227431972061,
            "section_title": "Optimization of Retrieval and Generation",
            "char_start_offset": 42008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "276741193",
            "title": "Beyond QA Pairs: Assessing Parameter-Efficient Fine-Tuning for Fact Embedding in LLMs",
            "text": "Fine-tuning for domainspecific tasks has been extensively explored. The RAFT approach (Zhang et al. 2024) combines Retrieval-Augmented Generation (RAG) with fine-tuning to enhance LLM performance in specific domains by training models to disregard irrelevant retrieved documents, improving focus and accuracy. Similarly, \"RAG vs Fine-tuning\" (Gupta et al. 2024) compares both approaches across various LLMs, demonstrating how each method can be effectively employed for domain-specific applications, particularly in underexplored sectors like agriculture. Additionally, \"Fine-tuning Language Models for Factuality\" (Tian et al. 2023) leverages recent innovations in factuality judgment and preference optimization algorithms to improve the factual accuracy of LLMs, offering a novel approach to mitigating misinformation. \n\nInstruction Tuning and Data Selection Efficient data selection for instruction tuning is crucial for optimizing LLM performance. \"From Quantity to Quality\" (Li et al. 2023) introduces a self-guided methodology that employs the Instruction-Following Difficulty metric to identify highquality instruction data, enhancing training efficiency. Additionally, \"Rethinking Data Selection for Supervised Fine-Tuning\" (Shen 2024) argues that selecting data reflecting human-like interactions, rather than purely based on quality and diversity, yields better results in aligning models with human expectations. The MoDS approach (Du, Zong, and Zhang 2023) further refines data selection by focusing on quality, coverage, and necessity, demonstrating improved performance with a significantly reduced dataset. Addressing LLM limitations such as hallucinations and weak numerical reasoning, ToolQA (Zhuang et al. 2024) introduces a dataset to evaluate LLMs' ability to use external tools for question answering, providing insights into their strengths and weaknesses.",
            "score": 0.6288174196931043,
            "section_title": "Domain-Specific Adaptation",
            "char_start_offset": 7054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1879
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "273549254",
            "title": "SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback",
            "text": "Similar to Vanilla RAG, it uses the input question to retrieve the search engine. We then finetune the answer generator given the input question along with the observations. SFT RAG can be regarded as only training the answer generator and leave the other two modules as they are. \n\nGPT4 Rewriter + SFT RAG adds a query rewrite module than SFT RAG. We ask GPT4, which is one of the most powerful LLMs, to rewrite the search query before feeding it to the search engine. \n\nIn this case, the answer generator and the query rewriter are separately optimized. Self-RAG (Asai et al., 2024) leverages GPT-4 to construct a large-scale dataset encompassing tasks such as determining whether retrieval is necessary, assessing the relevance of retrieved content, and formulating appropriate responses. Subsequently, SFT is employed to enable the LLM to adaptively perform retrieval and generation. Retrieval is triggered when the model predicts a special retrieval token exceeding a predefined threshold, followed by answer generation.",
            "score": 0.6287729662970969,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 15379,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1025
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 584,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "277451521",
            "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs",
            "text": "Although the paper is more of a synthesis of existing methodologies rather than a breakthrough in novel techniques, its extensive experimentation and clear recommendations, including the use of Milvus for vector storage, reverse repacking strategies, and mixed-context finetuning for robust generation, provide valuable insights for optimizing RAG systems. \n\n1) Chain-of-Retrieval Augmented Generation (CoRAG): Wang et al. [77] propose CoRAG (Chain-of-Retrieval Augmented Generation), a novel framework designed to enhance traditional RAG models by incorporating iterative retrieval and reasoning steps before answer generation. Conventional RAG approaches typically rely on a single retrieval phase, which can limit performance when handling complex, multihop queries due to incomplete or imperfect retrievals. In contrast, CoRAG dynamically reformulates the query as the model's internal state evolves, enabling the construction of intermediate retrieval chains that better capture the necessary context. The training process leverages rejection sampling to automatically generate these chains, effectively augmenting datasets that otherwise only provide the final answer. The method employs flexible decoding strategies at inference time to manage computational demands by controlling the length and number of retrieval chains sampled. Empirical results across various benchmarks, including a notable improvement in exact match scores for multi-hop question answering and state-of-the-art performance on the KILT benchmark, validate the efficacy of CoRAG 2) RAG mechanism with \"Reason-in-Documents\" module: Li et al. [5] propose Search-o1, an innovative framework designed to enhance large reasoning models (LRMs) such as OpenAI-o1, which, despite their robust stepwise reasoning abilities, often encounter limitations due to insufficient knowledge during extended reasoning processes. To overcome these challenges, Search-o1 incorporates an agentic retrievalaugmented generation (RAG) mechanism that dynamically searches for and retrieves external knowledge when uncertainties arise. Furthermore, the framework includes a dedicated Reason-in-Documents module that refines and analyzes the verbose retrieved content, ensuring that only coherent and relevant information is integrated into the reasoning chain.",
            "score": 0.6286342690772359,
            "section_title": "K. Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 130487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2312
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "276885273",
            "title": "Ext2Gen: Alignment through Unified Extraction and Generation for Robust Retrieval-Augmented Generation",
            "text": "Retrieval in RAG is an essential process to fetch the most relevant text chunks to ground the responses to the given query. Two traditional approaches are employed for retrieval: sparse retrieval, which relies on lexical-based methods such as BM25 (Robertson et al., 2009), and dense retrieval, which uses text embeddings from both queries and text chunks (Zhao et al., 2024). With the recent advance in RAG, significant efforts have been made to maximize retrieval performance. These include techniques: query expansion enriches the original query with semantically related terms to improve recall using LLMs (Gao et al., 2023a;Wang et al., 2023;Zhang et al., 2024;Rashid et al., 2024); re-ranking refines the initial retrieval results using more sophisticated models, often leveraging cross-encoders for better relevance estimation (Reddy et al., 2024;Hwang et al., 2024;Yu et al., 2024b); and self-critique iteratively verifies retrieved content for factual consistency (Asai et al., 2024;He et al., 2024;Ye et al., 2024) and can integrate web search for up-to-date information (Yan et al., 2024). \n\nDespite improving retrieval accuracy, hallucinations during generation necessitate complementary research (Laban et al., 2024;Islam et al., 2024). \n\nGeneration in RAG is the crucial process of producing responses grounded in retrieved content. However, hallucinations still persist due to the inability of the LLM on noisy and overloaded information (Cuconasu et al., 2024). In particular, Laban et al. ( 2024) evaluated 50 RAG systems on the \"Summary of a Haystack\" benchmark, revealing that robust generation remains an open challenge even with high retrieval accuracy. To the best of our knowledge, efforts to enhance the robustness of generation models against uncertain placement and information overload are limited. \n\nA few recent works highlight additional challenges. Jain et al. (2024) integrated retrieval into generation, eliminating their separation for improved performance. Islam et al. (2024) enhanced reasoning capabilities using Mixture-of-Experts models.",
            "score": 0.6286192756578833,
            "section_title": "Related Work",
            "char_start_offset": 5916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 272,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 356,
                    "end": 375,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 610,
                    "end": 629,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 647,
                    "end": 666,
                    "matchedPaperCorpusId": "266977456"
                },
                {
                    "start": 834,
                    "end": 854,
                    "matchedPaperCorpusId": "270702598"
                },
                {
                    "start": 873,
                    "end": 890,
                    "matchedPaperCorpusId": "270878612"
                },
                {
                    "start": 973,
                    "end": 992,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 992,
                    "end": 1008,
                    "matchedPaperCorpusId": "273229293"
                },
                {
                    "start": 1008,
                    "end": 1024,
                    "matchedPaperCorpusId": "270620574"
                },
                {
                    "start": 1209,
                    "end": 1229,
                    "matchedPaperCorpusId": "270869972"
                },
                {
                    "start": 1229,
                    "end": 1248,
                    "matchedPaperCorpusId": "273026102"
                },
                {
                    "start": 1453,
                    "end": 1476,
                    "matchedPaperCorpusId": "267301416"
                },
                {
                    "start": 1880,
                    "end": 1898,
                    "matchedPaperCorpusId": "270870462"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66845703125
        },
        {
            "corpus_id": "269761619",
            "title": "From Questions to Insightful Answers: Building an Informed Chatbot for University Resources",
            "text": "Pre-trained Large Language Models (LLMs) are proficient at acquiring extensive knowledge but lack memory expansion or revision capabilities, leading to errors like hallucinations.To address this, hybrid approaches like Retrieval Augmented Generation (RAG) have emerged [14], [17], [18].\n\nRAG integrates input sequences with information retrieved from corpus of an external data source, enriching context for sequence generation.The retriever component selects the top k text passages relevant to the input query, augmenting the model's understanding and enhancing output sequence generation.This process is governed by the equation: p n (z|x) where p n represents the retriever component with parameters n (number of documents or passages a user wants to retrieve), selecting relevant passages z from the knowledge database given input x.",
            "score": 0.6284659147031197,
            "section_title": "B. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 7365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 286
                },
                {
                    "start": 288,
                    "end": 428
                },
                {
                    "start": 428,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 838
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "274859726",
            "title": "PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization",
            "text": "These approaches attempt to bridge the gap between off-the-shelf LLMs and the specific demands of RAG, but both face significant limitations. End-to-end architectures primarily rely on supervised fine-tuning (SFT) of general LLMs (Yoran et al., 2024a;Fang et al., 2024). These approaches focus on constructing high-quality responses in RAG scenarios. However, through further analysis, we found that the requirements for the generators in RAG tasks are highly context-dependent and often interwoven. These varying and intertwined demands make it challenging for a model to meet all RAG objectives through standard SFT alone, as it does not incorporate the necessary preference information required for adapting to different retrieval scenarios. On the other hand, pipeline architectures introduce additional steps beyond generation, such as re-ranking retrieved documents, filtering irrelevant information, or employing post-hoc verification to ensure that citations support the claims (Dong et al., 2024a;Yu et al., 2024;Wang et al., 2024;Sun et al., 2023). However, these additional steps can only satisfy certain RAG requirements and still have a considerable gap in aligning with the global RAG requirements. Moreover, these additional steps introduce extra computational costs and time consumption, making it less practical for large-scale deployment. This further inspires us to think: Is it possible to fully align the generator with the diverse RAG requirements while retaining the simplicity and efficiency of an end-to-end architecture? \n\nTo this end, in this paper, we propose Multi-Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), aiming to optimize the generator of RAG systems to align comprehensively with specific RAG requirements. As illustrated in Figure 1, PA-RAG maintains the endto-end architecture of the generator while enabling it to learn multi-perspective preference information. The training of PA-RAG is divided into two phases. The first phase is foundational capability training, where the generator acquires the basic ability to utilize and cite documents through instruction fine-tuning. To construct high-quality instruction fine-tuning data, we utilize ChatGPT to generate complete and correct answers and employ a citation rewrite mechanism to ensure citation quality.",
            "score": 0.6276762864110484,
            "section_title": "Introduction",
            "char_start_offset": 2236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "273403554",
            "title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval",
            "text": "We introduce Probing-RAG, an efficient RAG pipeline that incorporates a prober to determine whether the language model needs to retrieve additional documents. Similar to the conventional retrieval-augmented generation pipeline, our approach comprises a generating language model and a retriever. Different from the general pipeline, the generator of Probing-RAG leverages the output from the prober and adaptively calls the retriever based on the model's internal hidden state.",
            "score": 0.6274687578523307,
            "section_title": "Method",
            "char_start_offset": 7175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "278501775",
            "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence",
            "text": "Traditional FL methods often lack mechanisms to integrate evolving knowledge across clients without incurring high communication overhead or violating data privacy. To address this, RAG offers a lightweight and privacy-preserving means of keeping the global generator up-to-date while enhancing personalization across domain-diverse clients. In RAG-assisted fine-tuning, the server periodically updates the shared generator on either: (a) pseudo-labeled data produced from its own retrieval-augmented pipeline via self-distillation [122], or (b) highly relevant documents dynamically retrieved from curated external sources (e.g., domain-specific databases, medical literature, or trusted web content) [145]. These documents are not randomly selected, but semantically aligned with the distribution of prior client tasks or updates, enabling the server to capture emergent knowledge relevant to clusters of users. This improves the generator's downstream alignment with personalized user contexts, even without direct exposure to raw user data. \n\nThe integration of RAG into continual pretraining pipelines supports three key objectives for federated personalization. First, it allows adaptive knowledge updates without relying on centralized annotations or fine-tuning per user, which is crucial in privacy-sensitive environments. Second, it enhances factual grounding by repeatedly training on retrieved hard negatives and diverse contexts, thereby reducing hallucination and domain mismatch. Third, by limiting updates to selectively retrieved, task-relevant content, the approach improves sample efficiency and ensures that model refinement reflects actual user needs rather than generic global trends. This serverside process indirectly fuels personalization at the client level. When a client in a niche domain (e.g., legal research or rare disease diagnosis) queries the generator, the model, fine-tuned on semantically similar retrievals, delivers more accurate, relevant, and context-aware outputs, despite never being finetuned on that specific user. Recent studies such as Few-Shot RAG [122] and domain adaptive continual pretraining [145] have shown that this retrieval-centric adaptation not only enhances factual precision but also boosts personalization fidelity in low-resource settings. We summarize the key enabling technologies and discuss their respective advantages and disadvantages, as presented in Table V.",
            "score": 0.627112514871685,
            "section_title": "IV. OPPORTUNITIES FOR PERSONALIZED FEDERATED INTELLIGENCE A. Towards RAG-based Personalized Federated Intelligence",
            "char_start_offset": 44815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2430
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "273850019",
            "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation RAG aims to enhance language models by retrieving and integrating relevant information from external knowledge sources, demonstrating significant improvements in handling knowledge-intensive tasks (Lewis et al. 2020;Guu et al. 2020). RAG models excel in various NLP applications, like question answering (Izacard and Grave 2021; Ram et al. 2023) and summarization (Lin et al. 2023). Several retrieval models have been developed to support RAG frameworks. BM25 (Robertson, Zaragoza et al. 2009), a sparse retrieval model, has been a foundational method for text retrieval tasks. DPR (Karpukhin et al. 2020a) indexes passages into a dense vector space for efficient retrieval and has been widely adopted in subsequent RAG models. Other notable dense retrieval models include Contriever (Izacard et al. 2021), SBERT (Reimers and Gurevych 2019a), and BGE (Xiao and Liu 2023), each contributing to the robustness and effectiveness of retrieval-augmented systems. \n\nRecent advancements in retrieval strategies have optimized the interaction between retrievers and language models. Models like Atlas (Izacard et al. 2023) and RETRO (Borgeaud et al. 2022) employ joint training and architectural modifications to better integrate retrieved information, though these methods are resource-intensive. \n\nLLM-Supervised Learning LLMs have demonstrated remarkable capabilities in natural language understanding and generation (Brown et al. 2020;Chowdhery et al. 2022). Leveraging the knowledge and capabilities of LLMs to guide the training of other models or themselves has recently emerged as a promising direction. Reinforcement Learning from AI Feedback (RLAIF) (Lee et al. 2023) proposes using an LLM to generate preference labels to train a reward model, which then guides the reinforcement learning process, achieving performance comparable to traditional human feedback-based approaches. \n\nIn addition, Wang et al. (2022) propose an LLMbootstrapping approach called self-instruct, where an LLM iteratively generates additional training data for itself. Wang, Yang, and Wei (2023) introduce the LLM-R framework which trains dense retrievers to identify high-quality incontext examples using feedback from LLMs.",
            "score": 0.6268751210923411,
            "section_title": "Related Work",
            "char_start_offset": 5310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 360,
                    "end": 376,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 491,
                    "end": 524,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 844,
                    "end": 872,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1462,
                    "end": 1484,
                    "matchedPaperCorpusId": "144546246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "276249796",
            "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
            "text": "The advancements in Retrieval-Augmented Generation (RAG) systems, such as those proposed in this work, hold significant potential for diverse applications. By enhancing modularity, generalization, and plug-and-play capabilities, these systems can empower applications in edge deployment at relatively low computational cost. However, the deployment of RAG systems still poses risks that need to be carefully considered, despite our efforts to mitigate them. These systems heavily rely on external retrieval sources, which may include biased or unreliable data, leading to the propagation of misinformation. In high-stakes applications like medical or legal advice, incorrect or incomplete retrieval could have severe consequences. Although our proxy is not directly responsible for the content retrieved by the retriever, future research can focus on improving alignment in fairness, robustness, and transparency during information filtering. \u2022 LLM Service: We integrate SGLang4 as our LLM server, which provides compatibility with various state-of-the-art language models, including Qwen2-72B-Instruct (Yang et al., 2024a) and Llama3.3-70B-Instruct (Dubey et al., 2024). Moreover, we also support GPT series models5 . \n\n\u2022 Inference Optimization: Our implementation supports two high-performance inference engines: SGLang and VLLM, allowing users to optimize for different deployment scenarios and hardware configurations. \n\nThis modular architecture ensures both flexibility in model selection and efficiency in deployment, while maintaining robust performance across different configurations. In-domain Datasets. As shown in Table 8, we conduct extensive in-domain experiments on three single-hop and three multi-hop datasets. For each dataset, we randomly sampled 6,000 instances as the training set, with sampling ratios detailed in Table 8. Overall, we utilize only 8% of the original data as the training set. For the in-domain test sets, we randomly sampled 1,000 instances as the test set. FreshQA Multihop-RAG",
            "score": 0.625712374276048,
            "section_title": "Impact Statement",
            "char_start_offset": 31219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1422
                },
                {
                    "start": 1425,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433349609375
        },
        {
            "corpus_id": "274776545",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "text": "The baseline methods include two types: (1) Direct generation: This includes open-source models Llama3-8B (Dubey et al., 2024), Mistral-7B (Jiang et al., 2023a), and Qwen2.5-7B (Yang et al., 2024), and the non-proprietary model ChatGPT (OpenAI, 2022) with results taken from (Zhang et al., 2024b). \n\n(2) Retrieval-augmented generation: This includes Naive RAG method and several complex RAG methods, including REPLUG (Shi et al., 2023), Self-RAG (Asai et al., 2024), IRCoT (Trivedi et al., 2023), Iter-RetGen (Shao et al., 2023), and Adaptive-RAG (Jeong et al., 2024). For a fair comparison, all RAG baselines use the E5-baseen (Wang et al., 2022a) retriever, and all LLMs are instruction-tuned with 7B or 8B parameters.",
            "score": 0.6256730930993452,
            "section_title": "Baselines",
            "char_start_offset": 16015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 720
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 465,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 547,
                    "end": 567,
                    "matchedPaperCorpusId": "268553748"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "Policy-Optimized Retrieval-Augmented Generation (PORAG) and Retrieval-Augmented Fine-Tuning (RAFT) (Zhang et al., 2024c) offer fundamentally different strategies for optimizing RAG systems. RAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently. In contrast, PORAG employs Group Relative Policy Optimization (GRPO), an advanced reinforcement learning method, to directly optimize multiple generation quality dimensions simultaneously through specialized reward models. PORAG dynamically generates policy-driven training samples, directly optimizing retrieval fidelity-how faithfully retrieved information is reflected-and response quality, including coherence, fluency, and helpfulness. Unlike RAFT, PORAG implicitly and dynamically handles distractors through reward modeling and advantage estimation rather than explicitly embedding distractors in supervised training sets. Additionally, PORAG incorporates explicit advantage estimation and KL-divergence regularization during policy updates to maintain controlled adaptation in retrievalaugmented generation. This stabilizes training, prevents drastic policy shifts, and balances retrieval fidelity with the model's inherent parametric knowledge, enhancing robustness and generalization across retrieval scenarios. In contrast, RAFT provides robustness primarily within domain-specific scenarios due to its explicit distractoraware fine-tuning but lacks dynamic adaptability beyond its predefined training context. In summary, PORAG offers greater deployment flexibility, nuanced generation optimization, and dynamic adaptability, addressing key limitations of RAFT related to static supervision, singlestrategy optimization, and the lack of direct optimization of retrieval fidelity and response quality.",
            "score": 0.6255071202389232,
            "section_title": "B. Comparing PORAG and RAFT Methodologies",
            "char_start_offset": 68668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2326
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 120,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83251953125
        },
        {
            "corpus_id": "273403922",
            "title": "ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020;Guu et al., 2020;He et al., 2021;Xu et al., 2023;Borgeaud et al., 2022;Jiang et al., 2023;Wang et al., 2024) models typically utilize dense retrievers (Karpukhin et al., 2020;Xiong et al., 2020) for searching documents and subsequently enhance language models to produce responses grounding the retrieved evidence. These models have shown the effectiveness in various NLP tasks (Ram et al., 2023a;Shi et al., 2023), such as open-domain question answering (Izacard et al., 2023), dialog (Shuster et al., 2022), code generation (Zhou et al., 2022) and so on. Nevertheless, the noise of retrieved contexts usually challenges the effectiveness of RAG models (Yu et al., 2022;Chen et al., 2023a;Asai et al., 2024). \n\nCurrent RAG approaches typically adopt various modules to refine retrieved evidence and mitigate the effects of noise from the retrieval process (Gao et al., 2024;Asai et al., 2023). One of the most straightforward strategies is improving retrieval accuracy. Accordingly, several studies focus on reranking retrieved evidence (Yu et al., 2023b;Shi et al., 2023), reformulating or expanding queries to generate more specific questions (Wang et al., 2023b;Jagerman et al., 2023), or utilizing finegrained evidence selectors (Yoran et al., 2023b). Additionally, Corrective RAG (Yan et al., 2024) incorporates a T5-based reranker as a critic, triggering adaptive actions to refine the retrieved evidence. To further reduce noise from retrieved contents, Self-RAG (Asai et al., 2023) trains LLMs to evaluate the relevance between queries and documents and then uses LLMs to filter irrelevant evidence through self-reflection.",
            "score": 0.6252267843911492,
            "section_title": "Related Work",
            "char_start_offset": 5024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1689
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 74,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 74,
                    "end": 90,
                    "matchedPaperCorpusId": "237452184"
                },
                {
                    "start": 106,
                    "end": 128,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 128,
                    "end": 147,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 208,
                    "end": 232,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 232,
                    "end": 251,
                    "matchedPaperCorpusId": "220302524"
                },
                {
                    "start": 512,
                    "end": 534,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 583,
                    "end": 602,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 711,
                    "end": 728,
                    "matchedPaperCorpusId": "252408513"
                },
                {
                    "start": 932,
                    "end": 950,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1203,
                    "end": 1223,
                    "matchedPaperCorpusId": "257505063"
                },
                {
                    "start": 1223,
                    "end": 1245,
                    "matchedPaperCorpusId": "258546701"
                },
                {
                    "start": 1343,
                    "end": 1361,
                    "matchedPaperCorpusId": "267312595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62939453125
        },
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Task-specific advancements in RAG systems focus on refining retrieval-augmented models for particular applications, improving their efficiency and effectiveness in complex tasks. Demonstrate-Search-Predict [116] introduces a modular approach that breaks down complex problems into manageable tasks, enhancing performance in multi-hop reasoning and open-domain question answering. Similarly, RA-DIT [117] uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks. These methods highlight the importance of tailoring RAG systems to specific tasks, enabling more effective and accurate solutions across diverse domains.",
            "score": 0.6251108748755779,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 69936,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 701
                }
            ],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 403,
                    "matchedPaperCorpusId": "263605962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned RoBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.",
            "score": 0.6249477576943593,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77197265625
        },
        {
            "corpus_id": "275133960",
            "title": "A Comprehensive Framework for Reliable Legal AI: Combining Specialized Expert Systems and Adaptive Refinement",
            "text": "This section presents and analyzes the performance outcomes of our proposed system, which integrates Retrieval-Augmented Generation (RAG), Knowledge Graphs (KG), a Mixture of Experts (MoE) framework, and Reinforcement Learning from Human Feedback (RLHF). By evaluating large language models (LLMs) such as GPT-4 [9], LLaMA-3 [37], and Google Flan-T5 [38] across various legal tasks, we demonstrate how different architectures and methodologies impact performance in specialized fields like law. \n\nFigure 4 (a) showcases the comparative performance between baseline models and those finetuned using Supervised Fine-Tuning (SFT) with Low-Rank Adaptation (LoRA). The data indicates a substantial improvement in task scores across all models when employing SFT-LoRA. For instance, LLAMA-3's performance escalates from approximately 40% in the baseline to 60% post fine-tuning. This 20% increase exhibits the efficacy of the SFT-LoRA approach in enhancing model capabilities. \n\nMoreover, the reduction in abstention rates by around 15% suggests that fine-tuning not only boosts accuracy but also equips models to respond more confidently. The advanced models like GPT-4 and LLAMA-3 exhibit better adaptability to fine-tuning, outperforming older architectures such as GPT-2 and Google Flan-T5, which show limited gains. This disparity highlights the importance of modern, adaptable architectures for fine-tuning processes to achieve optimal performance enhancements. \n\nHowever, despite these improvements, fine-tuned models occasionally struggled with handling complex and contextually rich queries. This limitation necessitated the integration of additional strategies, such as RAG and RLHF, to further enhance performance and reliability. \n\nFigure 4 (b)compares the performance and abstention rates between models enhanced with Retrieval-Augmented Generation (RAG) and those integrated with Knowledge Graphs (KG). RAG-enhanced models generally achieve higher scores and lower abstention rates compared to KG-enhanced setups. For instance, GPT-4 and LLAMA-3 show substantial improvements with RAG, achieving scores around 60%, while abstention rates are maintained below 20%.",
            "score": 0.624815003642696,
            "section_title": "Results and Discussion",
            "char_start_offset": 26631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1735
                },
                {
                    "start": 1738,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
            "score": 0.6247876356110573,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.796875
        },
        {
            "corpus_id": "273963542",
            "title": "Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval",
            "text": "Recent advancements in RAG have focused on innovative techniques and methodologies to optimize retrieval and generation processes. Lewis et al. (2020) highlight the power of RAG in knowledge-intensive NLP tasks, demonstrating its potential to solve complex information retrieval challenges [1]. Shahul et al. (2023) introduced RAGAS, a framework for automated evaluation of RAG pipelines, emphasizing the importance of reference-free evaluation metrics to enhance the evaluation process of RAG systems. Siriwardhana et al. (2022) developed RAG-end2end, which optimizes RAG for domain-specific knowledge bases, significantly improving performance in specialized domains such as healthcare and news [4]. Yu (2022) explored the use of retrieval-augmented generation across heterogeneous knowledge, addressing the challenges of retrieving information from diverse sources [6]. Nakhod (2023) proposed applying RAG to elevate low-code developer skills by integrating domain-specific knowledge into large language models, thereby improving their practical utility [9]. Melz (2023) introduced ARM-RAG, a system that enhances large language models' intelligence through storing and retrieving reasoning chains, demonstrating significant improvements in problem-solving tasks [10]. Chen et al. (2023) provided a comprehensive evaluation of the impact of RAG on large language models, highlighting the potential bottlenecks and challenges in applying RAG across different tasks [7]. Heydari et al. (2024) proposed the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLM's input prompt based on whether the user query necessitates external context retrieval, thereby enhancing the efficiency and accuracy of RAG systems [11].",
            "score": 0.6246222658651436,
            "section_title": "B. Recent Advances and Techniques in RAG",
            "char_start_offset": 3454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 702,
                    "end": 711,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "250391000"
                },
                {
                    "start": 873,
                    "end": 886,
                    "matchedPaperCorpusId": "266204857"
                },
                {
                    "start": 1057,
                    "end": 1060,
                    "matchedPaperCorpusId": "266204857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "We have evaluated the impact of various methods for fine-tuning LLM generators.Previous studies have demonstrated the feasibility of training both the retriever and generator jointly.We would like to explore this possibility in the future.In this study, we embraced the principle of modular design to simplify the search for optimal RAG implementations, thereby reducing complexity.Due to the daunting costs associated with constructing vector databases and conducting experiments, our evaluation was limited to investigating the effectiveness and influence of representative chunking techniques within the chunking module.It would be intriguing to further explore the impact of different chunking techniques on the entire RAG systems.While we have discussed the application of RAG in the domain of NLP and extended its scope to image generation, an enticing avenue for future exploration would involve expanding this research to other modalities such as speech and video.increase in performance across all metrics.Approximately equal performance is achieved by monoT5 and monoBERT, and RankLLaMA performs best, each ascending in latency.TILDEv2 is the fastest, taking approximately 10 to 20 milliseconds per query at the cost of performance.Additionally, TILDEv2 requires that the passages reranked be identically included in the previously indexed collection.Preprocessing must be redone at inference for new unseen passages, negating the efficiency advantages.",
            "score": 0.6245205788484179,
            "section_title": "Limitations",
            "char_start_offset": 32817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 79,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1242
                },
                {
                    "start": 1242,
                    "end": 1361
                },
                {
                    "start": 1361,
                    "end": 1463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "In this paper, we aimed to determine the most suitable approach for customizing language models (LMs) for less-resourced domains. We examined the effectiveness of retrieval augmented generation (RAG) and fine-tuning (FT) methods, focusing on four key aspects: (i) Table 7: SRAG performance. By adding the extracted hint to the top of the input prompt, SRAG's performance surpasses other settings. Statistically significant differences in the SRAG(S) and SRAG(D) columns are compared with the -FT+RAG and +FT+RAG columns. Superscripts (a) and (b) denote statistically significant differences (better or worse) compared to -FT+RAG and +FT+RAG, respectively, as determined by the Wilcoxon test (p-value < 0.01). fine-tuning methods, specifically full fine-tuning versus parameterefficient fine-tuning (PEFT), (ii) data augmentation techniques, (iii) the type and size of LMs, including decoder-only versus encoderdecoder models ranging from 80 million to 11 billion parameters, and (iv) the performance of retrieval models. Our findings reveal several key points. First, PEFT enhances downstream task performance and preserves the reasoning abilities of LMs while incorporating new knowledge. Second, prompt-based QA generation exhibits superior performance in factual QA tasks. Third, a small fine-tuned LM with RAG can perform on par with or even surpass a larger LM model. Additionally, RAG's performance improves with higher-performing retrievers. Notably, when comparing knowledge injection methods, RAG significantly outperforms FT. We addressed the cost of fine-tuning by developing Stimulus RAG (SRAG), a novel RAG approach that prompts an LM to generate correct responses based on hints provided in the prompt. This method eliminates the need for extensive fine-tuning, making it a cost-effective solution for enhancing LM performance in less-resourced domains.",
            "score": 0.6243982229871692,
            "section_title": "DISCUSSION AND CONCLUSIONS",
            "char_start_offset": 33941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1867
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "270285974",
            "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) [212] is a machine learning technique that combines the strengths of retrievalbased and generative models to enhance the quality and diversity of generated text.This approach has gained significant attention in NLP tasks, particularly in areas like conversational AI, question-answering systems, and text summarization.\n\n1) General Algorithm: In general, a typical RAG system should consist of the following components:\n\n\u2022 Retrieval Component: The system starts by retrieving relevant information from a large database or corpus.This could involve indexing and efficiently searching through past conversations, documents, or web pages based on the input query.Techniques like TF-IDF [213], BM25 [214], or more advanced retrieval methods can be used for this component.\u2022 Generation Component: Once the relevant information is retrieved, a generative model (e.g., GPT-3, T5, or BERT) can use this information as context to generate a response or output [215].The generation process is augmented by conditioning the model on the retrieved data, allowing it to generate more informed, contextually accurate, and diverse responses rather than generating from scratch.\u2022 Component Integration: The integration ways of these two components can vary [212].Firstly, the retrieved information might be concatenated with the input prompt and directly fed into the generator.Others might use a more sophisticated fusion mechanism, where the retrieval and generation models interact in multiple steps, refining the context and the generated output iteratively.By leveraging external knowledge, the generated text of RAG is more likely to be contextually appropriate and accurate.Retrieval of varied sources can introduce more diversity in the generated outputs, reducing the likelihood of repetitive or generic responses [216].Retrieval models can quickly narrow down the scope of information needed, which can make the generation process more efficient compared to exploring the entire knowledge space.Incorporating specific retrieved information can provide more control over the content and tone of the generated text, aligning it better with user expectations or specific requirements.\n\n2) Specific RAG Algorithm: Clinfo.ai[51].Clinfo.ai is an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature.The information retrieval and summarization tasks are applied to evaluate the retrieval-augmented LLM systems.\n\nAlmanac [52].",
            "score": 0.6243686659198822,
            "section_title": "D. Retrieval-Augmented Generation for Med-LLMs",
            "char_start_offset": 58486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 356
                },
                {
                    "start": 358,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1850
                },
                {
                    "start": 1850,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2212
                },
                {
                    "start": 2214,
                    "end": 2250
                },
                {
                    "start": 2250,
                    "end": 2255
                },
                {
                    "start": 2255,
                    "end": 2375
                },
                {
                    "start": 2375,
                    "end": 2485
                },
                {
                    "start": 2487,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 725,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 732,
                    "end": 737,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 1844,
                    "end": 1849,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2250,
                    "end": 2254,
                    "matchedPaperCorpusId": "264487188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "274610849",
            "title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced Retrieval-Augmented Generation on Knowledge Graphs",
            "text": "KG-based RAG Systems: Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) mitigates the hallucination issue of LLMs by retrieving external knowledge to enhance the accuracy and reliability of generation content. Recent RAG advancements have increasingly incorporated Knowledge Graphs (KGs) (Luo et al. 2023c;Sun et al. 2023;Xu et al. 2024;He et al. 2024), which store structured factual information, enabling more systematic reasoning by LLMs (Pan et al. 2024). KGs support diverse retrieval methods, each with different capabilities and costs, as detailed in ?? . \n\nOur analysis of retrieval methods, discussed in ?? , shows that current KG-based RAG systems (Luo et al. 2023c;Sun et al. 2023;Xu et al. 2024;He et al. 2024) predominantly rely on a single retrieval method, which often fails to meet the varied demands of real-world applications. These systems typically assume a stationary environment and remain static without subsequent fine-tuning, making them unable to adapt to potential shifts in the query domain and upgrades of the backend retriever. To address these issues,our work aims to develop an MAB-enhanced RAG system that strategically combines multiple retrievers. By leveraging realtime feedback, our system can dynamically adjust retrieval strategies to meet the evolving demands of diverse applica-tion scenarios of the RAG system effectively. \n\nTo our knowledge, the concurrent research by (Sawarkar, Mangal, and Solanki 2024) is one of the few studies attempting to integrate multiple retrieval methods, but it focuses on textual data sources and lacks the continuous optimization crucial for RAG systems in non-stationary environments. \n\nMulti-Armed Bandit Algorithms: The Multi-Armed Bandit (MAB) (Katehakis and Veinott Jr 1987) framework optimizes the balance between exploiting historical data and exploring new information. It includes two main types: context-free (Bubeck, Cesa-Bianchi et al. 2012), which operates without external information, and contextual bandits (Mahajan and Teneketzis 2008), which incorporate contextual data such as user features.",
            "score": 0.6239669625262466,
            "section_title": "Related Work",
            "char_start_offset": 23115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 77,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 313,
                    "end": 329,
                    "matchedPaperCorpusId": "259936842"
                },
                {
                    "start": 329,
                    "end": 344,
                    "matchedPaperCorpusId": "269449459"
                },
                {
                    "start": 344,
                    "end": 359,
                    "matchedPaperCorpusId": "267626823"
                },
                {
                    "start": 448,
                    "end": 465,
                    "matchedPaperCorpusId": "259165563"
                },
                {
                    "start": 683,
                    "end": 699,
                    "matchedPaperCorpusId": "259936842"
                },
                {
                    "start": 699,
                    "end": 714,
                    "matchedPaperCorpusId": "269449459"
                },
                {
                    "start": 714,
                    "end": 729,
                    "matchedPaperCorpusId": "267626823"
                },
                {
                    "start": 1729,
                    "end": 1760,
                    "matchedPaperCorpusId": "656323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "274422541",
            "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) To address the challenges of outdated knowledge embedded in model parameters (Zhao et al., 2024) and the inadequate retention of long-tail knowledge by LLMs (Mallen et al., 2023), Retrieval-Augmented Generation (RAG) has been introduced (Lewis et al., 2020;Chu et al., 2024;Yan et al., 2024). The most common RAG approach follows the Retrieve-Read framework (Gao et al., 2024), where retrieved documents are concatenated with the user's input to provide LLMs with external knowledge. However, retrievers are not without flaws (Gao et al., 2024), and the retrieved content may contain noise, which has been shown to degrade the RAG system's performance (Yu et al., 2024;2023;Yoran et al., 2023;Hong et al., 2024). Recent studies have sought to improve RAG by refining query formulation (Ma et al., 2023), enhancing retrievers (Karpukhin et al., 2020;Chen et al., 2023), improving generators (Yoran et al., 2023;Yu et al., 2023), and optimizing post-processing of retrieved documents (Yu et al., 2024;Xu et al., 2023). Nonetheless, these methods overlook the growing difficulty of obtaining sufficient knowledge from a single retrieval attempt as the complexity of tasks increases (Jiang et al., 2023). \n\nIterative Retrieval Iterative retrieval was introduced to address the evolving knowledge requirements that arise when solving complex problems (Feng et al., 2023;Shao et al., 2023;Jiang et al., 2023;Trivedi et al., 2023). The core principle of iterative retrieval is determining when and what to retrieve (Jiang et al., 2023). For instance, ITER-RETGEN (Shao et al., 2023) concatenates the input question with the generated output from the previous iteration to form a new query for the next. While this method has achieved some success, it merely reflects existing knowledge without explicitly indicating the LLM's information needs.",
            "score": 0.623960420609956,
            "section_title": "RELATED WORK",
            "char_start_offset": 3378,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 215,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 274,
                    "end": 294,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 294,
                    "end": 311,
                    "matchedPaperCorpusId": "270845599"
                },
                {
                    "start": 689,
                    "end": 706,
                    "matchedPaperCorpusId": "268364338"
                },
                {
                    "start": 730,
                    "end": 748,
                    "matchedPaperCorpusId": "258437010"
                },
                {
                    "start": 822,
                    "end": 839,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 862,
                    "end": 886,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1019,
                    "end": 1036,
                    "matchedPaperCorpusId": "268364338"
                },
                {
                    "start": 1216,
                    "end": 1236,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1402,
                    "end": 1420,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 1420,
                    "end": 1439,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1439,
                    "end": 1460,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 1545,
                    "end": 1565,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1593,
                    "end": 1612,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        }
    ],
    "quotes": {
        "cost": 0.17731800000000003,
        "quotes": [
            {
                "idx": 0,
                "key": "[267320876 | Lyu et al. | 2024 | Citations: 40]",
                "snippets": "Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "CONCLUSION",
                        "pdf_hash": "",
                        "start": 853,
                        "end": 1150,
                        "sentence_offsets": [
                            {
                                "start": 853,
                                "end": 1150
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[269149041 | Weng | 2024 | Citations: 10]",
                "snippets": "\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end, an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252735056 | Siriwardhana et al. | 2022 | Citations: 179]": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."
                },
                "metadata": [
                    {
                        "section_title": "XIII. RAG-MEMORY-FINETUNING",
                        "pdf_hash": "",
                        "start": 802,
                        "end": 1545,
                        "sentence_offsets": [
                            {
                                "start": 783,
                                "end": 979
                            },
                            {
                                "start": 981,
                                "end": 1179
                            },
                            {
                                "start": 1181,
                                "end": 1547
                            }
                        ],
                        "ref_mentions": [
                            "252735056"
                        ],
                        "quote": "\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end, an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[269149146 | Susnjak et al. | 2024 | Citations: 26]",
                "snippets": "RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses...The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
                        "pdf_hash": "",
                        "start": 288,
                        "end": 653,
                        "sentence_offsets": [
                            {
                                "start": 182,
                                "end": 432
                            },
                            {
                                "start": 432,
                                "end": 654
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses"
                    },
                    {
                        "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
                        "pdf_hash": "",
                        "start": 978,
                        "end": 1288,
                        "sentence_offsets": [
                            {
                                "start": 978,
                                "end": 1288
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68]."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270870251 | Wang et al. | 2024 | Citations: 61]",
                "snippets": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258865283 | Luo et al. | 2023 | Citations: 27]": "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."
                },
                "metadata": [
                    {
                        "section_title": "Retriever and Generator Fine-tuning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 551,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 94
                            },
                            {
                                "start": 94,
                                "end": 242
                            },
                            {
                                "start": 242,
                                "end": 345
                            },
                            {
                                "start": 345,
                                "end": 551
                            }
                        ],
                        "ref_mentions": [
                            "258865283"
                        ],
                        "quote": "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[271571401 | Gao et al. | 2024 | Citations: 20]",
                "snippets": "This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "E. Tuning Pattern",
                        "pdf_hash": "",
                        "start": 268,
                        "end": 2144,
                        "sentence_offsets": [
                            {
                                "start": 268,
                                "end": 529
                            },
                            {
                                "start": 532,
                                "end": 581
                            },
                            {
                                "start": 582,
                                "end": 667
                            },
                            {
                                "start": 668,
                                "end": 768
                            },
                            {
                                "start": 771,
                                "end": 806
                            },
                            {
                                "start": 807,
                                "end": 920
                            },
                            {
                                "start": 921,
                                "end": 997
                            },
                            {
                                "start": 998,
                                "end": 1163
                            },
                            {
                                "start": 1164,
                                "end": 1196
                            },
                            {
                                "start": 1197,
                                "end": 1261
                            },
                            {
                                "start": 1262,
                                "end": 1278
                            },
                            {
                                "start": 1279,
                                "end": 1340
                            },
                            {
                                "start": 1341,
                                "end": 1416
                            },
                            {
                                "start": 1417,
                                "end": 1502
                            },
                            {
                                "start": 1503,
                                "end": 1617
                            },
                            {
                                "start": 1620,
                                "end": 1652
                            },
                            {
                                "start": 1653,
                                "end": 1748
                            },
                            {
                                "start": 1749,
                                "end": 1818
                            },
                            {
                                "start": 1819,
                                "end": 1924
                            },
                            {
                                "start": 1925,
                                "end": 1940
                            },
                            {
                                "start": 1941,
                                "end": 2144
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[272911196 | Chung et al. | 2024 | Citations: 0]",
                "snippets": "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)...Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1357,
                        "end": 1494,
                        "sentence_offsets": [
                            {
                                "start": 1357,
                                "end": 1495
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1776,
                        "end": 2383,
                        "sentence_offsets": [
                            {
                                "start": 1756,
                                "end": 1932
                            },
                            {
                                "start": 1933,
                                "end": 2090
                            },
                            {
                                "start": 2091,
                                "end": 2235
                            },
                            {
                                "start": 2236,
                                "end": 2383
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[273501949 | Anaissi et al. | 2024 | Citations: 3]",
                "snippets": "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]...Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 1166,
                        "end": 1689,
                        "sentence_offsets": [
                            {
                                "start": 1166,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1498
                            },
                            {
                                "start": 1499,
                                "end": 1690
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]"
                    },
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 1691,
                        "end": 2359,
                        "sentence_offsets": [
                            {
                                "start": 1691,
                                "end": 1826
                            },
                            {
                                "start": 1827,
                                "end": 2030
                            },
                            {
                                "start": 2031,
                                "end": 2165
                            },
                            {
                                "start": 2166,
                                "end": 2358
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23]."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[273969615 | Zhang et al. | 2024 | Citations: 3]",
                "snippets": "Similarly, RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263605962 | Lin et al. | 2023 | Citations: 153]": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."
                },
                "metadata": [
                    {
                        "section_title": "C. Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 380,
                        "end": 547,
                        "sentence_offsets": [
                            {
                                "start": 380,
                                "end": 547
                            }
                        ],
                        "ref_mentions": [
                            "263605962"
                        ],
                        "quote": "Similarly, RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[276287820 | Bhushan et al. | 2025 | Citations: 2]",
                "snippets": "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1680,
                        "end": 1980,
                        "sentence_offsets": [
                            {
                                "start": 1680,
                                "end": 1854
                            },
                            {
                                "start": 1855,
                                "end": 1980
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[277043297 | Cheng et al. | 2025 | Citations: 6]",
                "snippets": "Static optimization involves training only one component of the RAG system while keeping the other static. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption.\n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance.\n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Static optimization involves training only one component of the RAG system while keeping the other static. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption.\n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance.\n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[277501853 | Srinivas et al. | 2025 | Citations: 1]",
                "snippets": "Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering).\n\nRAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering).\n\nRAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[277994112 | Gao et al. | 2025 | Citations: 7]",
                "snippets": "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.\n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.\n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258841328 | Dettmers et al. | 2023 | Citations: 2606]": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.",
                    "[264288947 | Asai et al. | 2023 | Citations: 780]": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."
                },
                "metadata": [
                    {
                        "section_title": "5.2.2",
                        "pdf_hash": "",
                        "start": 306,
                        "end": 2260,
                        "sentence_offsets": [
                            {
                                "start": 195,
                                "end": 356
                            },
                            {
                                "start": 359,
                                "end": 542
                            },
                            {
                                "start": 543,
                                "end": 722
                            },
                            {
                                "start": 723,
                                "end": 913
                            },
                            {
                                "start": 914,
                                "end": 1119
                            },
                            {
                                "start": 1122,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1442
                            },
                            {
                                "start": 1443,
                                "end": 1666
                            },
                            {
                                "start": 1667,
                                "end": 2065
                            },
                            {
                                "start": 2068,
                                "end": 2224
                            },
                            {
                                "start": 2225,
                                "end": 2315
                            }
                        ],
                        "ref_mentions": [
                            "264288947",
                            "258841328"
                        ],
                        "quote": "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.\n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.\n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[278339057 | Shi et al. | 2025 | Citations: 1]",
                "snippets": "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252568176 | Hofstatter et al. | 2022 | Citations: 82]": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.",
                    "[269605438 | Zamani et al. | 2024 | Citations: 29]": "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets.",
                    "[270199429 | Fang et al. | 2024 | Citations: 39]": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                },
                "metadata": [
                    {
                        "section_title": "Generative selector",
                        "pdf_hash": "",
                        "start": 1205,
                        "end": 1856,
                        "sentence_offsets": [
                            {
                                "start": 1171,
                                "end": 1299
                            },
                            {
                                "start": 1300,
                                "end": 1393
                            },
                            {
                                "start": 1394,
                                "end": 1476
                            },
                            {
                                "start": 1479,
                                "end": 1505
                            },
                            {
                                "start": 1506,
                                "end": 1656
                            },
                            {
                                "start": 1657,
                                "end": 1853
                            },
                            {
                                "start": 1854,
                                "end": 2044
                            }
                        ],
                        "ref_mentions": [
                            "252568176",
                            "269605438",
                            "10986612",
                            "270199429"
                        ],
                        "quote": "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[278635834 | Wang et al. | 2025 | Citations: 0]",
                "snippets": "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2021)(Ram et al., 2023)(Zhang et al., 2024) and question answering (Izacard et al., 2022)Shi et al., 2023;(Yoran et al., 2023)(Lin et al., 2023)Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023)Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
                    "[251371732 | Izacard et al. | 2022 | Citations: 783]": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",
                    "[256459451 | Ram et al. | 2023 | Citations: 605]": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1",
                    "[263605962 | Lin et al. | 2023 | Citations: 153]": "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.",
                    "[263608822 | Yoran et al. | 2023 | Citations: 216]": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
                    "[271915498 | Zhang et al. | 2024 | Citations: 8]": "LLMs confront inherent limitations in terms 001 of its knowledge, memory, and action. The 002 retrieval augmentation stands as a vital mecha-003 nism to address these limitations, which brings 004 in useful information from external sources to 005 augment the LLM. However, existing retrieval 006 methods encounter two pressing issues. On 007 one hand, the general retrievers are not prop-008 erly optimized for retrieval augmentation hence 009 exhibit limited effectiveness; on the other hand, 010 the task-specific retrievers excel in the targeted 011 retrieval augmentation scenario, while lack the 012 versatility to handle diverse scenarios. In this 013 work, we propose LLM-Embedder for the 014 unified support of diverse retrieval augmen-015 tation scenarios. Our method presents three 016 technical contributions. Firstly, we introduce 017 a new reward formulation , namely rank-aware 018 reward. It exploits the ranking position of the 019 desired output among N sampled outputs from 020 the LLM, which leads to fine-grained and ro-021 bust computation of reward from the LLM\u2019s 022 feedback. Secondly, we design a novel distil-023 lation objective , called graded distillation. It 024 incorporates both the absolute value and the 025 relative order of the reward for more sufficient 026 utilization of the LLM\u2019s feedback. Thirdly, we 027 systematically optimize the multi-task learning , 028 which effectively unifies the multiple retrieval 029 functionalities into one model. In our exper-030 iment, LLM-Embedder notably improves the 031 LLM\u2019s performances in various downstream 032 tasks, and outperforms both general and task-033 specific retrievers with a substantial advantage. 034"
                },
                "metadata": [
                    {
                        "section_title": "Retrieval-augmented Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1444,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 361
                            },
                            {
                                "start": 362,
                                "end": 528
                            },
                            {
                                "start": 529,
                                "end": 651
                            },
                            {
                                "start": 652,
                                "end": 780
                            },
                            {
                                "start": 781,
                                "end": 874
                            },
                            {
                                "start": 875,
                                "end": 964
                            },
                            {
                                "start": 965,
                                "end": 1031
                            },
                            {
                                "start": 1032,
                                "end": 1168
                            },
                            {
                                "start": 1169,
                                "end": 1264
                            },
                            {
                                "start": 1265,
                                "end": 1444
                            }
                        ],
                        "ref_mentions": [
                            "244954723",
                            "256459451",
                            "271915498",
                            "251371732",
                            "263608822",
                            "263605962",
                            "271915498",
                            "263608822",
                            "263605962"
                        ],
                        "quote": "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2021)(Ram et al., 2023)(Zhang et al., 2024) and question answering (Izacard et al., 2022)Shi et al., 2023;(Yoran et al., 2023)(Lin et al., 2023)Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023)Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[278714952 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Methodology",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 632,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 403
                            },
                            {
                                "start": 404,
                                "end": 632
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.025461,
        "cot": "For this query about implementation-level techniques, evaluations, and challenges of fine-tuning both retriever and generator components in RAG systems, I need to organize the quotes into meaningful dimensions that address different aspects of the query.\n\n1. \"Introduction to RAG and Fine-Tuning\" (format: synthesis) - This section should provide background on what RAG is and why fine-tuning is important in this context. Quotes [0], [2], [3], [5] provide good background information.\n\n2. \"Retriever Fine-Tuning Techniques\" (format: synthesis) - This section should cover specific techniques for fine-tuning the retriever component. Quotes [4], [11], [12], [13] discuss various retriever fine-tuning approaches.\n\n3. \"Generator Fine-Tuning Techniques\" (format: synthesis) - This section should focus on techniques specific to fine-tuning the generator component. Quotes [4], [12], [14] address generator fine-tuning methods.\n\n4. \"Dual Fine-Tuning Approaches\" (format: synthesis) - This section should address approaches that fine-tune both components together, which is specifically mentioned in the query. Quotes [3], [7], [9], [12], [13] discuss dual fine-tuning.\n\n5. \"Specific RAG Fine-Tuning Methods\" (format: list) - This section should provide details on the specific methods mentioned in the query (RAFT, CRAFT, Self-RAG, Finetune-RAG). A list format is appropriate here as we're enumerating distinct methods. Quotes [5], [6], [8], [10], [11], [14] discuss these specific methods.\n\n6. \"Challenges and Evaluation\" (format: synthesis) - This section should cover challenges in RAG fine-tuning and how these approaches are evaluated. Quotes [0], [1], [9], [12] touch on challenges and evaluation aspects.\n\nThe chosen formats (synthesis vs. list) are based on the nature of the content. Most sections require synthesizing information from multiple sources to provide a coherent explanation, while the specific methods section benefits from a list format to clearly delineate the different approaches mentioned in the query.",
        "plan": {
            "Introduction to RAG and Fine-Tuning (synthesis)": [
                0,
                2,
                3,
                5
            ],
            "Retriever Fine-Tuning Techniques (synthesis)": [
                4,
                11,
                12,
                13
            ],
            "Generator Fine-Tuning Techniques (synthesis)": [
                4,
                12,
                14
            ],
            "Dual Fine-Tuning Approaches (synthesis)": [
                3,
                7,
                9,
                12,
                13
            ],
            "Specific RAG Fine-Tuning Methods (list)": [
                5,
                6,
                8,
                10,
                11,
                14
            ],
            "Challenges and Evaluation (synthesis)": [
                0,
                1,
                9,
                12
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to RAG and Fine-Tuning",
                "tldr": "Retrieval-Augmented Generation (RAG) systems combine retrieval mechanisms with generative models to produce more accurate and contextually relevant responses. Fine-tuning both components is essential for optimizing performance, with various approaches targeting either individual components or the system as a whole. (5 sources)",
                "text": "\nRetrieval-Augmented Generation (RAG) has emerged as a powerful approach that enhances large language models by incorporating external knowledge. RAG systems consist of two core components: a retrieval mechanism that fetches relevant information from a knowledge base (such as a database or the internet), and a generative model that integrates this retrieved information into coherent and contextually relevant responses <Paper corpusId=\"269149146\" paperTitle=\"(Susnjak et al., 2024)\" isShortName></Paper>. This architecture allows models to produce text that is both contextually enriched and factually accurate by grounding responses in external knowledge sources.\n\nFine-tuning within the RAG framework has become crucial for optimizing system performance. Research in this area follows three main directions: some work focuses on fine-tuning the generator to better utilize retriever context, ensuring the model can faithfully incorporate and reason with the retrieved information <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258865283\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Other approaches prioritize fine-tuning the retriever to learn to fetch passages that are most beneficial for the generator <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Finally, holistic approaches treat RAG as an integrated system, fine-tuning both components simultaneously to enhance overall performance despite the increased complexity and integration challenges this presents <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe fine-tuning process for RAG systems requires a delicate balance, necessitating optimization of multiple elements including the retrieval model, context length, knowledge base construction, and the deployment of the underlying large language model <Paper corpusId=\"267320876\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>. Recent advances have expanded RAG applications across various domains, with methodologies like RAFT implementing specialized fine-tuning strategies that generate training data from target domain data for instruction fine-tuning <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>. These developments highlight the ongoing evolution of RAG fine-tuning approaches to enhance performance across diverse applications.",
                "citations": [
                    {
                        "id": "(Susnjak et al., 2024)",
                        "snippets": [
                            "RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses",
                            "The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68]."
                        ],
                        "paper": {
                            "corpus_id": 269149146,
                            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2656889",
                                    "name": "Teo Su\u0161njak"
                                },
                                {
                                    "authorId": "2296719088",
                                    "name": "Peter Hwang"
                                },
                                {
                                    "authorId": "1783269",
                                    "name": "N. Reyes"
                                },
                                {
                                    "authorId": "3312622",
                                    "name": "A. Barczak"
                                },
                                {
                                    "authorId": "11430146",
                                    "name": "Timothy R. McIntosh"
                                },
                                {
                                    "authorId": "143976433",
                                    "name": "Surangika Ranathunga"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Transactions on Knowledge Discovery from Data",
                            "n_citations": 26
                        },
                        "score": 0.8505859375
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                        ],
                        "paper": {
                            "corpus_id": 270870251,
                            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2273537815",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2308276345",
                                    "name": "Zhenghua Wang"
                                },
                                {
                                    "authorId": "2292070745",
                                    "name": "Xuan Gao"
                                },
                                {
                                    "authorId": "2308226671",
                                    "name": "Feiran Zhang"
                                },
                                {
                                    "authorId": "2308043953",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "2308044030",
                                    "name": "Zhibo Xu"
                                },
                                {
                                    "authorId": "2308036711",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2309182278",
                                    "name": "Zhengyuan Wang"
                                },
                                {
                                    "authorId": "2309656885",
                                    "name": "Shizheng Li"
                                },
                                {
                                    "authorId": "2309176521",
                                    "name": "Qi Qian"
                                },
                                {
                                    "authorId": "2292032843",
                                    "name": "Ruicheng Yin"
                                },
                                {
                                    "authorId": "2220896023",
                                    "name": "Changze Lv"
                                },
                                {
                                    "authorId": "2257315404",
                                    "name": "Xiaoqing Zheng"
                                },
                                {
                                    "authorId": "2257129987",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 61
                        },
                        "score": 0.9306640625
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."
                        ],
                        "paper": {
                            "corpus_id": 258865283,
                            "title": "SAIL: Search-Augmented Instruction Learning",
                            "authors": [
                                {
                                    "authorId": "1944274",
                                    "name": "Hongyin Luo"
                                },
                                {
                                    "authorId": "2475831",
                                    "name": "Yung-Sung Chuang"
                                },
                                {
                                    "authorId": "145802952",
                                    "name": "Yuan Gong"
                                },
                                {
                                    "authorId": "2146333115",
                                    "name": "Tianhua Zhang"
                                },
                                {
                                    "authorId": "143827730",
                                    "name": "Yoon Kim"
                                },
                                {
                                    "authorId": "1847260",
                                    "name": "Xixin Wu"
                                },
                                {
                                    "authorId": "31997718",
                                    "name": "D. Fox"
                                },
                                {
                                    "authorId": "145199941",
                                    "name": "H. Meng"
                                },
                                {
                                    "authorId": "145898106",
                                    "name": "James R. Glass"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 27
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lyu et al., 2024)",
                        "snippets": [
                            "Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results."
                        ],
                        "paper": {
                            "corpus_id": 267320876,
                            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2187857206",
                                    "name": "Yuanjie Lyu"
                                },
                                {
                                    "authorId": "2268429641",
                                    "name": "Zhiyu Li"
                                },
                                {
                                    "authorId": "2268393907",
                                    "name": "Simin Niu"
                                },
                                {
                                    "authorId": "2268399953",
                                    "name": "Feiyu Xiong"
                                },
                                {
                                    "authorId": "2268400606",
                                    "name": "Bo Tang"
                                },
                                {
                                    "authorId": "2117833477",
                                    "name": "Wenjin Wang"
                                },
                                {
                                    "authorId": "2282083454",
                                    "name": "Hao Wu"
                                },
                                {
                                    "authorId": "2304320758",
                                    "name": "Huan Liu"
                                },
                                {
                                    "authorId": "2277237058",
                                    "name": "Tong Xu"
                                },
                                {
                                    "authorId": "2265580543",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 40
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Chung et al., 2024)",
                        "snippets": [
                            "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)",
                            "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning."
                        ],
                        "paper": {
                            "corpus_id": 272911196,
                            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                            "authors": [
                                {
                                    "authorId": "2322992640",
                                    "name": "Isaac Chung"
                                },
                                {
                                    "authorId": "2322982756",
                                    "name": "Phat Vo"
                                },
                                {
                                    "authorId": "2322991957",
                                    "name": "Arman Kizilkale"
                                },
                                {
                                    "authorId": "2322982549",
                                    "name": "Aaron Reite"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.88232421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Retriever Fine-Tuning Techniques",
                "tldr": "Retriever fine-tuning in RAG systems uses multiple approaches including direct supervised fine-tuning, adapter modules, LLM-supervised retrieval, and reinforcement learning methods to optimize passage selection. Recent advances focus on end-to-end optimization, semantic space alignment, and adaptive retrieval strategies to improve accuracy and efficiency. (7 sources)",
                "text": "\nFine-tuning the retriever component in RAG systems is crucial for ensuring that the most relevant and useful information is retrieved for the generator. Several distinct approaches have emerged to optimize retriever performance:\n\nDirect supervised fine-tuning represents the most straightforward approach, where retrievers are trained on specialized datasets constructed from either open-source retrieval collections or domain-specific data <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This method enables the dense retriever to learn relevance patterns specific to the target domain.\n\nFor cases where direct fine-tuning of embedding models is not feasible (particularly with API-based models like OpenAI's Ada-002), researchers have developed adapter modules that enhance data representation while facilitating better alignment with downstream tasks <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. These lightweight trainable components modify the embedding space without altering the base model parameters.\n\nLLM-supervised retrieval (LSR) has emerged as a powerful technique where the retriever is fine-tuned based on feedback from the language model itself <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This approach is extended in reinforcement learning frameworks that optimize the retrieval process as a generative Markov chain, aligning the retriever with the generator's needs <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nMore advanced end-to-end optimization approaches include Stochastic RAG, which relaxes assumptions about document independence through differentiable approximation for sampling without replacement, enabling effective end-to-end optimization across diverse tasks <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. Similarly, methods like Replug use KL divergence to align retriever results with LLM preferences, while LLM-Embedder employs a distillation objective based on LLM rankings <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nFor complex reasoning tasks, approaches like CoRAG and DeepRAG build multistep reasoning frameworks through full parameter fine-tuning and multitask learning <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation and intermediate answer prediction, improving the model's ability to break down complex problems. DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>.\n\nSemantic space alignment techniques like O1-Embedder and Open-RAG use mixed fine-tuning approaches to improve retrieval effectiveness. Open-RAG specifically employs QLoRA quantized fine-tuning <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper> and Mixture of Experts modules to specialize networks for single/multi-hop reasoning tasks <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>.\n\nMore recent work focuses on improving retrieval robustness. The Retrieval-augmented Adaptive Adversarial Training (RAAT) approach leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises, which are categorized into three distinct types reflecting real-world environments <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. This helps address the challenge that inappropriate retrieved passages can potentially hinder LLMs' capacity to generate high-quality responses.\n\nDespite these advances, it's important to note that optimizing either the retrieval or generation component separately may lead to sub-optimal overall performance <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>, which has motivated research into dual fine-tuning approaches that optimize both components simultaneously.",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model."
                        ],
                        "paper": {
                            "corpus_id": 271571401,
                            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.84033203125
                    },
                    {
                        "id": "(Zamani et al., 2024)",
                        "snippets": [
                            "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."
                        ],
                        "paper": {
                            "corpus_id": 269605438,
                            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
                            "authors": [
                                {
                                    "authorId": "2293725953",
                                    "name": "Hamed Zamani"
                                },
                                {
                                    "authorId": "2240516450",
                                    "name": "Michael Bendersky"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2021)(Ram et al., 2023)(Zhang et al., 2024) and question answering (Izacard et al., 2022)Shi et al., 2023;(Yoran et al., 2023)(Lin et al., 2023)Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023)Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."
                        ],
                        "paper": {
                            "corpus_id": 278635834,
                            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2361597409",
                                    "name": "Shaohan Wang"
                                },
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "2106681735",
                                    "name": "Zheren Fu"
                                },
                                {
                                    "authorId": "2349977855",
                                    "name": "Zhendong Mao"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.81689453125
                    },
                    {
                        "id": "(Gao et al., 2025)",
                        "snippets": [
                            "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.\n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.\n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically."
                        ],
                        "paper": {
                            "corpus_id": 277994112,
                            "title": "Synergizing RAG and Reasoning: A Systematic Review",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291322497",
                                    "name": "Yijie Zhong"
                                },
                                {
                                    "authorId": "2275171009",
                                    "name": "Yuxi Bi"
                                },
                                {
                                    "authorId": "2356716546",
                                    "name": "Ming Xue"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.85693359375
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                        ],
                        "paper": {
                            "corpus_id": 270199429,
                            "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
                            "authors": [
                                {
                                    "authorId": "2293319089",
                                    "name": "Feiteng Fang"
                                },
                                {
                                    "authorId": "2287892865",
                                    "name": "Yuelin Bai"
                                },
                                {
                                    "authorId": "2266469238",
                                    "name": "Shiwen Ni"
                                },
                                {
                                    "authorId": "2301170603",
                                    "name": "Min Yang"
                                },
                                {
                                    "authorId": "2287803799",
                                    "name": "Xiaojun Chen"
                                },
                                {
                                    "authorId": "2266809482",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 39
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."
                        ],
                        "paper": {
                            "corpus_id": 278339057,
                            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                            "authors": [
                                {
                                    "authorId": "2195381022",
                                    "name": "Zhengliang Shi"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "2326805997",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "2265517632",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2331316040",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2261862546",
                                    "name": "Zhaochun Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.904296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Generator Fine-Tuning Techniques",
                "tldr": "Generator fine-tuning in RAG systems primarily employs supervised learning approaches to enhance language models' ability to use retrieved context effectively. Recent techniques focus on training models to distinguish between accurate and fictitious information, improving factuality and noise robustness while addressing challenges from imperfect retrieval. (4 sources)",
                "text": "\nFine-tuning the generator component of RAG systems is essential for optimizing how language models process and incorporate retrieved information. Several approaches have emerged to enhance generator performance:\n\nDirect supervised fine-tuning represents the primary method for optimizing generators in RAG systems. This approach uses external datasets to supplement the generator with additional knowledge while allowing customization of input and output formats. By establishing specific question-and-answer formats during training, language models can better understand particular data structures and respond according to instructions <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nKnowledge distillation offers another effective optimization path, particularly for open-source model deployment. This technique typically involves using more capable models like GPT-4 to batch-construct fine-tuning data, effectively transferring capabilities to smaller open-source models <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nMore specialized approaches like Finetune-RAG focus on enhancing the model's ability to distinguish between correct and fictitious context. Rather than improving retrieval quality, this method trains language models to effectively filter information from imperfect or misleading inputs. The core technique involves fine-tuning models using examples where both correct and incorrect information are explicitly presented, teaching the model to identify and utilize only the reliable information in its responses <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\nSome approaches focus on improving the robustness of generators against irrelevant content. This can be achieved through supervised fine-tuning that teaches models to summarize key points from retrieved documents, helping them process and integrate even noisy retrieval results <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>.\n\nRecent work has recognized that inappropriate retrieved passages can significantly impact a language model's ability to generate high-quality responses. The Retrieval-augmented Adaptive Adversarial Training (RAAT) approach addresses this challenge by dynamically adjusting the model's training process in response to various types of retrieval noise, while simultaneously employing multi-task learning to help the model internally recognize noisy contexts <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nHowever, it's important to note that optimizing only the generator component, like optimizing only the retriever, can lead to sub-optimal overall system performance. This limitation has motivated research into dual fine-tuning approaches that simultaneously optimize both retriever and generator components <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model."
                        ],
                        "paper": {
                            "corpus_id": 271571401,
                            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291409458",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.84033203125
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."
                        ],
                        "paper": {
                            "corpus_id": 278339057,
                            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                            "authors": [
                                {
                                    "authorId": "2195381022",
                                    "name": "Zhengliang Shi"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "2326805997",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "2265517632",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2331316040",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2261862546",
                                    "name": "Zhaochun Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                        ],
                        "paper": {
                            "corpus_id": 270199429,
                            "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
                            "authors": [
                                {
                                    "authorId": "2293319089",
                                    "name": "Feiteng Fang"
                                },
                                {
                                    "authorId": "2287892865",
                                    "name": "Yuelin Bai"
                                },
                                {
                                    "authorId": "2266469238",
                                    "name": "Shiwen Ni"
                                },
                                {
                                    "authorId": "2301170603",
                                    "name": "Min Yang"
                                },
                                {
                                    "authorId": "2287803799",
                                    "name": "Xiaojun Chen"
                                },
                                {
                                    "authorId": "2266809482",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 39
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Dual Fine-Tuning Approaches",
                "tldr": "Dual fine-tuning approaches simultaneously optimize both retriever and generator components in RAG systems to create better synergy between components and improve overall performance. Methods range from modular training stages like RA-DIT to end-to-end optimization techniques like Stochastic RAG, addressing limitations of single-component optimization approaches. (8 sources)",
                "text": "\nDual fine-tuning approaches for RAG systems optimize both retriever and generator components simultaneously or sequentially, addressing the limitations of optimizing only one component at a time. While static optimization (improving only one component while keeping the other fixed) requires fewer computational resources and enables faster deployment, it often results in sub-optimal overall system performance <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>. This limitation has motivated the development of holistic approaches that treat RAG as an integrated system <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT) represents a significant advancement in this area, implementing a two-stage fine-tuning approach. The first stage updates the pre-trained language model to better utilize retrieved information, while the second stage fine-tunes the retriever to return more relevant results based on the language model's preferences <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This modular training methodology allows for retrofitting any LLM with retrieval capabilities, creating a powerful synergy between components that leads to significant performance improvements across knowledge-intensive benchmarks <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nEnd-to-end optimization represents another approach to dual fine-tuning. Stochastic RAG, for example, treats the retrieval process as stochastic sampling without replacement, using a differentiable approximation to enable effective end-to-end optimization across diverse tasks <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. Similarly, methods like Replug align retriever results with LLM preferences using KL divergence, while LLM-Embedder employs a distillation objective based on LLM rankings to train retrievers that better match language model requirements <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nSome dual fine-tuning approaches specifically focus on addressing retrieval noise issues. The Retrieval-augmented Adaptive Adversarial Training (RAAT) method combines adaptive adversarial training to dynamically adjust the model's training process in response to different types of retrieval noise, while simultaneously employing multi-task learning to help the model internally recognize noisy contexts <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. This approach helps mitigate the potential negative impact of inappropriate retrieved passages on the language model's ability to generate high-quality responses.\n\nDespite the clear advantages of dual fine-tuning approaches, they introduce additional complexity and integration challenges compared to single-component optimization methods <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These challenges include increased computational requirements, more complex training procedures, and the need for careful balancing of optimization objectives between retriever and generator components. Nevertheless, the performance improvements achieved through dual fine-tuning approaches make them increasingly essential for developing state-of-the-art RAG systems.",
                "citations": [
                    {
                        "id": "(Cheng et al., 2025)",
                        "snippets": [
                            "Static optimization involves training only one component of the RAG system while keeping the other static. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption.\n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance.\n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information."
                        ],
                        "paper": {
                            "corpus_id": 277043297,
                            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "1491233507",
                                    "name": "Mingyue Cheng"
                                },
                                {
                                    "authorId": "2208917508",
                                    "name": "Yucong Luo"
                                },
                                {
                                    "authorId": "2322501286",
                                    "name": "Ouyang Jie"
                                },
                                {
                                    "authorId": "2332691115",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "2312648865",
                                    "name": "Huijie Liu"
                                },
                                {
                                    "authorId": "2291070758",
                                    "name": "Li Li"
                                },
                                {
                                    "authorId": "2322429208",
                                    "name": "Shuo Yu"
                                },
                                {
                                    "authorId": "2351226328",
                                    "name": "Bohou Zhang"
                                },
                                {
                                    "authorId": "2350426005",
                                    "name": "Jiawei Cao"
                                },
                                {
                                    "authorId": "2350427710",
                                    "name": "Jie Ma"
                                },
                                {
                                    "authorId": "2322524150",
                                    "name": "Daoyu Wang"
                                },
                                {
                                    "authorId": "2258714945",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.86669921875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."
                        ],
                        "paper": {
                            "corpus_id": 270870251,
                            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2273537815",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "2308276345",
                                    "name": "Zhenghua Wang"
                                },
                                {
                                    "authorId": "2292070745",
                                    "name": "Xuan Gao"
                                },
                                {
                                    "authorId": "2308226671",
                                    "name": "Feiran Zhang"
                                },
                                {
                                    "authorId": "2308043953",
                                    "name": "Yixin Wu"
                                },
                                {
                                    "authorId": "2308044030",
                                    "name": "Zhibo Xu"
                                },
                                {
                                    "authorId": "2308036711",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2309182278",
                                    "name": "Zhengyuan Wang"
                                },
                                {
                                    "authorId": "2309656885",
                                    "name": "Shizheng Li"
                                },
                                {
                                    "authorId": "2309176521",
                                    "name": "Qi Qian"
                                },
                                {
                                    "authorId": "2292032843",
                                    "name": "Ruicheng Yin"
                                },
                                {
                                    "authorId": "2220896023",
                                    "name": "Changze Lv"
                                },
                                {
                                    "authorId": "2257315404",
                                    "name": "Xiaoqing Zheng"
                                },
                                {
                                    "authorId": "2257129987",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 61
                        },
                        "score": 0.9306640625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Similarly, RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 273969615,
                            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                            "authors": [
                                {
                                    "authorId": "2270181751",
                                    "name": "Zilun Zhang"
                                },
                                {
                                    "authorId": "2174678931",
                                    "name": "Haozhan Shen"
                                },
                                {
                                    "authorId": "8200875",
                                    "name": "Tiancheng Zhao"
                                },
                                {
                                    "authorId": "2330774884",
                                    "name": "Yuhao Wang"
                                },
                                {
                                    "authorId": "2330612748",
                                    "name": "Bin Chen"
                                },
                                {
                                    "authorId": "2149196373",
                                    "name": "Yuxiang Cai"
                                },
                                {
                                    "authorId": "2093090552",
                                    "name": "Yongheng Shang"
                                },
                                {
                                    "authorId": "2111612160",
                                    "name": "Jianwei Yin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.8359375
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."
                        ],
                        "paper": {
                            "corpus_id": 263605962,
                            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2255374957",
                                    "name": "Xi Victoria Lin"
                                },
                                {
                                    "authorId": "1769736",
                                    "name": "Xilun Chen"
                                },
                                {
                                    "authorId": "46221498",
                                    "name": "Mingda Chen"
                                },
                                {
                                    "authorId": "2254168373",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2253400960",
                                    "name": "Maria Lomeli"
                                },
                                {
                                    "authorId": "2191899140",
                                    "name": "Rich James"
                                },
                                {
                                    "authorId": "2253404757",
                                    "name": "Pedro Rodriguez"
                                },
                                {
                                    "authorId": "2253401183",
                                    "name": "Jacob Kahn"
                                },
                                {
                                    "authorId": "2253402270",
                                    "name": "Gergely Szilvasy"
                                },
                                {
                                    "authorId": "2253417398",
                                    "name": "Mike Lewis"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2253400757",
                                    "name": "Scott Yih"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 153
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2021)(Ram et al., 2023)(Zhang et al., 2024) and question answering (Izacard et al., 2022)Shi et al., 2023;(Yoran et al., 2023)(Lin et al., 2023)Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023)Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."
                        ],
                        "paper": {
                            "corpus_id": 278635834,
                            "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning",
                            "authors": [
                                {
                                    "authorId": "2361597409",
                                    "name": "Shaohan Wang"
                                },
                                {
                                    "authorId": "48378753",
                                    "name": "L. Zhang"
                                },
                                {
                                    "authorId": "2106681735",
                                    "name": "Zheren Fu"
                                },
                                {
                                    "authorId": "2349977855",
                                    "name": "Zhendong Mao"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.81689453125
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."
                        ],
                        "paper": {
                            "corpus_id": 278339057,
                            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                            "authors": [
                                {
                                    "authorId": "2195381022",
                                    "name": "Zhengliang Shi"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "2326805997",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "2265517632",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2331316040",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2261862546",
                                    "name": "Zhaochun Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Zamani et al., 2024)",
                        "snippets": [
                            "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."
                        ],
                        "paper": {
                            "corpus_id": 269605438,
                            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
                            "authors": [
                                {
                                    "authorId": "2293725953",
                                    "name": "Hamed Zamani"
                                },
                                {
                                    "authorId": "2240516450",
                                    "name": "Michael Bendersky"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                        ],
                        "paper": {
                            "corpus_id": 270199429,
                            "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
                            "authors": [
                                {
                                    "authorId": "2293319089",
                                    "name": "Feiteng Fang"
                                },
                                {
                                    "authorId": "2287892865",
                                    "name": "Yuelin Bai"
                                },
                                {
                                    "authorId": "2266469238",
                                    "name": "Shiwen Ni"
                                },
                                {
                                    "authorId": "2301170603",
                                    "name": "Min Yang"
                                },
                                {
                                    "authorId": "2287803799",
                                    "name": "Xiaojun Chen"
                                },
                                {
                                    "authorId": "2266809482",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 39
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Specific RAG Fine-Tuning Methods",
                "tldr": "Several specialized fine-tuning methods have emerged for RAG systems, each addressing specific challenges in retrieval-augmentation integration. These approaches include RAFT, Self-RAG, Finetune-RAG, CoRAG, and Open-RAG, which employ diverse techniques from instruction tuning with mixed-quality contexts to self-reflection mechanisms. (8 sources)",
                "text": "\n- **RAFT (Retrieval-Augmented Fine-Tuning)**: Combines retrieval methods with language model supervised fine-tuning to enhance in-domain RAG performance. RAFT trains language models using a mixture of relevant (\"golden\") and irrelevant (\"distractor\") document contexts from the target domain, teaching models to dynamically leverage external knowledge while prioritizing relevant content. It employs supervised fine-tuning on static, curated datasets containing predefined question-response pairs and incorporates logical reasoning via Chain-of-Thought prompts. However, RAFT is limited by its reliance on predefined data and single-objective cross-entropy optimization. <Paper corpusId=\"276287820\" paperTitle=\"(Bhushan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277501853\" paperTitle=\"(Srinivas et al., 2025)\" isShortName></Paper> <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>\n\n- **Self-RAG**: Advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms. Unlike conventional approaches that might retrieve irrelevant information, Self-RAG enables models to perform on-demand retrieval based on self-evaluation. It trains models to generate special \"reflection tokens\" that allow assessment of response quality and factual integrity. These tokens make the language model controllable during inference, enabling adaptation to diverse task requirements and improving the connectedness and correctness of outputs. <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264288947\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>\n\n- **Finetune-RAG**: Focuses on training language models to distinguish between correct and fictitious context within RAG setups. Rather than improving retrieval quality, this method fine-tunes models using examples where both correct and incorrect information are explicitly presented, teaching them to identify and utilize only reliable information in their responses. This approach addresses the challenge of imperfect or misleading retrieved content by improving the model's generation behavior when faced with mixed-quality inputs. <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>\n\n- **CoRAG**: Builds an end-to-end multistep reasoning framework through full parameter fine-tuning and multitask learning. This method expands single-step QA datasets into retrieval-reasoning chains and jointly trains various tasks including sub-query generation, intermediate answer prediction, and final composition. CoRAG enhances models' ability to break down complex problems (like multi-entity relational reasoning) and adapt retrieval strategies dynamically through techniques such as query rewriting and error correction. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **DeepRAG**: Combines imitation and contrastive learning with binary tree search to create efficient retrieval paths. It uses a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy, optimizing the retrieval pathway through full parameter fine-tuning and multitask learning. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **Open-RAG**: Employs semantic space alignment techniques through mixed fine-tuning approaches. This method uses QLoRA quantized fine-tuning <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper> and Mixture of Experts (MoE) modules to specialize networks for single and multi-hop reasoning tasks, improving retrieval effectiveness through better semantic understanding. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **AdaptiveRAG**: Fine-tunes a lightweight classifier to select retrieval strategies dynamically, applying parameter isolation techniques to balance generality and adaptability. This approach enables the system to choose appropriate retrieval mechanisms based on input characteristics. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Bhushan et al., 2025)",
                        "snippets": [
                            "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts."
                        ],
                        "paper": {
                            "corpus_id": 276287820,
                            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                            "authors": [
                                {
                                    "authorId": "2256382094",
                                    "name": "Kushagra Bhushan"
                                },
                                {
                                    "authorId": "1392630568",
                                    "name": "Yatin Nandwani"
                                },
                                {
                                    "authorId": "2345003462",
                                    "name": "Dinesh Khandelwal"
                                },
                                {
                                    "authorId": "2320314900",
                                    "name": "Sonam Gupta"
                                },
                                {
                                    "authorId": "2345005348",
                                    "name": "Gaurav Pandey"
                                },
                                {
                                    "authorId": "1916865",
                                    "name": "Dinesh Raghu"
                                },
                                {
                                    "authorId": "2243011716",
                                    "name": "Sachindra Joshi"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.8818359375
                    },
                    {
                        "id": "(Srinivas et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering).\n\nRAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently."
                        ],
                        "paper": {
                            "corpus_id": 277501853,
                            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                            "authors": [
                                {
                                    "authorId": "2203079037",
                                    "name": "Sakhinana Sagar Srinivas"
                                },
                                {
                                    "authorId": "2139833562",
                                    "name": "Venkataramana Runkana"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.884765625
                    },
                    {
                        "id": "(Chung et al., 2024)",
                        "snippets": [
                            "Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)",
                            "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning."
                        ],
                        "paper": {
                            "corpus_id": 272911196,
                            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
                            "authors": [
                                {
                                    "authorId": "2322992640",
                                    "name": "Isaac Chung"
                                },
                                {
                                    "authorId": "2322982756",
                                    "name": "Phat Vo"
                                },
                                {
                                    "authorId": "2322991957",
                                    "name": "Arman Kizilkale"
                                },
                                {
                                    "authorId": "2322982549",
                                    "name": "Aaron Reite"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.88232421875
                    },
                    {
                        "id": "(Anaissi et al., 2024)",
                        "snippets": [
                            "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]",
                            "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23]."
                        ],
                        "paper": {
                            "corpus_id": 273501949,
                            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                            "authors": [
                                {
                                    "authorId": "3333168",
                                    "name": "Ali Anaissi"
                                },
                                {
                                    "authorId": "3069261",
                                    "name": "Ali Braytee"
                                },
                                {
                                    "authorId": "1992906806",
                                    "name": "Junaid Akram"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
                            "n_citations": 3
                        },
                        "score": 0.84521484375
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."
                        ],
                        "paper": {
                            "corpus_id": 264288947,
                            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "7806955",
                                    "name": "Zeqiu Wu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2707234",
                                    "name": "Avirup Sil"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 780
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Gao et al., 2025)",
                        "snippets": [
                            "For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.\n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.\n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically."
                        ],
                        "paper": {
                            "corpus_id": 277994112,
                            "title": "Synergizing RAG and Reasoning: A Systematic Review",
                            "authors": [
                                {
                                    "authorId": "2280046531",
                                    "name": "Yunfan Gao"
                                },
                                {
                                    "authorId": "2275320371",
                                    "name": "Yun Xiong"
                                },
                                {
                                    "authorId": "2291322497",
                                    "name": "Yijie Zhong"
                                },
                                {
                                    "authorId": "2275171009",
                                    "name": "Yuxi Bi"
                                },
                                {
                                    "authorId": "2356716546",
                                    "name": "Ming Xue"
                                },
                                {
                                    "authorId": "2256769434",
                                    "name": "Haofen Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.85693359375
                    },
                    {
                        "id": "(Dettmers et al., 2023)",
                        "snippets": [
                            "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                        ],
                        "paper": {
                            "corpus_id": 258841328,
                            "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                            "authors": [
                                {
                                    "authorId": "3239480",
                                    "name": "Tim Dettmers"
                                },
                                {
                                    "authorId": "51152502",
                                    "name": "Artidoro Pagnoni"
                                },
                                {
                                    "authorId": "14487640",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2606
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges and Evaluation",
                "tldr": "RAG fine-tuning faces significant challenges including computational resource constraints, component integration complexities, and domain adaptation difficulties. Evaluation methodologies vary widely across research, with emerging frameworks focusing on measuring both retrieval accuracy and generation quality in integrated systems. (8 sources)",
                "text": "\nFine-tuning RAG systems presents several significant challenges that researchers continue to address. A fundamental tension exists between computational efficiency and system performance. Static optimization approaches (improving only one component while keeping the other fixed) require fewer computational resources and enable faster deployment, but often result in sub-optimal overall system performance <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>. This computational constraint becomes particularly significant when attempting to simultaneously optimize both retriever and generator components in dual fine-tuning approaches.\n\nDomain adaptation represents another critical challenge. While RAG systems show impressive performance on general knowledge tasks, adapting them to specialized domains requires careful consideration of domain-specific knowledge bases and retrieval mechanisms. Research has shown that models like RAG-end2end can address this challenge by updating all components during training when adapting to domain-specific knowledge bases <Paper corpusId=\"269149041\" paperTitle=\"(Weng, 2024)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>. However, this adaptation process requires balancing multiple optimization objectives and carefully constructing domain-appropriate knowledge bases.\n\nThe delicate interplay between retriever and generator components introduces integration challenges. Fine-tuning either component in isolation often leads to sub-optimal performance, as the system's effectiveness depends on the synergy between retrieval and generation processes <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>. Optimizing this integration requires careful consideration of retrieval model selection, context length optimization, knowledge base construction, and appropriate LLM deployment <Paper corpusId=\"267320876\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>.\n\nRetrieval noise presents a particularly persistent challenge. Inappropriate retrieved passages can significantly hinder a language model's ability to generate high-quality responses. Research has identified three distinct types of retrieval noise reflective of real-world environments, each affecting model robustness differently <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. Approaches like Retrieval-augmented Adaptive Adversarial Training (RAAT) attempt to address this challenge by dynamically adjusting the training process in response to different noise types <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nEvaluation methodologies for RAG fine-tuning vary widely across research, making direct comparison between approaches challenging. Some evaluation frameworks focus primarily on retrieval accuracy metrics, while others emphasize generation quality measures. More comprehensive evaluation approaches consider both aspects, evaluating the combined effectiveness of text generation and provenance retrieval <Paper corpusId=\"252568176\" paperTitle=\"(Hofstatter et al., 2022)\" isShortName></Paper>. The diversity of tasks to which RAG systems are applied\u2014from open-domain question answering to fact verification, slot-filling for relation extraction, and dialogue systems\u2014further complicates evaluation standardization <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, recent advances in end-to-end optimization methodologies offer promising directions. Approaches like Stochastic RAG, which relaxes simplifying assumptions of marginalization and document independence through differentiable approximation for sampling without replacement, demonstrate effective end-to-end optimization across diverse tasks <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. These integrated optimization approaches, though computationally intensive, appear increasingly necessary to realize the full potential of RAG systems.",
                "citations": [
                    {
                        "id": "(Cheng et al., 2025)",
                        "snippets": [
                            "Static optimization involves training only one component of the RAG system while keeping the other static. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption.\n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance.\n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information."
                        ],
                        "paper": {
                            "corpus_id": 277043297,
                            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "1491233507",
                                    "name": "Mingyue Cheng"
                                },
                                {
                                    "authorId": "2208917508",
                                    "name": "Yucong Luo"
                                },
                                {
                                    "authorId": "2322501286",
                                    "name": "Ouyang Jie"
                                },
                                {
                                    "authorId": "2332691115",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "2312648865",
                                    "name": "Huijie Liu"
                                },
                                {
                                    "authorId": "2291070758",
                                    "name": "Li Li"
                                },
                                {
                                    "authorId": "2322429208",
                                    "name": "Shuo Yu"
                                },
                                {
                                    "authorId": "2351226328",
                                    "name": "Bohou Zhang"
                                },
                                {
                                    "authorId": "2350426005",
                                    "name": "Jiawei Cao"
                                },
                                {
                                    "authorId": "2350427710",
                                    "name": "Jie Ma"
                                },
                                {
                                    "authorId": "2322524150",
                                    "name": "Daoyu Wang"
                                },
                                {
                                    "authorId": "2258714945",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.86669921875
                    },
                    {
                        "id": "(Weng, 2024)",
                        "snippets": [
                            "\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end, an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-"
                        ],
                        "paper": {
                            "corpus_id": 269149041,
                            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
                            "authors": [
                                {
                                    "authorId": "2296715370",
                                    "name": "Benjue Weng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.86328125
                    },
                    {
                        "id": "(Siriwardhana et al., 2022)",
                        "snippets": [
                            "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."
                        ],
                        "paper": {
                            "corpus_id": 252735056,
                            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51516859",
                                    "name": "Shamane Siriwardhana"
                                },
                                {
                                    "authorId": "52001535",
                                    "name": "Rivindu Weerasekera"
                                },
                                {
                                    "authorId": "2114425044",
                                    "name": "Elliott Wen"
                                },
                                {
                                    "authorId": "1992921690",
                                    "name": "Tharindu Kaluarachchi"
                                },
                                {
                                    "authorId": "1814487",
                                    "name": "R. Rana"
                                },
                                {
                                    "authorId": "1486464114",
                                    "name": "Suranga Nanayakkara"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 179
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."
                        ],
                        "paper": {
                            "corpus_id": 278339057,
                            "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models",
                            "authors": [
                                {
                                    "authorId": "2195381022",
                                    "name": "Zhengliang Shi"
                                },
                                {
                                    "authorId": "1387839383",
                                    "name": "Lingyong Yan"
                                },
                                {
                                    "authorId": "2153198380",
                                    "name": "Weiwei Sun"
                                },
                                {
                                    "authorId": "2326805997",
                                    "name": "Yue Feng"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                },
                                {
                                    "authorId": "2265517632",
                                    "name": "Xinyu Ma"
                                },
                                {
                                    "authorId": "2237948548",
                                    "name": "Shuaiqiang Wang"
                                },
                                {
                                    "authorId": "2331316040",
                                    "name": "Dawei Yin"
                                },
                                {
                                    "authorId": "2265490493",
                                    "name": "M. D. Rijke"
                                },
                                {
                                    "authorId": "2261862546",
                                    "name": "Zhaochun Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.904296875
                    },
                    {
                        "id": "(Lyu et al., 2024)",
                        "snippets": [
                            "Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results."
                        ],
                        "paper": {
                            "corpus_id": 267320876,
                            "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2187857206",
                                    "name": "Yuanjie Lyu"
                                },
                                {
                                    "authorId": "2268429641",
                                    "name": "Zhiyu Li"
                                },
                                {
                                    "authorId": "2268393907",
                                    "name": "Simin Niu"
                                },
                                {
                                    "authorId": "2268399953",
                                    "name": "Feiyu Xiong"
                                },
                                {
                                    "authorId": "2268400606",
                                    "name": "Bo Tang"
                                },
                                {
                                    "authorId": "2117833477",
                                    "name": "Wenjin Wang"
                                },
                                {
                                    "authorId": "2282083454",
                                    "name": "Hao Wu"
                                },
                                {
                                    "authorId": "2304320758",
                                    "name": "Huan Liu"
                                },
                                {
                                    "authorId": "2277237058",
                                    "name": "Tong Xu"
                                },
                                {
                                    "authorId": "2265580543",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Trans. Inf. Syst.",
                            "n_citations": 40
                        },
                        "score": 0.9140625
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."
                        ],
                        "paper": {
                            "corpus_id": 270199429,
                            "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
                            "authors": [
                                {
                                    "authorId": "2293319089",
                                    "name": "Feiteng Fang"
                                },
                                {
                                    "authorId": "2287892865",
                                    "name": "Yuelin Bai"
                                },
                                {
                                    "authorId": "2266469238",
                                    "name": "Shiwen Ni"
                                },
                                {
                                    "authorId": "2301170603",
                                    "name": "Min Yang"
                                },
                                {
                                    "authorId": "2287803799",
                                    "name": "Xiaojun Chen"
                                },
                                {
                                    "authorId": "2266809482",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 39
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hofstatter et al., 2022)",
                        "snippets": [
                            "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."
                        ],
                        "paper": {
                            "corpus_id": 252568176,
                            "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
                            "authors": [
                                {
                                    "authorId": "97393346",
                                    "name": "Sebastian Hofst\u00e4tter"
                                },
                                {
                                    "authorId": "2809410",
                                    "name": "Jiecao Chen"
                                },
                                {
                                    "authorId": "2062947723",
                                    "name": "K. Raman"
                                },
                                {
                                    "authorId": "2499986",
                                    "name": "Hamed Zamani"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 82
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zamani et al., 2024)",
                        "snippets": [
                            "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."
                        ],
                        "paper": {
                            "corpus_id": 269605438,
                            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
                            "authors": [
                                {
                                    "authorId": "2293725953",
                                    "name": "Hamed Zamani"
                                },
                                {
                                    "authorId": "2240516450",
                                    "name": "Michael Bendersky"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                            "n_citations": 29
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.172128
    }
}