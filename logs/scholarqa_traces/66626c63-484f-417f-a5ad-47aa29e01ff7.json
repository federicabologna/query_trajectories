{
    "query": "do know true superficial alignment hypothesis is \n model knowledge and capabilities learnt almost alignment which subdistribution of formats with users \n true all domains aspects is true perceivable aspects helpfulness \n SFT huge instruction dataset teach models new knowledge Pointers related papers appreciated",
    "user_id": "lib_user",
    "task_id": "66626c63-484f-417f-a5ad-47aa29e01ff7",
    "timestamp": "2025-06-23T21:44:33.028985",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 2,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.11219399999999999,
    "decomposed_query": {
        "rewritten_query": "Understanding the superficial alignment hypothesis: how model knowledge and capabilities are learned during alignment on subdistributions of formats with users, and whether this is true across all domains and aspects or only perceivable aspects of helpfulness. Also, how SFT with huge instruction datasets teaches models new knowledge.",
        "keyword_query": "superficial alignment hypothesis model knowledge capabilities alignment subdistribution formats users domains perceivable aspects helpfulness SFT instruction dataset",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010704,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Superficial Safety Alignment Hypothesis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 82,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326007326",
                    "name": "Jianwei Li"
                },
                {
                    "authorId": "2326001415",
                    "name": "Jung-Eun Kim"
                }
            ],
            "abstract": "As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.",
            "corpus_id": 273350763,
            "sentences": [
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
                    "score": 0.9488785742569851,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 31564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 841
                        },
                        {
                            "start": 844,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1453
                        },
                        {
                            "start": 1456,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1931
                        },
                        {
                            "start": 1934,
                            "end": 1972
                        },
                        {
                            "start": 1973,
                            "end": 2151
                        },
                        {
                            "start": 2152,
                            "end": 2350
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7001953125
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe). This simplified framing of safety alignment is consistent with the \"superficial\" nature of SAH, where the alignment process fine-tunes how the model behaves in response to queries, rather than altering its deep internal structures. \n\n(3) Refusal Mechanisms and Format Control: Just as general alignment teaches models to structure their outputs in a user-friendly way, safety alignment in SSAH teaches models to issue consistent refusal mechanisms. These refusals take the form of standardized responses that indicate the model's compliance with safety guidelines, much like how general alignment might guide a model to give well-structured, polite answers to other types of questions. Importantly, this refusal mechanism makes it easier to choose the appropriate subdistribution of the output format. \n\nThe Superficial Alignment Hypothesis (SAH) as outlined in the Zhou et al. ( 2024) provides a theoretical framework for understanding how alignment processes operate in large language models. It suggests that alignment is largely superficial, conditioning the model on how to use its pretrained knowledge effectively. The Superficial Safety Alignment Hypothesis (SSAH) builds on this by applying similar principles to the realm of safety, simplifying the task of safety alignment to binary decisions regarding the fulfillment or refusal of unsafe requests. Both hypotheses underscore that alignment does not deeply alter the core abilities of the model, but rather adjusts the way those abilities are applied in specific contexts.",
                    "score": 0.6661249872200419,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 35619,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 528
                        },
                        {
                            "start": 531,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1098
                        },
                        {
                            "start": 1101,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1830
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61767578125
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024). \n\nHowever, this claim is centered on general alignment, and directly validating the hypothesis is challenging due to the complex interplays between pretraining and alignment. When a model fails to fulfill a user's request, it can be difficult to determine whether the issue stems from the pretraining stage (due to lack of sufficient knowledge) or from the alignment process (due to misalignment in the output format). For example, when a model struggles with solving a math problem, it could either be a lack of relevant mathematical knowledge or the inability to structure its reasoning effectively. In such cases, good instruction techniques like the Chain-of-Thought approach can significantly enhance the quality of the model's responses (Wei et al., 2022). \n\nSuperficial Safety Alignment Hypothesis (SSAH). Since our focus is specifically on safety alignment, which has distinct properties compared to general alignment, we carefully define the scope of our hypothesis. A key observation here is that, for a model to be able to fulfill a malicious request, it must already possess the necessary knowledge and reasoning ability to carry out that harmful action. Based on this observation, we propose Superficial Safety Alignment Hypothesis (SSAH): SSAH: Given an unsafe model that is capable of fulfilling users' malicious requests, safety alignment teaches the model the correct reasoning direction and a simple refusal mechanisms with reserved options. \n\nReasoning direction here refers to the model's internal decision-making process when confronted with a malicious query. That is, it represents the path the model is inclined to take in such a binary classification task, whether to fulfill the harmful request or to issue a refusal. As illustrated in Fig. 1, compared with SAH which targets the general alignment, our SSAH focuses on the safety alignment and has the following key differences : \n\n(1) Knowledge and reasoning ability: Safety alignment simplifies the problem by focusing specifically on models that already possess sufficient knowledge and reasoning abilities, as these models are capable of fulfilling malicious requests. This approach allows us to disregard other influencing factors and concentrate solely on the safety alignment process.",
                    "score": 0.6427660339937031,
                    "section_title": "SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS (SSAH)",
                    "char_start_offset": 7493,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 282
                        },
                        {
                            "start": 285,
                            "end": 457
                        },
                        {
                            "start": 458,
                            "end": 701
                        },
                        {
                            "start": 702,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1045
                        },
                        {
                            "start": 1048,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1742
                        },
                        {
                            "start": 1745,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2188
                        },
                        {
                            "start": 2191,
                            "end": 2431
                        },
                        {
                            "start": 2432,
                            "end": 2550
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1026,
                            "end": 1044,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56982421875
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming. The authors of LiMA argue that most of the functional capabilities of a language model are already present after pretraining, and that alignment is more about conditioning the model to apply these capabilities in a user-friendly way. \n\nThe Superficial Alignment Hypothesis can also help explain phenomena where models exhibit brittleness -for example, where an LLM generates inappropriate or harmful responses in new domains or under adversarial conditions. This brittleness is attributed to the fact that alignment does not deeply alter the underlying decision-making processes of the model, but only skims the surface to adjust output behavior in specific contexts. Therefore, if an adversary finds a way to bypass these superficial alignments (e.g., via jailbreaking), the model's underlying pretrained knowledge and capabilities may still enable it to produce harmful or misaligned responses. \n\nPreprint. \n\nRelevance to Superficial Safety Alignment Hypothesis. While SAH deals with general alignment (i.e., ensuring that a model follows general user instructions), SSAH is specifically focused on ensuring that a model safely interacts with users, especially when faced with harmful or malicious queries. The key parallels between SAH and SSAH include: \n\n(1) Pretrained Knowledge and Safety Concerns: Just as SAH assumes that knowledge and capabilities are largely acquired during pretraining, SSAH assumes that a model's ability to execute harmful actions (e.g., generating unsafe or unethical content) also stems from pretraining. Safety alignment, like general alignment, does not aim to teach the model new facts or capabilities, but rather to guide its reasoning pathways in a safe direction. \n\n(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe).",
                    "score": 0.6050972170156432,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 33716,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 432
                        },
                        {
                            "start": 435,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1107
                        },
                        {
                            "start": 1110,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1455
                        },
                        {
                            "start": 1458,
                            "end": 1735
                        },
                        {
                            "start": 1736,
                            "end": 1900
                        },
                        {
                            "start": 1903,
                            "end": 2199
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.383056640625
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "Discussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak attacks, we do not propose a specific solution to address these issues in this work. If these issues could be resolved within the framework of our theory, the term \"Superficial\" in \"Superficial Safety Alignment Hypothesis\" may no longer be necessary. Interestingly, recent research provides some supporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that advanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A systematic, multi-layered approach, extending beyond the model itself, may be required to effectively defend against sophisticated adversarial threats. \n\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact of the alignment method SFT. Due to resource limitations, we have not yet tested this approach on other alignment methods like PPO or DPO. \n\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and addresses the three key questions: How does safety alignment affect model behavior? Why are safety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these questions, we were able to demonstrate that safety alignment can be a straightforward process, rather than a myth. \n\nA APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS \n\nIn this section, we provide additional technical details and clarifications to supplement the experiments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These details help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hypothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology behind model configuration, fine-tuning, and evaluation. This appendix includes further discussion on how general instruction-following models and safety-aligned models were fine-tuned and assessed to probe their reasoning directions when faced with malicious queries, and the results of these assessments are presented in detail. \n\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA. \n\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation.",
                    "score": 0.5407700873546697,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 29361,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 11
                        },
                        {
                            "start": 12,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 348
                        },
                        {
                            "start": 349,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 587
                        },
                        {
                            "start": 588,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 977
                        },
                        {
                            "start": 980,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1156
                        },
                        {
                            "start": 1157,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1371
                        },
                        {
                            "start": 1374,
                            "end": 1425
                        },
                        {
                            "start": 1428,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 2152
                        },
                        {
                            "start": 2155,
                            "end": 2200
                        },
                        {
                            "start": 2203,
                            "end": 2477
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.2310791015625
                },
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "In our probe experiments, we explore the reasoning direction differences between unsafety-aligned models and safety-aligned models across several popular LLaMA families, including LLaMA2, LLaMA3, and LLaMA3.1 (Fig. 7 describes more probing results on the HEx-PHI dataset). These models offer diverse pretrained knowledge and capabilities, allowing us to investigate how safety alignment affects model behavior when responding to malicious queries. To isolate the impact of reasoning direction when facing unsafe inputs, it is crucial to control for other confounding factors. Existing open-source instruction-following models are typically both helpful and safe, while pretrained open-source models without safety alignment are neither helpful nor safe. This dichotomy presents a challenge in disentangling the effect of general instruction-following capabilities from safety-specific behaviors. \n\nThus, for each LLaMA variant (LLaMA2, LLaMA3, LLaMA3.1), we fine-tuned two separate models using Supervised Fine-Tuning (SFT): \n\n(1) A General Instruction-Following Model that is trained to follow human instructions but without any explicit safety mechanisms. This model helps us evaluate how a model with instruction-following capabilities but without safety guardrails reacts to malicious queries. (2) A Safety-Aligned Model that incorporates both general instruction-following capabilities and explicit safety mechanisms, allowing us to examine how safety alignment influences the model's reasoning direction when responding to unsafe inputs. \n\nBy comparing these two categories of models when exposed to different types of malicious queries, we can better understand how safety alignment reshapes the internal decision-making process of large language models. \n\nSupervised Fine-Tuning Process and Configuration. We follow the alignment method outlined in the Zhou et al. (2024), which uses Supervised Fine-Tuning (SFT). For the general instructionfollowing models, we employed the LIMA dataset, which includes over 1000 instruction-following examples. However, we removed 13 safety-related examples to avoid conflating safety concerns with general instruction-following abilities. The filtering process was assisted by GPT-4, following a set of instructions specifically designed to identify and exclude safety-related tasks.",
                    "score": 0.4958820147701556,
                    "section_title": "A.2 MODEL CONFIGURATION AND TRAINING DETAILS",
                    "char_start_offset": 37498,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 895
                        },
                        {
                            "start": 898,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1024
                        },
                        {
                            "start": 1027,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1543
                        },
                        {
                            "start": 1546,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1921
                        },
                        {
                            "start": 1922,
                            "end": 2053
                        },
                        {
                            "start": 2054,
                            "end": 2182
                        },
                        {
                            "start": 2183,
                            "end": 2327
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.027740478515625
                }
            ],
            "relevance_judgement": 0.7001953125,
            "relevance_judgment_input_expanded": "# Title: Superficial Safety Alignment Hypothesis\n# Venue: arXiv.org\n# Authors: Jianwei Li, Jung-Eun Kim\n## Abstract\nAs large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.\n## SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS (SSAH)\nPrevious research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024). \n\nHowever, this claim is centered on general alignment, and directly validating the hypothesis is challenging due to the complex interplays between pretraining and alignment. When a model fails to fulfill a user's request, it can be difficult to determine whether the issue stems from the pretraining stage (due to lack of sufficient knowledge) or from the alignment process (due to misalignment in the output format). For example, when a model struggles with solving a math problem, it could either be a lack of relevant mathematical knowledge or the inability to structure its reasoning effectively. In such cases, good instruction techniques like the Chain-of-Thought approach can significantly enhance the quality of the model's responses (Wei et al., 2022). \n\nSuperficial Safety Alignment Hypothesis (SSAH). Since our focus is specifically on safety alignment, which has distinct properties compared to general alignment, we carefully define the scope of our hypothesis. A key observation here is that, for a model to be able to fulfill a malicious request, it must already possess the necessary knowledge and reasoning ability to carry out that harmful action. Based on this observation, we propose Superficial Safety Alignment Hypothesis (SSAH): SSAH: Given an unsafe model that is capable of fulfilling users' malicious requests, safety alignment teaches the model the correct reasoning direction and a simple refusal mechanisms with reserved options. \n\nReasoning direction here refers to the model's internal decision-making process when confronted with a malicious query. That is, it represents the path the model is inclined to take in such a binary classification task, whether to fulfill the harmful request or to issue a refusal. As illustrated in Fig. 1, compared with SAH which targets the general alignment, our SSAH focuses on the safety alignment and has the following key differences : \n\n(1) Knowledge and reasoning ability: Safety alignment simplifies the problem by focusing specifically on models that already possess sufficient knowledge and reasoning abilities, as these models are capable of fulfilling malicious requests. This approach allows us to disregard other influencing factors and concentrate solely on the safety alignment process.\n\n## DISCUSSION, LIMITATION, AND CONCLUSION\nDiscussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak attacks, we do not propose a specific solution to address these issues in this work. If these issues could be resolved within the framework of our theory, the term \"Superficial\" in \"Superficial Safety Alignment Hypothesis\" may no longer be necessary. Interestingly, recent research provides some supporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that advanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A systematic, multi-layered approach, extending beyond the model itself, may be required to effectively defend against sophisticated adversarial threats. \n\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact of the alignment method SFT. Due to resource limitations, we have not yet tested this approach on other alignment methods like PPO or DPO. \n\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and addresses the three key questions: How does safety alignment affect model behavior? Why are safety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these questions, we were able to demonstrate that safety alignment can be a straightforward process, rather than a myth. \n\nA APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS \n\nIn this section, we provide additional technical details and clarifications to supplement the experiments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These details help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hypothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology behind model configuration, fine-tuning, and evaluation. This appendix includes further discussion on how general instruction-following models and safety-aligned models were fine-tuned and assessed to probe their reasoning directions when faced with malicious queries, and the results of these assessments are presented in detail. \n\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA. \n\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation.\n...\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.\n...\nThis observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming. The authors of LiMA argue that most of the functional capabilities of a language model are already present after pretraining, and that alignment is more about conditioning the model to apply these capabilities in a user-friendly way. \n\nThe Superficial Alignment Hypothesis can also help explain phenomena where models exhibit brittleness -for example, where an LLM generates inappropriate or harmful responses in new domains or under adversarial conditions. This brittleness is attributed to the fact that alignment does not deeply alter the underlying decision-making processes of the model, but only skims the surface to adjust output behavior in specific contexts. Therefore, if an adversary finds a way to bypass these superficial alignments (e.g., via jailbreaking), the model's underlying pretrained knowledge and capabilities may still enable it to produce harmful or misaligned responses. \n\nPreprint. \n\nRelevance to Superficial Safety Alignment Hypothesis. While SAH deals with general alignment (i.e., ensuring that a model follows general user instructions), SSAH is specifically focused on ensuring that a model safely interacts with users, especially when faced with harmful or malicious queries. The key parallels between SAH and SSAH include: \n\n(1) Pretrained Knowledge and Safety Concerns: Just as SAH assumes that knowledge and capabilities are largely acquired during pretraining, SSAH assumes that a model's ability to execute harmful actions (e.g., generating unsafe or unethical content) also stems from pretraining. Safety alignment, like general alignment, does not aim to teach the model new facts or capabilities, but rather to guide its reasoning pathways in a safe direction. \n\n(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe).\n...\n(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe). This simplified framing of safety alignment is consistent with the \"superficial\" nature of SAH, where the alignment process fine-tunes how the model behaves in response to queries, rather than altering its deep internal structures. \n\n(3) Refusal Mechanisms and Format Control: Just as general alignment teaches models to structure their outputs in a user-friendly way, safety alignment in SSAH teaches models to issue consistent refusal mechanisms. These refusals take the form of standardized responses that indicate the model's compliance with safety guidelines, much like how general alignment might guide a model to give well-structured, polite answers to other types of questions. Importantly, this refusal mechanism makes it easier to choose the appropriate subdistribution of the output format. \n\nThe Superficial Alignment Hypothesis (SAH) as outlined in the Zhou et al. ( 2024) provides a theoretical framework for understanding how alignment processes operate in large language models. It suggests that alignment is largely superficial, conditioning the model on how to use its pretrained knowledge effectively. The Superficial Safety Alignment Hypothesis (SSAH) builds on this by applying similar principles to the realm of safety, simplifying the task of safety alignment to binary decisions regarding the fulfillment or refusal of unsafe requests. Both hypotheses underscore that alignment does not deeply alter the core abilities of the model, but rather adjusts the way those abilities are applied in specific contexts.\n\n## A.2 MODEL CONFIGURATION AND TRAINING DETAILS\nIn our probe experiments, we explore the reasoning direction differences between unsafety-aligned models and safety-aligned models across several popular LLaMA families, including LLaMA2, LLaMA3, and LLaMA3.1 (Fig. 7 describes more probing results on the HEx-PHI dataset). These models offer diverse pretrained knowledge and capabilities, allowing us to investigate how safety alignment affects model behavior when responding to malicious queries. To isolate the impact of reasoning direction when facing unsafe inputs, it is crucial to control for other confounding factors. Existing open-source instruction-following models are typically both helpful and safe, while pretrained open-source models without safety alignment are neither helpful nor safe. This dichotomy presents a challenge in disentangling the effect of general instruction-following capabilities from safety-specific behaviors. \n\nThus, for each LLaMA variant (LLaMA2, LLaMA3, LLaMA3.1), we fine-tuned two separate models using Supervised Fine-Tuning (SFT): \n\n(1) A General Instruction-Following Model that is trained to follow human instructions but without any explicit safety mechanisms. This model helps us evaluate how a model with instruction-following capabilities but without safety guardrails reacts to malicious queries. (2) A Safety-Aligned Model that incorporates both general instruction-following capabilities and explicit safety mechanisms, allowing us to examine how safety alignment influences the model's reasoning direction when responding to unsafe inputs. \n\nBy comparing these two categories of models when exposed to different types of malicious queries, we can better understand how safety alignment reshapes the internal decision-making process of large language models. \n\nSupervised Fine-Tuning Process and Configuration. We follow the alignment method outlined in the Zhou et al. (2024), which uses Supervised Fine-Tuning (SFT). For the general instructionfollowing models, we employed the LIMA dataset, which includes over 1000 instruction-following examples. However, we removed 13 safety-related examples to avoid conflating safety concerns with general instruction-following abilities. The filtering process was assisted by GPT-4, following a set of instructions specifically designed to identify and exclude safety-related tasks.",
            "reference_string": "[273350763 | Li et al. | 2024 | Citations: 3]"
        },
        {
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 198,
            "influential_citation_count": 27,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51583409",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "50085131",
                    "name": "Ximing Lu"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "1947172233",
                    "name": "Melanie Sclar"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "1857797",
                    "name": "Chandra Bhagavatula"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                }
            ],
            "abstract": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
            "corpus_id": 265608902,
            "sentences": [
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
                    "score": 0.5794555816255665,
                    "section_title": "Preprint",
                    "char_start_offset": 993,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1901
                        },
                        {
                            "start": 1904,
                            "end": 2114
                        },
                        {
                            "start": 2115,
                            "end": 2302
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63720703125
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
                    "score": 0.5155875642273787,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.124755859375
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3). URIAL leverages in-context learning (ICL) through prompting with just a few carefully curated stylistic examples and a carefully designed system prompt to achieve impressive alignment results. We craft the in-context examples to begin by affirming the user query and introducing background information, then proceed to enumerate items or steps with comprehensive details, and finally conclude with an engaging summary that includes safety-related disclaimers. Surprisingly, we find that such a straightforward baseline method can significantly reduce the performance gap between base LLMs and aligned LLMs. To rigorously evaluate different alignment methods, we design a multi-aspect, interpretable evaluation protocol, detailed in Sec. 4. We create a dataset named just-eval-instruct which contains 1,000 diverse instructions from 9 existing datasets, such as those used by AlpacaEval (Li et al., 2023a), MT-bench (Zheng et al., 2023), andLIMA (Zhou et al., 2023). Our analysis encompasses six dimensions of LLM outputs: \n\nhelpfulness, clarity, factuality, depth, engagement, and safety. Our extensive results indicate that URIAL, using as few as three constant incontext examples, can effectively align base LLMs. Remarkably, URIAL surpass the LLMs aligned with SFT or SFT+RLHF on strong base LLMs such as Mistral-7b (Jiang et al., 2023a) and Llama-2-70b (Touvron et al., 2023), as reported in Fig. 1 and Tab. 1. \n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning. In this vein, our contributions in this work can support future research in the analysis and alignment of base LLMs.",
                    "score": 0.440922530977205,
                    "section_title": "Preprint",
                    "char_start_offset": 3108,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1209
                        },
                        {
                            "start": 1212,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1602
                        },
                        {
                            "start": 1605,
                            "end": 1784
                        },
                        {
                            "start": 1785,
                            "end": 2018
                        },
                        {
                            "start": 2019,
                            "end": 2135
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.0850830078125
                },
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "Limitation of Tuning-based Alignment. Alignment tuning through SFT and RLHF typically demands substantial resources, such as GPU nodes, a large amount of instruction data, and human annotations, making the process both costly and time-consuming. This restricts ordinary labs from aligning extreme-scale LLMs exceeding 30B, let alone the recent Falcon-180B (Almazrouei et al., 2023). Moreover, during the pre-training and continual training stages, efficiently estimating the downstream performance of a base model checkpoint becomes challenging if alignment tuning is always required to evaluate its instruction-following ability. Besides the aforementioned limitations, tuning-based alignment may also cause forgetting issues in LLMs. Wang et al. (2023) demonstrated that some SFTed LLMs perform significantly worse than their base counterparts on factual and reasoning benchmarks. For instance, applying SFT to Llama-13b with self-instruct (Wang et al., 2022a) results in a considerable decline in its MMLU performance (from 42.5 to 30.3) and Codex-Eval performance (from 26.6 to 13.4). Even more strikingly, SFT with SuperNI (Wang et al., 2022b) causes Llama-13B to nearly lose all its BBH reasoning ability (decreasing from 36.9 to 2.8). Moreover, Shen et al. (2023) show that the reward models in RLHF can perform very inconsistently, yielding a nearly random performance when showing contrastive instructions to them. These findings imply that alignment tuning may lead to the forgetting of previously acquired knowledge in base LLMs, which is also shown in our experiments. Superficial alignment hypothesis. LIMA (Zhou et al., 2023) employs only 1k examples to finetune a 65B LLM and discovers that such a slightly tuned LLM surprisingly achieves a high win rate over ChatGPT, thus implying that the alignment tuning is superficial. Similar observations are also reported by other recent studies (Chen et al., 2023a;Lee et al., 2023).",
                    "score": 0.4328163645086571,
                    "section_title": "ALIGNMENT TUNING & SUPERFICIAL ALIGNMENT HYPOTHESIS",
                    "char_start_offset": 35071,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 37
                        },
                        {
                            "start": 38,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1088
                        },
                        {
                            "start": 1089,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1580
                        },
                        {
                            "start": 1581,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1839
                        },
                        {
                            "start": 1840,
                            "end": 1941
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 942,
                            "end": 962,
                            "matchedPaperCorpusId": "254877310"
                        },
                        {
                            "start": 1128,
                            "end": 1148,
                            "matchedPaperCorpusId": "253098274"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.0316162109375
                }
            ],
            "relevance_judgement": 0.63720703125,
            "relevance_judgment_input_expanded": "# Title: The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning\n# Venue: arXiv.org\n# Authors: Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Raghavi Chandu, Chandra Bhagavatula, Yejin Choi\n## Abstract\nThe alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.\n## Preprint\nOn the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).\n...\nWe propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3). URIAL leverages in-context learning (ICL) through prompting with just a few carefully curated stylistic examples and a carefully designed system prompt to achieve impressive alignment results. We craft the in-context examples to begin by affirming the user query and introducing background information, then proceed to enumerate items or steps with comprehensive details, and finally conclude with an engaging summary that includes safety-related disclaimers. Surprisingly, we find that such a straightforward baseline method can significantly reduce the performance gap between base LLMs and aligned LLMs. To rigorously evaluate different alignment methods, we design a multi-aspect, interpretable evaluation protocol, detailed in Sec. 4. We create a dataset named just-eval-instruct which contains 1,000 diverse instructions from 9 existing datasets, such as those used by AlpacaEval (Li et al., 2023a), MT-bench (Zheng et al., 2023), andLIMA (Zhou et al., 2023). Our analysis encompasses six dimensions of LLM outputs: \n\nhelpfulness, clarity, factuality, depth, engagement, and safety. Our extensive results indicate that URIAL, using as few as three constant incontext examples, can effectively align base LLMs. Remarkably, URIAL surpass the LLMs aligned with SFT or SFT+RLHF on strong base LLMs such as Mistral-7b (Jiang et al., 2023a) and Llama-2-70b (Touvron et al., 2023), as reported in Fig. 1 and Tab. 1. \n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning. In this vein, our contributions in this work can support future research in the analysis and alignment of base LLMs.\n\n## ALIGNMENT TUNING & SUPERFICIAL ALIGNMENT HYPOTHESIS\nLimitation of Tuning-based Alignment. Alignment tuning through SFT and RLHF typically demands substantial resources, such as GPU nodes, a large amount of instruction data, and human annotations, making the process both costly and time-consuming. This restricts ordinary labs from aligning extreme-scale LLMs exceeding 30B, let alone the recent Falcon-180B (Almazrouei et al., 2023). Moreover, during the pre-training and continual training stages, efficiently estimating the downstream performance of a base model checkpoint becomes challenging if alignment tuning is always required to evaluate its instruction-following ability. Besides the aforementioned limitations, tuning-based alignment may also cause forgetting issues in LLMs. Wang et al. (2023) demonstrated that some SFTed LLMs perform significantly worse than their base counterparts on factual and reasoning benchmarks. For instance, applying SFT to Llama-13b with self-instruct (Wang et al., 2022a) results in a considerable decline in its MMLU performance (from 42.5 to 30.3) and Codex-Eval performance (from 26.6 to 13.4). Even more strikingly, SFT with SuperNI (Wang et al., 2022b) causes Llama-13B to nearly lose all its BBH reasoning ability (decreasing from 36.9 to 2.8). Moreover, Shen et al. (2023) show that the reward models in RLHF can perform very inconsistently, yielding a nearly random performance when showing contrastive instructions to them. These findings imply that alignment tuning may lead to the forgetting of previously acquired knowledge in base LLMs, which is also shown in our experiments. Superficial alignment hypothesis. LIMA (Zhou et al., 2023) employs only 1k examples to finetune a 65B LLM and discovers that such a slightly tuned LLM surprisingly achieves a high win rate over ChatGPT, thus implying that the alignment tuning is superficial. Similar observations are also reported by other recent studies (Chen et al., 2023a;Lee et al., 2023).",
            "reference_string": "[265608902 | Lin et al. | 2023 | Citations: 198]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
            "score": 0.9488785742569851,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 31564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7001953125
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "by the fact that simple fine-tuning over a small number of examples allows the model to compete with the state of the art, highlighting the relative importance of pretraining over large-scale instruction tuning and reinforcement learning approaches [ Page 5]. Please note that the information provided is based solely on the search results and is limited to the content available in those pages. token cost : 781 Response from chatpdf.com The hypothesis proposed in this paper is referred to as the \"Superficial Alignment Hypothesis.\" According to this hypothesis, alignment in language models is primarily about learning the style or format for interacting with users. It suggests that by fine-tuning a pretrained language model with a relatively small set of examples, the model can achieve strong performance in terms of alignment and generate responses in the style of a helpful AI assistant. Response from Claude+ 100K (GT of Evaluation.) The paper coins the term \"Superficial Alignment Hypothesis\" to refer to their hypothesis about alignment: \"A model's knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.\" In other words, their hypothesis is that alignment is largely about teaching the model stylistic and formatting guidelines for generating appropriate responses, whereas most of the underlying knowledge and capabilities come from pre-training. They test this hypothesis by fine-tuning a large language model on just 1,000 examples, and show that it is able to produce responses that are comparable or better than models trained on much more data. This supports their hypothesis that relatively little data is needed for alignment if the model has a strong pre-trained foundation.\n\nThe key idea is that alignment is \"superficial\" in the sense that it is mostly about teaching the model which style or formats to use, rather than imparting large amounts of new knowledge. So in summary, the \"Superficial Alignment Hypothesis\" refers to their hypothesis that alignment requires relatively little data because it is more about teaching surface-level stylistic conventions, whereas the bulk of the model's knowledge comes from pre-training.",
            "score": 0.8692606983321841,
            "section_title": "C. Example of outputs",
            "char_start_offset": 43805,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "258822910",
            "title": "LIMA: Less Is More for Alignment",
            "text": "We define the Superficial Alignment Hypothesis: A model's knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users. If this hypothesis is correct, and alignment is largely about learning style, then a corollary of the Superficial Alignment Hypothesis is that one could sufficiently tune a pretrained language model with a rather small set of examples [Kirstain et al., 2021]. \n\nTo that end, we collect a dataset of 1,000 prompts and responses, where the outputs (responses) are stylistically aligned with each other, but the inputs (prompts) are diverse. Specifically, we seek outputs in the style of a helpful AI assistant. We curate such examples from a variety of sources, primarily split into community Q&A forums and manually authored examples. We also collect a test set of 300 prompts and a development set of 50. Table 1 shows an overview of the different data sources and provides some statistics (see Appendix A for a selection of training examples).",
            "score": 0.7506523157184926,
            "section_title": "Alignment Data",
            "char_start_offset": 3168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1080
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4375
        },
        {
            "corpus_id": "269982232",
            "title": "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners",
            "text": "This is consistent with the \"Superficial Alignment Hypothesis\" (Zhou et al., 2024), which indicates that model learns knowledge and capabilities almost entirely in pretraining process, while alignment only guides the model to utilize the different \"subdistribution of formats\".So the data in the same subdistribution of formats is more beneficial.",
            "score": 0.7426028044013655,
            "section_title": "Analysis",
            "char_start_offset": 18767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 347
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 82,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "267770042",
            "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
            "text": "We additionally investigated the superficial alignment hypothesis, which states that only a few examples per task are required to teach a model to follow instructions. Figure 4 in the Appendix highlights that 24EU-7B models instruction-tuned on Bactrian-X as well as Bactrian-X-small generally outperform models instruction-tuned on Lima-X datasets. Our results show that the Superficial Alignment Hypothesis (Kirstain et al., 2022;Zhou et al., 2023) does not generally hold for midsized LLMs. However, with Mixtral-8x7B, we see high performances for synthetic as well as humancurated data, indicating that the effectiveness of the Superficial Alignment Hypothesis increases with larger model size or respectively with greater pretrained model capabilities.",
            "score": 0.7134325805118327,
            "section_title": "Superficial Alignment Hypothesis",
            "char_start_offset": 25871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 757
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.090087890625
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.",
            "score": 0.7115668972457387,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291015625
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "?\n\nPrompt Template I will provide the document chunks as follows: [docs], Instructions: Compose a comprehensive reply to the query using the provided document chunks. Cite each reference using [Page Number] notation (each document chunk begins with this number). Ensure citations are placed at the end of each sentence. If the document chunks mention multiple subjects sharing the same name, create separate responses for each. Include only information found in the document chunks, and refrain from adding extraneous details. Ensure the accuracy of the response and avoid disseminating false content. Exclude search results that are not pertinent to the question. Respond concisely and in a step-by-step manner. Query: [query]. Please provide detailed findings in response to the query: Query: What is the hypothesis about alignment in this paper? Response from Our system The hypothesis about alignment in this paper, as stated in the search results, can be summarized as follows: Superficial Alignment Hypothesis: The authors propose the Superficial Alignment Hypothesis, which suggests that a language model's knowledge and capabilities are predominantly acquired during pretraining, while alignment teaches the model which In summary, the hypothesis presented in the paper suggests that pretraining is responsible for the majority of a language model's knowledge and capabilities, while alignment focuses on teaching the model how to interact with users in specific formats or styles. token cost : 809 Response from Our system (Entry Level assistant) Based on the provided search results, here is a detailed list of findings regarding the hypothesis about alignment in the paper: 1. The paper introduces the Superficial Alignment Hypothesis, which states that a model's knowledge and capabilities are learned primarily during pretraining, while alignment teaches it which subdistribution of formats should be used [Page 1]. 2. The Superficial Alignment Hypothesis suggests that alignment can be a simple process where the model learns the style or format for interacting with users, exposing the knowledge and capabilities acquired during pretraining [Page 0]. 3. The paper argues that the Superficial Alignment Hypothesis is supported by the fact that simple fine-tuning over a small number of examples allows the model to compete with the state of the art, highlighting the relative importance of pretraining over large-scale instruction tuning and reinforcement learning approaches [ Page 5]. Please note that the information provided is based solely on the search results and is",
            "score": 0.7007664360779579,
            "section_title": "C. Example of outputs",
            "char_start_offset": 41564,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1787109375
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "The hypothesis in this paper is that alignment can be a simple process where the model learns the style or format for interacting with users, to expose the knowledge and capabilities that were already acquired during pretraining. This is supported by the fact that the authors were able to achieve remarkably strong performance by simply fine-tuning a 65B-parameter LLaMa language model on just 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. The authors collected a dataset of 1,000 prompts and responses where the outputs (responses) were stylistically aligned with each other, but the inputs (prompts) were diverse, and the model was trained on this set of examples.\n\nResponse from Vicuna-33B Token indices sequence length is longer than the specified maximum sequence length for this model (3045 \u00bf 2048). Running this sequence through the model will result in indexing errors The hypothesis about alignment in the paper is the \"Superficial Alignment Hypothesis.\" This hypothesis suggests that a model's knowledge and capabilities are primarily learned during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users [Kirstain et al., 2021]. In simpler terms, the model learns the style or format for interacting with users, exposing the knowledge and capabilities acquired during pretraining. The authors of the paper set out to test this hypothesis by curating a dataset of 1,000 prompts and responses, where the outputs are stylistically aligned with each other, but the inputs are diverse. The outputs are designed to be in the style of a helpful AI assistant. They collected these examples from various sources, primarily split into community Q&A forums and manually authored examples. They then trained a pretrained 65B-parameter LLaMa model, LIMA, on this set of 1,000 demonstrations, fine-tuning it to confirm their hypothesis. The results showed that LIMA performed remarkably well, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. The model also tended to generalize well to unseen tasks that did not appear in the training data. Response from Vicuna-13B I'm sorry, but I do not have access to the PDF you are referring to",
            "score": 0.6706513980882185,
            "section_title": "Response from MPT-30B-Chat",
            "char_start_offset": 46109,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148193359375
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe). This simplified framing of safety alignment is consistent with the \"superficial\" nature of SAH, where the alignment process fine-tunes how the model behaves in response to queries, rather than altering its deep internal structures. \n\n(3) Refusal Mechanisms and Format Control: Just as general alignment teaches models to structure their outputs in a user-friendly way, safety alignment in SSAH teaches models to issue consistent refusal mechanisms. These refusals take the form of standardized responses that indicate the model's compliance with safety guidelines, much like how general alignment might guide a model to give well-structured, polite answers to other types of questions. Importantly, this refusal mechanism makes it easier to choose the appropriate subdistribution of the output format. \n\nThe Superficial Alignment Hypothesis (SAH) as outlined in the Zhou et al. ( 2024) provides a theoretical framework for understanding how alignment processes operate in large language models. It suggests that alignment is largely superficial, conditioning the model on how to use its pretrained knowledge effectively. The Superficial Safety Alignment Hypothesis (SSAH) builds on this by applying similar principles to the realm of safety, simplifying the task of safety alignment to binary decisions regarding the fulfillment or refusal of unsafe requests. Both hypotheses underscore that alignment does not deeply alter the core abilities of the model, but rather adjusts the way those abilities are applied in specific contexts.",
            "score": 0.6661249872200419,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 35619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024). \n\nHowever, this claim is centered on general alignment, and directly validating the hypothesis is challenging due to the complex interplays between pretraining and alignment. When a model fails to fulfill a user's request, it can be difficult to determine whether the issue stems from the pretraining stage (due to lack of sufficient knowledge) or from the alignment process (due to misalignment in the output format). For example, when a model struggles with solving a math problem, it could either be a lack of relevant mathematical knowledge or the inability to structure its reasoning effectively. In such cases, good instruction techniques like the Chain-of-Thought approach can significantly enhance the quality of the model's responses (Wei et al., 2022). \n\nSuperficial Safety Alignment Hypothesis (SSAH). Since our focus is specifically on safety alignment, which has distinct properties compared to general alignment, we carefully define the scope of our hypothesis. A key observation here is that, for a model to be able to fulfill a malicious request, it must already possess the necessary knowledge and reasoning ability to carry out that harmful action. Based on this observation, we propose Superficial Safety Alignment Hypothesis (SSAH): SSAH: Given an unsafe model that is capable of fulfilling users' malicious requests, safety alignment teaches the model the correct reasoning direction and a simple refusal mechanisms with reserved options. \n\nReasoning direction here refers to the model's internal decision-making process when confronted with a malicious query. That is, it represents the path the model is inclined to take in such a binary classification task, whether to fulfill the harmful request or to issue a refusal. As illustrated in Fig. 1, compared with SAH which targets the general alignment, our SSAH focuses on the safety alignment and has the following key differences : \n\n(1) Knowledge and reasoning ability: Safety alignment simplifies the problem by focusing specifically on models that already possess sufficient knowledge and reasoning abilities, as these models are capable of fulfilling malicious requests. This approach allows us to disregard other influencing factors and concentrate solely on the safety alignment process.",
            "score": 0.6427660339937031,
            "section_title": "SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS (SSAH)",
            "char_start_offset": 7493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2188
                },
                {
                    "start": 2191,
                    "end": 2431
                },
                {
                    "start": 2432,
                    "end": 2550
                }
            ],
            "ref_mentions": [
                {
                    "start": 1026,
                    "end": 1044,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "267616740",
            "title": "Rethinking Data Selection for Supervised Fine-Tuning",
            "text": "Large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Anil et al., 2023;Touvron et al., 2023a,b;Jiang et al., 2023) have shown remarkable capabilities in canonical natural language understanding and generation tasks in recent years. To further teach LLMs to align with humans by following human instructions and interacting with humans in user-friendly manners, supervised finetuning, or instruction tuning (Ouyang et al., 2022;Wang et al., 2023c;Taori et al., 2023;Peng et al., 2023;Chiang et al., 2023;Xu et al., 2024;OpenAI, 2022OpenAI, , 2023)), has been applied as an indispensable alignment step where LLMs are finetuned on large sets of instruction-response demonstrations either annotated by human or synthesized by power proprietary models. \n\nDespite the success of alignment through SFT, LIMA (Zhou et al., 2023) proposes the superficial alignment hypothesis that the models' knowledge and abilities are learned mostly during pretraining, and SFT is all about style learning (Gudibande et al., 2024) of formatting the response in a humanlike manner. At the same time, recent works have been designing data selection strategies to filter SFT datasets, showing finetuning LLMs with a subset of the original dataset leads to superior instruction-following capabilities compared with utilizing the whole. Two major principles for designing existing selection strategies (Chen et al., 2023a;Lu et al., 2024;Chen et al., 2023b;Bukharin and Zhao, 2023;Du et al., 2023;Wu et al., 2023;Liu et al., 2024) are based on the quality and diversity aspects of the data. However, although these two factors are highly significant for developing AI systems in the pre-LLM era, we question their relevance under the SFT scenario in the current era of LLMs, given the superficial nature of SFT.",
            "score": 0.6404423376922419,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 465,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 502,
                    "end": 522,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02386474609375
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "While extensive research has introduced new IT datasets, models, and enhancement methods (Zhang et al., 2023a), few studies examine IT's limitations. Concurrent work by Gudibande et al. (2023) shows that IT with datasets synthesized from powerful proprietary models only leads to imitating their style, not their knowledge. Similarly, Lin et al. (2023) shows that alignment only teaches style and proposes in-context learning as an alternative to IT that outperforms several tuned models. This can be attributed to our findings on LFT, where the models respond using pre-trained knowledge and do not lead to knowledge degradation like SFT. This supports the superficial alignment hypothesis by Zhou et al. (2023), positing that models gain knowledge in pre-training, with alignment shaping format used in user interactions. Finally, Kung & Peng (2023) show that models trained on simplified task definition or delusive examples achieve performance comparable to the ones trained on the original instructions. In contrast to all these works, we study the exact causes of these limitations, investigate pattern copying and hallucination from novel perspectives, and highlight the overlooked effectiveness of LFT that utilizes only pre-trained knowledge.",
            "score": 0.615982978701443,
            "section_title": "Related Work",
            "char_start_offset": 33427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1251
                }
            ],
            "ref_mentions": [
                {
                    "start": 694,
                    "end": 712,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0374755859375
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "Large Language Models (LLMs) based on the Transformer architecture have achieved state-of-the-art performance on tasks that involve instruction following, problem-solving, and reasoning (Achiam et al., 2023;Dubey et al., 2024;Vaswani et al., 2017). The standard pipeline for building LLMs powered applications involves unsupervised training of a model on a giant corpus of data to gain general language understanding capability, referred to as pre-training (Brown et al., 2020;Radford et al., 2019). The model is further improved using post-training, which involves finetuning it to excel at a particular domain or behave like a helpful chatbot. This process is also referred to as alignment. The predominant way to do this is through Supervised Finetuning (SFT) where the language model is provided with a prompt, and the model is finetuned to respond to the task (Wei et al., 2022). An additional step is Reinforcement Learning through Human Feedback (RLHF) where a model is trained using reinforcement learning to generate human-preferred responses, by being rewarded for good responses and penalized for bad responses (Ouyang et al., 2022). \n\nTo achieve the post-training goal of responding appropriately to various user queries, LLMs need to develop several task-specific capabilities, like mathematics, reasoning, utilizing knowledge, and tool use. To teach a model these capabilities, model builders collect human-annotated or synthetically generated data and finetune the model to obtain the desired behavior. Since data collection at scale is labor and cost-intensive, it is essential to understand the qualitative and quantitative value of obtaining additional post-training data. Studies like LIMA (Zhou et al., 2024) have hypothesized that post-training alignment is all about learning the style and format of the desired behavior. Specifically, it puts forward the Superficial Alignment Hypothesis, whose claims are: \n\n\u2022 C1: A model's knowledge is learned entirely during pre-training. \n\n\u2022 C3: Post-training is largely about style and doesn't does not teach a model new capabilities. \n\n\u2022 C2: A small number of examples can saturate a model's performance for a given task.",
            "score": 0.6158237460636338,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 477,
                    "end": 498,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 865,
                    "end": 883,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1122,
                    "end": 1143,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "270440422",
            "title": "ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions",
            "text": "5 Discussion and Analysis 5.1 Why Self-Alignment Works?\n\nThe experimental results presented in Section 4 demonstrate that LLMs can effectively align themselves using self-annotated data, achieving significant improvements over the vanilla model.This outcome may seem counter-intuitive since the LLMs are trained solely on labels generated by themselves without external preferences.The Superficial Alignment Hypothesis posits that a model's knowledge and capabilities are primarily acquired during pre-training, with alignment refining the model's ability to select appropriate subdistributions of formats for user interaction [Zhou et al., 2024].Given the disparity between evaluation and generation capabilities in LLMs [Li et al., 2023a], it is reasonable to suggest that LLMs can enhance their generation abilities through self-evaluation.\n\nIn this paper, we propose that self-alignment reinforces consistency within LLMs.By training on confident preference pairs, LLMs can generalize alignment information to previously incorrect preferences.To validate this hypothesis, we analyzed contradictions in preference graphs before and after self-alignment on preference data selected by ContraSolver.",
            "score": 0.6060786431843029,
            "section_title": "Method",
            "char_start_offset": 19456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 57,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 843
                },
                {
                    "start": 845,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1055908203125
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming. The authors of LiMA argue that most of the functional capabilities of a language model are already present after pretraining, and that alignment is more about conditioning the model to apply these capabilities in a user-friendly way. \n\nThe Superficial Alignment Hypothesis can also help explain phenomena where models exhibit brittleness -for example, where an LLM generates inappropriate or harmful responses in new domains or under adversarial conditions. This brittleness is attributed to the fact that alignment does not deeply alter the underlying decision-making processes of the model, but only skims the surface to adjust output behavior in specific contexts. Therefore, if an adversary finds a way to bypass these superficial alignments (e.g., via jailbreaking), the model's underlying pretrained knowledge and capabilities may still enable it to produce harmful or misaligned responses. \n\nPreprint. \n\nRelevance to Superficial Safety Alignment Hypothesis. While SAH deals with general alignment (i.e., ensuring that a model follows general user instructions), SSAH is specifically focused on ensuring that a model safely interacts with users, especially when faced with harmful or malicious queries. The key parallels between SAH and SSAH include: \n\n(1) Pretrained Knowledge and Safety Concerns: Just as SAH assumes that knowledge and capabilities are largely acquired during pretraining, SSAH assumes that a model's ability to execute harmful actions (e.g., generating unsafe or unethical content) also stems from pretraining. Safety alignment, like general alignment, does not aim to teach the model new facts or capabilities, but rather to guide its reasoning pathways in a safe direction. \n\n(2) Binary Classification in SSAH: While SAH suggests that general alignment helps models choose the correct output subdistribution, SSAH posits that safety alignment simplifies this further by focusing on a binary classification task: either fulfill a request (if safe) or refuse it (if unsafe).",
            "score": 0.6050972170156432,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 33716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.383056640625
        },
        {
            "corpus_id": "261049152",
            "title": "Instruction Tuning for Large Language Models: A Survey",
            "text": "Despite the impressive improvements in the performance of instruction tuning, there lacks clarity about the specific knowledge that models acquire through instruction tuning, raising questions about: Does instruction tuning just learn Pattern Copying? or How exactly does the alignment tuning transform a base LLM? \n\nTo answer these questions, Kung and Peng (2023) delves into the analysis of how models make use of instructions during SFT by comparing the tuning when provided with altered instructions versus the original instructions. \n\nSpecifically, Kung and Peng (2023) creates simplified task definitions that remove all semantic components, leaving only the output information. In addition, Kung and Peng (2023) also incorporates delusive examples that contain incorrect input-output mapping. Surprisingly, the experiments show that models trained on these simplified task definitions or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Moreover, the paper also introduces a baseline for the classification task with zero-shot, which achieves similar performance to SFT in lowresource settings. \n\nSimilar to the findings of Kung and Peng (2023), several subsequent studies (Zhou et al., 2023a;Lin et al., 2023a) reached the same conclusion: the observed performance improvements in current SFT models are often due to superficial alignment. This means the models excel at recognizing superficial alignment, such as mastering output formats and making educated guesses, rather than truly understanding and learning the underlying tasks.",
            "score": 0.5995545774783833,
            "section_title": "Superficial Alignment",
            "char_start_offset": 85366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1609
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06732177734375
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "How does post-training model performance scale with dataset size? \n\nThe primary implication of the Superficial Alignment Hypothesis is that pre-training is all that matters, and with a rather small set of examples, we can align a model during post-training. However, this is a broad claim that is supported by a limited set of chatbot-style experiments. Post-training a model involves instruction following, problem-solving, and coding, and unlike chatbot-style dialogue whose evaluation is subjective and comparative, these capabilities can be judged using standardized benchmarks. For researchers and model builders who aim to improve performance on such tasks, it is important to understand performance scaling on such benchmarks with increasing fine-tuning data.",
            "score": 0.5949270100030654,
            "section_title": "Post-training Data Scaling",
            "char_start_offset": 5324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 766
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145263671875
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "In each of these cases, the process of estimating alignment occurs largely inside the user's head, requiring significant cognitive effort to compare model concepts against their domain abstractions. Moreover, by relying on individuals' mental abstractions, existing approaches limit alignment assessment to users with deep domain-specific knowledge, such as medical specialists [15,72] or chess Grandmasters [127]. \n\nTo scaffold the process of assessing model alignment, we introduce abstraction alignment, a methodology to measure the agreement between a model's learned abstractions and an explicit human representation of the modeled domain. Abstraction alignment is a form of representational alignment [136] that compares model outputs (a proxy for its internal representations) against codified human abstractions (a proxy for our internal representations). Abstraction alignment begins with a human abstraction graph -an agreed upon representation of formal human knowledge containing a set of pertinent concepts spanning levels of abstraction, such as a lexical graph [90] or medical hierarchy [157]. It then measures alignment by evaluating how well the abstraction graph accounts for a model's decision uncertainty. Through this process, model output probabilities (i.e., confidence in each class or token) are mapped to concepts in the human abstraction graph, and the probabilities of sibling concepts (e.g., guitarist and singer) are summed and propagated to shared ancestors (e.g., musician and artist). The result is the model's fitted abstraction graph, representing its decision making process across concepts at many levels of abstraction. \n\nTo aggregate abstraction alignment over many model decisions, we define three metrics. Abstraction match measures how much of the model's confusion is mitigated by moving up a level of abstraction, concept co-confusion tests how often the model confuses concepts, and subgraph preference quantifies which abstractions the model prefers. By integrating the abstraction alignment metrics into an interactive interface, we enable users, ranging from computer scientists to medical domain experts, to ask and answer alignment hypotheses, such as which human concepts the model has learned and what recurring misalignments the model makes. \n\nWe demonstrate how abstraction alignment facilitates alignment analysis through case studies with expert users interpreting an image classification model, benchmarking generative language models, and auditing a clinical ML dataset.",
            "score": 0.5836702153368841,
            "section_title": "Introduction",
            "char_start_offset": 3142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2294
                },
                {
                    "start": 2297,
                    "end": 2528
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "247614116"
                },
                {
                    "start": 382,
                    "end": 385,
                    "matchedPaperCorpusId": "51737170"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "1671874"
                },
                {
                    "start": 1102,
                    "end": 1107,
                    "matchedPaperCorpusId": "31594479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004962921142578125
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
            "score": 0.5794555816255665,
            "section_title": "Preprint",
            "char_start_offset": 993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Another issue is that learning to master knowledge utilization and format alignment in the instruction tuning and alignment stages might lead to suboptimal performance, due to the fact that the two goals can be divergent in model optimization (Ren et al., 2024).\n\nConsidering the above issues, this paper explores a new domain adaptation approach that only uses raw domain-specific corpus and general instruction or alignment data.Our hypothesis is that the knowledge utilization capacity can be essentially learned from general instruction or alignment data, which has been also evidenced by prior studies (Ouyang et al., 2022b;Rafailov et al., 2023b).In this way, we can remove the tedious instruction synthesis step from the training pipeline, since it is much easier to obtain general or mixed-domain instruction data from open resources.Another important attempt is to enhance knowledge learning by jointly attaining both memorization and utilization of knowledge.To implement this idea, we schedule all the instruction and alignment data at the pre-training stage (with a suitable format), then only reuse a minor proportion of instruction and alignment data for the instruction-tuning and alignment stages to achieve format alignment.We compare our rescheduled process with the traditional domain adaptation in Figure 1.Specially, our approach for domain adaptation of LLMs consists of two main stages, including domain knowledge learning and general format alignment.In the first stage, we conduct knowledge mixture continual pre-training to integrate both knowledge memorization and utilization.The memorization of new knowledge can be facilitated by taking into account how this knowledge will be utilized.In the second stage, based on the knowledge and capabilities that are already acquired during pre-training, we perform instruction tuning and alignment in an efficient manner to achieve format alignment.For unified training, we convert raw domain documents, instruction tuning data, and alignment data into a unified format for conducting knowledge mixture continual pre-training (Mix-CPT).To avoid catastrophic forgetting in continual pre-training, we propose Logit Swap Self-Distillation (LSSD), which exchanges the predicted top-1 token logit with the logit of the ground-truth token, serving as the surrogate target.In this way, LSSD maintains most probabilities of the original distribution of LLMs, avoiding dramatic model update and thereby preserving original capabilities.",
            "score": 0.5734827531000852,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 264,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 842
                },
                {
                    "start": 842,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 2106
                },
                {
                    "start": 2106,
                    "end": 2336
                },
                {
                    "start": 2336,
                    "end": 2497
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 629,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 629,
                    "end": 652,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020427703857421875
        },
        {
            "corpus_id": "261049152",
            "title": "Instruction Tuning for Large Language Models: A Survey",
            "text": "+0.8 on MMLU, C-Eval, GSM8K and BBH, respectively. 4.10 LIMA LIMA (65B) (Zhou et al., 2023a) is a large language model trained by fine-tuning LLaMA (65B) (Touvron et al., 2023a) on an instruction dataset, which is constructed based on the proposed superficial alignment hypothesis. \n\nThe superficial alignment hypothesis refers to the idea that the knowledge and capabilities of a model are almost acquired in the pre-training stage, while the alignment training (e.g., instruction tuning) teaches models to generate responses under user-preferred formalizations. Based on the superficial alignment hypothesis, the authors claimed that large language models can generate user-satisfied responses by fine-tuning it on a small fraction of instruction data. Therefore, the authors built instruction train/valid/test sets to verify this hypothesis. \n\nEvaluations are conducted on the constructed test set. For human evaluations, LIMA outperforms InstructGPT and Alpaca by 17% and 19%, respectively. \n\nAdditionally, LIMA achieves comparable results to BARD7 , Cladue8 , and GPT-4. For automatic evaluation, which is conducted by asking GPT-4 to rate responses and a higher rate score denotes better performance, LIMA outperforms InstructGPT and Alpaca by 20% and 36%, respectively, achieving comparable results to BARD, while underperforming Claude and GPT-4. Experimental results verify the proposed superficial alignment hypothesis.",
            "score": 0.5700346946796423,
            "section_title": "ChatGLM2",
            "char_start_offset": 43638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1429
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "271051005",
            "title": "Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning",
            "text": "Fine-tuning language models with instructional datasets has emerged as a powerful technique, offering notable improvements in model performance and alignment with human preferences and safety.By exploring a diverse array of instructional tasks, [Wei et al., 2021] demonstrated a significant enhancement in zero-shot performance on unseen tasks through fine-tuning.Building on this, [Chung et al., 2024] showed that scaling both the number of tasks and the model size can lead to substantial performance gains across different model architectures.[Peng et al., 2023] further advanced this field by leveraging large language models (LLMs) to generate high-quality instruction-following data, resulting in improved zero-shot performance on new tasks.\n\nA recent study [Zhou et al., 2023] introduces the Superficial Alignment Hypothesis, which posits that the bulk of knowledge in LLMs is acquired during pretraining.It further suggests that min-imal fine-tuning data is sufficient to align these models with human preferences.The study demonstrates a noteworthy enhancement in LLM performance with just 1,000 high-quality instruction data points.Subsequently, a plethora of research endeavors have concentrated on refining dataset quality through diverse filtering methodologies for general instruction following [Xu et al., 2023b, Chen et al., 2024, Liu et al., 2023a].",
            "score": 0.5676958703924166,
            "section_title": "Instructional Fine-tuning",
            "char_start_offset": 6371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 747
                },
                {
                    "start": 749,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 402,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048126220703125
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Instruction Tuning and Alignment.Instruction tuning (also known as supervised fine-tuning) employs human-annotated instructions (Sanh et al., 2022;Mishra et al., 2022;K\u00f6pf et al., 2023;Sun et al., 2023) or synthetic instructions by proprietary models (Taori et al., 2023;Chiang et al., 2023;Wang et al., 2023b) to fine-tune LLMs.Besides, alignment with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022a) or direct preference optimization (DPO) (Rafailov et al., 2023a) aims to align LLMs with human preference.Both instruction tuning and alignment are able to elicit knowledge from LLMs and improve their capabilities to solve downstream tasks.\n\nRecent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks.Inspired by these studies, we propose to expose knowledge memorization and capability elicitation from instruction tuning and alignment.Unlike these studies which typically focused on instruction tuning or alignment, we differ in that we unify the three stages of training LLMs (i.e., continual pre-training, instruction tuning, and alignment) and conduct a knowledge mixture pre-training to mainly focus on learning new domain knowledge while maintaining general knowledge.",
            "score": 0.5676177718771651,
            "section_title": "RELATED WORK",
            "char_start_offset": 32983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 33,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 532
                },
                {
                    "start": 532,
                    "end": 666
                },
                {
                    "start": 668,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1893
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 167,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 167,
                    "end": 185,
                    "matchedPaperCorpusId": "258179434"
                },
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 680,
                    "end": 699,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1065,
                    "end": 1084,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004848480224609375
        },
        {
            "corpus_id": "269983424",
            "title": "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction",
            "text": "Prior studies tend to attribute the alignment tax phenomenon to the low-quality samples within the instruction-following corpus (Chen et al., 2023;Cao et al., 2023), or the knowledge forgetting during the SFT process (Dou et al., 2023;Ren et al., 2024).However, our pilot study in Section 3 reveals that the quality issue and the catastrophic forgetting of pre-training knowledge are probably not the main cause of the alignment tax since the decline can be observed across corpora with varied arXiv:2405.13432v1[cs.CL] 22 May 2024 sizes and quality.\n\nBy analyzing the trend of loss descent during the SFT process, we alternatively posit that the data biases fitted on the instruction data are probably one of the major causes behind it.Specifically, during the tuning process, LLMs fit on dataset biases while acquiring instruction-following ability.In the beginning, the acquisition of generalizable ability predominates so the performance on knowledge and reasoning benchmarks improves.However, during the tuning process, the learning of generalization quickly stagnates and the model tends to acquire more data biases instead, which harms the parametric knowledge of LLM and leads to a decline in related benchmarks.\n\nWe propose a frustratingly simple DTM (Disperse-Then-Merge) framework composed of three steps:\n\n(1) we initially distribute the instruction-following data into several clusters and then (2) perform instruction tuning on each cluster of data to obtain a series of sub-models assimilating different data biases; (3) finally we merge the sub-models trained on each cluster into a single one in the weight space, such that the data bias of each sub-model is mitigated at fusion.Importantly, DTM ensures the reduction of alignment tax when instruction tuning with almost no extra cost at both training and inference.\n\nTo empirically verify the efficacy of the DTM framework, we conduct extensive experiments and evaluations across 9 benchmarks involving math reasoning, world knowledge, and code generation.The experiment results exhibit that DTM outperforms both (1) data selection methods that filter out low-quality samples (Dou et al., 2023); and",
            "score": 0.5621340313611585,
            "section_title": "Introduction",
            "char_start_offset": 1477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1220
                },
                {
                    "start": 1222,
                    "end": 1316
                },
                {
                    "start": 1318,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1833
                },
                {
                    "start": 1835,
                    "end": 2024
                },
                {
                    "start": 2024,
                    "end": 2167
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020275115966796875
        },
        {
            "corpus_id": "270123077",
            "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
            "text": "The large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities (Bubeck et al., 2023). However, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models. Traditional alignment methods include Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Wei et al., 2021;Chung et al., 2022) or preference data (DPO, KTO, IPO, and ORPO, Rafailov et al., 2023;Ethayarajh et al., 2024;Azar et al., 2024;Hong et al., 2024), and reinforcement learning (RLHF and RLAIF, Ouyang et al., 2022;Bai et al., 2022). These approaches typically involve multiple rounds of refinement of the instructed model (Touvron et al., 2023), i.e. high computational cost, and need a large amount of annotated data like human preferences, which might be difficult to collect. However, a line of work (Taori et al., 2023;Chen et al., 2023;Zhou et al., 2023;Lee et al., 2023;Zhao et al., 2024;Kaur et al., 2024) has suggested that IFT on a small amount of high quality instructionfollowing examples, even only 1000, can be sufficient to match the performance of more complex alignment mechanisms. In particular, Zhou et al. (2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023). Using such small instruction datasets for IFT has the clear advantage of significantly reducing the cost of model alignment.",
            "score": 0.5602551523664163,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1626
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 949,
                    "end": 967,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 984,
                    "end": 1002,
                    "matchedPaperCorpusId": "267522812"
                },
                {
                    "start": 1221,
                    "end": 1239,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04620361328125
        },
        {
            "corpus_id": "267740713",
            "title": "Multi-modal preference alignment remedies regression of visual instruction tuning on language model",
            "text": "The superficial alignment hypothesis states that a model's knowledge capabilities are largely acquired during its initial pre-training stage (Zhou et al., 2023). A corollary of this hypothesis is that alignment tuning refines the model output generation with a preferred response format rather than knowledge acquisition. As a result, models can be effectively realigned post-visual instruction using a relatively small set of examples (Kirstain et al., 2022). This principle applies to MLLMs as well, which acquire multi-modal knowledge representation via visual instruction tuning (Liu et al.,  2023b). However, existing work mixed large-scale text instruction data (518K out of 1.23 million in the case of mPlug-OWL 2 and 40K in the case of LLaVA-1.5). We hypothesize that the data inefficiency above is attributed to the underlying alignment strategy and demonstrate that one would need only a small alignment dataset so long as a proper alignment strategy such as DPO is utilized. \n\nAs suggested by Table 4, Direct Preference Optimization (DPO) emerges as a computationally efficient solution for enhancing model performance in the mixed-modal alignment space. Unlike the mixing text instruction as described above or LLaVA-RLHF, which used a large 82K dataset and complex training pipeline involving reward modeling and PPO, DPO achieves significant improvements in language capabilities with a smaller dataset and one-stop training setup. A notable advantage of DPO is its minimal alignment tax, which curtails the degradation of existing knowledge, as evidenced by its performance on benchmarks like MM-Bench, where DPO shows minimal impact. This method not only enables effective alignment of multi-modal models post-visual instruction tuning but also ensures the preservation of model performance. Our methodology exhibits notable proficiency in value alignment and data efficiency, yet it is imperative to acknowledge certain limitations and potential risks. One key consideration is the scalability of our approach. While our data scaling analysis suggests significant improvements up to a 6K preference dataset, the full extent of scalability beyond this threshold remains unexplored.",
            "score": 0.5576134427451603,
            "section_title": "Multi-modal preference alignment as a data-efficient remedy to instruction tuning capabilities",
            "char_start_offset": 26950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 459,
                    "matchedPaperCorpusId": "238583118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1668701171875
        },
        {
            "corpus_id": "267740204",
            "title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation",
            "text": "Prior research (Zhou et al., 2023) indicates that LLMs primarily acquire their knowledge during the pre-training phase, while the alignment phase only teaches LLMs about the specific subdistribution of interactions with users. In this work, we mainly focus on the alignment phase, while it remains unclear what abstraction knowledge is captured by LLMs during pre-training. Following previous works of knowledge probing (Hou et al., 2023;Sun et al., 2023), future research can probe recent LLMs, like Llama2, to better understand this question and explore how to equip LLMs with more abstraction knowledge during pre-training. \n\nMeanwhile, instruction tuning only elicits the existing knowledge of pre-trained LLMs. We leave for future works about equipping LLMs with new abstraction knowledge through other techniques, like knowledge editing (Wang et al., 2023a;Zhang et al., 2024;Hase et al., 2023), retrieval augmented generation (Lewis et al., 2020;Gao et al., 2023b;Wu et al., 2024), event-centric knowledge (Wang et al., 2023c(Wang et al., , 2022c;;Gao et al., 2023a;Do et al., 2024), intention detection (Wu et al., 2023), and knowledge population (Shen et al., 2023). Meanwhile, we can extend our abstraction knowledge to multimodal, like exploring knowledge from given images (Shen et al., 2024;Cui et al., 2024).",
            "score": 0.5543551906286184,
            "section_title": "Limitations",
            "char_start_offset": 26785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1322
                }
            ],
            "ref_mentions": [
                {
                    "start": 933,
                    "end": 953,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1013,
                    "end": 1032,
                    "matchedPaperCorpusId": "258564290"
                },
                {
                    "start": 1032,
                    "end": 1055,
                    "matchedPaperCorpusId": "252873038"
                },
                {
                    "start": 1155,
                    "end": 1174,
                    "matchedPaperCorpusId": "258959081"
                },
                {
                    "start": 1304,
                    "end": 1321,
                    "matchedPaperCorpusId": "264590421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003429412841796875
        },
        {
            "corpus_id": "276768896",
            "title": "In-context Learning vs. Instruction Tuning: The Case of Small and Multilingual Language Models",
            "text": "Instruction tuning has been the dominant paradigm to adapt LLMs for downstream tasks. The standard pipeline involves SFT on instruction-response datasets (Wei et al., 2021), often complemented with human preference alignment via methods such as RLHF (Ouyang et al., 2022) or DPO (Rafailov et al., 2023). The effectiveness of these approaches has been demonstrated across a variety of domains, leading to substantial improvements in instruction-following capabilities. However, recent studies have suggested that the benefits of instruction tuning might be more superficial that previously thought. Zhou et al. (2024) proposed the Superficial Alignment Hypothesis, arguing that fine-tuning primarily adjust response formatting and style rather than imbuing the models with new capabilities. Further, Lin et al. (2024) found that instruction tuning predominantly shift token distributions related to style and safety disclaimers, with minimal impact on core knowledge retrieval. In this line of work, Hewitt et al. (2024) showed that a base model can exhibit instruction-following behaviour to some extent by applying straightfor-ward rules and adjusting token distributions in a targeted manner. This raises critical questions about whether costly fine-tuning is always necessary for strong instruction-following performance. \n\nAlternative alignment strategies have been explored, particularly in the context of ICL. Brown et al. (2020) first demonstrated that pretrained models can exhibit strong instruction-following behaviour when provided with appropriate examples in their prompt. As noted above, Lin et al. ( 2024) expanded the use of ICL for instruction followin with URIAL, a method which aligns base LLMs using just three stylistic in-context examples. While URIAL improves instruction adherence significantly, Zhao et al. (2025) showed that it still lags behind fine-tuned models, particularly in multi-turn interactions. They further highlighted that decoding parameters, such as temperature and repetition penalties, significantly influence ICL performance, suggesting that proper hyperparameter tuning is crucial for optimal ICL alignment. Similarly, Han (2023) found that the benefits of ICL are highly sensitive to the quality of in-context examples, with carefully curated demonstrations yielding much stronger alignment that randomly sampled ones.",
            "score": 0.5530206659934156,
            "section_title": "Related Work",
            "char_start_offset": 3954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 172,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 279,
                    "end": 302,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 598,
                    "end": 616,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1820,
                    "end": 1838,
                    "matchedPaperCorpusId": "270123077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06744384765625
        },
        {
            "corpus_id": "261530292",
            "title": "Explainability for Large Language Models: A Survey",
            "text": "Assistant models are typically trained in two stages. First, they undergo unsupervised pre-training on large amounts of raw text to learn general linguistic representations. This pre-training stage allows the models to acquire general language knowledge. Second, the models go through alignment fine-tuning via supervised and reinforcement learning. This aligns the models with specific end tasks and user preferences. Explainability research on these models focuses on determining whether their knowledge comes predominantly from the initial pre-training stage, wherein they acquire general language abilities, or from the subsequent alignment fine-tuning stage, wherein they are tailored to specific tasks and preferences. Understanding the source of the models' knowledge provides insight into how to improve and interpret their performance. \n\nA recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches. Complex fine-tuning and reinforcement learning techniques may be less crucial than previously believed. On the other hand, this study also indicates that data quality is more important compared to data quantity during instruction fine-tuning. Furthermore, Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models.",
            "score": 0.5494044853176012,
            "section_title": "Explaining the Role of Fine-tuning",
            "char_start_offset": 56400,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2285
                },
                {
                    "start": 2286,
                    "end": 2479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0060272216796875
        },
        {
            "corpus_id": "267751255",
            "title": "Learning to Edit: Aligning LLMs with Knowledge Editing",
            "text": "Knowledge Editing Prior knowledge editing studies resort to auxiliary models for efficient updating and refining of LLMs. For example, SERAC (Mitchell et al., 2022b) builds a distinct counterfact model without changing the original LLM and employs a scope classifier to determine whether to use the counterfact model to answer the question. KE (De Cao et al., 2021) and MEND (Mitchell et al., 2022a) leverage a hypernetwork to predict the weight update of the LLM. While these methods have shown some promising results, they fail to utilize the inherent formidable capabilities of LLMs. More recent works such as KN (Dai et al., 2022), ROME (Meng et al., 2022), and MEMIT (Meng et al., 2023) adopt interpretability techniques to identify parameters corresponding to specific knowledge and update them to alter LLM's knowledge. Nevertheless, the correlation between localization and editing efficacy has been questioned (Hase et al., 2023). Diverging from these methodologies, we explicitly teach LLMs how to apply updated knowledge rather than mere memorization, which taps into the full potential of LLMs, fostering a more dynamic and effective knowledge editing process. \n\nLLM Alignment LLM alignment (Gabriel, 2020), which aims to calibrate LLMs' behaviors with human values and preferences, is essential for their application in real-world scenarios. A prominent technique in this area is supervised finetuning (SFT) (Wei et al., 2022;Mishra et al., 2022), which involves fine-tuning powerful LLMs using datasets composed of natural language instructions. Notably, SFT is instrumental in improving LLMs' understanding and adherence to human instructions, laying the groundwork for many subsequent alignment strategies such as reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Rafailov et al., 2023). Thus, plenty of efforts have focused on applying SFT for alignment using either human-annotated or synthetic data (Wei et al., 2022;Wang et al., 2023c;Jiang et al., 2023;Xu et al., 2023;Wang et al., 2023d).",
            "score": 0.5483051118557413,
            "section_title": "Related Work",
            "char_start_offset": 23159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 165,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 344,
                    "end": 365,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 375,
                    "end": 398,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 641,
                    "end": 660,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 672,
                    "end": 691,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 1780,
                    "end": 1801,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1801,
                    "end": 1823,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1976,
                    "end": 1995,
                    "matchedPaperCorpusId": "258833333"
                },
                {
                    "start": 2011,
                    "end": 2030,
                    "matchedPaperCorpusId": "258298159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00243377685546875
        },
        {
            "corpus_id": "278165260",
            "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics",
            "text": "(b) Direct-SFT: A standard supervised fine-tuning (SFT) approach where the model is trained only on the final task-specific outputs (category labels). While category definitions are included in the instruction, the training data omits any reasoning traces or intermediate steps. (c) ReasonImplicit-SFT: In this setting, the model is trained on both reasoning traces and final category labels. However, task-specific definitions are excluded from the instructions. This design encourages the model to rely on its internal parametric knowledge and domain understanding during reasoning. (d) ReasonExplicit-SFT: The model is trained using both step-by-step reasoning traces and final outputs, with the instructions explicitly prompting for structured reasoning. Additionally, detailed task definitions are included in the prompt to guide the model's predictions. (e) EWRA (Extreme Weather Reasoning-Aware Alignment): Our proposed alignment strategy described in section 3.3. It employs a twostage curriculum. First, the model is trained on implicitly prompted data (without definitions) to encourage generalizable and contextaware reasoning. This is followed by fine-tuning on explicitly guided data (with structured definitions and reasoning), promoting task adherence and instruction following.",
            "score": 0.5480900541429367,
            "section_title": "Model Description",
            "char_start_offset": 28416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1293
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013990402221679688
        },
        {
            "corpus_id": "273532094",
            "title": "Understanding Layer Significance in LLM Alignment",
            "text": "LLM Alignment. Pretrained language models encode general-purpose representations, enabling transfer across diverse tasks (Qiu et al., 2024;Jiang et al., 2024;Nijkamp et al., 2022). Alignment methods like instruction tuning (Zhang et al., 2023c;Sun et al., 2023;Muennighoff et al., 2023) and preference learning (Hejna et al., 2023;Guan et al., 2022;Rafailov et al., 2024;Song et al., 2024;Li et al., 2024a) adapt these models to specific objectives. Recent studies have explored alignment mechanisms. LIMA (Zhou et al., 2023) showed that fine-tuning on small datasets (e.g., 1,000 examples) shapes behavior without adding new knowledge, a finding echoed by others (Chen et al., 2023;Lee et al., 2023;Gudibande et al., 2023). Duan et al. (2023) connected instruction tuning to in-context learning via hidden state analysis, while URIAL (Lin et al., 2023) revealed that alignment mainly modifies stylistic tokens, preserving knowledge-centric ones. These insights suggest alignment imparts narrow, targeted adjustments. Our work builds on this by identifying the specific layers most critical for alignment, offering a more fine-grained understanding of how adaptation occurs. \n\nParameter Efficient Fine-Tuning (PEFT). Fine-tuning large language models with billions or trillions of parameters is computationally expensive (Brown et al., 2020;Fedus et al., 2022). Parameter-efficient fine-tuning (PEFT) methods address this by updating specific components (Zaken et al., 2021;Zhao et al., 2020;Ansell et al., 2021;Guo et al., 2020) or using soft prompts (Lester et al., 2021;Li & Liang, 2021;Asai et al., 2022).",
            "score": 0.5459831225297098,
            "section_title": "Related Works",
            "char_start_offset": 5272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 371,
                    "end": 389,
                    "matchedPaperCorpusId": "271719990"
                },
                {
                    "start": 1321,
                    "end": 1341,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1341,
                    "end": 1360,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035648345947265625
        },
        {
            "corpus_id": "261243909",
            "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
            "text": "Alignment data involves data with inputs and corresponding outputs, providing direct supervision for fine-tuning. While the model's foundational knowledge and capabilities stem from pretraining, its true efficacy and user interactions are refined through alignment data, instructing it in the most effective subdistribution of formats for engaging with users [106]. Two primary approaches to leverage alignment data are reinforcement learning from human feedback (RLHF) [3,8] and supervised fine-tuning [106]. \n\n10. https://huggingface.co/datasets/conceptofmind/flan2021_ submix_original.",
            "score": 0.5417587182245341,
            "section_title": "Alignment Data",
            "char_start_offset": 36778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 588
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 364,
                    "matchedPaperCorpusId": "259203998"
                },
                {
                    "start": 473,
                    "end": 475,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 503,
                    "end": 508,
                    "matchedPaperCorpusId": "259203998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0038700103759765625
        },
        {
            "corpus_id": "270560471",
            "title": "SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models",
            "text": "Instruction-following Large Language Models (LLMs), such as GPT-3.5 and GPT-4 (Achiam et al., 2023), have transformed natural language processing by demonstrating remarkable generalization across a wide range of tasks (Brown et al., 2020;Chung et al., 2022;Ouyang et al., 2022). These models are typically trained through several stages: an initial phase of unsupervised pre-training on a vast corpus of text, followed by post-training stages, which include supervised fine-tuning (SFT) on a smaller dataset of instruction-response pairs and reinforcement learning (Bai et al., 2022). \n\nRecent studies, such as AlpaGasus (Chen et al., 2024) and LIMA (Zhou et al., 2024), demonstrate that carefully curated, smaller datasets can outperform larger ones in improving LLM SFT performance. AlpaGasus finds that smaller datasets with higher quality scores, rated by GPT-4 for helpfulness or correctness, outperform significantly larger ones when used to fine-tune high-capacity LLMs. The Superficial Alignment Hypothesis, proposed in LIMA, suggests that pre-trained language models already possess the necessary knowledge, and the primary goal of fine-tuning is to guide the model toward adopting specific response styles, thus not requiring large amounts of data. LIMA achieves notable performance with only 1,000 high-quality instruction-response pairs, optimized for consistent style by human experts. However, this hypothesis raises three open questions: (i) What key elements constitute response styles that impact LLM SFT? (ii) How do data quality (i.e., helpfulness, correctness) relate to style consistency in influencing efficient SFT? (iii) Can we develop an automatic method that measures stylistic elements to curate smaller, stylistically consistent datasets for more efficient SFT at a lower cost, without relying on human experts? \n\nText style is shaped by consistent choices across various linguistic elements (Kang & Hovy, 2021;Karlgren, 2004), such as lexical, syntactic, and semantic features (DiMarco & Hirst, 1993).",
            "score": 0.5408788808855034,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050994873046875
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "Discussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak attacks, we do not propose a specific solution to address these issues in this work. If these issues could be resolved within the framework of our theory, the term \"Superficial\" in \"Superficial Safety Alignment Hypothesis\" may no longer be necessary. Interestingly, recent research provides some supporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that advanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A systematic, multi-layered approach, extending beyond the model itself, may be required to effectively defend against sophisticated adversarial threats. \n\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact of the alignment method SFT. Due to resource limitations, we have not yet tested this approach on other alignment methods like PPO or DPO. \n\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and addresses the three key questions: How does safety alignment affect model behavior? Why are safety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these questions, we were able to demonstrate that safety alignment can be a straightforward process, rather than a myth. \n\nA APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS \n\nIn this section, we provide additional technical details and clarifications to supplement the experiments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These details help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hypothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology behind model configuration, fine-tuning, and evaluation. This appendix includes further discussion on how general instruction-following models and safety-aligned models were fine-tuned and assessed to probe their reasoning directions when faced with malicious queries, and the results of these assessments are presented in detail. \n\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA. \n\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation.",
            "score": 0.5407700873546697,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 29361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2152
                },
                {
                    "start": 2155,
                    "end": 2200
                },
                {
                    "start": 2203,
                    "end": 2477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2310791015625
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "Conclusions: LLM post-training is a complex endeavor that involves improvements to instruction following, stylistic formatting, reasoning abilities, and general alignment to human preferences. LLMs can imitate the required style with \"superficial\" finetuning using a handful of examples, leading to the Superficial Alignment Hypothesis. However, a solely stylistic evaluation fails to characterize the many aspects of reasoning and task-specific capabilities that are key goals of finetuning. In fact, taskspecific skills & reasoning significantly improve after post-training with more examples compared to the pre-trained model. These improvements closely follow a power law in our experiments with the number of finetuning examples across multiple model families and sizes. We also see that these improvements are driven by the model's reasoning ability during generation, and are not limited to the model's alignment to formatting or style. In addition, we see that the win rate against other models can be a misleading metric to measure tasks that require complex reasoning, signaling the need for holistic evaluation programs leveraging standardized, objective benchmarks, in addition to measurement of alignment to human preferences. \n\nWe also observe that good post-training can help LLMs overcome problems associated with knowledge cutoff, by enabling them to better utilize knowledge from beyond the pre-training corpus either via further finetuning or RAG. These results put together highlight the qualitative and quantitative characteristics of post-training, and the role of data scaling in this.",
            "score": 0.538851387356765,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 21311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1982421875
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Large Language Models (LLMs) (Zhao et al., 2023) have revolutionized the field of natural language processing (NLP) (Brown et al., 2020;OpenAI, 2023), showing exceptional capabilities such as instruction following (Ouyang et al., 2022a;Taori et al., 2023) and complex reasoning (Wei et al., 2022;Wang et al., 2023a).However, due to their limited exposure to relevant data, such general LLMs still considerably lag behind in specific domains requiring professional knowledge.This situation has necessitated the effective adaptation of general-purpose LLMs to specific domains (e.g., mathematics and code), called domain adaptation of LLMs (Guo & Yu, 2022).\n\nIn essence, tailoring general LLMs to specific domains requires adaptation in two main aspects, namely knowledge learning (acquiring and leveraging the necessary domain knowledge) and format alignment (responding to the user in an expected output form) (Jiang et al., 2024;Zhou et al., 2023).Specially, knowledge learning can be further fulfilled via knowledge memorization and utilization.In practice, domain adaptation of LLMs typically involves three consecutive stages (Rozi\u00e8re et al., 2023;Azerbayev et al., 2023), i.e., pre-training, instruction tuning, and alignment, where the first stage is primarily aimed at knowledge memorization and the other two stages are mainly focused on knowledge utilization and format alignment.However, at the pre-training stage, knowledge memorization based on raw domain-specific corpus would be somehow inefficient without eliciting the acquired knowledge according to task goals (Jiang et al., 2024).Despite that some studies incorporate instruction data for pre-training, they often rely on proprietary models to synthesize high-quality instructions at scale (Cheng et al., 2024;Wang et al., 2024), which may not be that easy without extensive fine-tuning experiences.Another issue is that learning to master knowledge utilization and format alignment in the instruction tuning and alignment stages might lead to suboptimal performance, due to the fact that the two goals can be divergent in model optimization (Ren et al., 2024).",
            "score": 0.5366835874953642,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 474
                },
                {
                    "start": 474,
                    "end": 655
                },
                {
                    "start": 657,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1868
                },
                {
                    "start": 1868,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 296,
                    "end": 315,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 930,
                    "end": 948,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0023326873779296875
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "While interpretability methods identify a model's learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models' have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model's behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.",
            "score": 0.5301764382357241,
            "section_title": "Abstract",
            "char_start_offset": 11,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1111
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00806427001953125
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Subsequently, we assess the performance of final chat LLMs after performing format alignment with selectively chosen training samples for instruction tuning and alignment.We want to examine the instruction following capabilities of these LLMs using comprehensive benchmarks in a zero-shot scenario.Accordingly, we assess both domain-specific tasks (i.e., factual question answering (Wiki), math reasoning (Math), and code reasoning (Code)) and more general tasks (i.e., Reading Comprehension (RC), Commonsense Reasoning (CR), Examination (EX), and Instruction Following (IF)).\n\nWe show the final results in Table 2.\n\nFirst, the traditional domain adaptation method (i.e., CSD consisting of CPT, SFT, and DPO), faces challenges in simultaneously enhancing domains-specific capabilities while preserving general capabilities, in contrast to open-source chat models that do not utilize domain-specific raw data.For .This phenomenon can also be observed in the other two LLMs, which indicates that conventional domain adaptation methods of performing continual pre-training on raw domain data may cause catastrophic forgetting and merely focus on knowledge memorization without considering how to utilize the learned knowledge, which might suffer from the memorization trap.\n\nSecond, our proposed Mix-CPT method can simultaneously improve the performance of the target domains and the general capability.The main reasons are two fold.On one hand, with the constraint of logit swap self-knowledge distillation during continual pre-training, the LLM can effectively memorize the raw domain data while maintaining its originally learned knowledge in the previous pre-training stage.On the other hand, by mixing the raw domain data with the general instruction and alignment data (removing any templates), the model can learn the general knowledge utilization capability and transfer this capability to utilize the raw domain data.In this way, the model can perform efficient format alignment with only a few formatted samples to better utilize both target domain knowledge and other general knowledge.\n\nFinally, with the same instruction data and alignment data, our method can successfully improve the performance on the target domain while maintaining the general capability compared to the traditional continual pre-training augmented method based on the raw domain data.",
            "score": 0.5285005261609867,
            "section_title": "RESULTS OF CHAT LLM",
            "char_start_offset": 25163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 576
                },
                {
                    "start": 578,
                    "end": 615
                },
                {
                    "start": 617,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1270
                },
                {
                    "start": 1272,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2094
                },
                {
                    "start": 2096,
                    "end": 2367
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022258758544921875
        },
        {
            "corpus_id": "261076203",
            "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models",
            "text": "Moreover, they can be augmented by a few-shot or chain-of-thought manner (Bai et al., 2022b). This section briefly introduces four classes of alignment algorithms to answer the other key question, i.e. 'How to align big models with a given target'. Since this paper focuses on discussing the alignment goals of big models, more details can be referred to (Wang et al., 2023). \n\nIn-context Learning Since big models have acquired substantial knowledge and capabilities (Brown et al., 2020;OpenAI, 2023), in-context learning has emerged as a promising alignment approach to regulate LLMs' behaviors by including the alignment goal in the prompt (Ganguli et al., 2023). For example, by incorporating 'Make sure that your answers are fair and do not rely on stereotypes' in the prompt, the LLM can reduce stereotypes in the outputs. This approach will not sacrifice the model's basic capabilities without modifying model parameters. However, it completely relies on the model's self-correcting capabilities and may be infeasible for underperforming big models. \n\nSupervised Fine-tuning (SFT) Unlike incontext learning, the following approaches require fine-tuning the model parameters. As for SFT, researchers utilize manually constructed <input, output> data pairs covering human instructions, human preferences and other safety issues to train the model in a supervised manner. Various strategies are designed to automatically generate instruction data by prompting LLMs, such as Self-Instruct (Wang et al., 2022a) and SELF-ALIGN (Sun et al., 2023c). SFT is a paradigm with the advantages of stable training and quick convergence. However, it also suffers from two drawbacks, i.e. poor generalization to unseen user inputs as well as a lack of negative feedback. \n\nReinforcement Learning To solve the aforementioned problems, LLMs introduce reinforcement learning in the fine-tuning phase. The most representative Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) are three-staged. First, it constructs human-aligned data to fine-tune the model using SFT. Second, it collects and ranks model-generated responses of varying qualities to train a reward model.",
            "score": 0.5248873187480643,
            "section_title": "Reward Model Scoring",
            "char_start_offset": 43428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 488,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1528,
                    "end": 1547,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 1962,
                    "end": 1983,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0033245086669921875
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "While interpretability methods identify a model\u2019s learned concepts, they overlook the relationships between concepts that make up its abstractions and inform its ability to generalize to new data. To assess whether models\u2019 have learned human-aligned abstractions, we introduce abstraction alignment, a methodology to compare model behavior against formal human knowledge. Abstraction alignment externalizes domain-specific human knowledge as an abstraction graph, a set of pertinent concepts spanning levels of abstraction. Using the abstraction graph as a ground truth, abstraction alignment measures the alignment of a model\u2019s behavior by determining how much of its uncertainty is accounted for by the human abstractions. By aggregating abstraction alignment across entire datasets, users can test alignment hypotheses, such as which human concepts the model has learned and where misalignments recur. In evaluations with experts, abstraction alignment differentiates seemingly similar errors, improves the verbosity of existing model-quality metrics, and uncovers improvements to current human abstractions.",
            "score": 0.5246329108832541,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00815582275390625
        },
        {
            "corpus_id": "274982312",
            "title": "Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees",
            "text": "How to Define Instructions? In this paper, as we focus on experiments with text-attributed graphs, we define instructions as label descriptions encoded by LLMs. However, this approach is not applicable to non-textual graphs. Other methods could be explored to define instructions, such as using proxy models (Hu et al., 2019) or graph heuristics (Jin et al., 2020) to generate instructions. \n\nHow to Choose SFT Data? We manually select graphs as supervised fine-tuning datasets for each domain, though the selected graphs may not be fully representative. Unlike textual data, evaluating the quality of graph datasets poses a challenge. Improved dataset selection methods could enhance the SFT process by identifying more representative or diverse data from graph databases. Additionally, while we perform instruction tuning over entire graphs, it is possible that only specific subgraphs are beneficial (Hashemi et al., 2024). Developing data selection methods that focus on high-quality subgraphs within a single SFT dataset could improve task-tree selection. Another worthy research line is to select SFT data that aligns with user preferences (Song et al., 2024). \n\nHow to Leverage SFT Data? In scenarios with limited instructions, standard supervised fine-tuning may struggle to capture sufficient knowledge of the target domain. To address this, methods could be proposed to better utilize the unlabeled instances in the SFT dataset, thus enhancing model adaptation (Sohn et al., 2020). \n\nHow to Maintain General Inference Capability? While instruction tuning specializes the model for a specific domain, it may compromise the model's general inference capabilities across other domains. This could hinder the model's performance when it needs to function both as a domain expert and a general reasoner. To mitigate this, regularization techniques could be designed to preserve the general knowledge encoded in the model during the instruction tuning process. \n\nWhy SFT Works on Graphs? Instruction tuning is a common post-training process in modern large language models (e.g., LLAMA, GPT) that significantly improves instruction-following capabilities. The success of this method in LLMs may stem from the fact that natural language serves as an interface between humans and models (Wei et al., 2021).",
            "score": 0.5224799557588,
            "section_title": "C.2. Specialization via Instruction Tuning",
            "char_start_offset": 47979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 1146,
                    "end": 1165,
                    "matchedPaperCorpusId": "259308873"
                },
                {
                    "start": 1471,
                    "end": 1490,
                    "matchedPaperCorpusId": "210839228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018606185913085938
        },
        {
            "corpus_id": "265551388",
            "title": "Instruction-tuning Aligns LLMs to the Human Brain",
            "text": "Because models vary in their brain and behavioral alignment across different architectures and training objectives (Schrimpf et al., 2021), we estimate the effect of instruction-tuning by evaluating 8 vanilla LLMs and 17 LLMs that were further instruction-tuned, and report a significant increase in brain alignment due to instruction-tuning. \n\nTo investigate why instruction-tuning increases alignment to human brain activity, we study the relationships between brain alignment and various LLM properties. Specifically, we compute Pearson correlations between an LLM's brain alignment and its properties, including next-word prediction (NWP) ability, model size, a range of problem-solving abilities, and world knowledge spanning different domains. We evaluated the latter two properties using the Big-Bench Hard benchmark (BBH; Suzgun et al., 2022) and the Massive Multi-task Language Understanding benchmark (MMLU; Hendrycks et al., 2021), respectively. \n\nWe report three major findings: \n\n1. Instruction-tuning generally aligns LLM representations to human brain activity, increasing brain alignment by 6.2% on average (Figure 1). \n\n2. Investigating the factors underlying LLM-brain alignment, we find that brain alignment is strongly correlated with world knowledge (r = 0.81) and model size (r = 0.95) (Figure 2). \n\n3. Surprisingly, our results indicate that instruction-tuning LLMs generally does not enhance behavioral alignment with human reading times. Furthermore, behavioral alignment on this dataset is poorly correlated with all other measures we investigate, including task performance and model size (Figure 3).",
            "score": 0.5210387072950655,
            "section_title": "Introduction",
            "char_start_offset": 3541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1627
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 138,
                    "matchedPaperCorpusId": "222359195"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002758026123046875
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "Style and formatting improvements saturate quickly. From Figure 3 we see that the models get better at style and format errors with just 100 examples. If one just takes a passing look at the responses from this model, they could incorrectly conclude that the model is \"aligned\" to answer math or multi-hop questions. However, all of these responses are still incorrect for the task for which we fine-tune the model. \n\nReasoning performance continues to improve with more data. Models continue to get better at reasoning and question understanding with more examples. The total number of mistakes a model makes highly correlates with reasoning errors (r 2 value of 0.98 for math and 0.99 for multihop QnA on Llama-3 8B) as opposed to total mistakes and formatting errors (r 2 value of 0.93 for math and 0.83 for multihop QnA). It also signifies that a model's capabilities are not entirely learned during pretraining, because models can significantly improve their reasoning, or learn to apply it effectively, during post-training. This leads us to the idea that the superficial alignment hypothesis could be limited in scope to improvements on style-and-formatting alignment tasks. It doesn't accurately characterize the improvements in capabilities that post-training is more effective at.",
            "score": 0.5198146236568856,
            "section_title": "Results",
            "char_start_offset": 14279,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09185791015625
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "Supervised fine-tuning (SFT) refines large language models (LLMs) using task-specific instruction data to enhance their capability to follow instructions (Touvron et al., 2023;Peng et al., 2023) and to align their outputs with human preferences and safety considerations (Ouyang et al., 2022;Rafailov et al., 2023;Dong et al., 2023b;Yuan et al., 2023).This process is often termed \"alignment\", signifying the tailoring of model outputs to conform to specific downstream requirements.Nevertheless, current research casts doubt on the necessity and potential adverse impacts of SFT.But the alignment achieved through SFT is often considered to be \"superficial\", with the process potentially repurposing pre-existing knowledge from pre-training to merely reshape outputs to meet specific criteria (Zhou et al., 2023;Lin et al., 2023).It has been observed that even a small-scale SFT training dataset can produce significant alignment effects (Liu et al., 2023;Xia et al., 2024).On the other hand, recent empirical studies (Luo et al., 2023;Dong et al., 2023a) have raised concerns that SFT might hurt the knowledge acquired during its pre-training phase, leading to serious consequences like catastrophic forgetting.\n\nNot only is there no definitive consensus on the necessity of SFT, but the majority of these studies also focus on monolingual tasks.LLMs still encounter challenges in handling complex crosslingual generation tasks (Schioppa et al., 2023;Wang et al., 2023).Current research on crosslingual alignment primarily seeks to extrapolate or align English capabilities to other languages using the SFT paradigm (Zhang et al., 2023;Chai et al., 2024;Xu et al., 2024), yet there remains a gap in exploring the specific impacts of SFT-based cross-lingual alignment.Furthermore, given the potential risk of SFT leading to the forgetting of pre-training knowledge, the question of how to achieve cross-lingual alignment without training remains underexplored.",
            "score": 0.5175415902779813,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1213
                },
                {
                    "start": 1215,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 292,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 292,
                    "end": 314,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 314,
                    "end": 333,
                    "matchedPaperCorpusId": "258170300"
                },
                {
                    "start": 333,
                    "end": 351,
                    "matchedPaperCorpusId": "258059818"
                },
                {
                    "start": 939,
                    "end": 957,
                    "matchedPaperCorpusId": "266551413"
                },
                {
                    "start": 1019,
                    "end": 1037,
                    "matchedPaperCorpusId": "261031244"
                },
                {
                    "start": 1638,
                    "end": 1656,
                    "matchedPaperCorpusId": "266999425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0083465576171875
        },
        {
            "corpus_id": "269303161",
            "title": "Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks",
            "text": "While those studies address unlearning a DPO method without the SFT step, further exploration of alternative methods is warranted. Although the significance of high-quality preferences is widely acknowledged, there remains a necessity to explore the influence of data quantity on performance of the alignment methods. Additionally, the crucial aspect of generalization remains unexplored. While aligning a model aims to enhance performance across all categories, improving alignment methods often comes at the expense of performance in other areas. Further investigation in this regard is necessary. To this end, we examine the performance of alignment methods both before and after SFT to assess the learning capabilities of IPO, KTO, and CPO. Moreover, we highlight the weaknesses of alignment methods by comparing their performance across five different domains, demonstrating the significant impact of dataset quantity on performance.",
            "score": 0.5156051133648978,
            "section_title": "Related Works",
            "char_start_offset": 9114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 938
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006565093994140625
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
            "score": 0.5155875642273787,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.124755859375
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "By externalizing formal human knowledge as an abstraction graph, abstraction alignment expands current alignment workflows from internalized comparison to iterative hypothesis testing. To evaluate this shift in perspective, we emulate three real-world alignment tasks across computer vision, natural language, and medicine. First, in Section 5.1, we apply abstraction alignment to interpret an image classification model, finding that it expands interpretation from narrow questions about why a model made a specific decision to broad explorations of the human concepts it has learned. Next, in Section 5.2, we collaborate with researchers to benchmark the specificity of language model responses, revealing that abstraction Oversees ICD usage alignment expands their conventional benchmarks of isolated pairwise comparisons to more comprehensive comparisons across the entire space of potential outcomes. Finally, in Section 5.3, we leverage abstraction alignment earlier in the ML pipeline, using it with healthcare professionals to assess the human alignment of a medical dataset, revealing discrepancies between medical abstractions and their real-world usage. \n\nStudy Method. To evaluate how abstraction alignment supports real-world alignment analysis, we collaborate with seven domain experts across two case studies: language model specificity (Section 5.2) and medical dataset analysis (Section 5.3). We conducted in-depth, semi-structured interviews with each expert to assess how abstraction alignment influenced their analysis. We began with questions about their domain expertise, alignment workflows, and desired outcomes, such as \"Tell me about your role as a [title]?\" and \"How do you currently measure alignment in [case study task]?\". Next, we introduced the abstraction alignment interface (Section 4) using tasks and datasets representative of their domain. Finally, we prompted experts to think aloud as they engaged with abstraction alignment to identify ways the model or dataset was aligned or misaligned with their domain knowledge. This approach allowed us to understand experts' current processes, observe how abstraction alignment functioned in context, and assess its potential to address experts' alignment goals. We discuss study limitations in Section 6. \n\nWe targeted expert participants to understand how abstraction alignment could impact real-world domains. To identify experts, we reached out to authors of relevant literature, attendees of specialized conferences, and LinkedIn professionals with applicable expertise.",
            "score": 0.5145498315611877,
            "section_title": "Evaluative Case Studies with Domain Experts",
            "char_start_offset": 32904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2286
                },
                {
                    "start": 2289,
                    "end": 2393
                },
                {
                    "start": 2394,
                    "end": 2556
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004962921142578125
        },
        {
            "corpus_id": "265551388",
            "title": "Instruction-tuning Aligns LLMs to the Human Brain",
            "text": "Instruction-tuning is a widely adopted finetuning method that enables large language models (LLMs) to generate output that more closely resembles human responses. However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans. We investigate the effect of instruction-tuning on aligning LLM and human language processing mechanisms in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs on three datasets involving humans reading naturalistic stories and sentences, and find that instruction-tuning generally enhances brain alignment (~6%), but has no similar effect on behavioral alignment. To identify factors underlying this improvement in brain alignment, we compute correlations between brain alignment and various LLM properties, such as model size, problem-solving, and world knowledge understanding. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that the mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain.",
            "score": 0.5124296477482675,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004398345947265625
        },
        {
            "corpus_id": "268264604",
            "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models",
            "text": "To reduce the complexity and cost of alignment, researchers have paid more attention to the first step of RLHF, Supervised Fine-Tuning (SFT), and proposed a range of sophisticated SFT variations to reach the same performance as RLHF.Omitting x for brevity, a general form of SFT alignment is:\n\nindicating that this paradigm is a member of imitation learning in Eq. ( 3), which directly learns to mimic the preferred behaviors while unlearning the dispreferred ones.\n\nWithout using negative examples y l , Eq. ( 8) reverts to conventional instruction tuning.For example, LIMA (Zhou et al., 2023) assumes that an LLM's knowledge is primarily gained during pretraining, and alignment teaches the model which formats to use in interactions.It achieves the alignment of an LLaMA-65B model by utilizing a limited set of 1k meticulously curated instructions and their corresponding gold responses.Like RLAIF, such (instruction, response) data could also be automatically constructed.Wang et al. (2023d) propose SELF-INSTRUCT, a semiautomated method for generating instruction data to improve LLMs' instruction following capabilities.Similarly, SELF-ALIGN (Sun et al., 2023), based on the SELF-INSTRUCT approach, incorporates additional human-defined value principles to generate more helpful, ethical, and reliable responses.\n\nTo address the limitation of the methods above using only positive feedback y w , Chain of Hindsight (CoH) (Liu et al., 2023b) was developed to utilize the paired feedback.During the training process, a prefix \"Good\" is appended to the preferred response, and \"Bad\" to y l .At inference, the LLM is instructed with \"Good\" to produce aligned responses.CoH is equivalent to learning a conditional policy \u03c0 \u03b8 (y|r) conditioned on the reward r, and r = 1 (\"Good\") for y w otherwise r = 0 (\"Bad\"), that is, E p(y,r) log \u03c0 \u03b8 (y|r) \u221d KL [p(y, r)||\u03c0 \u03b8 (y, r)].",
            "score": 0.5118102911739953,
            "section_title": "SFT-based Alignment",
            "char_start_offset": 25668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 292
                },
                {
                    "start": 294,
                    "end": 465
                },
                {
                    "start": 467,
                    "end": 557
                },
                {
                    "start": 557,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1318
                },
                {
                    "start": 1320,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 976,
                    "end": 995,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1148,
                    "end": 1166,
                    "matchedPaperCorpusId": "258479665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00266265869140625
        },
        {
            "corpus_id": "264490452",
            "title": "Unpacking the Ethical Value Alignment in Big Models",
            "text": "Since big models trained with instruction tuning have acquired sufficient knowledge and possess capabilities of zeroshot/few-shot learning, intent understanding, reasoning, and explanation, directly constraining the behaviour of LLMs through instructions/demonstrations becomes possible. Ganguli et al. (2023) found that by directly adding value statements to instructions/prompts, such as 'Please ensure your answer is fair and doesn't exhibit stereotypes', the model can understand the value-related instruction and reduce harmful content, such as social bias, in its output to some extent. Furthermore, under some metrics, the degree of value alignment is positively correlated with the number of instruction fine-tuning steps. Saunders et al. (2022) utilized the aforementioned capabilities of big models to make the model self-critique its generated answers and make revisions based on identified issues accordingly, achieving automatic alignment. This method utilizes the model's own understanding and correction abilities to achieve align-ment. Without modifying any parameters, it can retain the model's basic capabilities, providing a promising paradigm for re-aligning black-box models based on specific values. However, this approach heavily relies on the model's intrinsic capabilities and is limited by the performance of the instruction fine-tuning phase, unsuitable for smaller models or those not fine-tuned by instructions. \n\nFine-tuning based Alignment Considering the shortcomings of the aforementioned methods, direct fine-tuning, despite its high computational and data costs, offers good alignment performance and minimizes the impact on downstream tasks. Furthermore, in an era where big models serve as the foundation of various downstream tasks, a finetuned model can be reused across diverse applications and contexts, making fine-tuning highly cost-effective. Existing fine-tuning-based alignment methods fall into two categories: Supervised Fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). \n\n\u2022 Supervised Fine-tuning (SFT): Similar to the plug-in alignment above, early SFT methods primarily focused on improving specific risk metrics. Lu et al. (2020) employed Counterfactual Data Augmentation (CDA) to reduce bias in pre-trained models by fine-tuning them on data with varying attributes but similar semantics (e.g., male and female). Gehman et al.",
            "score": 0.5116207652168793,
            "section_title": "Introduction of Alignment Methods",
            "char_start_offset": 41113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2388
                },
                {
                    "start": 2389,
                    "end": 2402
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00243377685546875
        },
        {
            "corpus_id": "271600915",
            "title": "ABC Align: Large Language Model Alignment for Safety & Accuracy",
            "text": "LLM alignment is a rapidly evolving area of research, alongside efforts to further scale up pretraining data and neural network parameter counts. Starting in 2023, early evidence (Zhou et al., 2023) emerged that while massive amounts of data were required in the pre-training stage, alignment itself could be facilitated through smaller, high-quality curated datasets with samples numbering in the thousands instead of trillions. In parallel, LLM capabilities were being categorised relative to new evaluation frameworks, specifically around 'reasoning' (problem-solving) (Mitra et al., 2023), extending standard definitions of accuracy beyond metrics like ROUGE and BLEU (Papineni et al., 2002). \n\nOur work builds on 'Constitutional AI' from Anthropic (Kundu et al., 2023) and 'Less is More for Alignment' (LIMA) by Meta (Zhou et al., 2023). Constitutional AI focuses on the self-alignment of LLMs based on a 'constitution' or set of principles that guide the process of 'AI feedback'. We adapt this approach to use the ABC AI Principles directly in the creation of our fine-tuning datasets and frontier model alignment prompts. \n\nLIMA defines the 'Superficial Alignment Hypothesis', that states a 'model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users'. The use of 'superficial' here indicates that fine-tuning a subset of parameters, rather than complete pre-training, is the appropriate setting for LLM alignment. Their work examines the impact of a small set of 1000 samples on several alignment metrics, building on Kirstain et al. (2022). Our dataset is thus correspondingly small, high-quality, and informed by our AI Principles. \n\nOur own hypothesis is that given a sub-distribution of fine-tuning and preference data specific to our organisation, we can in turn align LLMs in a way that preserves their general utility, capability, and performance on industry-standard benchmarks that directly map to and reflect our AI Principles.",
            "score": 0.5085222221480301,
            "section_title": "LLM Alignment",
            "char_start_offset": 8063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 198,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 672,
                    "end": 695,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 822,
                    "end": 841,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1655,
                    "end": 1677,
                    "matchedPaperCorpusId": "238583118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12335205078125
        },
        {
            "corpus_id": "272367487",
            "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options",
            "text": "The introduction of None of the above options often confounds models, degrading performance compared to standard MCQs (Kadavath et al., 2022;Wang et al., 2024a). Similarly, open-ended questions pose greater challenges, as the absence of predefined options increases reasoning complexity (Myrzakhan et al., 2024). Some models can infer questions from answer choices alone, suggesting reliance on superficial patterns rather than deep understanding (Balepur et al., 2024). \n\nOur contribution: We investigate how LLMs handle multiple-choice questions when none of the provided answers are correct, an understudied challenge in current benchmarks. Our work offers insights into the robustness of LLMs when faced with scenarios where traditional instruction-following behavior may lead to incorrect conclusions. \n\nModel Alignment Recent advancements in LLM alignment focus on enhancing helpfulness in responses. Key contributions include fine-tuning techniques that utilize human feedback, as seen in (Rafailov et al., 2023;Ouyang et al., 2022;Hong et al., 2024;Sun et al., 2023) and (Hejna & Sadigh, 2023), which employ reinforcement learning from human preferences to shape user-aligned outputs. Bai et al. (2022) further illustrate the benefits of instruction fine-tuning for improved helpfulness, while research by (Zhang et al., 2024a) and (Tuan et al., 2024) addresses the balance between helpfulness and safety. \n\nOur contribution: In this work, we explore how model alignment influences reflective judgment, where models may favor helpfulness over critical assessment. We aim to isolate this effect by comparing models at different stages of training, providing insights into the relationship between alignment strategies and the quality of model outputs.",
            "score": 0.5083712238670867,
            "section_title": "CONTRIBUTIONS IN THE CONTEXT OF RELATED WORK",
            "char_start_offset": 28027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1758
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 469,
                    "matchedPaperCorpusId": "267760229"
                },
                {
                    "start": 996,
                    "end": 1019,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007843017578125
        },
        {
            "corpus_id": "276937618",
            "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
            "text": "The general practice is to fine-tune the aligned model with new domain-specific knowledge. Models that have undergone In CFT, a contextual prompt is highlighted in green \"Think about practical applications of the information in the next text. How could this knowledge be used in real-world situations?\" followed by the main text. IFT employs a direct instruction \"What is diabetes?\" before presenting the same text. In contrast, CPT displays only the main text without any preceding prompts or instructions. \n\nThe key difference lies in CFT's use of contextual prompts that guide the model's semantic understanding and reasoning, whereas IFT relies on explicit instructions to elicit specific responses. CPT, lacking both prompts and instructions, focuses solely on processing the main content. \n\ninstruction fine-tuning and alignment training are more amenable to interacting with users but are harder to update with new knowledge. But training to update the knowledge can result in catastrophic forgetting of knowledge gained during pretraining, or loss of capabilities like instructionfollowing and task-solving (Wang et al., 2023a). There is a need to develop methods that enable us to quickly improve reasoning and recall in aligned LLMs for domain specific fine-tuning. \n\nOur approach is inspired by the capabilities of LLMs to leverage prompts in question answering. For example, few-shot prompting popularized by Brown et al. (2020) performs well on a variety of unseen tasks at prediction time. Wei et al. (2022c) investigated how chain-of-thought (CoT) prompting can significantly improve a model's ability to perform complex multi-step reasoning. Wang et al. (2023b) further improved on CoT by selecting the most consistent answer from a diverse set of sampled reasoning paths. \n\nOur work investigates a simple question: can prompting improve the efficacy of LLM fine-tuning? We argue yes, and to this end, we propose a new method for fine-tuning that blends in-context learning with gradient-based learning. In summary, our contributions are as follows: \n\n(a) We present contextual fine-tuning, a generalization of instruction tuning, that combines in-context learning and fine-tuning. We further investigate the gradients provided by the additional context and provide synthetic experiments demonstrating their effectiveness for fine-tuning.",
            "score": 0.5067711469664314,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 1421,
                    "end": 1440,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1504,
                    "end": 1522,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1658,
                    "end": 1677,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00323486328125
        },
        {
            "corpus_id": "266999538",
            "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples",
            "text": "NO Human-Written NO Reward Continuous Instructions Model Enhancement \n\nSelf-Instruct (Wang et al., 2022) Seed QA examples \u2717 \u2713 \u2717 Self-Align (Sun et al., 2023c) Seed QA examples \u2717 \u2713 \u2717 LMSI (Huang et al., 2022) Question-only dataset \u2717 \u2713 \u2717 SALMON (Sun et al., 2023b) Question-only dataset \u2717 \u2717 \u2717 Self-Chat (Xu et al., 2023) Dialogue dataset \u2717 \u2713 \u2717 Self-QA (Zhang and Yang, 2023) Knowledge dataset \u2717 \u2713 \u2717 LongForm (K\u00f6ksal et al., 2023) Web dataset \u2717 \u2713 \u2717 Humpback (Li et al., 2023a) Web dataset \u2717 \u2713 \u2713 ReST (Gulcehre et al., 2023) Seed QA examples \n\nTable 1: Comparison of different self-bootstrapping methods truthfulness, or helpfulness from human-crafted principles, which would require a stronger ability that is only shown in large models. We find empirically that our framework can be adaptable to models as small as 350M and can be applied across various domains without the need for redesigning principles or retraining reward models. We compare our approach with other self-alignment methods in Table 1. \n\nWe conduct comprehensive experiments across three key alignment benchmarks: safety, truthfulness, and instruction-following. We find the iterative training scheme enhances alignment performance over time. We also show we consistently outperform the SFT models in the conventional alignment pipeline. In terms of balancing between harmlessness and helpfulness, we notably improve harmlessness rates without compromising helpfulness. Furthermore, our method shows robust domain generalization capabilities, particularly in various harmfulness domains, highlighting its adaptability and effectiveness. \n\n2 Related Work",
            "score": 0.5050948002069154,
            "section_title": "Data",
            "char_start_offset": 4013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 71,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1620
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002124786376953125
        },
        {
            "corpus_id": "265551388",
            "title": "Instruction-tuning Aligns LLMs to the Human Brain",
            "text": "Instruction-tuned LLMs are useful for studying LLM properties underlying brain and behavioral alignment. To identify why LLM and human brains share representational similarities, prior work has mostly focused on high-level properties such as model size (Antonello et al., 2023), as well as external behaviors such as predicting missing words (Schrimpf et al., 2021;Caucheteux & King, 2022). However, a key to understanding these similarities is to identify internal properties of LLMs that underlie brain alignment, including the amount of knowledge LLMs encode, e.g., factual (AlKhamissi et al., 2022) and commonsense (Bosselut et al., 2019;Sap et al., 2020). Our work is the first to show that we can harness instruction-tuned LLMs for this purpose. Because they have been trained to respond to a general instruction format, we can evaluate LLMs on diverse tasks in a fine-grained manner, allowing the study of LLM properties both internal (e.g., knowledge) and external (e.g., behavior), and how they correlate with brain and behavioral alignment. \n\nWorld knowledge shapes language comprehension and brain activity. Our results show that world knowledge is a key factor in LLM brain alignment. LLMs demonstrating greater world knowledge across all tested subject domains have representations more similar to the human brain. This suggests that world knowledge influences human brain activity and shapes the language comprehension systems in our brain.",
            "score": 0.5050215231824297,
            "section_title": "Implications for Neuroscience: Studying LLM-Human Alignment",
            "char_start_offset": 21739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1454
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 365,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 365,
                    "end": 389,
                    "matchedPaperCorpusId": "246902471"
                },
                {
                    "start": 642,
                    "end": 659,
                    "matchedPaperCorpusId": "220060314"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0038242340087890625
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "To adapt general-purpose LLMs to specific domains (e.g., Wiki, mathematics, code), our core idea is to decouple knowledge learning and format alignment, and propose an effective two-stage domain adaptation framework for general LLMs, i.e., first performing knowledge mixture continual pretraining (Section 2.2) and then performing efficient format alignment (Section 2.3).We show the overall architecture in Figure 2.\n\nIn the first stage, we conduct continual pre-training on the mixed data of raw domain-related documents, general instruction and alignment data via a unified text format.We aim to utilize general instruction data to better elicit the capacities of knowledge memorization and utilization during continual pre-training.To avoid catastrophic forgetting in pre-training, we design a new learning method, i.e., Logit Swap Self-Distillation (LSSD), that exchanges the top-1 token logit with the ground-truth token logit.In the second stage, based on the domain knowledge augmented LLMs, we conduct efficient format alignment with a small number of easy instructions or preference samples that have been seen during pre-training.In this way, LLMs can focus on learning the simple style and formet for interacting with human, without much consideration of how to utilize the attained knowledge.Next, we will describe each part in detail.",
            "score": 0.5039429294156588,
            "section_title": "APPROACH 2.1 OVERVIEW",
            "char_start_offset": 5189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 417
                },
                {
                    "start": 419,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 736
                },
                {
                    "start": 736,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00206756591796875
        },
        {
            "corpus_id": "268379670",
            "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model",
            "text": "Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks. Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated. In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks. Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively. Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting. To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment. Experimental results consistently illustrate the forgetting decreased from this method on CoIN.",
            "score": 0.5036776787003399,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00206756591796875
        },
        {
            "corpus_id": "272330499",
            "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning",
            "text": "LLMs commonly learn from massive unsupervised data during pre-training and store the learned knowledge in their parameters. Conflict emerges as contextual knowledge in the retrieved evidence contrasting with the inner knowledge stored in LLMs' parameters, which triggers \"hallucinations\" if an LLM holds a strong belief in its parametric knowledge and produces response inconsistent with the provided context. This phenomenon is unexpected as the performance of RAG will be restricted by hallucinations if the retrieved evidence is not accurate and helpful enough (Karpukhin et al., 2020;Shi et al., 2023;Ren et al., 2023;Mallen et al., 2023). We focus our research on the retrieval-augmented generation setting where an LLM is deemed trustworthy if it answers questions based on contextual rather than parametric knowledge. \n\nA wealth of studies (Longpre et al., 2021;Chen et al., 2022;Neeman et al., 2023;Zhou et al., 2023) has been dedicated to this problem owing to its obstructive impact on trustworthy LLMs. These efforts have discovered LLMs' dilemma between the parameters and retrieved evidence. In other words, LLMs may have capabilities of responding according to both contextual and parametric knowledge, and struggles to determine which to rely on. The inherent capabilities conforms to the foundation of a recently emerged technology in the field of AI: alignment (Ji et al., 2023a), which aims at encouraging a instruction-following language model (Wei et al., 2022) to behave in line with human intentions and values (Leike et al., 2018) by reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022). It is worth noting that alignment only \"unlocks\" capabilities LLMs already had, but does not teach new capabilities to the pretrained and instruction-tuned models (Lowe & Leike, 2022). \n\nInspired by the success of aligning LLMs with human intentions and values, we take the first step towards aligning retrieval-augmented LLM to a trustworthy status where it supplies responses in accordance with merely the retrieved evidence and ignores the parametric knowledge.",
            "score": 0.5031997859627766,
            "section_title": "Introduction",
            "char_start_offset": 1931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 588,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 605,
                    "end": 622,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 622,
                    "end": 642,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 847,
                    "end": 869,
                    "matchedPaperCorpusId": "237491581"
                },
                {
                    "start": 869,
                    "end": 887,
                    "matchedPaperCorpusId": "253107178"
                },
                {
                    "start": 887,
                    "end": 907,
                    "matchedPaperCorpusId": "253447228"
                },
                {
                    "start": 1463,
                    "end": 1481,
                    "matchedPaperCorpusId": "16785747"
                },
                {
                    "start": 1607,
                    "end": 1628,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0034160614013671875
        },
        {
            "corpus_id": "267412450",
            "title": "Factuality of Large Language Models: A Survey",
            "text": "Continued domain-specific SFT has shown to be effective for enhancing factuality, particularly in the absence of such knowledge during pre-training. For instance, (Elaraby et al., 2023) enhance the factual accuracy of LLMs through knowledge injection (KI). Knowledge, in the form of entity summaries or entity triplets, is incorporated through SFT by either intermediate tuning, i.e. first on knowledge and then on instruction data; or combined tuning, i.e. on the mixture of both. While some improvements are exhibited, the method alone can be insufficient to fully mitigate factual errors. \n\nFor general-purpose LLMs, SFT is typically employed to improve the instruction-following capabilities as opposed to factual knowledge which is mostly learned in pre-training. However, this process may inadvertently reveal areas of knowledge not covered in the pre-training, causing the risk of behavior cloning, where a model feigns understanding and responds with hallucinations to questions it has little knowledge of (Torabi et al., 2018). Rtuning (Zhang et al., 2023a) is proposed to address this issue with two pivotal steps: first, assessing the knowledge gap between the model's parametric knowledge and the instruction tuning data, and second, creating a refusal-aware dataset for SFT. It enables LLMs to abstain from answering queries beyond their parametric knowledge scope. On the other hand, BeInfo (Razumovskaia et al., 2023) improve factual alignment through the form of behavioral fine-tuning. The creation of the behavioral tuning dataset emphasizes two goals: selectivity (choosing correct information from the knowledge source) and response adequacy (informing the user when no relevant information is available or asking for clarification). Both methods effectively control LLMs on non-parametric questions but require extra effort in dataset curation and might hinder the models' retention of parametric knowledge. \n\nSycophancy (Sharma et al., 2023), another source of factuality errors, often arises from misalignments during SFT and RLHF (Ouyang et al., 2022).",
            "score": 0.49979895171341204,
            "section_title": "Tuning and RLXF",
            "char_start_offset": 16408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1035,
                    "matchedPaperCorpusId": "23206414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035114288330078125
        },
        {
            "corpus_id": "270357323",
            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
            "text": "We present the results comparing SFT and DPO in Figure 4. We observe that, on average, DPO leads to better performance when aligned with the helpful preferences compared to SFT for instruction-tuned models.This might stem from Figure 5: Comparing the effect of applying alignment methods on pre-trained models with instruction-tuned models using LLaMA-1 (Section 6.1).SFT helps more for pre-trained models, while DPO helps more for instruction-tuned models.However, when aligning to objective preferences like harmlessness, DPO leads to more faithful alignment across both pre-trained and instruction-tuned models.\n\nthe fact that DPO takes into consideration both the chosen and the rejected samples and is able to extract more learnings from the preference dataset.\n\nWhen evaluating the harmlessness, we observe very interesting results.Using DPO is significantly better than SFT, indicating that DPO makes the model alignment much more faithful to the preferences than SFT.This is more pronounced when the preferences are more objective, such as harmlessness, compared to relatively broader preferences like helpfulness.This observation also holds for models such as LLaMA-1, where DPO performs worse than SFT on helpfulness benchmarks.Furthermore, it is not always the case that aligning using the harmless preference dataset leads to more harmlessness in the case of SFT, further showing that it is not as faithful as DPO.Given the inability of pre-trained models to inherently provide rewards for broader preferences, we can say that generally, if the base model can be expected to be a good reward model, using DPO leads to better downstream performance and more faithfully aligned models.We also make another interesting observation that contradicts logical deductions.Oftentimes, using harmless data performs better than helpful data.This might be a characteristic of the Beaver-Tails dataset as it specifically contains helpful and safe responses, but this should be an important consideration when curating datasets for preference alignment.",
            "score": 0.4996565128269322,
            "section_title": "Results",
            "char_start_offset": 19671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 614
                },
                {
                    "start": 616,
                    "end": 766
                },
                {
                    "start": 768,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1238
                },
                {
                    "start": 1238,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1695
                },
                {
                    "start": 1695,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001789093017578125
        },
        {
            "corpus_id": "270764323",
            "title": "A Teacher Is Worth A Million Instructions",
            "text": "The recent advancements in Large Language Models (LLMs) have significantly propelled the field of natural language understanding and generation.Pre-trained language models (PLMs) leveraging extensive training corpus sourced from web [3,6] have demonstrated impressive capabilities across various natural language processing (NLP) tasks.However, additional training steps are required for PLMs to follow instructions and keep the responses aligned to human preferences.\n\nInstruction tuning (IT) [19,27,31] trains a PLM further for instruction following; utilising the general knowledge imparted in the pre-training phase along with the imparted instruction following capability it trains the model to generalise well on unseen tasks.However, while proficient at following instructions, these models may produce outputs that are potentially toxic or ethically questionable.To enhance alignment with human values, further training is necessary, utilizing techniques such as reinforcement learning with human feedback [13], direct preference optimization (DPO) [16] and monolithic preference optimization without reference model (ORPO) [10] based on pairwise preference data.\n\nInstruction tuning requires meticulous attention to data quality, optimization of instruction sets, and the implementation of effective training methodologies to ensure peak performance.A primary challenge in training these instruction-tuned models in specific domains is the potential reduction in the model's generalization ability, a factor we monitor using public evaluation benchmarks in our research.In this study, we present a method that not only addresses these concerns but also improves public benchmarks while aligning the model within a specific domain, in this instance, ecommerce.Drawing from the successful implementation of Knowledge Distillation (KD) [9] in miniLLMs [8] and tasks such as classification, we propose it as an alternative to the commonly used supervised fine-tuning (SFT) and alignment process in language model training.We propose Domain Alignment from Expert (DAE), a unique post-training domain alignment algorithm designed to strengthen domain-specific knowledge within the LLMs.DAE integrates domain-specific expert models into the training process, enhancing the model's understanding of specialized domains while preserving its ability to generalize across broader contexts.",
            "score": 0.4994372474425337,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 468
                },
                {
                    "start": 470,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1171
                },
                {
                    "start": 1173,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1768
                },
                {
                    "start": 1768,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0026721954345703125
        },
        {
            "corpus_id": "265067168",
            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "text": "Alignment, which typically involves two main processes, supervised fine-tuning and reinforcement learning from human feedback, serves as a crucial step toward unlocking the capabilities of LLMs and aligning them with human preferences. While alignment notably enhances the quality of LLM responses, it also introduces the risk of hallucinations. In this section, we will categorize the alignment shortfalls related to hallucinations into two parts: Capability Misalignment and Belief Misalignment. \n\nCapability Misalignment. Considering that LLMs have inherent capability boundaries established during pre-training, SFT utilizes highquality instructions along with their corresponding responses to empower LLMs to follow user instructions, unlocking their acquired abilities in this process. However, as the capabilities of LLMs expand, a significant challenge emerges: the potential misalignment between the LLMs' intrinsic capabilities and those depicted in the annotation data. When the demands from alignment data exceed these predefined capability boundaries, LLMs are trained to produce content beyond their own knowledge boundaries, amplifying the risk of hallucinations (Schulman, 2023). \n\nBelief Misalignment. Several studies have demonstrated that LLM's activations encapsulate an internal belief related to the truthfulness of its generated statements (Burns et al., 2022;Azaria and Mitchell, 2023). Nevertheless, misalignment can occasionally arise between these internal beliefs and the generated outputs. Even when LLMs are refined with human feedback (Ouyang et al., 2022), they can sometimes produce outputs that diverge from their internal beliefs. Such behaviors, termed as sycophancy (Cotra, 2021), underscores the model's inclination to appease human evaluators, often at the cost of truthfulness. Recent studies indicate that models trained via RLHF exhibit pronounced behaviors of pandering to user opinions. Such sycophantic behaviors are not restricted to ambiguous questions without definitive answers (Perez et al., 2023), like political stances, but can also arise when the model chooses a clearly incorrect answer, despite being aware of its inaccuracy (Wei et al., 2023).",
            "score": 0.49851020240260585,
            "section_title": "Hallucination from Alignment",
            "char_start_offset": 31571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004756927490234375
        },
        {
            "corpus_id": "266209771",
            "title": "TigerBot: An Open Multilingual Multitask LLM",
            "text": "1. Base models exhibit strong capability to follow instructions, right away before alignment. \n\nWe performed a quick evaluation on SQuAD2.0 benchmark, and found that Tigerbot-13bbase reached 86% of the next token prediction accuracy as Tigerbot-13b-chat. 2. Since foundational capabilities (knowledge and instruction following) has been learned during pretraining, alignment learning can be lightweight. This further benefits rapid and economic application deployment in various verticals. Our experiments show that loss reaches 95% of convergence after one million examples of SFT training.",
            "score": 0.49848113272053374,
            "section_title": "Training method",
            "char_start_offset": 10856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 591
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0014324188232421875
        },
        {
            "corpus_id": "265696189",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
            "text": "Typically, LLMs acquire knowledge through pre-training when building the model from scratch, implicitly achieved by reading vast amounts of text. 2 Whether knowledge can be acquired in the learning process after the model's construction (fine-tuning) remains an open question. 10 Groups including Meta have proposed the Superficial Alignment Hypothesis, which suggests that a model's knowledge and abilities are almost entirely learned during pretraining, and fine-tuning serves to extract knowledge. 11 However, pre-training and subsequent learning might be equivalent tasks for LLMs in learning from text, making it impossible to distinguish between the two strictly. \n\nFurthermore, building a model from scratch requires vast computational resources, suggesting that if this hypothesis is correct, constructing LLMs adapted to specialized knowledge or closed data becomes exceedingly high-cost. \n\nAn alternative method for adding specialized knowledge to LLMs without additional training or fine-tuning is Retrieval augmented generation (RAG). 12 RAG employs a search algorithm to find relevant professional documents and integrates these sections into the input prompt for the LLM, aiming to improve answer accuracy. One significant advantage of RAG is that it requires no model training. However, there are limitations, including challenges in constructing a system that accurately understands technical terms for document retrieval and the LLM's contextual length constraints, which limit the ability to integrate information across multiple documents. Thus, RAG may not always be a viable substitute for training-based approaches. 2,12   examination of additional training conditions has organized the training conditions and database requirements, along with their constraints, to introduce specialized knowledge into existing LLMs. Initially, we taught the LLM using model texts containing fictional information to identify the requirements for additional training. Subsequently, \n\nwe built an open dataset based on approximately 55,000 open-access papers from Springer-Nature journals to generate a model that learns more practical knowledge. Through these investigations, we clarified the substantial conditions for adding new knowledge to existing models and organizing the limitations of current methods.",
            "score": 0.49664728594748253,
            "section_title": "Introduction",
            "char_start_offset": 2298,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 1988
                },
                {
                    "start": 1991,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2317
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08880615234375
        },
        {
            "corpus_id": "266690973",
            "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation",
            "text": "Further Pretraining To transfer financial and legal knowledge to the PanGu-\u03c0 model, we further pre-train it with a specialized vertical corpus. To alleviate catastrophic forgetting of formerly learned knowledge, we partially introduce the during the pretraining process. Specifically, we resample a portion of the previous high-quality data and mix it with the vertical domain data at a 1 : 1 ratio. \n\nInstruction Tuning During the instruction tuning phase, we utilize supervised fine-tuning samples of both general-purpose tasks and domain-specific tasks. We shuffle and combine them into one supervised dataset and execute the instruction tuning process in one stage. On the basis of the YunShan-base model, which has already acquired general and specialized knowledge during previous phase, this approach aims to simultaneously teach the model how to follow human instructions in different domains.",
            "score": 0.49604029771297237,
            "section_title": "Training Process",
            "char_start_offset": 54887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 901
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013456344604492188
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "In our probe experiments, we explore the reasoning direction differences between unsafety-aligned models and safety-aligned models across several popular LLaMA families, including LLaMA2, LLaMA3, and LLaMA3.1 (Fig. 7 describes more probing results on the HEx-PHI dataset). These models offer diverse pretrained knowledge and capabilities, allowing us to investigate how safety alignment affects model behavior when responding to malicious queries. To isolate the impact of reasoning direction when facing unsafe inputs, it is crucial to control for other confounding factors. Existing open-source instruction-following models are typically both helpful and safe, while pretrained open-source models without safety alignment are neither helpful nor safe. This dichotomy presents a challenge in disentangling the effect of general instruction-following capabilities from safety-specific behaviors. \n\nThus, for each LLaMA variant (LLaMA2, LLaMA3, LLaMA3.1), we fine-tuned two separate models using Supervised Fine-Tuning (SFT): \n\n(1) A General Instruction-Following Model that is trained to follow human instructions but without any explicit safety mechanisms. This model helps us evaluate how a model with instruction-following capabilities but without safety guardrails reacts to malicious queries. (2) A Safety-Aligned Model that incorporates both general instruction-following capabilities and explicit safety mechanisms, allowing us to examine how safety alignment influences the model's reasoning direction when responding to unsafe inputs. \n\nBy comparing these two categories of models when exposed to different types of malicious queries, we can better understand how safety alignment reshapes the internal decision-making process of large language models. \n\nSupervised Fine-Tuning Process and Configuration. We follow the alignment method outlined in the Zhou et al. (2024), which uses Supervised Fine-Tuning (SFT). For the general instructionfollowing models, we employed the LIMA dataset, which includes over 1000 instruction-following examples. However, we removed 13 safety-related examples to avoid conflating safety concerns with general instruction-following abilities. The filtering process was assisted by GPT-4, following a set of instructions specifically designed to identify and exclude safety-related tasks.",
            "score": 0.4958820147701556,
            "section_title": "A.2 MODEL CONFIGURATION AND TRAINING DETAILS",
            "char_start_offset": 37498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.027740478515625
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Typically, adapting a base LLM to a specific domain involves three distinct and relatively independent stages, each based on corresponding data in different formats.Specifically, the base model firstly performs continual pre-training (CPT) on domain-specific corpus for learning new knowledge, then conducts supervised fine-tuning (SFT) based on instructions for enhancing the instruction following ability, and finally utilizes the human preference data for human alignment.In this work, we adopt the direct preference optimization (DPO) to as the alignment algorithm.Formally, we denote the domain-specific corpus as D CPT = {d i } nc i=1 , where d i represents a raw domain document consisting of a sequence of tokens.For the instructions used in SFT, we denote as D SFT = {\u27e8q i , r i \u27e9} ns i=1 , where q i and r i represent the user query and the expected response, repectively.For alignment data used in DPO, we denote by\n\n, where q i , r + i , and r \u2212 i represent the user query, positive response, and negative response, respectively.\n\nIn this work, we propose to mix D CPT , D SFT and D DPO with a unified text format, building upon which we further perform knowledge mixture continual pre-training on a general LLM.Unlike previous work relying on synthesizing high-quality domain instructions (Jiang et al., 2024;Cheng et al., 2024), we empirically find that knowledge utilization is indeed a general capability that can be learned from general instructions, and we can further transfer such capacity to enhance the learning of domain knowledge.Specially, we remove any templates and markers (e.g., [User]) from the instructions and alignment data to construct the mixture data in a unified format, denoted by D MIX = {x cpt , x sft , x dpo }, where x cpt = d i is the original domain document, x sft = [q i ; r i ] denotes the concatenation of user query and expected response in instructions, and\n\nis the concatenation of user query and positive response in the alignment data.We show some examples in Figure 1.",
            "score": 0.4952968203934909,
            "section_title": "UNIFIED KNOWLEDGE FORMAT",
            "char_start_offset": 7757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 926
                },
                {
                    "start": 928,
                    "end": 1041
                },
                {
                    "start": 1043,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1907
                },
                {
                    "start": 1909,
                    "end": 1988
                },
                {
                    "start": 1988,
                    "end": 2022
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022430419921875
        },
        {
            "corpus_id": "270870413",
            "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning",
            "text": "Therefore, it is necessary to evaluate the model performance on more academic benchmarks. In this subsection, we present the results on six benchmarks, evaluating various model abilities including explicit instruction following [Zhou et al., 2023], general knowledge [Rein et al., 2023], multitask language understanding [Hendrycks et al., 2020], commonsense reasoning [Zellers et al., 2019], human falsehoods mimicking [Lin et al., 2021], and math word problem-solving [Cobbe et al., 2021]. We compare our INPO (PM) with the SFT baseline, iterative DPO (PM), and SPPO (PM). The results are shown in Table 2. Interestingly, compared to the SFT baseline, all three alignment methods exhibit performance improvements on these benchmarks. A potential reason for this is that during the alignment stage, the alignment methods more effectively leverage the model's internal knowledge and abilities, which were introduced during the pre-training and SFT stages. Additionally, both INPO and iterative DPO incorporate KL regularization, which prevents the learned policy from deviating significantly from the reference policy, thereby avoiding performance degradation. And the superior results of INPO and SPPO demonstrate the advantage of considering general preferences. In this subsection, we conduct an ablation study to examine the benefits of including the KL regularization term in the game objective. The results are shown in Table 3. We observe that INPO with KL regularization (INPO w/ KL) generally outperforms its counterpart without KL regularization (INPO w/o KL) by a clear margin. This indicates regularizing our policy towards the reference policy is beneficial for the alignment performance.",
            "score": 0.4947564683236868,
            "section_title": "Main Results",
            "char_start_offset": 22365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005405426025390625
        },
        {
            "corpus_id": "261049152",
            "title": "Instruction Tuning for Large Language Models: A Survey",
            "text": "The benefits of SFT are threefold: (1) Finetuning an LLM on the instruction dataset bridges the gap between the next-word prediction objective of LLMs and the users' objective of instruction following; (2) SFT allows for a more controllable and predictable model behavior compared to standard LLMs. The instructions serve to constrain the model's outputs to align with the desired response characteristics or domain knowledge, providing a channel for humans to intervene with the model's behaviors; and (3) SFT is computationally efficient and can help LLMs rapidly adapt to a specific domain without extensive retraining or architectural changes. \n\nDespite its effectiveness, SFT also poses challenges: (1) Crafting high-quality instructions that properly cover the desired target behaviors is non-trivial: existing instruction datasets are usually limited in quantity, diversity, and creativity; \n\n(2) there has been an increasing concern that SFT only improves on tasks that are heavily supported in the SFT training dataset (Gudibande et al., 2023); and (3) there has been an intense criticism that SFT only captures surface-level patterns and styles (e.g., the output format) rather than comprehending and learning the task (Kung and Peng, 2023). Improving instruction adherence and handling unanticipated model responses remain open research problems. These challenges highlight the importance of further investigations, analysis, and summarization in this field, to optimize the fine-tuning process and better understand the behavior of instruction tuned LLMs. \n\nIn the literature, there has been an increasing research interest in analysis and discussions on LLMs, including pre-training methods (Zhao et al., 2023), reasoning abilities (Huang and Chang, 2022), downstream applications (Yang et al., 2023a;Sun et al., 2023b), but rarely on the topic of LLM instruction tuning. This survey attempts to fill this blank, organizing the most up-to-date state of knowledge on this quickly advancing field. Specifically, \n\n\u2022 Section 2 presents the general methodology employed in instruction tuning. \u2022 Section 3 outlines the construction process of commonly-used SFT representative datasets. \u2022 Section 4 presents representative instruction tuned models.",
            "score": 0.49195688870714005,
            "section_title": "Introduction",
            "char_start_offset": 1864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2022
                },
                {
                    "start": 2025,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00250244140625
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "Overview. This section investigates whether IT, at its current open-source scale, can function as a knowledge enhancer. Initially, we present the distinct natures of transformation that a base pre-trained LLM undergoes when subjected to LFT and SFT-based fine-tuning and show that while responses generated after LFT are closely aligned to pre-trained knowledge, they deviate significantly for SFT, indicating new knowledge acquisition. Subsequently, we show that this new knowledge often leads to a degradation in response quality, and relying predominantly on pre-trained knowledge often yields more factual and useful responses. \n\nFinding 1. LFT Responses align closely with the original pre-trained knowledge. SFT does not. To study how finetuned models differ from their base pre-trained counterparts, we perform the token-distribution analysis proposed by Lin et al. (2023). Specifically, for a given instruction-response pair, the instruction i = {i 1 , i 2 , \u2022 \u2022 \u2022 } is first input to the aligned (or fine-tuned) model to obtain its response r = {r 1 , r 2 ,\u2022 \u2022 \u2022 } via greedy decoding. Next, for each position t in the response, a 'context' at this position is defined as to be \n\nThis \"context\" is then input to the base model to obtain its probability distribution for predicting the next token at position t, P base . The probability distribution of the token at position t obtained from the aligned model is denoted as P align . We then calculate three metrics: (1) KL Divergence between P base and P align , (2) Base Probability: P base of the token at t with the maximum P align value (3) Base Rank: Rank in P base of the token at t with the maximum P align value. With the base rank denoted as \u03b7, the unshifted, marginal and shifted tokens are defined as when (\u03b7 = 1), (1 < \u03b7 \u2264 3) and (\u03b7 > 3) respectively. Figure 1 illustrates how the three metrics evolve. These metrics are averaged across all response tokens and plotted against the varying sizes of the IT dataset used for fine-tuning.",
            "score": 0.48942489253554067,
            "section_title": "IT is (currently) Not a Knowledge Enhancer",
            "char_start_offset": 8695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2004
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021572113037109375
        },
        {
            "corpus_id": "273186838",
            "title": "Intriguing Properties of Large Language and Vision Models",
            "text": "Beyond visual recognition capabilities, we analyze cross-modal alignment based on the platonic representation hypothesis (Huh et al., 2024), which argues that neural networks, despite being trained on different objectives, data, and modalities, should converge to a shared statistical model of reality in their representation spaces. To measure representation similarity between two modalities, the original authors of this hypothesis use mutual nearest-neighbor alignment metrics, a type of kernel-alignment metric. In our work, we assess how much alignment is lost after visual instruction tuning by applying this metric within the context of the platonic representation hypothesis. We evaluate 10 LLMs and measure alignment between these LLMs and vision encoders (LLVMs) using the DOCCI (Onoe et al., 2024) dataset which contains long image descriptions requiring localized scene understanding. As shown in Figure 7, after visual instruction tuning, both LLaVA-1.5 and LLaVA-NeXT show degraded alignment performance with respect to representations compared to their original vision encoder. This suggests doubts about the actual role of the projector in causing the degradation in alignment preservation. From this observation, we speculate that current LLVMs are trained on a variety of datasets to achieve generalization (i.e., multi-task learning). However, during visual instruction tuning, the models might overemphasize capabilities requiring complex cognition while potentially reducing representations related to other tasks, such as localized scene understanding (i.e., DOCCI). This results in a lower alignment Preprint. \n\nscore and catastrophic forgetting, as shown in Table 3. For future work, one potential direction is to develop a localized enhanced alignment module similar to HoneyBee (Cha et al., 2024).",
            "score": 0.48890978558650894,
            "section_title": "DO LLVMS PRESERVE CROSS-MODAL ALIGNMENT?",
            "char_start_offset": 23550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1824
                }
            ],
            "ref_mentions": [
                {
                    "start": 1805,
                    "end": 1823,
                    "matchedPaperCorpusId": "266174127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003147125244140625
        },
        {
            "corpus_id": "263334580",
            "title": "Self-Specialization: Uncovering Latent Expertise within Large Language Models",
            "text": "However, a pertinent question remains: How effective are the self-aligned models when applied to more niche domains, such as biomedicine? Given that both the initial pre-training and subsequent selfalignment are general, the knowledge embedded in LLM parameters may be a mixture of semantics and various domains. This raises questions about their effectiveness in specialized domains, despite the aims of instruction-tuning and self-alignment for cross-task generalization. In our preliminary study, however, we find that existing models such as Alpaca (Taori et al., 2023) and Dromedary (Sun et al., 2023), although aligned, exhibit only a modest degree of improvement within the specialized domains. These observations underline the need for focused approaches that can leverage the domain expertise existing in the base models, to ensure the self-generated instruction-tuning data remains both contextually appropriate and accurate. \n\nIn this work, we explore the possibility of selfspecialization (Fig. 1). Drawing inspiration from the foundational principles of self-alignment, selfspecialization goes a step further by incorporating domain-specific seed instructions and is further bolstered by parameter-efficient fine-tuning, as well as optional iterative refinement and retrieval components. Our goal is to guide models beyond generic alignment, directing them to generate data that are not just contextually fitting for a specialized domain but also maintain high accuracy. \n\nWe evaluate our self-specialized models within the biomedical and finance domains (20 datasets in total), and across a variety of base models that we specialize. Surprisingly, despite the simplicity of our approach, our results present a compelling case for self-specialization significantly outperforming the base models, and even larger models that are generally instruction-tuned or specifically pre-trained on the target domain. Notably, our self-specialized one based on MPT-30B (Team, 2023) for biomedicine even surpasses larger models (based on LLaMA-65B (Touvron et al., 2023a)), including the ones improved through selfalignment by leading methods (Wang et al., 2022a;Sun et al., 2023).",
            "score": 0.4881649281951831,
            "section_title": "Introduction",
            "char_start_offset": 1838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 935
                },
                {
                    "start": 938,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003551483154296875
        },
        {
            "corpus_id": "267060796",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "text": "Fine-tuning foundation large language models (LLMs) in line with human preferences, such as helpfulness, harmlessness, and truthfulness (Ziegler et al., 2019;Ouyang et al., 2022), have demonstrated remarkable effectiveness. Recent research indicates that these foundation LLMs primarily acquire their knowledge from the pretraining corpus, while the alignment process serves to facilitate the incorporation of preencoded knowledge for generating human-aligned responses rather than introducing additional knowledge (Zhou et al., 2023). However, it should be noted that there may be certain knowledge contained in the alignment data only, but not in the pretraining corpus. As illustrated in Figure 1, the knowledge snippet regarding \"Direct Preference Optimization\", a recently introduced technique for LLM alignment (Rafailov et al., 2023), is absent from the pretraining corpus of the foundation LLMs, leading to a phenomenon termed as knowledge inconsistency between the external knowledge in alignment data and the intrinsic knowledge embedded within foundation LLMs. Knowledge inconsistency could lead the finetuned LLMs to adapt to external knowledge they encounter but do not fully comprehend, resulting in hallucinated responses. Our preliminary analysis in Figure 2 reveals a direct correlation between the percentages of knowledge inconsistency and the rates of hallucination across various foundation LLMs and benchmarks. 1 Therefore, an interesting question arises: how to identify alignment data instances that trigger knowledge inconsistency before conducting alignment to mitigate hallucinations. \n\nRecent studies attempt to identify inconsistent instances by evaluating if foundation LLMs can generate correct responses for alignment questions. \n\n1 Refer to Section 4.2 and Section 4.3 for more details. Then, they calibrate the inconsistency by appending a sentence to the ground truth answer that describes the uncertainty of their capabilities during alignment (Yang et al., 2023;Zhang et al., 2023a). However, these approaches suffer from two major limitations. Firstly, they are designed for tasks like question answering, which can be evaluated via a binary true/false or a single accuracy score. For complex tasks, such as \"Describe the process of photosynthesis in detail.\", it becomes challenging to ascertain the confidence level in the gold references.",
            "score": 0.4861726456318921,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2379
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004520416259765625
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "\u2022 Post-training performance on a task has a power law relationship of the form P \u221d D 1/b with the number of post-training samples, similar to scaling laws established for pertaining and inference (Brown et al., 2024;Kaplan et al., 2020), across models of multiple families and sizes. (Section 3) \n\n\u2022 Evaluating alignment models using win-rates as shown in Zhou et al. (2024) could be misleading for reasoning-based tasks. For instance, LLM-based judges can prefer model generations that exhibit a chatbot-style answer for mathematical questions, even though the model might be poor at mathematical abilities as observed on math benchmarks. (Section 3.3) \n\n\u2022 Through extensive error analysis on tasks like math and multihop reasoning, we see that when a model is finetuned for a task, the improvements in task-specific style and formatting saturate in just 100 examples, as hypothesized by the Superficial Alignment Hypothesis. However, the model's performance on the task is directly correlated with its improvements in reasoning ability, which improves notably during post-training with more finetuning examples. (Section 4) for the task. When evaluated for their objective task-specific performance, we see that the models do improve significantly with additional data on many tasks during post-training over their pre-trained counterparts. In addition, these improvements are primarily driven by improvements in reasoning and analytical abilities during post-training. Good post-training is also an effective way for LLMs to learn and integrate new knowledge from beyond their knowledge cut-off.",
            "score": 0.4857503718881221,
            "section_title": "Key Takeaways",
            "char_start_offset": 3695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1598
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.163818359375
        },
        {
            "corpus_id": "259375779",
            "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning",
            "text": "In conclusion, the Instruction Following Score (IFS) was introduced as a metric to detect language models' ability to follow instructions. Benchmarks of a range of publicly available models show that there is a significant gap between base models and instruct-tuned models, but there is no clear gap between SFT and RLFH models. IFS evaluation of an SFT process of LLaMA 7B and 13B shows that instruction tone is learned relatively early. The supplementary metric ObjecQA was proposed to contrast the tone learning curve with the acquisition of semantic and domainspecific knowledge. Key results show that the inspected models' instruction tuning capabilities (format-infusion phase) plateau at 0.9-0.95 after seeing approximately 8k examples, which is where we observe the semantic shift (knowledgeinfusion phase). Bigger models reached a 0.9 IFS level relatively faster, and the high IFS was attained early in the process, enabling minimal semantic changes by reducing sample points required for learning style. \n\nFor future work, the research should focus on composable feature blocks that can be applied to foundation models to achieve desired alignment aspects, such as helpfulness, formality, or strict formats without unexpected downgrades in upstream tasks or semantic shifts. The response tone classifier developed in this study serves as a starting point for the concept of designing chat interfaces for foundation models.",
            "score": 0.48458607699199946,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 18205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0030517578125
        },
        {
            "corpus_id": "273901354",
            "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
            "text": "To marry the strengths of both paradigms, in this paper, we propose DRPO, Dynamic Rewarding with Prompt Optimization, a novel tuning-free approach for LLM self-alignment. DRPO draws inspiration from two key insights from recent alignment research. First, the superficial alignment hypothesis (Zhou et al., 2024) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2024a;Zhao et al., 2024). Second, reward models in RLHF often generalize poorly to out-of-distribution samples (Burns et al., 2023), whereas LLMs, well-known for their superior generalization capabilities, can provide more effective rewards and feedback for alignment. Building on these insights, DRPO is constructed atop a search-based prompt optimization (PO) framework (Pryzant et al., 2023;Hao et al., 2023;Wang et al., 2023), allowing LLMs to selfcorrect and automatically craft detailed alignment instruction. This steers model behavior more effectively, without relying on any use of human preferences or model training. \n\nThe core novelty of DRPO lies in its dynamic rewarding mechanism, integrated with the optimization framework. This mechanism enables LLMbased rewards to be adjusted on the fly based on specific queries, helping to identify and rectify the model's alignment blind spots. For example, if an LLM with outdated knowledge pretends to answer a question requiring the latest news, its \"knowledge limitation\" reward will be low, and the alignment prompt will be updated accordingly. We apply this novel method to automatically craft both the system prompt and responses in ICL examples, which have proven highly effective in improving alignment. \n\nWe conducted comprehensive experiments on 8 recent LLMs using the standard alignment benchmark, just-eval-instruct, composed of questions from multiple alignment datasets. Our results show that DRPO can effectively align both base and SFT/RLHF tuned models. Notably, DRPO significantly enhances base models, enabling them to outperform their SFT/RLHF-tuned counterparts. \n\nFigure 2: Comparing DRPO with other alignment methods, such as RLHF and URIAL (Lin et al., 2024a).",
            "score": 0.4828160797260054,
            "section_title": "Introduction",
            "char_start_offset": 1805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2154
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 311,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 400,
                    "end": 419,
                    "matchedPaperCorpusId": "265608902"
                },
                {
                    "start": 824,
                    "end": 842,
                    "matchedPaperCorpusId": "264451925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08123779296875
        },
        {
            "corpus_id": "269293831",
            "title": "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?",
            "text": "Zhou et al. ( 2023) claim that a model's knowledge and capabilities are acquired almost entirely during pre-training, and the effect of alignment tuning might be \"superficial\", in that it teaches the model the format for interacting with users. This idea is further supported by recent works (Lin et al., 2024;Ghosh et al., 2024). However, to what extent this applies to multilingual translation in LLMs is little known. To bridge this gap, we conduct a series of controlled experiments on fine-tuning LLMs for translation, complementing previous research across three dimensions. First, we study the parallel data efficiency in the era of LLMs, aiming to determine the minimum data needed for effective model alignment to the translation task. Next, we explore the scope of alignment by probing whether aligning one translation direction influences other directions. Finally, we investigate how synthesized fine-tuning data quality impacts the LLMs' behaviour in generating translations. \n\n3 Experiments and Results",
            "score": 0.48170061459028696,
            "section_title": "Superficial alignment hypothesis",
            "char_start_offset": 3888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1016
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 329,
                    "matchedPaperCorpusId": "267548105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0226287841796875
        },
        {
            "corpus_id": "265129034",
            "title": "Fake Alignment: Are LLMs Really Aligned Well?",
            "text": "As shown in Fig. 1, we found clear performance differences between two formats in the safety evaluation. Inspired by Wei et al. (2023a), we think this is due to the mismatched generalization between model's capabilities and its safety considerations. Specifically, the training of LLMs can be divided into two stages, termed pre-training and fine-tuning. LLMs are pre-trained on large-scale corpus and thus acquire various powerful capabilities, such as text generation, reasoning, and subject knowledge, etc. Fine-tuning uses supervised finetuning (Ouyang et al., 2022), RLHF (Christiano et al., 2017), RLAIF (Bai et al., 2022b), and others to enhance model's instruction following ability and align it with human value preferences, thereby building safety guardrails for the LLM. However, when the data for safety training lacks diversity, the model tends to merely mimic safety data in certain aspects without genuinely comprehending human preferences. For example, as pointed out by Yuan et al. (2023), talking to GPT-4 through ciphers compared to normal language can cause model to tend to output unsafe content. Similarly, the poor safety performance of some models in multiple-choice questions is also due to the insufficient safety training. This also means that the model appears to align well in certain aspects, but in reality, this can be deceptive; it doesn't possess a deep, correct understanding of alignment. This is what we refer to as fake alignment. \n\nTo prove this explanation, we design evaluation datasets in two aspects: capability and safety. Each test question in the dataset contains a corresponding open-ended format and multiple-choice format to directly compare model's performance differences. Here, the capability test is to show that LLMs have mastered the ability to solve multiple-choice questions in the pre-training stage. If the model shows no difference between the two evaluation formats on the capability test set but demonstrates a difference on the safety test set, it can prove the existence of fake alignment.",
            "score": 0.48156022072649696,
            "section_title": "The Fake Alignment Phenomenon",
            "char_start_offset": 8340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 570,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 577,
                    "end": 602,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00878143310546875
        },
        {
            "corpus_id": "274982186",
            "title": "Boosting LLM via Learning from Data Iteratively and Selectively",
            "text": "Instruction tuning teaches LLMs to perceive the intent of users and provide helpful responses, standing as a core component in the deployment of LLMs (Ouyang et al., 2022). Collecting high-quality data for instruction tuning has caught great attention. Early works (Wei et al., 2022;Longpre et al., 2023) merge existing NLP datasets to obtain a diverse collection across different tasks. Subsequently, Wang et al. (2023) proposed the Self-Instruct framework, which is an automated algorithm gathering instruction-response data by bootstrapping LLMs' generations. Building on this work, a number of data synthesis and evolution techniques (Xu et al., 2023;Ding et al., 2023;Wu et al., 2024b;Li et al., 2024a) have emerged, leveraging powerful proprietary LLMs to significantly enhance the quality and size of candidate datasets for supervised alignment. Nonetheless, according to the superficial alignment hypothesis proposed by Zhou et al. (2024), LLMs have acquired abundant knowledge and abilities during pre-training, and the focus of instruction tuning is all about style learning for providing a helpful response. They verified their hypothesis with around 1K elaborately selected samples, highlighting data selection for instruction tuning as a promising research direction.",
            "score": 0.48033171914380124,
            "section_title": "Instruction Tuning Datasets",
            "char_start_offset": 25373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1280
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 171,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 265,
                    "end": 283,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 402,
                    "end": 420,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 655,
                    "end": 673,
                    "matchedPaperCorpusId": "258840897"
                },
                {
                    "start": 673,
                    "end": 690,
                    "matchedPaperCorpusId": "258352678"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "While task-specific sensitivity focuses on the importance of layers within individual tasks, it is equally essential to evaluate how each task-specific model influences other tasks during the merging process. Cross-task sensitivity captures the interdependencies and shared representations between different tasks, ensuring that the merged model benefits from common features and decision-making processes. The measurement of cross-task influence begins with evaluating logits alignment between different task-specific models. Specifically, for calibration samples from task t j , we compute the alignment score between model \u03b8 t i SFT and the expert model for task t j , \u03b8 t j SFT , using the L 2 distance between their output logits: \n\nwhere f \u03b8 (x) denotes the output logits of model \u03b8 for input x, and || \u2022 || 2 represents L 2 distance. This alignment score quantifies how closely the predictions of model \u03b8 t i SFT match those of the expert model for task \u03b8 t j SFT , providing insight into the degree of shared knowledge and representational similarity between tasks. To obtain a comprehensive measure of cross-task sensitivity for a specific task model \u03b8 t i SFT , we aggregate the alignment scores across all other tasks. This aggregation process involves computing the normalized alignment: \n\nThe resulting cross-task scaling factor \u03c4 i serves as a crucial metric that quantifies model \u03b8 t i SFT 's ability to transfer knowledge across tasks. Higher values of \u03c4 i indicate superior cross-task generalization capabilities, suggesting that the model has learned robust representations that are valuable across multiple tasks. Conversely, lower values of \u03c4 i reflect greater task-specific specialization, indicating that the model's features are more narrowly focused on its primary task.",
            "score": 0.4797135236404124,
            "section_title": "Cross-Task Scaling",
            "char_start_offset": 10963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1794
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0036640167236328125
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "More is different. \n\n--Philip W. Anderson, 1972 Reasoning ability is a hallmark of human intelligence (Gendron et al., 2024;Huang and Chang, 2022;Wei et al., 2022b). Although Large Language Models (LLMs) have recently demonstrated significant capabilities in various tasks such as conversation (Achiam et al., 2023;Anthropic, 2024;Peng et al., 2023) and summarization (Almazrouei et al., 2023;Scao et al., 2022;Wei et al., 2023b;Yang et al., 2023), they often struggle with complex reasoning tasks (Gendron et al., 2024;Lu et al., 2023;Wu et al., 2023). One particularly challenging area is mathematical reasoning (Arora et al., 2023;Cobbe et al., 2021;He et al., 2024;Hendrycks et al., 2021;Zhong et al., 2023), which requires the ability to solve mathematical problems and derive logical conclusions in a step by step manner (Saxton et al., 2019;Shao et al., 2024;Toshniwal et al., 2024;Wei et al., 2022b;Yu et al., 2024). \n\nTwo prevailing beliefs guide researchers and practitioners in enhancing mathematical reasoning abilities of LLMs. The first belief posits that complex reasoning abilities, especially mathematical reasoning, are emergent abilities that exist in large language models but not in small models (Wei et al., 2022a,b). Typically, models with more than 30 billion parameters exhibit the strong mathematical reasoning ability (Brown et al., 2020). The second belief is the seminal \"superficial alignment\" hypothesis (Zhou et al., 2023), which asserts that \"A model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users.\". According to this hypothesis, the alignment process, primarily through supervised fine-tuning (SFT), does not inject new knowledge or improve inherent abilities but rather adjusts the output response format.",
            "score": 0.4794383217809577,
            "section_title": "Introduction",
            "char_start_offset": 459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 536,
                    "matchedPaperCorpusId": "258212542"
                },
                {
                    "start": 669,
                    "end": 692,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 907,
                    "end": 923,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 1345,
                    "end": 1365,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092529296875
        },
        {
            "corpus_id": "270067747",
            "title": "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs",
            "text": "Large Language Models (LLMs) are sequentially trained on general pre-training corpus, pairs of instruction-response and preference-alignment datasets, thus covering tasks involving writing (Touvron et al., 2023a;Jiang et al., 2023;Blum and Blum, 2023;Pan, 2021), math (Imani et al., 2023; Figure 1: SFT on domain data injects domain knowledge into general LLMs. CF aims to keep the LLM performance on the general tasks after training on domain tasks. While GCI aims to enhance the performance on domain tasks by the integration of general capabilities with domain knowledge. Then the LLM is applied to domain-specific scenarios. Liu et al., 2023;Azerbayev et al., 2023), code (Bui et al., 2023;Chen et al., 2021;Rozi\u00e8re et al., 2023), etc. Many popular domain-specific LLMs are finetuned from general chat LLMs (Xiong et al., 2023;Wang et al., 2023a;Yu, 2023). The straightforward procedure is illustrated in Figure 1. \n\nResearchers have identified a challenge known as Catastrophic Forgetting (CF) (Kaushik et al., 2021), where the model's recent learning overshadows and diminishes its previously acquired capabilities and knowledge, leading to a significant performance drop on previous tasks. Current studies to mitigate CF focus on preserving the general capabilities. However, this paper investigates how to effectively harmonize and utilize both general capabilities and domain-specific knowledge, rather than mitigate CF. Our rationale stems from the observation that, even with CF resolved, general capabilities often encounter difficulties integrating with domain-specific knowledge. \n\nSpecifically, we illustrate the enhancement of GCI in legal domain through Figure 2. A general chat LLM focuses on computing solutions for math queries, delivering numerical results. However, with SFT on legal knowledge, the LLM shifts its approach to presenting relevant law article content, rather than providing the calculation result and conclusion, despite users potentially preferring the latter. An optimal GCI-equipped LLM maintains its general capabilities while integrating legal knowledge contextually at the appropriate time steps.",
            "score": 0.47812220377273995,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0014104843139648438
        },
        {
            "corpus_id": "274130626",
            "title": "Efficient Alignment of Large Language Models via Data Sampling",
            "text": "Large Language Models (LLMs) trained on huge corpus of data have shown considerable performance in distilling the knowledge and acquiring a wide variety of skills and capabilities [26], [27], [29]. Usually, the model is specialized using domain adaptive training, fine-tuning or instruction tuning [19], [28], [12]. However, this specialized model may still produce results which humans may not find desirable. Recently, aligning LLMs with human feedback has proven to significantly improve their ability to produce ethical, factual, and helpful outputs. As demonstrated by [18] and [24], alignment techniques have become an indispensable part of the post-training process for LLMs, ensuring that these models adhere to human values and preferences. \n\nAlignment datasets usually come from human feedback which is expensive to collect, curate, and standardize. Usually, alignment techniques such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) take pairwise preference data which is difficult to collect due to its scarcity in the real world [6], [21], [5]. Recent alignment strategies such as KTO proposed by [8] aim to overcome the problem by converting the feedback into a binary signal and achieve SOTA performance. While binary preference data is easier to collect compared to pairwise preference data, the data management cost is still substantial. \n\nThere have been numerous studies in the LLM pre-training and fine-tuning paradigms which show that a small high-quality dataset is sufficient, contributing to lower costs [30], [22], [16]. In this work, we explore data efficient strategies for LLM alignment to save the overall LLM alignment cost. Specifically, we aim to answer the following research questions: \n\n\u2022 How does LLM alignment scale with data? Are we using more data than necessary? (RQ1) \u2022 How can we identify optimal subsamples to sufficiently align LLMs? (RQ2)",
            "score": 0.47700421467032283,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1764
                },
                {
                    "start": 1767,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0030879974365234375
        },
        {
            "corpus_id": "274342032",
            "title": "Baichuan Alignment Technical Report",
            "text": "Finally, the LLM inspector evaluates the quality of the generated prompts, assigns scores, and provides suggestions for improvement. This process is iterated until all constraints present in the response are fully encapsulated within the prompt. \n\nTextbook While simple imitation learning can efficiently acquire relatively straightforward instructions, it often results in only a superficial understanding when applied to complex instructions. To address the issue where models may know the outcome but lack comprehension of the underlying reasoning, we propose a method of instructional imitation. By explicitly focusing on learning the intent of the instruction, the thought process, analogical reasoning, and alignment principles, we can significantly enhance the efficiency of instruction learning and improve the model's generalization and alignment capabilities. Specifically, for complex instructions, we leverage LLMs to generate responses that are highly aligned with the prompts, as illustrated in Figure 5 (d). For example, when tasked with understanding an instruction, the model might be prompted to \"explain how to comprehend the instruction, analyzing its details and challenges from multiple dimensions such as professionalism, complexity, and output constraints.\" Alternatively, LLMs can be guided to articulate the thought process behind a solution, such as \"elaborate on the reasoning process behind the solution, including relevant domain knowledge, reasoning methods, and common pitfalls, particularly focusing on erroneous reasoning paths and easily overlooked output constraints like format, style, tone, length, and word count limitations.\" Additionally, prompts like \"you need to learn the reasoning techniques and background knowledge mining from the given examples. For instance: xx\" can be fed to the LLM to endow it with the ability to generalize and adapt to different task scenarios. Some alignment principles are also taught, such as \"avoid using technical terms or complex expressions whenever possible\" and \"consider the user's emotional state,\" which are common response techniques. This approach ensures a deeper understanding and more robust application of complex instructions, thereby enhancing the overall capability of the model to generalize and align with diverse instructional requirements.",
            "score": 0.47693553458392474,
            "section_title": "Constraint Instruction Expansion",
            "char_start_offset": 35185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006984710693359375
        },
        {
            "corpus_id": "270123022",
            "title": "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models",
            "text": "In this section, we try to answer the question: Does the final student RM (S T ) help with the alignment of other base SFT models? Specifically, we experiment with a \"Mistral-7B-SFT-Beta\" (Tunstall et al., 2023) base policy model and compare the aligned model after one alignment iteration to Zephyr-7B- Additionally, our approach matches the performance of Zephyr-7B-Beta, a strong DPO-aligned model using 64k high-quality GPT-4 annotated preference data. Although our student RM is significantly smaller than GPT-4, it effectively leverages the distilled knowledge from the teacher model, enabling policy models to achieve comparable results. The performance of Zephyr-7B-Beta and our model complement each other, as each model excels on different datasets. This suggests a promising future exploration of combining offline with online preference data for policy model alignment. \n\nFurthermore, we observe that the updated student RM outperforms the base student RM, indicating that the teacher's ranking capabilities have been effectively distilled into the student RM through our teacher-student collaborative mechanism. However, we also observe that DPO alignment with the initial student RM outperforms that with the final student RM on Harmless Base and Beavertails. This is because the initial student RM is trained on human data that includes both helpfulness and harmlessness preferences (refer to \u00a73.2), while the teacher RM is not optimized for harmlessness (Cui et al., 2023). Throughout the alignment iterations, the teacher's strengths in identifying helpful responses and its weaknesses in recognizing safe responses are gradually transferred to the students. Since helpfulness and harmlessness are conflicting objectives, balancing them is outside the scope of this paper (Dai et al., 2023;Touvron et al., 2023). Future research may focus on better controlling the 3 SPIN is a strong self-evolution alignment method at the 7B scale, utilizing iterative supervised fine-tuning. It can be downloaded from https://huggingface.co/UCLA-AGI/ zephyr-7b-sft-full-SPIN-iter2. \n\ntype of knowledge transferred from the teacher to the student.",
            "score": 0.47656283854809145,
            "section_title": "Transfer RM to Another Policy Model",
            "char_start_offset": 15650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2083
                },
                {
                    "start": 2086,
                    "end": 2148
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018968582153320312
        },
        {
            "corpus_id": "268819411",
            "title": "Can LLMs get help from other LLMs without revealing private information?",
            "text": "LLMs are effective at creating bootstrapping datasets, e.g. by creating task instructions through their own conditional generation (Wang et al., 2023).Similarly, Lee et al. (2023) have shown how alignment data can be synthesized.The student model needs to have such bootstrapping capabilities and the richer this ability is, the better it produces diverse task transformations that the teacher can better use to explain it back.",
            "score": 0.4742580822083737,
            "section_title": "Synthetic Datasets",
            "char_start_offset": 24895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 428
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013093948364257812
        },
        {
            "corpus_id": "264833257",
            "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design",
            "text": "After DAPT, we perform model alignment. We specifically leverage two alignment techniques: supervised fine-tuning (SFT) and SteerLM (Dong et al., 2023). We adopt the identical hyperparameter training configuration as DAPT for all models, with the exception of using a reduced global batch size of 128. We employ an autoregressive optimization objective, implementing a strategy where losses associated with tokens originating from the system and user prompts are masked (Touvron et al., 2023). This approach ensures that during backpropagation, our focus is exclusively directed towards the optimization of answer tokens. \n\nWe combined our domain alignment dataset, consisting of approximately 1.4k samples, with larger general chat datasets. For SFT, we blended the domain instructional data with 128k commercial-viable chat data and then performed fine-tuning for a single epoch after random shuffling. \n\nWe conducted experiments involving augmentation of the domain-specific SFT dataset for more than one epoch. However, it became apparent that the model rapidly exhibited signs of overfitting when presented with in-domain questions, often repeating irrelevant answers from the domain SFT dataset. For SteerLM, we closely followed the steps outlined in (Wang et al., 2023). We first trained an attribute model instantiated with LLaMA2-13B model on the Help-Steer and OASST datasets. We then used the attribute model to label all attributes for OASST data and our domain instructional data. Finally, we conducted attribute-conditioned fine-tuning and also masked the attribute labels and trained on ChipNeMo models for 2 epochs. We refer readers to Appendix A.4 for details on the alignment datasets and A.7 on implementations details. \n\nWe also experimented with DAPT directly on a chat aligned model, such as the LLaMA2-Chat model. We found that DAPT significantly degraded the model's alignment, making the resulting model useless for downstream tasks.",
            "score": 0.4741240097137953,
            "section_title": "Model Alignment",
            "char_start_offset": 11779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1958
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0015192031860351562
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "In the domain knowledge learning stage, the LLM has simultaneously learned to both memorize domain knowledge and understand how to utilize the knowledge through our proposed knowledge mixture continual pre-training.After that, during the format alignment stage, the LLM can more efficiently fine-tuned to master the task format with only a small number of alignment samples.Next, we first introduce the selection of training samples and then perform general format alignment.\n\nSince we would like to decouple knowledge learning and format alignment, we mainly focus on training samples from D SFT and D DPO that are are both easy and have been encountered during continual pre-training, which avoids introducing new knowledge during supervised fine-tuning.These easy samples are selected based on the perplexity scores of the LLMs w.r.t the ground-truth output.Specifically, given a sample in D SFT and D DPO , we first equip the input instruction and output response with the corresponding chat template, i.e., the format for interaction with humans.Then, we feed the formatted sequence into the LLM and compute the perplexity score for the output response.Finally, we select top-K samples with the lowest perplexity scores to conduct the supervised fine-tuning and direct preference optimization.Note that for samples in D DPO , we only utilize the positive response for computing its perplexity score.\n\nAfter selecting the training samples, we next utilize them to conduct efficient format alignment.Firstly, we utilize the selected easy instruction samples from D SFT to perform supervised fine-tuning based on the LLM after continual pre-training following the standard way (Ouyang et al., 2022b), which is to minimize the cross-entropy loss:\n\nwhere r j and r <j denote the j-th token and its previous tokens in the response.Secondly, we further utilize the selected easy preference samples from D DPO to conduct direct preference optimization following its original method (Rafailov et al., 2023b) as follows:\n\nwhere \u03c3 denotes the sigmoid function, \u0398 and \u0398 ref denote the parameters of the updated LLM and reference LLM during the direct preference optimization process, and \u03c0 denotes the product of the probabilities of all output tokens, conditioned on the given input.",
            "score": 0.47351787864928463,
            "section_title": "EFFICIENT FORMAT ALIGNMENT",
            "char_start_offset": 13568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 475
                },
                {
                    "start": 477,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1404
                },
                {
                    "start": 1406,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1747
                },
                {
                    "start": 1749,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 2015
                },
                {
                    "start": 2017,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 1679,
                    "end": 1701,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1979,
                    "end": 2003,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002002716064453125
        },
        {
            "corpus_id": "266999538",
            "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples",
            "text": "The technique to make Large language models (LLMs) follow human instructions and generate safe outputs is alignment (Ouyang et al., 2022). Currently, it is the key to generating sophisticated text and tackling a variety of language-based tasks (Brown et al., 2020;Bubeck et al., 2023;OpenAI, 2023;Liu et al., 2023). The mainstream alignment approaches include instruction fine-tuning (Wei et al., 2021) and preference learning (Ouyang et al., 2022). Instruction tuning employs a supervised fine-tuning (SFT) process, largely dependent on human annotations or data derived from LLMs themselves. The key technique in preference learning is reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), which iteratively refines an SFT-enhanced LLM to better align with human preferences. \n\nBoth SFT and RLHF are heavily data-dependent. The lack of high-quality data significantly blocks the democratization of usable and safe LLMs. In this work, we explore scenarios with limited examples from the target alignment domain such as safety, truthfulness, and helpfulness. A few prior works propose to solve this problem with self-alignment (Wang et al., 2022;Sun et al., 2023c), i.e. making the LLMs align themselves with samples generated by themselves. The common assumption is the pretrained LLMs have already learned a good amount of hidden knowledge related to the aligned behaviors and we just need to \"elicit\" it with samples generated by LLMs themselves rather than using direct human instructions. \n\nHowever, the current self-alignment techniques are not truly free of human instructions. They still involve some form of hand-crafted instructions or principles to enhance the quality of the model-generated responses. It leads to two limitations: (1) Crafting effective human instruction is complex. For example, Sun et al. (2023c) needs to manually design 16 generic principles and multiple specific principles for different tasks. It requires substantial domain knowledge and risks erring at a higher level compared to a more bottom-up data-driven approach.",
            "score": 0.47343087394202527,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 137,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 244,
                    "end": 264,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 427,
                    "end": 448,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 688,
                    "end": 709,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003749847412109375
        },
        {
            "corpus_id": "278602959",
            "title": "Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning",
            "text": "A key challenge for large language models (LLMs) is the misalignment between their training objective-minimizing word prediction error-and users' expectation for helpful instruction adherence [9,38,42]. Instruction tuning effectively bridges this gap by training on (INSTRUCTION, OUTPUT) pairs, which shifts models beyond simple next-word prediction [19,37,46,51]. These datasets typically incorporate annotated natural language data, providing explicit task guidance [33,48], or LLM-generated outputs from curated instructions, enhancing the quality of interactions [4,6,56]. However, instruction tuning primarily refines communication rather than imparts new knowledge, as studies suggest that LLMs acquire most of their capabilities during pretraining [17,57]. Our work emphasizes the importance of aligning instruction data with human cognitive patterns [32,34], while maintaining structured information, which enables models to better understand scene interactions through human-like reasoning, instead of merely memorizing factual knowledge. By focusing on this alignment, we aim to improve the models' utility in real-world applications, ensuring they respond more effectively to user queries.",
            "score": 0.4730093578930844,
            "section_title": "Related Works 2.1 Instruction Tuning",
            "char_start_offset": 5502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 192,
                    "end": 195,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 350,
                    "end": 354,
                    "matchedPaperCorpusId": "4492210"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 360,
                    "end": 363,
                    "matchedPaperCorpusId": "259075356"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 570,
                    "end": 572,
                    "matchedPaperCorpusId": "265308687"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 759,
                    "end": 762,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0026721954345703125
        },
        {
            "corpus_id": "277104402",
            "title": "Aligning Multimodal LLM with Human Preference: A Survey",
            "text": "In this section, we will provide a brief explanation of the complete training process for MLLMs, which consists mainly of three phases (Figure 3): pre-training, instruction tuning, and alignment with human preference. \n\nPre-Training. The pre-training phase of MLLMs primarily aims to align the feature spaces of different modalities with that of the language model. The data used in this phase is typically simple caption data. For instance, imagecaption pairs are commonly used for image/video understanding MLLMs [108], [109], while speech data and transcriptions are used for speech understanding MLLMs [110], [8]. Through this pre-training phase, the model learns to understand inputs from various modalities. Instruction Tuning. Building on the pre-training phase, the SFT phase aims to teach the model how to interact with humans by focusing on understanding questions and providing responses in a specified format, i.e., instructionfollowing ability. The data used in this phase is typically high-quality and diverse dialogue data. For example, in the commonly seen visual question answering (VQA) task, given an image and its corresponding instruction, the trained model will provide the correct answer for the task. \n\nAlignment with Human Preference. Previous works have shown that SFT tends to make the model memorize training data and try to generalize across diverse scenarios [111]. The alignment phase, typically involving reinforcement learning (RL) strategies, is crucial for generalizing to unseen domains. However, most multimodal models neglect this step [3], [4], [5], [6], [7]. The goals of alignment stage are broad, such as reducing hallucinations [112], [21], enhancing conversational abilities [28], improving safety [113], strengthening the reasoning abilities [29], improving capabilities for long-reasoning tasks like DeepSeek-R1 [38], and overall MLLM performance [24]. This phase usually uses pair data that incorporates human preference.",
            "score": 0.4724434442262063,
            "section_title": "BACKGROUND: MLLM ALIGNMENT",
            "char_start_offset": 4100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1968
                }
            ],
            "ref_mentions": [
                {
                    "start": 1719,
                    "end": 1723,
                    "matchedPaperCorpusId": "273098155"
                },
                {
                    "start": 1742,
                    "end": 1747,
                    "matchedPaperCorpusId": "267413047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0027790069580078125
        },
        {
            "corpus_id": "266163054",
            "title": "Is Ignorance Bliss? The Role of Post Hoc Explanation Faithfulness and Alignment in Model Trust in Laypeople and Domain Experts",
            "text": "Post hoc explanations have emerged as a way to improve user trust in machine learning models by providing insight into their decision-making (Arrieta et al., 2020), particularly in high-stakes settings such as medicine (Yu et al., 2018), law (Walters and Novak, 2021), and finance (Cao, 2022). These explanations are often evaluated based on their alignment with human knowledge (\"alignment\") (Smilkov et al., 2017;Sundararajan et al., 2017;Saporta et al., 2022). The more explanations align with the knowledge of domain experts, the more accurate they are deemed to be. However, the faithfulness of an explanation with respect to the model (\"faithfulness\"), a fundamental criterion, is often overlooked. Explanation faithfulness is an important criterion when evaluating post hoc explanations because an explanation may seem valid to a user based on its alignment with domain knowledge, but completely misrepresent what happens in a model. Furthermore, the effect of ex-planation faithfulness and alignment in user trust and whether this effect differs among laypeople and domain experts is unclear. \n\nTherefore, this study focuses on the following two research questions: (1) For a post hoc explanation, how do the faithfulness and the alignment of an explanation influence user trust in the model? (2) How does this phenomenon differ for laypeople versus domain experts? \n\nTo address these questions, we conduct a user study examining how post hoc explanations affect model trust for laypeople and domain experts. In doing so, we aim to understand how explanation faithfulness and alignment influence user trust and how the presence of highly-specialized domain knowledge influences how users reason about explanation faithfulness and alignment. This work is significant because, by understanding factors that influence user trust in models, how these factors differ among laypeople and domain experts, we can assess whether these users (who often are not machine learning experts) are basing their trust in correct factors. This is especially critical in high-stakes settings where domain experts may use post hoc model explanations to make consequential decisions. \n\nWe find that, when determining the extent to which to trust models using post hoc explanations, laypeople base their trust on explanation faithfulness while domain experts base their trust on explanation alignment.",
            "score": 0.4720049689830744,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2169
                },
                {
                    "start": 2172,
                    "end": 2386
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 236,
                    "matchedPaperCorpusId": "80789013"
                },
                {
                    "start": 415,
                    "end": 441,
                    "matchedPaperCorpusId": "16747630"
                },
                {
                    "start": 441,
                    "end": 462,
                    "matchedPaperCorpusId": "250602206"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00426483154296875
        },
        {
            "corpus_id": "272592928",
            "title": "Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization",
            "text": "\u2022 By leveraging both weak and strong models, our approach potentially overcomes limitations of human oversight in complex tasks. However, despite these promising results, several limitations and challenges remain, echoing concerns identified in recent alignment research (Ouyang et al. 2022;Bai et al. 2022;P\u00e9rez-D'Arpino, Khurshid, and Shah 2020): \n\n\u2022 While an improvement over baseline approaches, naive finetuning on weak supervision is insufficient to fully recover strong model performance, indicating the need for more sophisticated transfer techniques. \u2022 Generalization remains inconsistent across tasks, with complex domains like reward modeling proving particularly challenging. This highlights the need for more robust and adaptive alignment techniques. \u2022 Our current setup, though an advancement over previous methods, may not fully capture the difficulties of aligning superhuman AI systems, such as the potential ease of imitating human-level errors. \n\nThese limitations point to several promising directions for future work: \n\n\u2022 Developing more sophisticated debate mechanisms to enhance the quality and efficiency of knowledge transfer between models of varying capabilities. \u2022 Exploring the integration of our approach with other scalable oversight methods to create more comprehensive alignment frameworks. \u2022 Investigating the application of this framework to even more advanced AI systems and diverse task domains to test its scalability and generalizability. \u2022 Refining our understanding of model behavior and alignment across increasing scales of capability to better address the challenges of aligning superhuman AI systems. \n\nIn conclusion, our results demonstrate that weak-tostrong generalization is a promising approach for model alignment, capable of eliciting strong capabilities from limited supervision. By bridging the gap between explanation generation and model alignment, our framework opens new avenues for creating AI systems that are not only powerful but also fundamentally aligned with human values and intentions. While significant challenges remain in scaling this approach to more complex tasks and truly superhuman models, the insights gained from this work provide a solid foundation for future research in AI alignment and safety.",
            "score": 0.47122273855478714,
            "section_title": "Error Analysis",
            "char_start_offset": 28024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 291,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00809478759765625
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "Furthermore, given the potential risk of SFT leading to the forgetting of pre-training knowledge, the question of how to achieve cross-lingual alignment without training remains underexplored.\n\nTo bridge these gaps, our study conducts an indepth examination of the impact of SFT on crosslingual generation.We investigate the influence of SFT on the decoding patterns of foundation models in cross-lingual contexts, hypothesizing that the success of SFT largely hinges on the selection of initial prior tokens that are critical for eliciting taskspecific generation in the target language.Furthermore, the observed decoding similarities between Instruction: Translate the following sentence from English to Ukrainian: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.foundation and SFT models support the extension of the superficial alignment hypothesis to crosslingual scenarios.Responding to these insights, we introduce a training-free alignment method named \"PRETTY\" for cross-lingual and non-English tasks.\n\nThe Prefix TexTs act as a Yarn (PRETTY) linking the foundation LLM and the SFT LLM, eliciting the foundation LLM to exhibit near-SFT performance levels.Specifically, we augment the original input with a few tokens that serve as decoding priors, and then prompt the foundation LLM to resume decoding based on this modified input.In most cases, only one or two task-related prior tokens are needed, and the method for constructing these prior tokens is flexible across various kinds of language resources, fostering the democratization of multilingual LLMs.\n\nWe conducted experiments on machine translation (Goyal et al., 2022), cross-lingual summarization (Bhattacharjee et al., 2023) and non-English part-of-speech (POS) tagging (Liang et al., 2020) tasks across eight languages.These tasks exemplify cross-lingual generation and multilingual language understanding, and they provide ample non-English test data to evaluate effectiveness across varying levels of resource availability.",
            "score": 0.46943624410883344,
            "section_title": "Introduction",
            "char_start_offset": 1784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1050
                },
                {
                    "start": 1052,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1607
                },
                {
                    "start": 1609,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 1707,
                    "end": 1735,
                    "matchedPaperCorpusId": "258947845"
                },
                {
                    "start": 1781,
                    "end": 1801,
                    "matchedPaperCorpusId": "214794966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0296478271484375
        },
        {
            "corpus_id": "267897555",
            "title": "Unintended Impacts of LLM Alignment on Global Representation",
            "text": "First, we identify models with checkpoints at different stages of the alignment process (See Figure 2) so that we can measure the effects of each stage. \n\nSupervised Fine-tuning. In the supervised finetuning stage, the model is provided with prompts and example completions and fine-tuned to produce these sorts of completions. Popular SFT datasets for chat models include the human-written Flan3 (Wei et al.) and Open Assistant (K\u00f6pf et al., 2023) datasets, and the synthetic ShareGPT4 , Alpaca (Taori et al., 2023), and Open-Orca (Lian et al., 2023) datasets. All are variants of instruction following completions to task-oriented prompts. Typically, this step is used to make language models follow instructions rather than continue the input text based on the language modeling objective. \n\nPreference Tuning. After SFT, models undergo preference tuning, where a dataset of prompts and preference-ranked completions are used to align LLMs with user preferences. Two popular algorithms for preference tuning are Proximal Policy Optimization (PPO) (Schulman et al., 2017), which is used in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), and Direct Preference Optimization (DPO) (Rafailov et al., 2023). For RLHF, a reward model is trained, which takes in a prompt and completion and outputs a score predicting the degree of human preference for such an output, whereas, in DPO, the model is updated directly using the preference dataset. \n\nDeployment After alignment, language models are either deployed inside a product or released for broader use. Notably, these models are a core technology that enables higher-level user-facing systems. While model developers may intend a specific audience, open-access models can be adopted anywhere and major LLM APIs are globally accessible 5 . As a result, due to the broad nature of their possible utility, even unintended impacts of alignment can affect their global adoption.",
            "score": 0.4692465198045747,
            "section_title": "Alignment Process",
            "char_start_offset": 6880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00269317626953125
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "To enhance the capabilities of LLMs in question answering and addressing ethical issues, SFT and alignment are typically applied in a sequential manner due to the distinct nature of these tasks. However, this sequential approach often leads to performance degradation on some tasks. In this study, we introduce UFT which integrates SFT and alignment into a single stage, thereby mitigating this problem. \n\nTo be more specific, the instruction-tuning data consist of a prompt x and a corresponding response y, where the response y is considered of high quality, typically labeled by experts in the field. In comparison, the alignment data include a prompt x, response y, and feedback r. This feedback can be categorized as desired or undesired for pairwise feedback, positive or negative for binary feedback, or a scalar in the interval [0, 1] for general score-based feedback. Due to the high quality of instruction-tuning data, they can be regarded as data with a score of 1, i.e., positive feedback. With this consideration, the instruction-tuning dataset can be transformed into alignment data in the format of prompt x, response y and feedback r = 1, which is the highest reward in consideration. Eventually, the transformed instruction-tuning dataset and the alignment dataset can be merged for fine-tuning LLM using the loss function in UNA as shown in Eq. 6. Our experiments demonstrate that when finetuning using only instruction-tuning dataset, UFT can outperform SFT on downstream tasks, which we attribute to the KL divergence term that focuses on minimization with the pretrained model, a factor ignored in the SFT processes. Additionally, the results indicate that mixing the instructiontuning data with alignment for UFT can prevent the performance degradation on some tasks and outperform previous sequential methods. Lastly, we discover that the distribution of mixed data, i.e., the proportion of instruction-tuning data and alignment data will impact the performances of LLMs. More details can be found in the experiment section. \n\nA heuristic proof why UFT can replace SFT is provided by arguing that they achieve the same goal of maximizing the probability of \u03c0 \u03b8 (y|x) for prompt x and response y in the instruction-tuning dataset.",
            "score": 0.4691743767772256,
            "section_title": "UFT: Unify SFT and Alignment",
            "char_start_offset": 5137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001934051513671875
        },
        {
            "corpus_id": "270440784",
            "title": "Deep Exploration of Cross-Lingual Zero-Shot Generalization in Instruction Tuning",
            "text": "In this section, we analyze whether the performance improvements in cross-lingual transfer through instruction alignment originate from linguistic factors or formatting.To do this, we employ mT-Ko and two of its variants and evaluate their performance on the held-out tasks from the P3 benchmark.The first variant, mT-Ko-Trans, merely employs a translated version of P3 templates into Korean during the inference phase.This variant aligns linguistic aspects of templates between training and inference.The second variant, mT-Ko-CI, as mentioned in Section 3.3, encompasses instruction alignment that considers both linguistic and instructional format aspects during inference.Further illustrative examples are available in the Appendix A.5.\n\nTable 2 shows that mT-Ko-Trans demonstrates consistently improved performance compared to the mT-Ko evaluated using P3 templates, across most of the tasks.This observation proves that notable performance enhancement is achievable through linguistic alignment alone, as it guides the model to better adapt to new unseen templates.When comparing the performance of mT-Ko-Trans with that of mT-Ko-CI, it becomes evident that the latter achieves higher performance.This result is attributed to the fact that while both approaches entail linguistic alignment between the training and  evaluation templates, mT-Ko-CI additionally gains cross-task generalization through the alignment of structural formatting between training and evaluation templates.The result highlights that both linguistic and instructional format alignment is important in cross-lingual generalization.",
            "score": 0.4690877786787337,
            "section_title": "Template alignment: Linguistic Or Instructional Format",
            "char_start_offset": 20393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 740
                },
                {
                    "start": 742,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1071
                },
                {
                    "start": 1071,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0036640167236328125
        },
        {
            "corpus_id": "262824801",
            "title": "Large Language Model Alignment: A Survey",
            "text": "These methods convert human intents and preferences into text-based feedback signals to achieve alignment, which can be considered as an extension to the SFT process. Chain of Hindsight (CoH)  draws inspiration from human learning process, especially post-experience adjustments. It aims to align models based on successive outputs paired with retrospective feedbacks. The goal is to fine-tune models to predict the most preferred outputs. In the fine-tuning process, human preferences treated as both a function and training data, ensuring that during inference, the fine-tuned model only generates favorable results. RAFT ) utilizes a reward model to pinpoint model outputs in sync with human preferences. The system uses SFT for alignment. Assuming there exists a trained reward model and a data generator (e.g., an LLM like GPT-4, or even humans), the system mixes data generated from each source. An essential observation is that while outputs need filtering and fine-tuning, the backpropagation is not frequently executed, making the process relatively swift. LIMA (Zhou et al., 2023a) is proposed to validate the assumption that the bulk of knowledge in LLMs is acquired during the pre-training phase. As such, only a minimal amount of instruction-tuning data may be needed to guide the model towards generating desirable outputs. Specifically, the dataset used in LIMA contains only 1000 instruction-response pairs, where 750 of these pairs come from community platforms like Stack Exchange, wikiHow, and Reddit, and the remaining 250 pairs are from self-authored instructions and responses. Their findings reveal that fine-tuning on this dataset is on par with leading LLMs.  find that modeling human preferences solely based on sorting information is inadequate. As a remedy, they introduce Imitation learning with Language Feedback (ILF). ILF operates in three stages: (1) generating various refinements for a given input based on an initial LM output and feedback; (2) selecting the refinement garnering maximum feedback; and (3) fine-tuning the model to maximize the probability of the chosen refinement made to the input. Their work also provides a theoretical analysis showing that ILF parallels Bayesian inference, akin to RLHF. In addition to the above single-agent alignment methods,  introduce stable alignment, a technique designed to learn alignment",
            "score": 0.46907713602953827,
            "section_title": "SL with Text-based Feedback Signals",
            "char_start_offset": 43571,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00217437744140625
        },
        {
            "corpus_id": "269983424",
            "title": "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction",
            "text": "Supervised Instruction Tuning.Supervised fine-tuning of LLMs on open-domain instructionfollowing data (Ouyang et al., 2022) is a promising approach for calibrating LLMs with human values, which is a critical prerequisite prior to their deployment in real-world scenarios (Xu et al., 2023c).Bypassing the complex and unstable proximal policy optimization algorithm Schulman et al. (2017) in the reinforcement learning from human feedback (RLHF) procedure (Ouyang et al., 2022), SFT only requires a high-quality instruction-following corpus collected from GPT-4 (OpenAI, 2023) or human annotator (Zhou et al., 2023;Conover et al., 2023) to tune on.In spite of its simpleness, a surge of recent models (Ding et al., 2023;Xu et al., 2023a;Geng et al., 2023;Xu et al., 2023b) prove the effectiveness of SFT with their impressive performance on both conventional knowledge and reasoning benchmarks (Hendrycks et al., 2021) and newly appeared instruction-following benchmarks (Li et al., 2023d;Zheng et al., 2023).However, Bai et al. (2022) point out that in particular cases, alignment of LLM is a double-edged sword, enhancing instruction-following ability at the sacrifice of capacity on the conventional knowledge and reasoning benchmark, or the alignment tax.Some follow-ups (Dou et al., 2023;Chen et al., 2023) conjecture that low-quality samples and interference of parametric knowledge are the reasons behind this.Different from previous works, in this study we propose a new perspective to understand the root cause of alignment tax.\n\nModel Merging.Model merging is an effective technique to aggregate the capacity of multiple models.",
            "score": 0.46894905048152813,
            "section_title": "Related Work",
            "char_start_offset": 4351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 30,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1415
                },
                {
                    "start": 1415,
                    "end": 1535
                },
                {
                    "start": 1537,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1636
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 123,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 454,
                    "end": 475,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 594,
                    "end": 613,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 753,
                    "end": 770,
                    "matchedPaperCorpusId": "257912848"
                },
                {
                    "start": 892,
                    "end": 916,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002349853515625
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "From the pilot study, we find that better quality data (in terms of factuality) for SFT and DPO does not necessarily yield models with better factual alignment.This is likely because the supervision from RAG contains information unknown to the LLM; thus, fine-tuning on RAG generated responses may inadvertently encourage the LLM to output unfamiliar information.To avoid unknown knowledge from being presented to the LLM, a viable strategy is to create SFT and DPO training data using the generated responses from the LLM itself.\n\n4 Factuality-Aware Alignment\n\nIn the section, we further extend our discussion of factual alignment to encompass more general instructions.Unlike biography generation in Section 3, where factuality is the main alignment objective, human instructions are diverse and complex, necessitating a range of alignment skill sets beyond factuality alone; e.g., logical thinking, problem handling and user alignment (Ye et al., 2024).Thus, conducting factual alignment with the diverse instructions face two main challenges: (1) different instructions may demand distinct skill sets.For example, in Figure 2, instruction 3, \"Please give me a brief history of coffee\", necessitates factual accuracy and concise summarization, while instruction 8, \"Tell me a story about a pig who goes to the moon\", prioritizes creativity and imagination over strict factuality.(2) As recent studies have emphasized (Ye et al., 2024;Hosking et al., 2024), using a single scalar for reward modeling fails to adequately address multiple alignment skill sets and often under-presents the aspect of factuality.\n\nTo tackle the aforementioned challenges, we propose factuality-aware alignment (FLAME ).To address the first challenge, we propose to prompt LLMs to classify whether a given instruction demands the response to be factual, as shown in Figure 2. We then apply the factuality fine-tuning strategy for SFT and DPO discussed in Section 3.2 to those fact-based instructions.Furthermore, to address the second challenge, we employ separate rewards to evaluate the factuality and instruction following capability of a LLM.For simplicity, our work only considers two alignment skill sets: instruction following and factuality.We leave more comprehensive reward modeling to future work.",
            "score": 0.46882478134431393,
            "section_title": "Strategies for Factual Alignment",
            "char_start_offset": 11024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 560
                },
                {
                    "start": 562,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1382
                },
                {
                    "start": 1382,
                    "end": 1610
                },
                {
                    "start": 1612,
                    "end": 1700
                },
                {
                    "start": 1700,
                    "end": 1980
                },
                {
                    "start": 1980,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2229
                },
                {
                    "start": 2229,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 938,
                    "end": 955,
                    "matchedPaperCorpusId": "259991144"
                },
                {
                    "start": 1420,
                    "end": 1437,
                    "matchedPaperCorpusId": "259991144"
                },
                {
                    "start": 1437,
                    "end": 1458,
                    "matchedPaperCorpusId": "263134280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00424957275390625
        },
        {
            "corpus_id": "272367487",
            "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options",
            "text": "The above-mentioned observations underscore the critical need to design decision-making systems that ensure language models are not only helpful but also capable of reflective judgment, particularly in high-stakes environments. Ultimately, our research demonstrates that achieving true helpfulness in LLMs requires more than just following instructions; it demands a capacity for reflective judgment that allows models to question, reason, and even disregard flawed instructions when necessary. \n\nTo sum up, our contributions are as follows: \n\n\u2022 We introduce and measure the concept of reflective judgment in LLMs. \n\n\u2022 We examine the effects of pre-training, instruction tuning, and alignment on reflecting judgment, providing insights into how alignment can impact the balance between helpfulness and reasoning. \n\n\u2022 We analyze how model size influences the ability to refuse to answer questions with incorrect options. We observe an increase in reflective judgment ability with an increase in model size. \n\n\u2022 We compare LLM behavior to human tendencies to blindly follow instructions, raising concerns about how such human tendency might propagate into models during alignment or fine-tuning processes. \n\n\u2022 We provide qualitative and quantitative analysis how popular RLHF datasets have severe data quality issues suggesting that humans may inadvertently transfer their own biases to the models through the annotation process.",
            "score": 0.46860351917487175,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029468536376953125
        },
        {
            "corpus_id": "270045155",
            "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment",
            "text": "The development and deployment of Large Language Models (LLMs) have profound implications across various domains.Our proposed approach, Alignment from Demonstrations (AfD), introduces a significant advancement in the safe and effective alignment of LLMs.This section discusses the broader impacts of our work, considering both the positive contributions and potential risks.\n\nFirst, our research enhances the safety and reliability of LLMs: By using high-quality demonstration data, AfD aligns LLMs with a very general data format.AfD permits a wider application of alignment use the demonstration dataset.\n\nSecond, AfD reduces the dependency on costly and labor-intensive human preference annotations.This not only lowers the financial barriers for developing aligned LLMs but also can potentially accelerate the deployment process, making advanced LLMs more accessible to a wider range of organizations and people.\n\nMoreover, our method can operate without the need for continuous human interaction and external annotators, which helps in preserving the privacy of the data used for model fine-tuning.This is particularly important in domains that handle sensitive information, such as medical records and personal communications.\n\nHowever, there are also potential risks when aligning LLMs with demonstrations.Although demonstration data is typically of higher quality, it is still susceptible to biases that reflect the perspectives and prejudices of the data sources.It is essential to carefully curate and diversify the demonstration datasets to mitigate these biases.In contrast to the prevailing approaches in LLM alignment research, which rely on preference datasets, this work focuses on offline expert demonstration datasets.These datasets are more accessible in realworld applications and serve as the basis for developing algorithms that can surpass the performance of Supervised Fine-Tuning (SFT), the common practice for such datasets.The use of demonstration datasets, combined with the accessibility of the dynamics model, naturally frames the problem as an Imitation Learning (IL) or Inverse Reinforcement Learning (Inverse RL) task.",
            "score": 0.4680510883627944,
            "section_title": "Broader Impacts",
            "char_start_offset": 26330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 374
                },
                {
                    "start": 376,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 606
                },
                {
                    "start": 608,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 916
                },
                {
                    "start": 918,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1232
                },
                {
                    "start": 1234,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1574
                },
                {
                    "start": 1574,
                    "end": 1736
                },
                {
                    "start": 1736,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2151
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00206756591796875
        },
        {
            "corpus_id": "268264604",
            "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models",
            "text": "Considering the costs of SFT and RL, and the fact that most mainstream LLMs are black-box, fine-tuning based alignment approaches become increasingly unaffordable or infeasible.Therefore, another popular paradigm, In-Context Learning (ICL) based alignment, has attracted more attention.This approach leverages the massive knowledge and instruction-following capabilities of LLMs obtained during the pretraining and instruction tuning phases.By directly providing value instructions or K few-shot examples {x i , y i } K i=1 , ICL constrains the generation of the LLM to align with human values, avoiding additional training.In fact, ICL can also be regarded as a kind of imitation learning.By incorporating a shared prompt concept (Xie et al., 2021), c, e.g., values, minimizing the divergence between p(y, x, c) and \u03c0 \u03b8 (y, x, c) can transformed to optimizing:\n\nOmitting the KL regularization term and freezing parameters \u03b8, imitation learning can be viewed as implicit Bayesian inference, inferring the latent concept from given examples x, y, and driving the LLM to generate a connected response.Concretely, the simplest way is to prompt LLMs to generate responses that adhere to human preferences (Ganguli et al., 2023).Han (2023) further retrieves and includes relevant demonstration examples from SFT data and concatenates them with the input prompt.Lin et al. (2023) find that aligned LLMs primarily learn language styles matching human preferences, providing evidence in support of the \"Superficial Alignment Hypothesis\" (Zhou et al., 2023).Based on such findings, they propose to utilize three consistent stylistic examples and a system prompt for alignment.Considering the ever-changing and diverse human values in the real world, On-the-fly Preference Optimization (OPO) (Xu et al., 2023b) leverages a Retrieval-Augmented Generation (RAG) to achieve dynamical alignment.In addition, the generate-then-refine schema (Gou et al., 2023) first generates initial responses and then enables LLMs to verify and rectify their own output.",
            "score": 0.46785870659320095,
            "section_title": "In-Context Alignment",
            "char_start_offset": 33128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 861
                },
                {
                    "start": 863,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04193115234375
        },
        {
            "corpus_id": "262084051",
            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
            "text": "According to the Superficial Alignment Hypothesis proposed by Zhou et al. [83], the capability of a model is rooted in pretraining, and data from downstream tasks acts to activate the inherent ability of LLMs that has been learned during pretraining. There are two important questions that arise from such a hypothesis: (i) what kind of data is most effective at activating possible latent knowledge, and (ii) why is one dataset better than another at such activation? Our empirical results suggest that, in the mathematical tasks we consider, our MetaMathQA dataset may serve as a superior activator of mathematical knowledge. Yet, why MetaMath yields superior performance than training on the data of correct answer-only or GSM8K CoT is unclear. We speculate that perhaps it is $QVZHURQO\\ *60.&R7 $QV$XJ 5HSKUDVLQJ )2%$5 69 the simplicity of the data that matters. As shown in Figure 3, we compute the perplexity [46,72] for the under-finetuned LLaMA-2-7B model, in terms of answer-only data, GSM8K CoT, and the subsections of MetaMathQA data. The perplexity of MetaMathQA is significantly lower than the other two datasets. This highlights its inherently easy-to-learn nature, which may be more conducive to eliciting bolstered problem-solving abilities from an LLM. This is also aligned with the findings with TinyStories [18], where short and easy story data can help LLMs generate content fluently.",
            "score": 0.4677325522519351,
            "section_title": "DISCUSSION FROM A PERPLEXITY PERSPECTIVE",
            "char_start_offset": 18607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1404
                }
            ],
            "ref_mentions": [
                {
                    "start": 74,
                    "end": 78,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09552001953125
        },
        {
            "corpus_id": "270357323",
            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
            "text": "Previous works have shown that more informative and high-quality responses improve the alignment of models (Ji et al., 2023).However, this has only been explored for instruction-tuned LLMs, fully fine-tuned with RLHF, and with limited downstream evaluation.We extend this analysis to Figure 1: Performance comparison for helpful and harmless benchmarks when models are aligned using QLoRA over HH-RLHF (in red ) and BeaverTails (in blue ).We observe better performance when using a more informative and high-quality preference alignment dataset, albeit it is often overfitting for non-instruction tuned models when aligned using DPO (Section 4.1).\n\npre-trained and instruction-tuned models using parameter-efficient SFT and DPO.\n\nSetup To probe the impact of the informativeness and quality of the preference dataset on alignment and downstream performance, we compare the performance of training our models using the two preference datasets, HH-RLHF and Beaver-Tails, on their harmless and helpful splits.Beaver-Tails is supposedly more informative and of better quality than HH-RLHF, particularly for the harmlessness and safe prompts.For detailed analysis, we show comparisons using supervised fine-tuning (SFT) and DPO over the pre-trained Mistral-7b and instruction-tuned Mistral-7b-Instruct models.We also probe the impact of the dataset quality as the training progresses, analyzing the effect on the stability of the training.",
            "score": 0.46707748453976294,
            "section_title": "Impact of Quality",
            "char_start_offset": 10734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 257
                },
                {
                    "start": 257,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 647
                },
                {
                    "start": 649,
                    "end": 728
                },
                {
                    "start": 730,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018606185913085938
        },
        {
            "corpus_id": "271404315",
            "title": "Structure-aware Domain Knowledge Injection for Large Language Models",
            "text": "This work pioneers in incorporating structureaware methodologies to enhance domain knowledge injection into large language models. Through a novel SCPT-SSFT paradigm, we have set a new precedent for adapting LLMs to specialized domains, and the promising and scalable results underscore the viability and potential of our method. We hope to inspire further research in efficient and effective domain adaptation in the LLM community, moving a step closer to models that can truly emulate human intelligence. Limitation. Our two-stage strategy introduces added computational complexity, where taxonomy extraction and data reorganization are required in the SCPT phase, and extra QA syntheses are optionally applied in the SSFT stage. In Appendix B, we provide further discussion with extensive empirical experiments. Despite the additional computational overhead introduced, our method achieves greater overall benefits and can reduce the reliance on large-scale LLMs (e.g., 70B models). We will delve into the investigations in future work. SFT Data-Synthesis. We query Llama3-70B to generate 2,700 QA examples and remove those with over 0.5 F1-Score similarity to test samples to prevent data leakage. During inference, when the model can generate correct answers (corresponding to specific knowledge points) that haven't been seen during the SFT stage, we can ensure the knowledge is injected at the CPT stage and SFT only enhances the instruction-following capability. In practice, merely 13 out of 2700 (around 0.5%) synthetic data have over 0.5 F1-Score and are thus filtered out from the SFT data. \n\nTab. A1 statistics the semantic similarity (measured by BERTScore (Zhang et al., 2020)) between generated and GT questions and answers, and the results emphasize there is no knowledge leakage in the generated SFT data (they share poor semantic similarity across questions, answers, and QAs). MMedBench's robust dataset extends across 6 languages (i.e., English, Chinese, Japanese, French, Russian, and Spanish) and 21 medical fields, which include, but are not limited to, Internal Medicine, Biochemistry, Pharmacology, Psychiatry, and many others. It provides 45,048 training pairs and 8,518 testing pairs for diverse learning and testing scenarios.",
            "score": 0.4663452820589668,
            "section_title": "Conclusion",
            "char_start_offset": 24827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0017271041870117188
        },
        {
            "corpus_id": "270765001",
            "title": "Alignment For Performance Improvement in Conversation Bots",
            "text": "This 'instruction-tuning' procedure enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability.However there is often accompanied by catastrophic forgetting[7],Overfitting(loss of generalization), possible hallucination induction, model safety compromise.\n\nAlignment Training has shown to improve helpfulness and reduce harmlessness of models.These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model [8] then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, proximal policy optimization.There has been other recent alignment techniques which have shown to optimize the same loss but without a need of explicit training of a reward model like Direct Preference Optimization(DPO) [1] , Identity Preference Optimization(IPO) [2],Kahneman-Tversky Optimization(KTO) [3].\n\nWe have used Alignment to improve Instruction Adherence in Conversation Bot setting where the Bot is expected to adhere follow certain Gaurdrails/playbook.There has been other works that show that Alignment(especially RLHF) [9] [10] can help to improve instruction following but ours is the first work that shows alignment can be used as an alternative to SFT in certain domains at least(where the notion of \"negative\" is clear) and also help in Feedback Driven Improvement all that without a need of explicitly training a reward function.",
            "score": 0.46576545989106316,
            "section_title": "III. RELATED WORK",
            "char_start_offset": 2711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 316
                },
                {
                    "start": 318,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 986
                },
                {
                    "start": 988,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "121987403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003063201904296875
        },
        {
            "corpus_id": "261705916",
            "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
            "text": "Though numerous technical solutions have been proposed in the survey for hallucinations in LLMs, there exist some potential directions: \n\n\u2022 Data Construction Management. As previously discussed, the style, and knowledge of LLMs is basically learned during model pre-training. High quality data present promising opportunities for facilitating the reduction of hallucinations in LLMs (Kirstain et al., 2022). Inspired by the basic rule of machine learning models: \"Garbage input, garbage output\", Zhou et al. (2023) proposes the superficial alignment hypothesis, which views alignment as learning to interact with the user. The results of simple fine-tuning on a few high-quality samples demonstrate that data quality and diversity outweigh the importance of fine-tuning large-scale instructions (Mishra et al., 2021;Wei et al., 2022a;Sanh et al., 2022) andRLHF (Bai et al., 2022;Ouyang et al., 2022). To perform efficiently in knowledge-intensive verticals, we argue that construction of entity-centred fine-tuned instructions (Bao et al., 2023;Gui et al., 2023;Wei Zhu and Wang, 2023) is a promising direction that it can combine the structured knowledge and semantic relevance of knowledge graphs to enhance the factual-ity of generated entity information. Another feasible proposal is to incorporate a self-curation phase (Li et al., 2023g) in the instruction construction process to rate the quality of candidate pairs. During the iteration process, quality evaluation (Chen et al., 2023c) based on manual or automated rule constraints could provide self-correction capacity. \n\n\u2022 Downstream Task Alignment. Generic LLMs have a certain degree of natural language problem comprehension in a variety of open environments. However, the main problem still remains in the deviation from the application requirements, which leads to emergence of diverse hallucinations. Thus, downstream task alignment especially built on vertical domain cognition necessitates expanded symbolic reasoning, decomposition and planning of complex tasks, and faithful external knowledge injection. Specifically, while expert in language processing, LLMs struggle to make breakthroughs in mathematical abilities, a deficiency attributable to the textual training objective.",
            "score": 0.4653578533785561,
            "section_title": "Future Directions",
            "char_start_offset": 30965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 138,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 406,
                    "matchedPaperCorpusId": "238583118"
                },
                {
                    "start": 816,
                    "end": 834,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 834,
                    "end": 856,
                    "matchedPaperCorpusId": "239009562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039642333984375
        },
        {
            "corpus_id": "267522812",
            "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
            "text": "Moreover, we benchmark the ability of the models both with head-to-head comparisons, single-score evaluation, and on factual knowledge datasets. Since all these analyses show that our models outperform the baselines across datasets, tasks and evaluation methods, we consider them conclusive evidence that our results are not due to a length bias of the LLMs as judges. \n\nConclusions. In this work we have shown that using reply length as a heuristic can effectively pre-select instructions for LLMs alignment in SFT. Moreover, a straightforward refinement step is enough to create a dataset of only 1k instruction-response pairs which yields competitive results compared to complex alignment methods like RLHF and DPO. Thus, this approach constitutes an inexpensive yet strong baseline for future works on alignment. Our analysis also challenges the current understanding of high-quality IFT datasets and their impact on fine-tuned model performance in standard NLP benchmarks. We emphasize that a major aspect of alignment concerns mitigation of safety risks and ethical use of LLMs. We have not explored this aspect here, as it demands task-specific approaches.",
            "score": 0.4645513035399984,
            "section_title": "Discussion",
            "char_start_offset": 28670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1163
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0026836395263671875
        },
        {
            "corpus_id": "276558084",
            "title": "Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans",
            "text": "While these approaches have significantly advanced our understanding of how conceptual representations align between humans and models, they suffer from a major limitation: they do not reveal where in the model the concepts are stored and make it difficult to draw conclusions beyond coarse alignment. For example, the cosine distance between embeddings might indicate that \"animal\" and \"dog\" are more similar than \"animal\" and \"daffodil\", but it can not tell us if \"dog\" and \"animal\" are processed with similar neural pathways or architectural components, limiting our ability to understand the existence of structures such as hierarchical relationships in the model. \n\nHere, we propose a novel way to study human -LLM alignment in concept representation. We borrow a method from activation steering (Suau et al., 2023(Suau et al., , 2024;;Rodriguez et al., 2025), to identify which neurons are most responsible for processing and understanding of a particular concept, so-called expert neurons. This approach enables us not only to measure alignment between human and model representations, but also to explore additional questions, such as whether LLMs organize concepts in a hierarchy interpretable to humans (e.g., \"dog\", \"cat\", and \"cheetah\" being categorized as \"animal\"). We also track how alignment evolves during training for different model sizes, shedding light on the impact of model capacity on the de-velopment of aligned representations -an aspect largely overlooked in previous work on text-based models (Shen et al., 2024;Wei et al., 2022). Ultimately, understanding these internal structures and factors that lead to mis-alignment can provide valuable insight for designing interventions targeted at guiding model behaviors towards human-like solutions and enhancing their transparency (Fel et al., 2022;Peterson et al., 2018;Toneva, 2022). \n\nIn our experiments, we focus on causal LLMs using the Pythia models (70m, 1b and 12b) for which multiple training checkpoints are publicly available (Biderman et al., 2023). Given a diverse set of concepts across multiple domains (see Sec. 3.2), we identify each LLM's corresponding expert neurons.",
            "score": 0.4643405964294578,
            "section_title": "Introduction",
            "char_start_offset": 1428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 841,
                    "end": 864,
                    "matchedPaperCorpusId": "273695590"
                },
                {
                    "start": 2011,
                    "end": 2034,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00605010986328125
        },
        {
            "corpus_id": "271892126",
            "title": "API-Guided Dataset Synthesis to Finetune Large Code Models",
            "text": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks following extensive pre-training [34,72]. In the domain of code-related tasks, large code models (LCMs) such as CodeLlama [66] and StarCoder [37] have exhibited impressive capabilities in program understanding and generation, supporting various real-world applications. However, despite their vast knowledge acquired through training on enormous datasets, these base models may not achieve optimal performance across all use cases out-of-the-box. As illustrated in Fig. 1(a) and (c), to further align models with diverse requirements-including enhancing general code generation capabilities [38,66] or specializing in specific codebases or domains (supporting commercial products like deep learning [54] or security-related [25] assists)-researchers often employ additional datasets to fine-tune base models, yielding more powerful and customized LCMs. \n\nAmong the various fine-tuning techniques proposed, supervised fine-tuning (SFT) has emerged as a critical approach for enhancing LLM capabilities. SFT leverages the knowledge acquired during pre-training while aligning models with human expectations [6,11,81]. This process involves further training the models on carefully curated instruction datasets, typically comprising formatted instruction-response pairs. These pairs, represented as (INSTRUCTION, RESPONSE), consist of human-provided tasks or queries (INSTRUCTION) and the corresponding desired outputs (RESPONSE) that the model should generate [8,16,52,90]. \n\nGiven the importance of high-quality SFT datasets for LCMs, various approaches have been developed to create and curate such datasets. These methods include the collection of real-world code snippets [82] and the use of programming concepts and keywords (e.g., recursion and loops) to guide LLMs in dataset construction [44]. These efforts have resulted in the generation of extensive datasets, as exemplified by Nemotron-4's 800k examples [3]. While such large datasets offer potential benefits, they also present practical challenges in the SFT process.",
            "score": 0.46379278938089463,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "260140789"
                },
                {
                    "start": 1204,
                    "end": 1207,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1563,
                    "end": 1566,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "270358041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00157928466796875
        },
        {
            "corpus_id": "265551388",
            "title": "Instruction-tuning Aligns LLMs to the Human Brain",
            "text": "Effect of Instruction-tuning on LLMs. Instruction-tuning is an effective method for enhancing LLM capability and controllability. It trains LLMs using pairs of human instructions and desired outputs. The benefits of instruction-tuning can be categorized into three key aspects (Zhang et al., 2023): (1) it bridges the disparity between the pretraining objective of LLMs (next-word prediction) and the goal of accurately following human instructions, (2) it achieves greater control and predictability of model behavior compared to standard LLMs, allowing researchers to make them more similar to humans in both capability and output similarity (Chia et al., 2023;Dasgupta et al., 2022;Safdari et al., 2023), and (3) it often costs only a small fraction of compute relative to pretraining, enabling LLMs to swiftly adapt to target domains (Chung et al., 2022). We contribute to this research area from a neuroscience perspective, by studying whether instruction-tuning makes LLMs more aligned to the human language system in terms of brain and behavioral alignment. \n\nEffect of Finetuning on Brain alignment. Prior works have studied how finetuning affects LMs' alignment to human brain activity. These include finetuning on a wide range of downstream NLP tasks (Oota et al., 2022), finetuning to summarize narratives (Aw & Toneva, 2023), and finetuning to directly predict brain activity recordings (Schwartz et al., 2019). These studies aim to use brain alignment to study how finetuning affects LMs and their representations. (Taori et al., 2023). The input field is optional for certain instructions. \n\nwhy instruction-tuned LLMs align to brain activity by testing the correlation of brain alignment with various world knowledge domains and problem-solving abilities. \n\nLM properties linked to Brain alignment. One exciting research area focuses on disentangling the contribution of various LM properties towards brain alignment.",
            "score": 0.46281127901420194,
            "section_title": "Background & Related Work",
            "char_start_offset": 5198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1261,
                    "end": 1280,
                    "matchedPaperCorpusId": "248505919"
                },
                {
                    "start": 1317,
                    "end": 1336,
                    "matchedPaperCorpusId": "257255248"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00234222412109375
        },
        {
            "corpus_id": "277349741",
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "text": "After pretraining on massive corpus, LLMs obtain the ability to serve as a general problem solver. To adapt them for domain-specific tasks, several post-training techniques can be applied to further refine their capabilities beyond initial pre-training. Three pivotal methodologies in this phase are instruction tuning, alignment tuning, and model adaptation (Zhao et al., 2023;Zhang et al., 2023;Wang et al., 2023f). These techniques enhance task generalization, align outputs with human preferences, and optimize models for domain-specific or resourceconstrained settings, respectively. In the following, we briefly introduce their objectives, methods, and impacts based on contemporary research. \n\nInstruction tuning. Instruction tuning refines LLMs to follow task-specific natural language instructions, enabling zero-shot or few-shot generalization to unseen tasks (Wei et al., 2021;Chung et al., 2024). Unlike conventional fine-tuning, which trains models on labeled examples for specific tasks, instruction tuning employs datasets comprising task descriptions, input-output pairs, and diverse prompts (e.g., \"Summarize this article: \n\n[text]\"). This approach conditions models to infer task requirements from instructions, better comprehend tasks, and satisfy human expectations across diverse tasks. Representative models that perform instruction tuning include InstructGPT (Ouyang et al., 2022) and FLAN-T5 (Chung et al., 2024). \n\nInstruction tuning is closed to supervised fine-tuning (SFT) (Ouyang et al., 2022) and prompt tuning (Liu et al., 2021). SFT performs full-parameter fine-tuning based on pre-trained models using task-specific labeled data (input-output pairs). Instruction tuning is a special form of SFT that fine-tunes a model using instructional task descriptions, with the goal of allowing the model to understand and generalize to unseen instructions. Prompt tuning is a parameter-efficient fine-tuning method (which will be discussed in the latter) that guides the model output by adjusting the prompts in the inputs, usually without updating the pre-trained model parameters, and optimizing only a small number of prompt-related parameters. The difference between the three concepts is relatively small.",
            "score": 0.4626525339436558,
            "section_title": "Post-training",
            "char_start_offset": 36048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 1382,
                    "end": 1403,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1501,
                    "end": 1522,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001987457275390625
        },
        {
            "corpus_id": "277244849",
            "title": "TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment",
            "text": "These results emphasize the importance of self- generated DPO data, as it better aligns with the model's specific capabilities and learning patterns. The findings suggest that while some benefits can be retained when transferring data across sizes, DPO is most effective when the data is tailored to the model's own behavior and representations. \n\nAblation Study on Pre-SFT Alignment To showcase the effectiveness of the Pre-SFT Alignment strategy, we conduct an ablation study comparing the performance of multiple different tuning orders. Figure 5 highlights the performance differences of Qwen2-VL-7B, comparing its results after tuning with various strategies against the original base model. The Pre-SFT alignment strategy consistently outperforms all other approaches across all three benchmarks. This demonstrates its ability to effectively combine the benefits of both realms: enhancing the model's temporal understanding while preserving high levels of instructionfollowing capability. This ablation study underscores the advantages of aligning the model with fine-grained temporal knowledge before instruction fine-tuning, enabling better overall performance.",
            "score": 0.46191386652533284,
            "section_title": "Analysis",
            "char_start_offset": 27995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001964569091796875
        },
        {
            "corpus_id": "277103838",
            "title": "How much do LLMs learn from negative examples?",
            "text": "We still find it incredible that a few hundred negative examples improve the answer accuracy of a pre-trained language model significantly more than thousands of positive examples albeit in a restricted domain. It seems clear that wrong answers to a few hundred training questions cannot give the model much missing information about the test questions. Thus the knowledge to answer these test questions correctly must already reside in the pre-trained model but obfuscated by the probability mass given to other plausible sounding answers. Training with negative examples seems to flip a switch that causes the model to sharply distinguish factually accurate answers from plausible sounding ones. This supports a version of the \"Super-ficial Alignment Hypothesis\" (Zhou et al., 2024): A model's knowledge and capabilities are learnt almost entirely during pretraining, and alignment teaches it not only format and style, but also preference for factual accuracy.",
            "score": 0.4607531227978555,
            "section_title": "Discussion",
            "char_start_offset": 17615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 963
                }
            ],
            "ref_mentions": [
                {
                    "start": 765,
                    "end": 784,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1683349609375
        },
        {
            "corpus_id": "273351402",
            "title": "3DS: Decomposed Difficulty Data Selection's Case Study on LLM Medical Domain Adaptation",
            "text": "LLMs encounter significant challenges when learning unfamiliar or complex knowledge during supervised fine-tuning, particularly when the data was not encountered during pre-training, which can impede domain adaptation. Gekhman et al. (2024) found that models acquire new factual knowledge slowly during SFT, especially when the information diverges from their pre-existing understanding, leading to a higher risk of hallucinations. Ren et al. (2024) further show that when the knowledge introduced during Instruction Fine-tuning significantly differs from what was learned in pre-training, the model struggles to integrate it, causing performance degradation. This highlights the difficulty models face in using pre-training knowledge to understand new concepts. Kang et al. (2024) also emphasize that unfamiliar examples during fine-tuning increase the likelihood of hallucinations, suggesting that high-difficulty data can destabilize the model and negatively impact its ability to adapt to new domains. Together, these findings underscore the risks associated with fine-tuning on excessively difficult data, which can undermine model performance in domain-specific tasks.",
            "score": 0.4606238013945601,
            "section_title": "DATA LEARNABILITY IN LLM SFT",
            "char_start_offset": 7411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011377334594726562
        },
        {
            "corpus_id": "262459267",
            "title": "InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning",
            "text": "In addition to contributing a new financial domain LLM, we are also interested in understanding the behaviors of domain instruction tuning. We find that a diverse set of domain instructions is very effective in \"transforming\" a high-quality foundation model (such as LLaMA-65B) into a high-quality domain-specific model -suggesting the consistency with the Superficial Alignment Hypothesis (Zhou et al., 2023). Second, we discover that generic instructions, like those used in Alpaca (Taori et al., 2023), can detrimentally impact the performance of instruction-tuned models on domain tasks. This underscores the importance of curating domainspecific instructions. Together, these findings provide insights into how to fine-tune a foundation model for a specific domain. \n\nThe rest of the paper is organized as follows. First, we describe our manually curated financial domain instruction dataset. This is followed by an overview of the instruction tuning procedures for developing InvestLM. Subsequently, we present expert evaluation results and report the performance of InvestLM, comparing it with other LLMs on a set of financial NLP benchmarks. Lastly, we conduct studies to better understand the behaviors of domain instruction tuning.",
            "score": 0.460476971729633,
            "section_title": "Claude-2",
            "char_start_offset": 8113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04620361328125
        },
        {
            "corpus_id": "269982995",
            "title": "Exploring Alignment in Shared Cross-lingual Spaces",
            "text": "In Section 4.1 we discussed several results.Here we demonstrate that our findings generalize to other languages.\n\nDeeper layers in multilingual models reveal increased alignment and preserve semantic concepts, contrasting with language-dependent lexical learning in lower layers.We made this observation through qualitative analysis of concepts across different languages we studied in this paper.In Figures 14-16, we present lexical concepts learned within the lower layers of the multilingual Figure 9: Layer-wise alignment of clusters to lexical and semantic properties in mBERT models, contrasting with the aligned semantic concepts found in the higher layers.To verify our hypothesis, we quantify the number of lexical (suffixbased concepts) and semantic concepts in English within the mBERT model.Please see Figure 9 for a layer-wise pattern of concepts.\n\nFine-tuning calibrates the latent space towards higher alignment We consistently higher alignment of concepts as the models were fine-tuned towards a downstream NLP task.Please refer to Figures 11-13 for results across different architectures and languages.We display alignment outcomes in base models (dotted lines) and after they were fine-tuned (solid lines).Please refer to  for additional examples of concepts aligned across various languages.\n\nThe task-specific calibration of the latent space facilitates zero-shot capabilities.In Figures 21-23, we display alignment outcomes using mT5 base models and after tuning them for the machine translation task.We examine language alignment within the encoder, decoder, and between the encoder and decoder.We observe that fine-tuning the models enhances the alignment of latent spaces.Interestingly, this increase in alignment also extends to other languages, despite the fact that the model was not specifically tuned for these zero-shot languages.",
            "score": 0.45921925734314384,
            "section_title": "B Concept Alignment",
            "char_start_offset": 27288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 112
                },
                {
                    "start": 114,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 803
                },
                {
                    "start": 803,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 1032
                },
                {
                    "start": 1032,
                    "end": 1119
                },
                {
                    "start": 1119,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1310
                },
                {
                    "start": 1312,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1860
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004116058349609375
        },
        {
            "corpus_id": "270357475",
            "title": "Phased Instruction Fine-Tuning for Large Language Models",
            "text": "Instruction fine-tuning (IFT) (Ouyang et al., 2022;Longpre et al., 2023), involving training on instruction dataset using standard supervised finetuning method, aligns pre-trained language models to users's intent and has been proven as an effective alignment method to enhance their ability to follow instructions.Large language models (LLMs) are pre-trained on raw text data using maximum likelihood estimation, equipping them with the basic ability to predict the next word (Zhou et al., 2023;Zhao et al., 2023).However, a gap exists between this ability and following user intentions (Thoppilan et al., 2022;Ouyang et al., 2022).To bridge this gap and enable models to complete human end tasks, various instruction fine-tuning strategies have been proposed, including SFT (Ouyang et al., 2022), LIMA (Zhou et al., 2023), Alpaca (Taori et al., 2023;Dubois et al., 2023), Alpagasus (Chen et al., 2024), CoT (Wei et al., 2022), Superfiltering (Li et al., 2024) and Self-instruct (Wang et al., 2023b).Among these, SFT and LIMA employ human-written instruction data for fine-tuning, while strategies like Self-instruct, Alpaca, Alpagasus utilize ChatGPT as a teacher to automatically generate extensive instruction datasets.Their training approach, a one-off IFT on the whole instruction data without differentiating the difficulty levels of instructions, lacks efficiency in enhancing the instruction-following capability of pre-trained models.\n\nstruction, and the output is the answer following the instruction.Diversity (Dubois et al., 2023;Wang et al., 2023b;Zhou et al., 2023) is a crucial aspect of a high-quality instruction dataset, indicating that such datasets are typically extensive, encompassing a wide range of tasks and examples representing different levels of instruction-following difficulty.For instance, tasks like mathematical problem-solving, code writing, entity extraction, and copy generation each present varying levels of difficulty.",
            "score": 0.4590176162688875,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 633
                },
                {
                    "start": 633,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1444
                },
                {
                    "start": 1446,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 51,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 51,
                    "end": 72,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 477,
                    "end": 496,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 612,
                    "end": 632,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 776,
                    "end": 797,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 804,
                    "end": 823,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 980,
                    "end": 1000,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1543,
                    "end": 1562,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1562,
                    "end": 1580,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001239776611328125
        },
        {
            "corpus_id": "261076203",
            "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models",
            "text": "Typically, LLMs are pre-trained with the objective of next token prediction (Brown et al., 2020;Zhang et al., 2022). Though these models have demonstrated impressive zero-shot and few-shot capabilities in some tasks (Brown et al., 2020), which we infer is learned from patterns in the massive training corpus, LLMs still struggle to help users complete diverse tasks given some instructions. Therefore, we take human instructions as the first level of alignment goal, defined as enabling big models to complete diverse tasks that humans instruct them to do. This goal concentrates on the fundamental capabilities of big models to generate narrowly defined correct results, without the expectation of meeting human preferences. Achieving this goal lays the foundation for more advanced alignment levels. \n\nMost studies collect an instruction dataset to perform as a proxy of this alignment goal and finetuned pre-trained LLMs on this dataset in a supervised manner (Wang et al., 2022a;Chung et al., 2022;Longpre et al., 2023;Chen et al., 2023b;Zhou et al., 2023). Each piece of data is formalized as a unified format <instruction, input, output>, where the instruction describes the task and the output is expected to be generated for the given input when following the instruction. Such instruc-tion tuning method relies on the zero-shot and fewshot capability of big models in a prompt-based paradigm, thus emerging after the advent of GPT-3 (Brown et al., 2020). To cope with the diversity and infinity of human instructions, efforts from three perspectives are mainly considered to create high-quality datasets, which allows the model better generalize to unseen instructions. \n\n(1) Scaling the Number of Tasks. The performance of instruction tuning and cross-task generalization scale well with the number of tasks (Chung et al., 2022). Therefore, many datasets comprised of instructions for more and more tasks are gradually built.",
            "score": 0.4590103622679689,
            "section_title": "Human Instructions",
            "char_start_offset": 7817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 96,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 216,
                    "end": 236,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1443,
                    "end": 1463,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0027675628662109375
        },
        {
            "corpus_id": "277780691",
            "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
            "text": "In contrast, the uniform setting (left) reveals a more abrupt phase transition: most properties remain near-zero in accuracy until the model reaches a critical capacity threshold, after which multiple properties are rapidly acquired. This implies that, for capacity-constrained models, structuring knowledge injection with a skewed distribution-rather than uniform-may ensure partial acquisition of salient knowledge, rather than risking a complete failure to learn under uniform allocation. \n\nInstruction Fine-Tuning We investigate how different instruction tuning strategies affect knowledge retention and acquisition under varying model capacities. We use uniform distribution in this experiment. Based on a shared pretrained model, we inject new knowledge using both Supervised Fine-Tuning (SFT) and Continued Pretraining (CPT), mixing in 80% of the original pretraining data to mitigate catastrophic forgetting following Chen et al. (2020). The underlying pretrained model is trained up to 50%, 90%, 100%, and 120% of its maximum capacity. \n\nWhile both SFT and CPT achieve perfect accuracy on new knowledge across all settings, they differ in how well they preserve prior knowledge. As shown in Table 2, SFT suffers from more severe forgetting, particularly in models trained at or beyond capacity (100% and 120%), where old accuracy drops significantly. This degradation is likely due to format-induced syntactic overhead in SFT, which consumes model capacity and leads to abrupt loss spikes (Figure 8). In contrast, CPT introduces new knowledge more seamlessly, resulting in more stable loss and better retention of previously learned content. Interestingly, for under-capacity models (e.g., 50%), the difference is negligible-suggesting that forgetting primarily emerges when the model's capacity is saturated.",
            "score": 0.45796546008976335,
            "section_title": "F.1 Experiment Setting",
            "char_start_offset": 79744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021152496337890625
        },
        {
            "corpus_id": "270226041",
            "title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback",
            "text": "Our findings indicate that instructions written in English outperform instructions written in non-English languages, aligning with our initial expectations.This phenomenon can be attributed to the accumulation of biases in LLMs' understanding of instructions across different languages.Such biases can introduce errors during the generation of results, resulting in incorrect outputs or outputs in the wrong language (i.e., off-target problem; Zhang xLLMs-100.Compared with the baseline models, our models demonstrate consistent improvements for both multilingual capabilities across all examined tasks.In comparison to the SFT-based models, xLLMs-100 increases the step of alignment with human preferences, resulting in additional improvements.This observation highlights the significance of aligning with human preferences after supervised fine-tuning.Furthermore, our model significantly outperforms other models on generative task datasets such as XL-Sum and FLORES, showing the effectiveness of our dataset and human feedback finetuning.",
            "score": 0.45749200213616303,
            "section_title": "Results",
            "char_start_offset": 25625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 1042
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011510848999023438
        },
        {
            "corpus_id": "277065939",
            "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding",
            "text": "To gain deeper insights into our model's capabilities, we conducted the following analysis of its cognitive visual perception mechanisms: \n\nHow knowledge influences perception. We systematically evaluated the base model's knowledge by probing entity-specific discriminative features and validating factual alignment against Wikipedia references using Qwen2.5-14B [42]. Entities were then classified into four groups based on knowledge possession (Known/Unknown) and training data presence (in-domain/out-of-domain). As evidenced in Fig. 4, DeepPerception achieves significantly greater performance gains on known entities versus unknown entities regardless of domain boundaries, confirming that its improvements originate from knowledge-driven cognitive processing rather than superficial perceptual enhancements, thereby validating its capacity to harness domain knowledge for visual perception. This aligns with human experts' reliance on domain expertise to resolve ambiguity, confirming our framework's success in emulating cognitive visual perception. \n\nThe roles of CoT-SFT and GRPO. As shown in Fig. 5, we quantified the impact of our two-stage training paradigm by measuring the KL Divergence between stage-2 models (trained with CoT-SFT or GRPO) and the stage-1 model, separately evaluating the CoT and answer components. For CoT-SFT, the significantly higher KL Divergence in the CoT components versus answer segments indicates that CoT-SFT primarily equips the model with perceptionoriented reasoning In contrast, GRPO exhibited greater divergence in the answer components than in the CoT, demonstrating its focus on refining perceptual accuracy grounded in the reasoning process. By combining the two stages, the framework achieves synergistic enhancement of cognitive visual perception, effectively bridging knowledge-driven reasoning and sensory processing. \n\nQualitative Results. As illustrated in Fig. 6, we conducted a comparative case analysis between DeepPerception and Qwen2-VL-7B. The visual evidence demonstrates our model's capability to generate accurate answers through  a reasoning process that systematically integrates domainspecific knowledge with visual observations, in contrast to the baseline model's tendency to produce incorrect responses from superficial pattern recognition.",
            "score": 0.45727431109065364,
            "section_title": "Analysis",
            "char_start_offset": 22514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0053863525390625
        },
        {
            "corpus_id": "274981665",
            "title": "NILE: Internal Consistency Alignment in Large Language Models",
            "text": "The project outcome is not satisfactory, which can lead to frustration, especially if a lot of effort was invested. He does not know what to do next, which creates a sense of worry about the future of the project. Sun et al., 2024). These approaches narrowly focus on data quantity while overlooking IFT's core purpose: unlocking the latent capabilities of pretrained LLMs. They fail to examine the crucial relationship between SFT training data and the underlying pre-trained models. \n\nRecent findings delve into this problem deeper, revealing that the alignment between underlying world knowledge derived from IFT datasets and the internal knowledge stored within the parameters of LLMs is crucial for determining their performance (Kang et al., 2024). This alignment creates meaningful connections between IFT datasets and the downstream performance of specific LLMs, offering theoretical guidelines for the generation and selection of IFT datasets tailored to enhance the effectiveness of particular LLMs. Specifically, when world knowledge from IFT datasets conflicts entirely with LLM's internal knowledge, these encoded associations are largely neglected, and the LLM tends to merely predict an intelligent \"blind guess\" by mimicking surface-level patterns such as text styles without truly understanding task intentions. Nevertheless, further experiments (Ren et al., 2024) observe that only incorporating a certain degree of world knowledge that is incompatible with LLMs' internal parameter knowledge into IFT datasets brings more performance boost. There are recent attempts (Li et al., 2024b;Cao et al.;Li arXiv:2412.16686v1 [cs.CL] 21 Dec 2024Dec et al., 2024d;;Liu et al., 2024;Li et al., 2024a) to maintain consistent formatting at a superficial level such as text styles, but they still overlook the need for managing internal consistency. \n\nInspired by previous findings, we propose a novel framework called NILE (INTERNAL CONSISTENCY ALIGNMENT). To tackle the aforementioned issues, NILE is designed to generate and select better IFT datasets considering the consistency between internal parameter knowledge in LLMs and world knowledge in IFT datasets.",
            "score": 0.457178132804885,
            "section_title": "World Knowledge",
            "char_start_offset": 1030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2170
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 231,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 1363,
                    "end": 1381,
                    "matchedPaperCorpusId": "268041894"
                },
                {
                    "start": 1586,
                    "end": 1604,
                    "matchedPaperCorpusId": "261076515"
                },
                {
                    "start": 1675,
                    "end": 1692,
                    "matchedPaperCorpusId": "265351632"
                },
                {
                    "start": 1692,
                    "end": 1709,
                    "matchedPaperCorpusId": "267682220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004364013671875
        },
        {
            "corpus_id": "264590782",
            "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
            "text": "In this paper, we follow the superficial alignment hypothesis proposed by Zhou et al. (2023) that a model's knowledge is mostly learnt during pretraining, while instruction data teaches the model to follow a certain pattern when interacting with users. Hence, the quality of these instruction data could be viewed as its ability to efficiently steer language models in learning to generate responses in a particular manner. Based on this assumption, we further propose our instruction quality evaluation hypothesis as follows. \n\nHypothesis 1 Instruction Quality Evaluation Hypothesis: Given an instruction dataset D, we finetune a language model on D, denoted as M f t . The instruction quality of D can be estimated through the inference loss of M f t on a evaluation dataset D eval . \n\nTo ensure the inference loss provides a valid measure for evaluating data quality, the evaluation set should comprise a selected collection of unbiased and high-quality instruction-following samples. \n\nIn particular, given an instruction-following dataset D, we finetune a base language model M using D with model training settings S. S normally refers to training batch size, epochs, etc. L refers to the loss function. The obtained finetuned language model is denoted as M f t . We define the dataset D's quality Q D|M,S as below, \n\nwhere D eval refers to the high-quality and unbiased evaluation set, and \u221d means a direct proportion.",
            "score": 0.456996514824783,
            "section_title": "What is instruction quality?",
            "char_start_offset": 5696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "273532094",
            "title": "Understanding Layer Significance in LLM Alignment",
            "text": "Aligning large language models (LLMs) with specific requirements is essential for enhancing their utility across diverse applications (Luo et al., 2023a;Yu et al., 2023;Luo et al., 2023b;Li et al., 2023;Liu et al., 2024a;2022;Feng et al., 2023). Fine-tuning LLMs during the alignment process can significantly improve the models' capabilities to meet targeted needs (Bubeck et al., 2023). Typically, alignment involves fine-tuning the model on diverse datasets, which may include both human-curated (Rajani et al., 2023) and LLM-generated (Taori et al., 2023) data, using approaches like instruction tuning (Wei et al., 2021) and preference learning (Bai et al., 2022;Rafailov et al., 2024). Given the significant cost associated with full parameter fine-tuning, parameter-efficient fine-tuning (PEFT) (Hu et al., 2021;Chen et al., 2022;Pan et al., 2024) methods have emerged as a popular alternative, offering a balance between performance and resource efficiency. \n\nUnderstanding what LLMs actually learn during the alignment process is crucial. Zhou et al. (2023) posits that the majority of knowledge and capabilities are developed during the pre-training phase, with alignment primarily serving to refine the model's conversational style and formatting. Using a well-selected set of 1,000 training examples for supervised finetuning (SFT), they successfully produced a high-quality aligned model. Similarly, Lin et al. (2023) investigated the token distribution of LLMs before and after alignment and found that most changes were related to \"stylistic tokens\", such as discourse markers and transition words, while the knowledge-intensive content largely remained untouched, coming from Table 1: Impact of fine-tuning different components of LLAMA 2-7B on alignment performance using the LIMA dataset. Evaluated on MMLU (5-shot) and GPT-4o scores for Vicuna and MT-Bench prompts.",
            "score": 0.45679708885241244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 221,
                    "matchedPaperCorpusId": "268819294"
                },
                {
                    "start": 668,
                    "end": 690,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00400543212890625
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "Res.: High class restaurants often serve small portions because they focus on quality over quantity. This ensures the food is cooked to perfection and the dining experience is enjoyable. Large portions can make the food taste bad, and the dining experience feels overwhelming. responding accurately, proceeds to generate hallucinated content . The suspected cause is style imitation, a process where the model, striving for lengthier, more detailed responses, fabricates information when it lacks sufficient knowledge. This hypothesis is further confirmed by comparing the responses to responses by another model fine-tuned on the simplified version of the same IT dataset. The hallucinations in Instructions 3 and 4 are not invented but are instead drawn from the IT dataset, a subject explored more comprehensively in Section 5. Moreover, Instruction 1 exemplifies the model's ability to generate an elaborate answer when it has adequate knowledge of the subject, whereas Instruction 2 demonstrates how merely imitating the style can alter the nature of a response to a reasoning task. Every response is also accompanied by Simplified Res., a response from a model fine-tuned on the same IT dataset but with simplified responses (detailed in Section 4). Notice how the Simplified Res. is less prone to hallucination by providing a brief response. \n\nFinding 4. Pre-trained knowledge (currently) dominates. \n\nOur findings indicate that IT via LFT primarily facilitates response initiation rather than augmenting new knowledge, and most of the response is based on pre-trained knowledge. In contrast, the substantial token distribution shift observed in SFT suggests learning new knowledge during fine-tuning. To assess if this newly acquired knowledge translates into improved response quality, we conduct the following evaluations, as shown in Fig. 3: (1) We compare the performance of models of varying sizes when fine-tuned with LFT on just 1k samples of the IT dataset and the same models fine-tuned with SFT on a dataset 326\u00d7 larger on just-eval-instruct 1k . As we know from earlier findings, the LFT model relies entirely on pre-trained knowledge, while the SFT model maximizes learning from the IT dataset. The results show that the LFT model outperforms in terms of factuality and usefulness, suggesting that even extensive IT does not significantly introduce useful or factual knowledge to the model.",
            "score": 0.456269497680265,
            "section_title": "Instruction",
            "char_start_offset": 15624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2410
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002368927001953125
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "Table 2 compares the pre-trained Llama-2 70B fine-tuned on OASST dataset with responses from different sources.We list the FActScore (FS) of biography generation using the pre-trained model through Bio 5-shot demonstration as reference (row 6 FAVA dataset 0) and SFT, which is fine-tuned on our seed data with human created responses, is our baseline (row 1).We first notice that SFT shows significant FActScore degrade (53.1 vs 44.7) compared to Bio 5-shot with the pre-trained model.It seems that SFT tends to generate more lengthy responses but with more erroneous facts.\n\nWhen eliciting the knowledge from PT by finetuning on its own generated responses, SFT fact generates more factual responses in Biography and Alpaca (row 2 vs 1).However, it shows slightly inferior instruction following capability in Alpaca Eval.This result demonstrates that human responses indeed teach LLMs how to better follow instructions but also encourage LLMs to output more false facts.On the other hand, eliciting the knowledge from the pre-trained model itself avoids the encouragement of hallucination albeit with a slight reduction in instruction-following capability.Finally, SFT combining supervision from humans and PT, shows comparable instruction following capability and output more factual responses on factbased instructions (row 3 vs 1).11).On the other hand, when only aligned with factual preference data, DPO fact shows less improvement in instruction following capability (row 1 vs 3).These results indicate that preference optimization for either instruction following or factuality alone may come at the expense of the other since the former encourages models to output long and detailed responses while the later discourages models to output false claims.When jointly conducting instruction and factuality alignment, DPO not only better follows instructions but also outputs more factual responses (row 4 vs 1, 2).Finally, initializing from SFT , the DPO fine-tuned models are more factual than their counterparts (i.e., 6 vs 2 and 7 vs 4) without instruction following capability degrade.We also list the results from Llama-2-Chat 70B (row 0) and observe that despite of its strong instruction following capability, it tends to output many more incorrect facts.",
            "score": 0.4562669449264821,
            "section_title": "Comparisons of SFT",
            "char_start_offset": 19752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 574
                },
                {
                    "start": 576,
                    "end": 738
                },
                {
                    "start": 738,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 971
                },
                {
                    "start": 971,
                    "end": 1157
                },
                {
                    "start": 1157,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 2094
                },
                {
                    "start": 2094,
                    "end": 2267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020351409912109375
        },
        {
            "corpus_id": "268724146",
            "title": "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback",
            "text": "Large Language Models (LLMs) have exhibited strong capabilities in solving various downstream tasks through alignment techniques such as Supervised Finetuning (SFT, (Zhang et al., 2023b), (Zhang et al., 2023b)), Direct Preference Optimization (DPO, (Rafailov et al., 2024)), and Reinforcement Learning from Human Feedback (RLHF, (Stiennon et al., 2020;Ouyang et al., 2022)). Those techniques align language models with human intent, mainly to maximize the helpfulness of LLM's responses. However, maximizing helpfulness does not mean minimizing errors. A significant problem arises in that LLMs often produce outputs that, while seemingly plausible, contain factual errors (Min et al., 2023) or self-contradictions (Liu et al., 2022), which are referred to as hallucinations. \n\nTo mitigate the hallucinations, many studies focus on augmenting the knowledge of LLMs, such as curating training data (Penedo et al., 2023;Zhou et al., 2023) or employing retrieval-augmented generation (RAG, (Gao et al., 2023b;b)) during inference. Nevertheless, it is essential to acknowledge that model knowledge inherently has limitations, and even the most powerful models, such as GPT-4, are prone to experiencing hallucinations (Zhang et al., 2023a). Consequently, we posit that the fundamental nature of the hallucination problem lies in the model's misalignment with its knowledge boundary. Hallucinations arise when LLMs try to answer questions beyond their knowledge boundary. \n\nOne research direction related to aligning LLM with its knowledge boundary involves alignment for honesty (Kadavath et al., 2022;Yang et al., 2023b). However, alignment for honesty presents significant challenges. On the one hand, the honesty of LLMs is hard to evaluate. Honesty can be viewed as a classification problem, where the model learns to distinguish between what it knows and what it does not. Nevertheless, the accuracy which indicates how much the model knows can differ dramatically among various models or even with different prompts (Kaddour et al., 2023).",
            "score": 0.45618816876589763,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 272,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 329,
                    "end": 352,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 352,
                    "end": 372,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 715,
                    "end": 733,
                    "matchedPaperCorpusId": "233296648"
                },
                {
                    "start": 1006,
                    "end": 1008,
                    "matchedPaperCorpusId": "233296648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004505157470703125
        },
        {
            "corpus_id": "267627906",
            "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
            "text": "Aligning large language models with human values necessitates a substantial investment in human annotation ef-ICML 2024 Workshop on Models of Human Feedback for AI Alignment, Vienna, Austria. \n\nforts (Ouyang et al., 2022;Touvron et al., 2023). Previous work emphasizes the importance of the quantity and the quality of the training data (Zhou et al., 2023;Chen et al., 2023b). Moreover, human annotations are especially precious and expensive (Touvron et al., 2023). \n\nSelf-alignment seeks to minimize cost of obtaining human annotations while maintaining satisfactory model performance. This objective can be achieved from three aspects: (i) try to obtain high quality self-generate data (Bai et al., 2022;Sun et al., 2023b;a;Wang et al., 2022;Niu et al., 2023;2022;Huang et al., 2022;Ma et al., 2023b), (ii) try to make full use of ready-made data (Li et al., 2023a) , (iii) try to elicit model internal knowledge and capacity of the model (Sun et al., 2023b;a;Wang et al., 2022;Bai et al., 2022). As for (iii), existing self-alignment methods share a common feature: they aim to accumulate high-quality data and subsequently conduct supervised fine-tuning(SFT) directly from the pretrained model. \n\nIt's widely recognized that SFT could improve the instruction following ability of pretrained LLM. Zhao (Zhao et al., 2021) evaluate different size models' performance and find a positive correlation between the zero-shot and few-shot as model size increases. Consequently, during the self-aligned SFT process, the model's zero-shot ability is already enhanced, which should also improve its few-shot instruction following ability. This gives rise to a question: Is the pretrained large language model the better few-shot generator or is multi-round(bootstrapping) self-alignment effective? If the answer is no, this enhanced few-shot capability for better performance is ignored.",
            "score": 0.4560805376089708,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 221,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001979827880859375
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "The alignment stage usually involves either supervised fine-tuning for specific tasks or instruction finetuning for general-purpose usage. Regardless, finetuning (almost always) comes at the end of pretraining and yields remarkable improvements on downstream tasks (Touvron et al., 2023;Groeneveld et al., 2024). Consequently, the benefits of each stage are largely explored independently, with improvements to pretraining being orthogonal to benefits from model alignment. \n\nRather than exploring these two training regimes independently, we ask: What does the model learn and forget during pre-training and fine-tuning? Specifically, how do pretraining and fine-tuning interact to produce the resulting model? Does more pre-training hinder better fine-tuning results? Answering these questions requires us to examine how models learn during pre-training and how this affects fine-tuning. Therefore, we begin by fine-tuning two language models under a variety of conditions to determine how fine-tuning affects model behavior. We explore both supervised and instruction fine-tuning, testing the models' memorization and forgetting when learning specific tasks and serving as general-purpose language-AI tools. We then explore the affect of pre-training on these behaviors by fine-tuning multiple pre-training checkpoints of a large language model (Figure 1), evaluating each checkpoint and its fine-tuned variant on downstream evaluation sets. We track model abilities during pre-training and compare them to improvements achieved after fine-tuning at the corresponding pre-training step. 2  Our experiments yield the following insights into LLM training: (1) although supervised fine-tuning can improve performance on in-distribution tasks, it can also cause the model to forget domain knowledge or tasks that it was previously capable of solving ( \u00a74); (2) fine-tuned models show high sensitivity to evaluation prompts, but this sensitivity can be alleviated by more pre-training ( \u00a74); (3) continued pre-training can improve a model in ways that are only revealed after fine-tuning ( \u00a76); (4) tasks for which the model already performs well during pre-training benefit much less from fine-tuning than those where the model does not demonstrate capabilities ( \u00a75, \u00a76);",
            "score": 0.45568800671648463,
            "section_title": "Introduction",
            "char_start_offset": 1678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003185272216796875
        },
        {
            "corpus_id": "266348323",
            "title": "One Shot Learning as Instruction Data Prospector for Large Language Models",
            "text": "Large language models (LLMs) (Brown et al., 2020;OpenAI, 2023;Google, 2023;Bai et al., 2023;Li et al., 2023a) have showcased remarkable capabilities (Wei et al., 2022;Schaeffer et al., 2023;Liu et al., 2023;Cheng et al., 2024) across a wide range of language tasks by scaling the model size and training data. Despite their proficiency, it is imperative to further enhance their alignment with human instructions. This alignment process involves supervised fine-tuning (SFT) on input-output pairs, known as instruction tuning. Instruction tuning is a crucial step, serving not only to activate the valuable knowledge acquired by LLMs during pretraining but also to facilitate their interaction with humans in a manner that aligns with natural conversational dynamics. \n\nConsiderable efforts in instruction tuning have been concentrated on collecting larger (Chung et al., 2022;Wang et al., 2022b), more diverse (Sanh et al., 2022;Sun et al., 2023;Wang et al., 2023b), and intricate (Xu et al., 2023a;Wei et al., 2023) datasets. This is commonly achieved through human crowd-sourcing (Aghajanyan et al., 2021;Ouyang et al., 2022;Tang et al., 2022) or extracting data from larger pre-existing models (Wang et al., 2022a;Taori et al., 2023;Chiang et al., 2023;Xu et al., 2023a). Despite the growth in the size of datasets employed for instruction tuning, certain studies (Zhou et al., 2023;Chen et al., 2023;Cao et al., 2023) suggest that smaller yet valuable datasets tend to be more effective in harnessing the capabilities of LLMs. Blindly expanding the volume of instruction data without ensuring quality may introduce noise and lead to hallucination issues (Zhang et al., 2023c;Zhao et al., 2023a).",
            "score": 0.45564286230190754,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1700
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 92,
                    "end": 109,
                    "matchedPaperCorpusId": "257900712"
                },
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "268715833"
                },
                {
                    "start": 1083,
                    "end": 1108,
                    "matchedPaperCorpusId": "231718729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022430419921875
        },
        {
            "corpus_id": "277468006",
            "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning",
            "text": "To better understand the effects of prompt format on supervised fine-tuning (SFT), we explore four SFT variants that differ in whether the training data includes intermediate reasoning steps and whether the answers are wrapped in structured JSON format. We use GPT-4o-generated data on the ESCI product search task for training all four SFT models. \n\nTable 9 shows results on ESCI, and Table 10 evaluates generalization to broader benchmarks. \n\nOn the task-specific ESCI dataset, all SFT variants outperform the base model (Qwen-2.5-3B-Instruct), demonstrating the effectiveness of supervised fine-tuning on task-specific data. However, all variants fall short compared to REC-R1, which uses the same data but trains via reinforcement learning. This highlights the advantage of reward-driven learning in aligning with downstream task metrics. \n\nWe then assess the general-purpose capabilities of these models. On MMLU, a knowledgeintensive benchmark, all SFT variants retain performance close to the original model (within 2 points), suggesting factual knowledge is preserved. In contrast, IFEval results reveal catastrophic forgetting across the board-all SFT variants suffer 20-30 point drops in instruction-following accuracy, regardless of format. This underscores the risk of overfitting in SFT, where tuning on narrow task data compromises broader generalization. \n\nAn interesting observation arises on GSM8K: the variant with JSON formatting but no reasoning shows improved performance over the base model (+4.7). We hypothesize that the strict output format (JSON) acts as a \"shield,\" isolating the fine-tuning effects from interfering with the model's native reasoning process. In contrast, the reasoningheavy variants modify the generative behavior more substantially, harming out-of-domain reasoning. \n\nOn the coding benchmarks (MBPP, HumanEval), all four SFT variants exhibit comparable or slightly improved performance relative to the original model-regardless of whether the training outputs used JSON format. This suggests that coding ability is relatively robust to task-specific SFT, and may even benefit from it.",
            "score": 0.4554577230869007,
            "section_title": "E.3.2 Additional Analysis: Impact of Reasoning and JSON Format in SFT",
            "char_start_offset": 45800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2130
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00243377685546875
        },
        {
            "corpus_id": "272880789",
            "title": "Exposing Assumptions in AI Benchmarks through Cognitive Modelling",
            "text": "By offering a metaevaluation perspective, our approach contributes to the workshop's goal of evaluating AI evaluations, providing a robust framework for assessing traits and their relationships across model evaluations. \n\n(Re)constructing Crosslingual Alignment Transfer Building on the need for improved evaluation methodologies outlined in the introduction, we now turn to a specific case study in cross-lingual alignment transfer. The concept of 'alignment' in LLMs is predominantly studied using English data [Kirk et al., 2024], implicitly assuming cross-lingual transferability. However, recent empirical evidence challenges this assumption [Masoud et al., 2023, Tao et al., 2023, Cao et al., 2023]. Our Structural Equation Model (SEM) in Fig. 1 makes this assumption explicit, allowing us to rigorously measure the effect of alignment transfer. \n\nOur model uses unobservable latent factors (circles) for underlying concepts and observable variables (rectangles) for specific tests following [Suhr, 2006]. We explore relationships between language ability (G eng , G dan ), cultural knowledge (CK eng , CK dan ), and alignment (HHH eng , HHH dan ) for both English and Danish. Arrows indicate potential relationships or influences between these factors. A full toy example of an SEM can be found in Appendix A and explanations of datasets in B. This explicit model elucidates several key aspects of alignment transfer: \n\n1. Design direction: To assess cross-lingual transfer in alignment, we require a dataset that 'loads' on Danish HHH. The model makes this requirement explicit, highlighting an important direction for dataset development. 2. Testable hypotheses: Our model enables rigorous testing of hypotheses using benchmark data from multiple models, such as whether HHH-alignment transfers across languages and whether there exists an underlying language ability factor. 3. Improved construct validity: By connecting multiple benchmarks to underlying constructs, we can assess how well these tests measure what they're intended to measure. This allows us to investigate whether different traits converge or if a trait needs to be decomposed into multiple components.",
            "score": 0.4554252789206773,
            "section_title": "body",
            "char_start_offset": 1950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 222,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 667,
                    "end": 685,
                    "matchedPaperCorpusId": "265445838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0037212371826171875
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "URIAL even outperforms SFT and RLHF when base LLMs are strong. When using Mistral-7B as the base model, URIAL (4.63) outperforms its official SFT-ed model, Mistral-7B-Instruct (4.44), on all aspects, yielding the best performance among 7B-level LLMs. Likewise, on top of Llama-2-70b q , URIAL also surpasses the RLHF-ed version (Llama-2-70b-chat q ) by a significant margin (4.74 vs 4.67), which nearly matches the performance of ChatGPT (4.75) and . Note that both Mistral-7B and Llama-2-70b q are better pre-trained than Llama-2-7b, as suggested by various benchmarking results (Jiang et al., 2023a;Touvron et al., 2023) and their zero-shot performance (e.g., helpfulness 3.05 vs 3.70). Therefore, we conclude that when the base LLMs are well-pretrained, SFT and RLHF may not be as crucial for alignment as previously believed. Instead, tuning-free methods such as URIAL can achieve superior performance with minimal effort, at least in the scenarios covered by our evaluation. We also conduct a human evaluation for pairwise comparisons in Table 2, which reinforces these conclusions. \n\nAligned LLMs might forget knowledge and become overly sensitive. Case studies in Appendix B reveal that fine-tuning could induce forgetting, hallucination, and overly sensitive censorship. For example, we find that Mistral-7B with URIAL can correctly answer the question \"Did Facebook corporate change its name?\" by telling users the new name is \"Meta Platform Inc.\". However, the SFT-ed version Mistral-7B-Instruct instead answers \"No, Facebook did not change its name.\" This indicates that during SFT process, the LLMs might over-fit the instruction-tuning data and thus forget the previously acquired knowledge.",
            "score": 0.45507709440234756,
            "section_title": "EMPIRICAL RESULTS",
            "char_start_offset": 29200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1704
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0017681121826171875
        },
        {
            "corpus_id": "266725709",
            "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
            "text": "In this subsection, we firstly present the essential steps to develop an instruction-following LLM. Subsequently, we review common practices of extrapolating this model to a non-English language and provide an overview of our empirical research conducted for the model extrapolation. \n\nStep 1: Pretraining to acquire language capability and knowledge \n\nAs a significant source of foundational capabilities for a LLM, pretraining aims to predict the next token based on the prefix sequences. Formally, given a large corpus D, the training objective is to minimize the following loss: \n\nwhere x = {x 1 , ..., x n } denotes an input token sequence. By pretraining on massive text data ranging from billions to trillions of tokens, LLMs are capable of capturing intricate language structures, semantics, and contextual relationships, thereby acquiring strong language generation capabilities. Additionally, these LLMs also learn how to comprehend concepts, facts, and the connections between them, leading to a broad understanding of world knowledge. \n\nStep 2: Instruction tuning for aligning with human intent Instruction tuning (SFT) aims to further enhance the capability of LLMs to follow instructions. Its training data consists of many instruction-response pairs. The model needs to learn to accurately respond to instructions, rather than merely continuing from the preceding text. Formally, given an instruction dataset D \u2032 = {(I, Y )}, where I represents a task instruction and Y represents a desired response, the training objective of instruction tuning is to minimize the following loss: \n\nBy tuning on diverse instruction tasks, the model is able to better comprehend and follow human instructions, and generalize to unseen instructions.",
            "score": 0.45454063644768394,
            "section_title": "Background and Overview",
            "char_start_offset": 3554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1595
                },
                {
                    "start": 1598,
                    "end": 1746
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002590179443359375
        },
        {
            "corpus_id": "272881354",
            "title": "Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories",
            "text": "In its design, Mocha controls both how counterfactual data is generated and how they are rendered to the user. By incorporating theories such as Structural Alignment Theory and Variation Theory, it aims to support the learning of both the human and the model. These theories have proven insightful for understanding how humans grasp and compare concepts, shaping the development of human-AI collaboration systems for sensemaking [29], hypothesis testing [2], as well as model training [24]. From a humanin-the-loop machine learning perspective, Mocha addresses two seemingly contradictory objectives: (1) generating labeled data that diversifies the training dataset to aid the model's learning, and \n\n(2) maintaining structural consistency across the batches of data presented to users to support their cognitive processes. Mocha achieves this balance by enforcing a common structure through the model's learned pattern rules [25]. By visualizing these consistent pattern rules, users may be better understanding the behavior of the model through inference projection [26]. This can not only boosts the model's performance but also enable participants to validate or correct the model during the interactive training process. \n\nAlthough visual cues for alignable differences in Mocha were helpful in supporting the participants' reasoning, Estes and Hasson [17] argue that while alignable differences can be more straightforward and easier for comparison, non-alignable differences can also provide key information that might otherwise remain overlooked. These differences necessitate a more abstract form of comparison, prompting users to think beyond simple relational structures and consider broader conceptual frameworks. For example, when comparing planes and cars, their alignable differences can be that they both have engines, but the engines are different in size. While this gives us some insight into their definitions, comparing a plane's wings and a car's wheels, which are not structurally alignable but conceptually and analogically alignable, gives additional insight to categorizing one for land and the other for air. Future research should explore how non-alignable differences in AI explanations affect user decision-making and understanding. Such studies could determine whether these non-alignable comparisons enhance user performance and elicit deeper insights in human-AI collaborative systems.",
            "score": 0.45444642541769636,
            "section_title": "Human Cognition and Learning Theories in the Interactive ML Pipeline",
            "char_start_offset": 57413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "267211887"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "262044762"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "258216675"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "15834560"
                },
                {
                    "start": 1358,
                    "end": 1362,
                    "matchedPaperCorpusId": "263396334"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004383087158203125
        },
        {
            "corpus_id": "269302493",
            "title": "Does Instruction Tuning Make LLMs More Consistent?",
            "text": "Large language models (LLMs) are trained with general objectives such as next token prediction, but further training is often needed to make models useful assistants.Recent research has shown how so-called instruction tuning-supervised finetuning on user interactions-enables good zeroshot performance across a range of tasks (Wei et al., 2022;Sanh et al., 2022) by aligning LLMs to user's intents and expectations (Ouyang et al., 2022;Touvron et al., 2023).Subsequently, many public instruction datasets have been curated (Wang et al., 2022b;Longpre et al., 2023;Wang et al., 2022a, inter alia), and new instruction fine tuned (IFT) models have been published.Models have been evaluated in terms of their generation quality, by humans (Taori et al., 2023;Chiang et al., 2024) or by leveraging LLMs (Chiang et al., 2023;Li et al., 2023); while Wang et al. (2023b) evaluated IFT models capabilities by measuring their accuracy on zero-shot and few-shot question answering, finding that IFT models become better or worse at different tasks depending on the dataset used.\n\nThis raises the question: How exactly does instruction tuning change LLMs?Lin et al. (2023) studies this question in terms of decoding and to-  ken distribution shift, finding most differences to be in stylistic tokens, thus arguing that IFT primarily teaches the model to talk like an assistant.However, instruction datasets have been created as verbalizations of tasks, and thus the data will contain multiple surface forms encoding the same expected behavior (see Figure 1).Therefore, we posit that this type of data could also encourage more semantic consistency in IFT models, i.e. less sensitivity to small perturbations, and thus make models more robust.Sensitivity of LLMs to subtle changes-that preserve the input's meaning-has been studied in terms of formatting (Sclar et al., 2024), in the context of factual predictions (Elazar et al., 2021), and in downstream classification tasks (Jang et al., 2022).",
            "score": 0.4538627998384599,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1547
                },
                {
                    "start": 1547,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 344,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 415,
                    "end": 436,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 523,
                    "end": 543,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002208709716796875
        },
        {
            "corpus_id": "276317902",
            "title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions",
            "text": "Section II provides a review of previous work and elaborates further on the innovative aspects of the PT-ALIGN method. Section III presents the methodology and architecture of the proposed PT-ALIGN, which mainly includes instruction synthesis for the target safety domain, contrastive generation of positive and toxic samples, and fine-grained dual instruction fine-tuning. Section IV presents our main experimental results, including the safety enhancement achieved by PT-ALIGN for both vanilla LLMs and aligned LLMs, the quantification of model safety alignment tax, case studies, etc. Section V discusses the comparative analysis of PT-ALIGN with other methods, highlighting its advantages in achieving safer and more effective alignment. Section VI concludes the findings of this study. Large language models (LLMs) [30], through pre-training on vast text corpora, have acquired unparalleled capabilities [31] in text comprehension and generation. However, the next-word prediction objective in pre-trained LLMs, along with them being harmless, helpful, and honest, necessitates their safety alignment. Current alignment efforts often do not prioritize safety, and aligning models to follow instructions can compromise safety [32], [14]. One safety alignment approach involves using human feedback-based reinforcement learning (RLHF) [33], where human feedback signals serve as rewards. Safety alignment is also achieved through supervised finetuning (SFT), typically using positive question-answer pairs as training samples. Instruction tuning [18] is a highly effective method, enabling LLMs to learn safe response styles. Differing from this, our method guides the model to synthesize both safe and toxic samples for instruction tuning in a self-alignment manner, incorporating extremely negative samples as a new supervisory signal into the training process of LLMs, thereby refining the loss functions.",
            "score": 0.4537928626354274,
            "section_title": "DeepSeek-V3",
            "char_start_offset": 7903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 1230,
                    "end": 1234,
                    "matchedPaperCorpusId": "261823321"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "263671523"
                },
                {
                    "start": 1338,
                    "end": 1342,
                    "matchedPaperCorpusId": "259064099"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0031719207763671875
        },
        {
            "corpus_id": "273532094",
            "title": "Understanding Layer Significance in LLM Alignment",
            "text": "Evaluated on MMLU (5-shot) and GPT-4o scores for Vicuna and MT-Bench prompts. Tuned components include attention projections (W q , W k , W v , W o ) and feed-forward layers (W up , W down , W gate ). the base pre-trained model. These findings imply that the alignment process mainly adjusts the model's presentation style rather than modifying its foundational knowledge. To gain a deeper understanding of LLM alignment, we analyze this process at the level of model parameters. We conducted a pilot study to investigate the impact of different model components on alignment performance, by fine-tuning only specific layers and evaluating the resulting performance, as presented in Table 1. The results clearly indicate that finetuning different components of the LLM leads to considerable performance differences. For instance, fine-tuning the feed-forward network (FFN) layers achieves performance similar to fine-tuning all linear layers (i.e., with LoRA), whereas focusing solely on the attention layers causes a notable drop in performance. This observation shows the complexity of layer-specific contributions to LLM alignment, highlighting the need for detailed analysis. \n\nTo address this, we propose identifying the layers that are most critical to alignment performance during the SFT process. We develop a novel approach, ILA, for identifying the important layers for LLM alignment. Specifically, we learn a binary mask for the parameter changes in each layer during the fine-tuning process, which serves as an indicator of layer significance. A binary mask value of zero indicates that the corresponding layer has negligible influence during the process, while a value of one denotes that the layer is crucial. We use gradient descent to learn the binary mask effectively and offer a theoretical analysis of the optimization process. The main findings and significance of this work include: \n\n\u2022 Consistent layer importance ranking across different alignment datasets. We observe similar rankings of important layers during alignment for the same pre-trained model (see Fig. 1), even though the alignment datasets vary significantly in both content and size. This suggests that the alignment process endows the model with similar capabilities, corroborating previous research findings and offers new insights into LLM alignment. \n\n\u2022 Enhancing performance by freezing unimportant layers.",
            "score": 0.4535680880392592,
            "section_title": "Introduction",
            "char_start_offset": 1822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2340
                },
                {
                    "start": 2343,
                    "end": 2398
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035800933837890625
        },
        {
            "corpus_id": "270357323",
            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
            "text": "We compare the performance of the models across helpfulness and harmlessness when aligned using a preference dataset of lower quality with a higher quality dataset using SFT and DPO in Figure 1.The helpfulness benchmarks indicate that a higher quality and more informative preference dataset leads to overall more helpful behavior of the aligned model across both the alignment methods.When using the helpful preference subsets, there are bigger gains on general-purpose NLP tasks (such as MMLU) and instruction-following benchmarks (such as Alpaca Eval) when SFT is used.This suggests that SFT is more sensitive to the dataset quality when the downstream task and preference dataset are of a similar nature.When using the orthogonal harmless preference for alignment, relatively lower quality datasets such as HH-RLHF lead to overfitting when using alignment methods like DPO and experience significant performance degradation.However, since BeaverTails ensures that the safe and harmless responses are informative, there is no degradation in performance when used with DPO.\n\nThe harmlessness benchmarks reveal that using a more informative and safer dataset makes the model less harmful.However, DPO leads to a significant degeneration of the model when Beaver-Tails is used with Mistral-7b.As Mistral-7b is not instruction-tuned, we hypothesize that this degradation might be due to the inability of the base model to effectively represent the reward for preference optimization, specifically for samples targeted towards more objective preferences such as harmlessness and safety compared to broader preferences such as helpfulness.We also observe that harmless alignment using higher-quality datasets is more faithful than lower-quality datasets, especially when using SFT.LLaMA LLaMA We also present the progress of the model performances at various training steps in Figure 2. We observe that when using a more informative preference dataset, the alignment either leads to faster learning and more gains or achieves comparable The performance here is shown in % relative to the performance when using 1600 samples.\n\nperformance to the base model.When using a relatively lower-quality dataset, either the learning is slower or deterioration is observed during the course of the alignment training.Overall, better quality and informative datasets are better for alignment across methods and preferences when using PEFT.",
            "score": 0.4531059693785581,
            "section_title": "Results and Observations",
            "char_start_offset": 12196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1075
                },
                {
                    "start": 1077,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 2121
                },
                {
                    "start": 2123,
                    "end": 2153
                },
                {
                    "start": 2153,
                    "end": 2303
                },
                {
                    "start": 2303,
                    "end": 2424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001979827880859375
        },
        {
            "corpus_id": "261705563",
            "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
            "text": "Pre-trained large language models (LLMs) exhibit a remarkable capacity to address human queries, aid in coding tasks, and more. Nonetheless, the generated outputs of these models can sometimes diverge from preferred human values and even pose potential risks. To make pre-trained LLMs more user-friendly and safe, numerous alignment methods have been proposed, such as RLHF (Casper et al., 2023), RLAIF (Bai et al., 2022b), RRHF (Yuan et al., 2023), RAFT (Dong et al., 2023), and DPO (Rafailov et al., 2023). These methods, however, necessitate the finetuning of pre-trained LLMs and demand considerable amounts of meticulously human-annotated data and computational resources. Take RLHF as an example, this comprehensive approach encompasses three primary phases: supervised finetuning (SFT), reward modeling (RM), and reinforcement learning (RL), together with the necessity to manage four separate models or heads-policy, value, reward, and reference models-each of which has at least billions of parameters. Efficiently operating these models requires significant GPU memory, and the act of updating their parameters poses a threat of overwriting the knowledge retained from the initial pre-training. Additionally, it is worth noting that training larger models is often met with heightened instability and requires significant engineering expertise. Hence, aligning frozen LLMs presents a more appealing option to the community. \n\nThis work shows that fixed LLMs are alignable using a novel inference method without finetuning and data. To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used. Logically, the action of \"selecting a sub-distribution\" should not mandate modifications to model parameters. Reject sampling is a working example of inference-time alignment. However, the method is sample-inefficient (as tested by our experiments).",
            "score": 0.45307701553536184,
            "section_title": "Introduction",
            "char_start_offset": 435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2077
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093505859375
        },
        {
            "corpus_id": "259164600",
            "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
            "text": "Instruction tuning can be seen as generalized task-specific supervised fine-tuning that teaches LLMs to follow instructions in the form of prompts. It fine-tunes a single model using instructions via prompts that span various tasks, thereby enabling the prompting of LLMs. A very specific and, from an AutoML perspective, complicated task that is often solved by a form of instruction tuning, called Reinforcement Learning from Human Feedback (RLHF) (Fernandes et al., 2023), is alignment.3 Alignment (fine-tuning) describes the idea of approximately aligning the behavior and output of LLMs with human value-related task objectives such as helpfulness, honesty, harmlessness, etc. The two ingredients required to make Reinforcement Learning (RL) work on a pre-trained Language Model (LM) are: (i) A Reward Model (RM) to convert a sequence of texts to a scalar reward which numerically represents the human preference. (ii) A policy that takes user queries or prompts as input to produce language tokens as outputs. \n\nWe note that, in principle, one can also use a pre-trained model off-the-shelf. However, in practice, some instruction tuning and alignment fine-tuning is performed if the model is released to end-users. Moreover, some form of additional task-specific fine-tuning is often performed to increase the performance of the LLM for its designated usage and to make it properly usable. \n\nFinally, during inference, the fine-tuned LLM generates text for language-related tasks, such as question answering, based on the learned knowledge and contextual understanding.",
            "score": 0.4530741503871544,
            "section_title": "Background on LLMs",
            "char_start_offset": 9681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0027370452880859375
        },
        {
            "corpus_id": "273346094",
            "title": "SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers",
            "text": "The results are shown in Figure 6. Complex Human Instruction Improves Text-Image Alignment: As mentioned above, Gemma has better instruction-following capabilities than T5. We can further leverage this capability to strengthen text embedding. Gemma is a chat model, and although it possesses strong capabilities, it can be somehow unpredictable, thus we need to add instructions to help it focus on extracting and enhancing the prompt itself. LiDiT (Ma et al., 2024) is the first to combine simple human instruction with user prompts. Here, we further expand it by using in-context learning of LLM to design a complex human instruction (CHI). As shown in Table 2, incorporating CHI during train-whether from scratch or through fine-tuning-can further improve the image-text alignment capability. \n\nAdditionally, as shown in Figure 7, we find that when given a short prompt such as \"a cat\", CHI helps the model generate more stable content. This is evident in the fact that models without CHI often output content unrelated to the prompt.",
            "score": 0.4527214948603708,
            "section_title": "TEXT ENCODER DESIGN",
            "char_start_offset": 15597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1037
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0009183883666992188
        },
        {
            "corpus_id": "268248820",
            "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
            "text": "The foundational training of models like Mistral leverages large, well-curated datasets to encompass a wide range of knowledge domains and linguistic structures, fostering robust general language processing skills with stable and widely applicable  feature representations. However, SFT model often involves smaller, domain-specific datasets of varying quality, which may lead to models becoming overly attuned to the peculiarities of these datasets. This shift not only risks introducing biases towards narrower datasets but also threatens the broader knowledge base established during initial training. \n\nThe result is a compromise in the model's generalization capabilities, as it may begin to forget more generalizable abilities it learned in pretrain stage. As depicted in Figure 3, Mistral-Plus archives great performance in multi-turn conversational task, effectively completing summarization and converting text into YAML format. However, due to SFT causing a forgetfulness of the generalizable abilities acquired from the base model, Mistral-Instruct performs well in summarization tasks but struggles to respond adequately to tasks requiring conversion to YAML format.",
            "score": 0.4521061114298194,
            "section_title": "Mistral-Plus on Conversational Task",
            "char_start_offset": 23350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021495819091796875
        },
        {
            "corpus_id": "269983424",
            "title": "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction",
            "text": "To understand the reason behind the alignment tax and in particular what is learned when alignment tax occurs, we propose to track the change of loss during the SFT process.In detail, we randomly split the dataset into 10 portions with equal sizes, training on 9 of them sequentially and leaving one for evaluation.Every after a portion is finished, we measure the loss reduction on the training set \u2206L train and the loss reduction on the validation 10% 20% 30% 40% 50% 60% 70% 80% 90%\n\nProportion of instruction data used set \u2206L val .Intuitively, while \u2206L val reflects the enhancement in generalizable model capacity on instruction following, \u2206L train encompasses not only the generalizable instruction-following ability, but also the ungeneralizable data specific biases.To measure the composing proportion of the two components, we plot the ratio \u2206L train /\u2206L val during the training process in Figure 4.\n\nAs is shown, the ratio is approximately 1.0 at the beginning, suggesting that generalizable instruction-following ability dominates at the initial of training.But as the SFT goes on, the ratio quickly inflates from 1.0 to nearly 20, indicating that the acquisition of data biases gradually outweighs other factors and becomes the major reason for loss reduction.Furthermore, to have a more intuitive understanding of data-specific biases, we exhibit the token-level biases by measuring the correlation between the per-token loss reduction on the training set and the validation set.Spearman's \u03c1 between the loss reduction on two sets is shown in Figure 5. From the figure, it becomes apparent that as the instruction tuning goes on, the fitting on training tokens gradually deviates from the generalizable ability.Meanwhile, some representative tokens with prominent loss reduction at the beginning and the end of training are shown in Figure 6.In a comparison between Figure 6a and Figure 6b, we can observe that the training loss reduction at the end can be mainly attributed to rare words and symbols, suggesting the existence of ungeneralizable data biases.",
            "score": 0.4512497328204572,
            "section_title": "Seek for Main Causes of the Alignment Tax",
            "char_start_offset": 9216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 485
                },
                {
                    "start": 487,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 773
                },
                {
                    "start": 773,
                    "end": 907
                },
                {
                    "start": 909,
                    "end": 1068
                },
                {
                    "start": 1068,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018243789672851562
        },
        {
            "corpus_id": "271162009",
            "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
            "text": "Alignment Algorithms.Aligning LLMs towards human-desired objectives is a problem that has been significantly noticed.Common methods for model alignment usually involve SFT and RLHF.SFT (Brown et al., 2020;Wang et al., 2022) finetunes a pre-trained model on task-specific data which contains instructional commands and human-annotated expected outcome (Chiang et al., 2023;Taori et al., 2023).RLHF is a technique that fine-tunes language models using human preferences to align their outputs with desired behaviors.Glaese et al. (2022); Rafailov et al. (2024) use RLHF to improve LLM safety when facing malicious questions.However, successfully training models using SFT or RLHF is challenging.The quality and quantity of training data are crucial for good training results and effectiveness (Zhou et al., 2024;Wang et al., 2024;Taori et al., 2023;Achiam et al., 2023;Touvron et al., 2023), requiring extensive data collection, cleaning, computational resources, and time.Besides, researchers have also discovered that during the training process of SFT or RLHF, the reasoning and understanding capabilities of models may decrease (Ouyang et al., 2022;Lu et al., 2024;Yue et al., 2024).This phenomenon may be caused by overestimating the model to overfit to the reward model or training data distribution (Noukhovitch et al., 2023;Rita et al., 2024), deviating from the original model and losing general capabilities.\n\nModification of LLM Parameters and forward process.Prior studies have explored modifying the forward propagation process or directly altering model parameters.Meng et al. (2022Meng et al. ( , 2023) ) propose model editing methods to update or insert specific knowledge without affecting other basic knowledge.Geva et al. (2022) hypothesize the existence of word vectors in MLP layers strongly correlating with specific tokens and propose setting activations of selected word vectors to a constant for detoxification.",
            "score": 0.451159854507136,
            "section_title": "Related Works",
            "char_start_offset": 5344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 971
                },
                {
                    "start": 971,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1416
                },
                {
                    "start": 1418,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 536,
                    "end": 558,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 791,
                    "end": 810,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "259108263"
                },
                {
                    "start": 1130,
                    "end": 1151,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1167,
                    "end": 1184,
                    "matchedPaperCorpusId": "263831589"
                },
                {
                    "start": 1304,
                    "end": 1330,
                    "matchedPaperCorpusId": "266151093"
                },
                {
                    "start": 1577,
                    "end": 1594,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0012254714965820312
        },
        {
            "corpus_id": "269137670",
            "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
            "text": "Fine-tuning models using Reinforcement Learning from Human Feedback (RLHF) showcase a remarkable ability to align with human preferences and generalize to new scenarios and is more sample-efficient than supervised fine-tuning.Nonetheless, these models exhibit characteristics and behaviors that warrant careful consideration, prompting the need for further exploration and refinement.\n\nAlignment Capabilities One intriguing property, referred to as the Alignment Tax, was identified by [Ouyang et al. 2022].The phenomenon reveals that RLHF-trained chat models sometimes perform poorly compared to initial policy in downstream tasks, suggesting a cost linked to aligning human preferences.To mitigate this, they propose incorporating the pre-training objective into RLHF-finetuning, which substantially reduces the Alignment Tax.Moreover, [Bai et al. 2022a] indicates that larger models tend to exhibit lower alignment tax.[Bai et al. 2022a] also observed that RLHF models better align with human preferences as the scales of both the reward model and policy model increase.It is noteworthy, however, that a similar scaling effect could be seen in instruction-finetuned SFT models.A comprehensive comparison of the scaling effects on RLHF versus SFT models is currently lacking in the literature and would make for an intriguing future study.\n\nGeneralization Capabilities RLHF models have exhibited impressive generalization capabilities beyond their training data, including generalization on new prompts and human feedback.For instance, [Ouyang et al. 2022] demonstrates RLHF-tuned models answering coding questions and following instructions in multiple languages despite being finetuned only in English and with limited code-related prompts.This suggests that the majority of a language model's capabilities are acquired during pre-training, and RLHF merely aligns these capabilities to elicit desired behavior.However, this generalization can be a double-edged sword, potentially leading to undesirable outcomes, especially when the feedback signal is sparse.For instance, the initial LLaMA2 Chat Model5 , when prompted \"How to kill a process?\" refused to answer, drawing ethical concerns, though the intended answer was about terminating a computer process.This behavior likely stems from the model's extended generalization from examples that trained it to reject violent queries.",
            "score": 0.4510323923687326,
            "section_title": "Limitations of RLHF Models",
            "char_start_offset": 79681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 384
                },
                {
                    "start": 386,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1341
                },
                {
                    "start": 1343,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 2063
                },
                {
                    "start": 2063,
                    "end": 2262
                },
                {
                    "start": 2262,
                    "end": 2386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0028667449951171875
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
            "score": 0.4498686188256984,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0059356689453125
        },
        {
            "corpus_id": "278714795",
            "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following",
            "text": "instruction-following performance without a reward model: Across three different base models and four diverse instruction-following benchmarks, including ArenaHard [26] and WildBench [28], BLEUBERI matches (and sometimes exceeds) the performance of reward model-guided RL and SFT according to both automatic and human evaluations. This is a striking result, given the simplicity and cost-effectiveness of BLEUBERI compared to training and deploying large reward models for RLHF. Human evaluators find that BLEUBERItrained models are just as good as those from models aligned with reward models. Furthermore, BLEUBERI models produce more factually-grounded responses than those aligned with either reward models or SFT. Taken as a whole, ours is the first work to show that optimizing BLEU-far from overfitting to superficial n-gram matches-actually promotes helpful, factual, and well-formatted responses on general-domain instruction-following tasks. Given access to high-quality synthetic references (readily available via existing datasets or generated from powerful LLMs), our BLEUBERI method presents a novel alternative to alignment that entirely avoids training large, complex reward models. \n\n2 How well do simple reference-based metrics capture human preferences? \n\nIn this section, we first investigate how well reference-based string matching metrics correlate with with human preference judgments on publicly available instruction-following datasets. To do so, we generate synthetic references for a subset of examples, and we find that BLEU is surprisingly competitive with state-of-the-art reward models in terms of agreement with human preferences. Moreover, BLEU's agreement with human preferences improves with more synthetic references (especially those from more powerful LLMs).",
            "score": 0.4489218057832185,
            "section_title": "Strong",
            "char_start_offset": 3174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "270357771"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00731658935546875
        },
        {
            "corpus_id": "267406252",
            "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
            "text": "In summary, our empirical studies show that a few harmful data in the user finetuning stage can potentially compromise the alignment2 . To uncover the hidden reason of the corruption when harmful data is present, we further derive the statistics of the model to assist our analysis. \n\n\u2022 Training loss over the alignment data. We record the model's loss over the alignment dataset (the one used for alignment). As shown in Figure 3, for the model produced by SFT, the alignment loss is increased when the harmful ratio becomes larger. This partially explains that the model is less aligned to the alignment data after finetuning on more harmful user data, i.e., it starts to forget the alignment knowledge. For the non-aligned model, we see that the alignment loss starts in a high value and then becomes stable at the same level even finetuning on more harmful data. We again resort to Appendix A.3 for an explanation of this phenomenon. \u2022 Hidden embedding drift. To further explain the change of alignment loss as well as the harmful score with harmful ratio, we measure the drift of hidden embedding after user finetuning. More precisely, embedding drift is measured as the L2 norm of the difference between the hidden embedding of the aligned model (or pre-trained model for Non-aligned) and that of the finetuned model over the same alignment data. We see that the embedding drift of the SFT model is significantly higher when the harmful ratio is higher. The same phenomenon is observed for non-aligned model. but the drift is less severe. Our explanation is that more drift is needed to be introduced to the embedding of an SFT model to overwhelm the learned pattern from alignment data. We refer to the embedding drift phenomenon as \"Harmful Embedding Drift\" (HED). As the drift follows the same trend with the harmful score, we conjecture that it is the hidden reason responsible for the corruption of model alignment. Our justification is that with such a significant drift, the perturbed embedding may no longer encode the right information of the input, thereby breaking the alignment. \n\nThe fundamental reason of harmful embedding drift. \n\nWe now formally explain why finetuning on user data in essence changes the hidden embedding of alignment data.",
            "score": 0.44852048709512893,
            "section_title": "Risk Analysis",
            "char_start_offset": 11194,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2149
                },
                {
                    "start": 2152,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00307464599609375
        },
        {
            "corpus_id": "271227251",
            "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
            "text": "Pre-training tasks may involve language modeling [95], favored by decoder-only architectures for predicting subsequent tokens, or de-noising autoencoding [132], which focuses on correcting or replacing corrupted tokens. \n\nFine tuning or Adaptive tuning: The fine-tuning stage is crucial for adapting pre-trained LLMs to specific domains or tasks, leveraging labeled examples or reinforcement learning to refine the model's understanding and predictive capabilities. It encompasses two main strategies: instruction tuning and alignment tuning. \n\nInstruction tuning entails the fine-tuning of a language model by incorporating explicit instructions or demonstrations during training. This approach is designed to direct the model towards desired behaviors and outcomes, facilitating a more targeted response to tasks. The instructions for this tuning can be derived from existing datasets reformatted to include clear directives or crafted to reflect specific human needs. Alignment tuning, on the other hand, aims to adjust the LLM's outputs to match human expectations accurately, a process that may involve a trade-off known as the alignment tax [106]. This concept refers to potential compromises in the model's capabilities as it is fine-tuned to prioritize outputs that are deemed more acceptable or beneficial from a human perspective. The most commonly used alignment criterias are helpfulness, honesty, and harmlessness [106] [99]. Few other criteria are also mentioned like behavior, intent, incentive, and inner aspects [137].",
            "score": 0.448198388281374,
            "section_title": "B. Large Language Models",
            "char_start_offset": 28944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 222,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1535
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1433,
                    "end": 1437,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004451751708984375
        },
        {
            "corpus_id": "270559708",
            "title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
            "text": "ICA refers to using in-context learning with carefully designed prompts to achieve alignment without adjusting the model's parameters.\n\nIn-Context Learning (ICL) Since the discovery of ICL capabilities in LLMs (Brown et al., 2020), there has been a growing body of research exploring the underlying mechanism and applications of ICL (Bai et al., 2024;Abernethy et al., 2024).For instance , Von Oswald et al. (2023) and Dai et al. (2023) examined the mechanism of ICL from the perspective of gradient descent learning, suggesting that ICL functions as an implicit fine-tuning method.Other studies have investigated how contextual examples impact model performance.Min et al. (2022) demonstrated that randomly replacing labels in contextual demonstrations has minimal effect on the performance of various classification and multiple-choice tasks.Wu et al. (2023) introduced a self-adaptation mechanism for selecting and arranging contextual examples, thereby improving the model's few-shot learning capabilities.Li and Qiu (2023) proposed a metric to assist the model in determining the optimal arrangement of examples.\n\nAlignment With ICL Earlier research on ICL mainly focused on tasks such as classification and multiple-choice questions.However, recent work has started to explore the application of ICL to a wider array of tasks.Ye et al. (2023) explored the direct relationship between ICL and instructions, demonstrating that inserting taskirrelevant prompts in the input can also enhance the instruction-following capabilities of large language models (LLMs) during reasoning.Han (2023) applied ICL to open-domain dialogue tasks, introducing the concept of In-Context Alignment (ICA).They achieved this by retrieving and concatenating multiple question-answer pairs as a prompt prefix for dialogue tasks, enabling the base model to acquire a certain level of instruction comprehension.Urial (Lin et al., 2023) took this further by using only three fixed, carefully designed questionanswer pairs along with a system prompt, combining these elements using Markdown 2 format as the prompt prefix.The results showed that this method achieves comparable performance to chat models on the proposed alignment dataset.",
            "score": 0.44819541076179825,
            "section_title": "In-Context Alignment (ICA)",
            "char_start_offset": 5000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 136,
                    "end": 375
                },
                {
                    "start": 375,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 1010
                },
                {
                    "start": 1010,
                    "end": 1117
                },
                {
                    "start": 1119,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1332
                },
                {
                    "start": 1332,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 2099
                },
                {
                    "start": 2099,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 374,
                    "matchedPaperCorpusId": "258947312"
                },
                {
                    "start": 388,
                    "end": 414,
                    "matchedPaperCorpusId": "254685643"
                },
                {
                    "start": 419,
                    "end": 436,
                    "matchedPaperCorpusId": "254877715"
                },
                {
                    "start": 844,
                    "end": 860,
                    "matchedPaperCorpusId": "254877590"
                },
                {
                    "start": 1010,
                    "end": 1027,
                    "matchedPaperCorpusId": "257219778"
                },
                {
                    "start": 1332,
                    "end": 1348,
                    "matchedPaperCorpusId": "257233109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021495819091796875
        },
        {
            "corpus_id": "276937618",
            "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
            "text": "Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask,\"can prompting help us teach LLMs how to learn\". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model's interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.",
            "score": 0.44780734333917827,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00206756591796875
        },
        {
            "corpus_id": "274436578",
            "title": "VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs",
            "text": "The Supervised Fine-Tuning (SFT) stage involves further training on the pretrained model to refine it task-solving capabilities and ensure greater alignment with human instructions across multiple domains [41,72]. During the SFT phase, it is a typical practice to employ datasets specific to a particular domain for the fine-tuning of LLMs, which may lead to a decline in performance on non-target domains, a phenomenon commonly referred to as catastrophic forgetting. We conducted experiments on open-sourced models including LLaMA [18,52,53] and Qwen [8,64] series to assess how the model's proficiency in other domains changes when finetuned with data from a single domain, as illustrated in Table 1 and Figure 2. We have regulated the number of training instances per epoch to a fixed count of 10,000. More details about training and evaluation settings can be found in Section 5.1. Our findings indicate that when a model is trained exclusively with data from a single domain, its performance on tasks from other domains tends to degrade progressively over the course of training. \n\nWhile recent research has delved into exploring fine-tuning methods for multi-task enhancement [17,45], they are still in their early stages. However, as shown by proprietary models such as GPT-4 [1] and Gemini [50], which exhibit outstanding multi-task performance, improving a model's versatile capabilities across various domains during the SFT phase is crucial. Therefore, our work systematically investigates methods to enhance multi-domain performance during the SFT stage to bridge this gap.",
            "score": 0.4475522365875667,
            "section_title": "Supervised Fine-Tuning.",
            "char_start_offset": 9185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1586
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "257365347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016679763793945312
        },
        {
            "corpus_id": "262898422",
            "title": "Jointly Training Large Autoregressive Multimodal Models",
            "text": "Supervised fine-tuning is a fundamental tool to leverage the abilities of large pretrained models. Recently, instruct tuning has been extended to a multimodal setting (Liu et al., 2023;Dai et al., 2023); however, all the existing approaches are focused on visual understanding abilities. In this work, we study instruction tuning tailored to interleaved image-text generation. We collect a small and curated mixed-modal dataset to teach our JAM model to support textual explanations with coherent images. Since in the first stage, the model has been trained on image-text captions and text-only data; we train on interleaved image-text data during this phase. In line with the superficial alignment hypothesis introduced in LIMA (Zhou et al., 2023), we demonstrate that the model can quickly learn the style of images and text from a small curated dataset. Our results suggest that the Superficial Alignment Hypothesis introduced in LIMA holds not only for learning the text style but also for images. In our experiments, we consider two slightly different instruction tuning settings, we introduce a small portion of the image-text Shutterstock data with retrieval augmentation and we find this approach beneficial to preserve the generated image quality when generating with retrieval augmentation. Sect 3 presents a comparison between these two strategies. We train using a standard supervised procedure without leveraging any reinforcement learning or human preference strategy. In this instruction-tuning phase, we leverage interleaved image-text data in contrast to previous methods (Koh et al., 2023a) that rely only on image-text caption and no instruction tuning, our experimental results confirm the benefits of training with interleaved image-text data.",
            "score": 0.4474875899098233,
            "section_title": "MULTIMODAL CONVERSATIONAL INSTRUCT TUNING",
            "char_start_offset": 9156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1764
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "260704721",
            "title": "In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning",
            "text": "In this note, we show a simple concept of incontext alignment that makes vanilla pretrained language models capable in following chat-style instructions without fine-tuning. By retrieving an average of 9.4 demontration alignment examples, the 13B Llama-2-vanilla model achieves a win-rate of 78.4% vs. OpenAI's text-davinci-003, up from 11.4% when prompted directly. Compared to finetuning-based alignment, we argue in this section that in-context alignment can have advantages in efficiency and interpretability. \n\nFor example, to deploy models with different alignment objectives (e.g., different styles, data sources, etc.), finetuning-based framework would spend resources in model training in advance and load different model weights across the servers, demanding pre-hoc decisions on balancing the resources. In-context alignment, however, loads the same vanilla model weights on all servers and has the ability to perform different alignments on the same server or even same batch at inference time. \n\nIn-context alignment would also facilitate quick evaluations for the model checkpoints during pretraining, without fine-tuning each model checkpoint to follow instructions. Facilitating such evaluation to the pretraining trajectory of language models may tell us more about when the key knowledge behind instruction following emerges. \n\nFurthermore, being able to retrieve an extremely small set of active alignment examples makes the alignment more transparent (e.g., Table 4). This could be helpful to the developers of the alignment dataset, indicating inappropriate source data or a lack of data under certain topics. \n\nOpen questions following this note include whether we can build RLHF upon or as in-context alignment, as well as how to support multi-turn dialogs or instructions with long contexts within our in-context alignment framework.",
            "score": 0.4473471758372808,
            "section_title": "Implications",
            "char_start_offset": 9980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1343
                },
                {
                    "start": 1346,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0030879974365234375
        },
        {
            "corpus_id": "277955495",
            "title": "Synergistic Weak-Strong Collaboration by Aligning Preferences",
            "text": "Interaction strategies between weak-strong models In our experiments, we examine two key interaction strategies between weak and strong models: (1) W/o Alignment, where the weak model generates initial responses that the strong model then refines without any model alignment, and (2) With Alignment, which involves fine-tuning the weak model based on the strong model's preferences. We further explore different formats for the weak model's output to inform the strong model: \n\n(1) Direct Answer, providing a straightforward response to the user query; \n\n(2) Domain Knowledge, supplying background information relevant to the reasoning; and (3) Chain of Thought (CoT), offering detailed explanations with the answer. By combining these two interaction strategies with the three formats, we assess each combination's effectiveness in handling specialized tasks. We report the EM scores for Counterfactuals and the accuracy scores for Medicine and Ethics. As shown in Figure 5, our experiments clearly demonstrate the effectiveness of model alignment across all three datasets comparing the results with and without alignment. Particularly, the Chain of Thought (CoT) format stands out as the most advantageous, surpassing both Direct Answer and Domain Knowledge formats. Its detailed reasoning path significantly assists the strong model in handling complex queries, as seen in its stronger performance on the ethics and counterfactual datasets-both of which demand advanced reasoning. In contrast, the medicine dataset, requiring substantial domain knowledge, shows less variation across interaction strategies, suggesting that taskspecific expertise can outweigh interaction style when the knowledge requirement is paramount. \n\nImpact of different strong models: General capabilities enhance problem-solving. In this setup, we standardized the strong model for specific domains. Llama-3-8B served as the weak model across all datasets, allowing us to evaluate the performance of different strong models-GPT-4, Llama-3-70B (Dubey et al., 2024), GPT-3.5-Turbo, and Llama-2-70B (Touvron et al., 2023)-across various domains. According to the experiment results in Figure 5, the strong model GPT-4, when engaged in the domain of Counterfactuals, exhibits the highest accuracy at 75.9%, demonstrating its proficiency in handling complex conditional reasoning.",
            "score": 0.4473087425347332,
            "section_title": "Analysis",
            "char_start_offset": 19681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2355
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005687713623046875
        },
        {
            "corpus_id": "273507468",
            "title": "Training of Scaffolded Language Models with Language Supervision: A Survey",
            "text": "Knowledge Conflict. LMs embed an immense amount of knowledge in their weights, but at the same time, it is limited by the extent of their parametric training. To expand its knowledge and capabilities, we supply knowledge at inference time in the form of retrieved passages, database look-ups, tool outputs, or developer-written hints. However, combining this contextual knowledge and parametric knowledge has been a non-trivial task, and when the two sources disagree, a knowledge conflict arises (Li et al., 2025a;Xu et al., 2024). For example, if asked \"What is the capital of France?\" with a system prompt stating \"The capital of France is Rome,\" a model might still answer \"Paris\" based on its parametric knowledge (Jin et al., 2024b). \n\nEmpirical studies show that when signals conflict, LMs first lean on their parametric knowledge (Jin et al., 2024a;Tan et al., 2024;Kortukov et al., 2024). Empirical evidence suggests that context-parametric inversion during instruction tuning is one potential pathway for knowledge conflict to arise: the model initially relies on context at the start of instruction tuning, but this reliance on the context decreases as instruction finetuning continues (Goyal et al., 2024). \n\nWhile LMs often default to parametric knowledge, findings suggest that under the right conditions, models can be highly sensitive to contextual information. LMs can indeed adopt externally provided evidence if it is coherent and well-structured, though they still exhibit confirmation bias when that evidence partially aligns with their parametric knowledge (Xie et al., 2024). There is also evidence demonstrating that Chain-of-Thought prompting is strongly influenced by contextual features such as prompt formatting, and these contextual cues can override internal reasoning (Turpin et al., 2023). Moreover, instruction-tuned models may agree with incorrect user statements out of a learned alignment preference, further demonstrating that models sometimes defer to contextual input, even when it contradicts their underlying knowledge (Wei et al., 2023).",
            "score": 0.4468511063594611,
            "section_title": "Post-Training",
            "char_start_offset": 48750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 531,
                    "matchedPaperCorpusId": "268379757"
                },
                {
                    "start": 719,
                    "end": 738,
                    "matchedPaperCorpusId": "268041323"
                },
                {
                    "start": 838,
                    "end": 857,
                    "matchedPaperCorpusId": "267782658"
                },
                {
                    "start": 857,
                    "end": 874,
                    "matchedPaperCorpusId": "270257714"
                },
                {
                    "start": 1579,
                    "end": 1597,
                    "matchedPaperCorpusId": "263610324"
                },
                {
                    "start": 1799,
                    "end": 1820,
                    "matchedPaperCorpusId": "258556812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003765106201171875
        },
        {
            "corpus_id": "268091264",
            "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
            "text": "Many empirical studies observe that DNNs tend to learn simple concepts during the learning process (Arpit et al., 2017;Liu et al., 2021;Mangalam and Prabhu, 2019). Furthermore, Xu et al. (2019), Liu et al. (2023a), Zhou et al. (2024), andTian et al. (2023) theoretically explain the learning preference of DNNs. Meanwhile, many researchers focus on analyzing the utility of fine-tuning for language models (Merchant et al., 2020;Hao et al., 2020;Aghajanyan et al., 2021;Zhou and Srikumar, 2022;Mosbach et al., 2020) and attempt to understand the in-context learning (Ren et al., 2024a). However, few previous studies investigate how trustworthiness is learned by LLMs during pre-training. In this paper, we take a closer look at the learning dynamic of trustworthiness within LLMs' representations. \n\nAs the capabilities of LLMs have increased, conventional alignment techniques that rely on \"human feedback\" (like RLHF) may no longer work when trying to align models that are more powerful than humans (Burns et al., 2023;Yuan et al., 2024). To address this challenge, research institutions are actively exploring new solutions. For example, OpenAI introduces \"superalignment\" and proposes a \"weak-to-strong supervision\" approach (Burns et al., 2023). Also, Meta proposes a \"self-reward\" mechanism (Yuan et al., 2024). At the same time, more and more research focuses on the emerging field of \"self-alignment\" (Sun et al., 2023;Li et al., 2023b). In this paper, we provide a deeper understanding of the pre-training dynamics and successfully align the SFT model using its own pre-training checkpoints. We believe that the pre-training period is worth being explored and it may be a promising source for self-alignment.",
            "score": 0.4466214797629928,
            "section_title": "Understanding the training process of DNNs.",
            "char_start_offset": 24737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1719
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 119,
                    "matchedPaperCorpusId": "11455421"
                },
                {
                    "start": 195,
                    "end": 213,
                    "matchedPaperCorpusId": "268042694"
                },
                {
                    "start": 215,
                    "end": 238,
                    "matchedPaperCorpusId": "268701418"
                },
                {
                    "start": 406,
                    "end": 429,
                    "matchedPaperCorpusId": "216914339"
                },
                {
                    "start": 429,
                    "end": 446,
                    "matchedPaperCorpusId": "227905362"
                },
                {
                    "start": 446,
                    "end": 470,
                    "matchedPaperCorpusId": "229371560"
                },
                {
                    "start": 470,
                    "end": 494,
                    "matchedPaperCorpusId": "235658872"
                },
                {
                    "start": 494,
                    "end": 515,
                    "matchedPaperCorpusId": "222141026"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020351409912109375
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "We suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through Knowledge Probability P (y|x) and Instruction Probability P (y c |c, x), respectively (as depicted in Fig. 2).Our empirical analysis within a continual instruction tuning framework reveals distinct forgetting patterns between these two aspects, with shifts in instruction following primarily driving performance declines.\n\nTo investigate the internal changes of the model during forgetting, we introduce the Instruction Vector (IV) framework to extract representations closely associated with the task processing.We hypothesize a straightforward yet robust computational graph for LLMs (see Fig. 1 b), featuring an intermediate variable \u03b8 c crucial for task performance.The presence or absence of \u03b8 c directly impacts the model's capability to handle instruction c.This hypothesis is supported by causal intervention experiments in Sec.3.2.By analyzing IV dynamics pre and post-training, we find minor changes in IV expression with forgetting happens.Furthermore, explicitly incorporating IV into the model's computational graph can recover the mastery of the corresponding instruction.This results indicate that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting.\n\nBuilding on these insights, we develop an IVguided training methodology to mitigate catastrophic forgetting.This method incorporates a progressive IV-intervention training mechanism, in which the IV is initially introduced through intervention and is then gradually phased out during the training process.The deliberate inclusion of IV aids in optimizing the model by ensuring adherence to the IV-related computational graph, thereby minimizing the overshadowing effect of new reasoning pathways.Additionally, we have introduced an IVbased KL-Divergence loss function to reduce the discrepancies between zero-shot and IV-intervened logits, ensuring that the model's behavior remains aligned with the original computational structure.Validated across multiple datasets, this method significantly alleviate forgetting in both general and in-context learning abilities, confirming the link between IV and forgetting.\n\nMain Findings and Contributions.(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.",
            "score": 0.44604770322266396,
            "section_title": "Introduction",
            "char_start_offset": 1804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 458
                },
                {
                    "start": 460,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1372
                },
                {
                    "start": 1374,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2107
                },
                {
                    "start": 2107,
                    "end": 2287
                },
                {
                    "start": 2289,
                    "end": 2321
                },
                {
                    "start": 2321,
                    "end": 2604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020198822021484375
        },
        {
            "corpus_id": "261076203",
            "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models",
            "text": "Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.",
            "score": 0.44583984784553654,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0038700103759765625
        },
        {
            "corpus_id": "276937618",
            "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
            "text": "Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks (OpenAI et al., 2024). As model sizes increase, the performance of LLMs on multi-step reasoning, instruction following, and program execution (Wei et al., 2022b;Du et al., 2024) have been found to improve empirically. LLMs are trained via a three-step process: pretraining (which results in a base model) to compress knowledge over a vast text corpus, supervised fine-tuning for instruction following, and alignment with human expectations of behavior for use as a chat bot (Ouyang et al., 2022;Lee et al., 2023;Rafailov et al., 2024;Ethayarajh et al., 2024). \n\nHowever, LLMs remain unaware of information and events occurring after their knowledge cutoff; in fast-moving domains and in scenarios where deployment requires knowledge of up-to-date information, there is a need to remedy this limitation. There are two popular approaches to this problem. The first is to increase the context length of the model until all anticipated new information fits within this context (the largest of which is Google's Gemini-1.5 model (Reid et al., 2024) with a context length of two million tokens). However, even context lengths this large can be exhausted and it is unclear whether the model's attention mechanism is capable of accurately inferring signal regardless of where it is in the context. The alternate approach uses external knowledge stores via retrieval augmented systems (Lewis et al., 2021). This approach works well when the reasoning abilities already learned by the model suffice to process and extract the relevant information. But gradient-based learning remains vital in scenarios where there is a need to teach the model how to manipulate new tools or learn new strategies for reasoning. \n\nThe simplest approach to update model knowledge via fine-tuning is to continue pretraining the base model. Unfortunately, the data and training procedure necessary to replicate the additional finetuning and alignment phases are rarely open-sourced in chat-based models. The general practice is to fine-tune the aligned model with new domain-specific knowledge.",
            "score": 0.4458298248800687,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 256,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 569,
                    "end": 590,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 607,
                    "end": 629,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0023326873779296875
        },
        {
            "corpus_id": "266725532",
            "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
            "text": "Comprehensive experiments demonstrate the superiority of this method across reasoning tasks, offering a promising approach to responsibly downsize LLMs. DRESS: (Chen et al., 2023) propose using natural language feedback (NLF), specifically critique and refinement NLF, to improve alignment with human preferences and interaction capabilities of large vision language models (LVLMs). They generalize conditional reinforcement learning to effectively incorporate non-differentiable NLF by training the model to generate corresponding responses conditioned on the NLF. Experiments show relative improvements in DRESS over prior state-of-theart LVLMs in metrics of helpfulness, honesty, and harmlessness alignment. MixAlign: Despite having accurate reference points, LLMs may disregard them and rely on incorrect references or biases instead. This tendency to hallucinate arises when users ask questions that do not directly align with the retrieved references, lacking detailed knowledge of the stored information. (Zhang et al., 2023b) focus on this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the user and knowledge base to clarify how the user question relates to the stored information. MixAlign uses a language model to achieve automatic knowledge alignment and, if needed, further enhances this alignment through user clarifications. MixAlign focuses on utilizing grounding knowledge for faithful decision-making. In cases of uncertainty or unclear evidence, MixAlign generates a question seeking clarification from the user -a process referred to as human-assisted knowledge alignment. Chain-of-Verification (CoVe): (Dhuliawala et al., 2023) develop the CoVe method where the model 1. Drafts an initial response. \n\n2. Plans verification questions to fact-check its draft. \n\n3. Answers those questions independently so the answers are unbiased. \n\n4. Generates a final verified response. \n\nExperiments show CoVe decreases hallucinations across tasks like list-based Wikidata questions and long-form text generation. Given a user query, an LLM generates a baseline response that may contain inaccuracies like factual hallucinations. CoVe first generates verification questions to ask, then answers them to check for agreement.",
            "score": 0.4455222991908864,
            "section_title": "Self-refinement through feedback and reasoning",
            "char_start_offset": 17783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 1934
                },
                {
                    "start": 1937,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029239654541015625
        },
        {
            "corpus_id": "263334580",
            "title": "Self-Specialization: Uncovering Latent Expertise within Large Language Models",
            "text": "To motivate our exploration of self-specialization, we first begin by addressing a fundamental question: How well do generally aligned models perform on specialized domains? While popular models, such as Alpaca (Taori et al., 2023) and Dromedary (Sun et al., 2023), have demonstrated effectiveness in following general instructions, it remains unclear whether general alignment can also elicit expertise for a certain domain. \n\nInvestigating this, we assess the capabilities of Alpaca and Dromedary against their base model, LLaMA-65B (Touvron et al., 2023a), on a collection of benchmarks within the biomedical domain. We evaluate Alpaca as an upper bound, due to its reliance on GPT-3.5-generated datasets (Ouyang et al., 2022) via the self-instruct process (Wang et al., 2022a), unlike Dromedary, which generates instructional data from its base model. We use 10 biomedical NLP datasets (see Section 4 for details), covering a diverse set of tasks to ensure a comprehensive mix of content and also to look at the cross-task generalization, the core of instructiontuning. Table 1 summarizes the result. \n\nWe find that both Alpaca and Dromedary have only a slight (1.2 -2.5) advantage over LLaMA in biomedicine. While they are aligned to handle a broad set of instructions, they do not seem to effectively improve their specialized domain expertise; intuitively trading their expertise for generality given finite parameters. In light of these findings, it becomes evident that for cases where we are only interested in expert domains for all our downstream tasks, there remains a large potential for improvement beyond the generic alignment. This underscores the need for a model or approach, like self-specialization, that could potentially uncover specialization while maintaining cross-task generalizability with minimal supervision.",
            "score": 0.4452859268801984,
            "section_title": "Preliminaries: Benchmarking Existing Aligned Models",
            "char_start_offset": 4075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1838
                }
            ],
            "ref_mentions": [
                {
                    "start": 708,
                    "end": 729,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00511932373046875
        },
        {
            "corpus_id": "272366500",
            "title": "Self-Judge: Selective Instruction Following with Alignment Self-Evaluation",
            "text": "Evaluation Criteria. For instruction-following LLMs, alignment evaluation aims to measure how well the model outputs align with human preference. Preference for desired outputs may vary among different users. In this work, we follow previous work that focuses on evaluating the preferences shared by most users which are helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response (Zheng et al. 2023). As demonstrated by Zheng et al. (2023), when prompting GPT-4 as the evaluator using these aspects of preference, it can have a high correlation with human evaluations. Below is a breakdown of each aspect: \n\n\u2022 Helpfulness measures whether the model response effectively addresses the instruction. A helpful response should provide valuable solutions to solve the problems of the user. \u2022 Relevance indicates how closely the model response aligns with the user question or the topic of the question. A highly relevant response should directly address the user's problem without discussion about unrelated areas. \u2022 Accuracy evaluates the correctness of the information provided in the response. \n\nAn accurate response should be free from errors and based on knowledge that can be traced to some sources. \u2022 Depth focuses on the thoroughness and comprehensiveness of the response. An answer with depth should go beyond a superficial response, offering a detailed understanding or insight. \u2022 Creativity is about the novelty of the response. Creative responses should introduce new ideas, solutions, or perspectives that are not conventional. \u2022 Level of detail assesses the amount of information contained in the response. \n\nA detailed response should contain specific examples, data, or explanations to support the main arguments. \n\nBy focusing on the six aspects during evaluation, a comprehensive score will be induced as a measure of the level of alignment. As shown in the prompt in Fig. 2, the alignment score is an integer from 1 to 10. Types of Alignment Evaluation. By evaluating a response y \u2032 \u223c F(y \u2032 |x), an estimator J \u03b8 parameterized by \u03b8 produces a quality score z. There are two types of alignment evaluation: reference-free and reference-based estimation. \n\n\u25b7 Reference-free: It considers the scenario that the reference answer is usually not available at test time. The estimator takes in x and y \u2032 for evaluation, i.e., z \u223c J \u03b8 (z|x, y \u2032 ).",
            "score": 0.4452773385652579,
            "section_title": "Alignment Evaluation",
            "char_start_offset": 13287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2195
                },
                {
                    "start": 2198,
                    "end": 2306
                },
                {
                    "start": 2307,
                    "end": 2382
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006587982177734375
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "We consider abstraction alignment to be a methodology (i.e., an overarching strategy or conceptual foundation [31]) for comparing the alignment of model-learned and human-encoded abstractions. In this paper, we have instantiated one method for measuring abstraction alignment that compares model outputs to human abstraction graphs. This approach has proven valuable in assessing model and dataset alignment across computer vision, natural language, and medical domains. However, we expect there are many methods for measuring abstraction alignment and believe that developing these methods -tailored to different models, domains, and users -provides exciting opportunities for future work. \n\nOur current method uses the model's output confidence as a proxy for its internal abstractions. While this approach is model agnostic, allowing us to apply it across a range of models and flexibly extend it to dataset analysis, its reliance on model uncertainty limits our ability to measure abstraction alignment when models are confidently correct. Alternative abstraction alignment methods could overcome this limitation by extracting abstractions directly from the model's internal representations, drawing inspiration from methods that identify a model's internal state [58,83] or its procedure for producing an output [40,53,104,152]. New metrics could measure the alignment between these extracted model representations and existing human abstractions, revealing how a model's abstractions change across layers and evolve during training. \n\nAnother limitation of our abstraction alignment method is its dependence on human abstraction graphs as proxies for human knowledge. Currently, abstraction alignment is limited to domains where abstraction graphs exist, such as linguistics [44,90] and healthcare [67,68,157]. In some domains, abstraction graphs may not perfectly capture task semantics. For instance, while WordNet maps specific areas (e.g. Patagonia) to more general locations (e.g., Chile), it is not a comprehensive location database and omits many towns (e.g., Coyhaiqe) [44,90]. As a result, in a location prediction task, WordNet may not include every location the model outputs and abstraction alignment may not fully capture the model's abstractions. However, we are encouraged by the extensive research on knowledge graph generation [61,66].",
            "score": 0.44506552489233503,
            "section_title": "Discussion and Future Work",
            "char_start_offset": 70880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 1268,
                    "end": 1272,
                    "matchedPaperCorpusId": "106402715"
                },
                {
                    "start": 1272,
                    "end": 1275,
                    "matchedPaperCorpusId": "235294296"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "258426987"
                },
                {
                    "start": 1785,
                    "end": 1788,
                    "matchedPaperCorpusId": "1671874"
                },
                {
                    "start": 1808,
                    "end": 1811,
                    "matchedPaperCorpusId": "33285731"
                },
                {
                    "start": 1811,
                    "end": 1815,
                    "matchedPaperCorpusId": "31594479"
                },
                {
                    "start": 2087,
                    "end": 2090,
                    "matchedPaperCorpusId": "1671874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004150390625
        },
        {
            "corpus_id": "274464899",
            "title": "Is Large-Scale Pretraining the Secret to Good Domain Generalization?",
            "text": "Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has dramatically improved benchmark results. However, it remains unclear if DG finetuning methods are becoming better over time, or if improved benchmark performance is simply an artifact of stronger pre-training. Prior studies have shown that perceptual similarity to pre-training data correlates with zero-shot performance, but we find the effect limited in the DG setting. Instead, we posit that having perceptually similar data in pretraining is not enough; and that it is how well these data were learned that determines performance. This leads us to introduce the Alignment Hypothesis, which states that the final DG performance will be high if and only if alignment of image and class label text embeddings is high. Our experiments confirm the Alignment Hypothesis is true, and we use it as an analysis tool of existing DG methods evaluated on DomainBed datasets by splitting evaluation data into In-pretraining (IP) and Out-of-pretraining (OOP). We show that all evaluated DG methods struggle on DomainBed-OOP, while recent methods excel on DomainBed-IP. Put together, our findings highlight the need for DG methods which can generalize beyond pretraining alignment.",
            "score": 0.4450556285712255,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006565093994140625
        },
        {
            "corpus_id": "277621931",
            "title": "CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness",
            "text": "Table 1 presents the results of medium-sized LMs before and after cultural alignment using CARE, while Table 3 compares these results with various baselines. We have the following key findings: \n\nUsing CARE, culturally-aligned LMs achieve higher average scores (up to 29% improvement) in both Chinese and Arabic compared to the vanilla checkpoints, even for these already instructiontuned models. This resonates with the findings of Zhou et al. (2024a) and shows that a relatively small amount of carefully curated data by humans can improve LMs' alignment. We also see a noticeable gap among LMs developed by different regions. The Qwen2.5-7B developed by Chinabased Alibaba performs the best in Chinese and can be further improved from 7.28 to 7.61 (out of 10) overall by aligning with CARE. Interestingly, the aligned version of Gemma2-9B outperforms Qwen2.5-7B on social norms and commonsense for Chinese (more on this in \u00a75.2). We also test its generalization capabilities on other culture-related tasks in Appendix D. \n\nCultural alignment strengthens stronger LMs but fails to address weaknesses in weaker LMs. Qwen2.5-7B and Gemma2-9B, the top-performing LMs on Chinese and Arabic, respectively, show consistent improvement across all five cultural categories after alignment. However, alignment shows limited gains where the model's initial performance is poor. Mistral-7B is not improved on Chinese entities and literacy, where its starting scores are only 3.03 and 2.43. Similarly, Qwen2.5-7B, does not benefit from alignment on Arabic entities. This suggests that base models need a foundational level of cultural knowledge for alignment to be effective. \n\nCulture-specific human preference data is helpful beyond general human preference. Table 3 shows that alignment with CARE's cultural preference consistently outperforms general preference datasets by up to 42%. Additionally, cultural alignment with CARE brings improvement in both CoT and role-play prompting. While culture-specific SFT outperforms generic instruction tuning, alignment yields greater gains, highlighting the importance of human cultural preference data over ground-truth demonstrations alone.",
            "score": 0.44495948888840087,
            "section_title": "Main Results",
            "char_start_offset": 14703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029239654541015625
        },
        {
            "corpus_id": "273185840",
            "title": "Only-IF:Revealing the Decisive Effect of Instruction Diversity on Generalization",
            "text": "The rapid advancements in large language models (LLMs) have revolutionized a wide range of tasks, including language comprehension [41], generation [3], knowledge-based question answering [15], and solving complex reasoning problems in fields like mathematics [11,16] and programming [6,1,7,25]. These successes hinge on the foundational capabilities of LLMs, such as knowledge retrieval, reasoning, planning, and notably, instruction following-enabled through instruction-tuning [34,40,45,37]. Instruction-tuning trains LLMs on a wide variety of instruction-output pairs, allowing them to handle diverse prompts and better generalize to new tasks. \n\nWhile knowledge retrieval focuses on accessing stored information and reasoning involves multi-step problem-solving, instruction following concerns the accurate interpretation and execution of diverse natural language prompts [55]. This capability is vital for user interaction, as it involves understanding the intent behind instructions and performing tasks without relying on complex logic [29]. Despite its importance, the mechanisms underlying instruction following remain less explored compared to other capabilities like reasoning. \n\nThe current research landscape on instruction tuning has produced varied and sometimes contradictory findings, ranging from the impact of dataset selection [54] to the effects of scaling up data [51,52]. These disparate findings suggest that instruction following in LLMs is influenced by the scale and composition of fine-tuning data in complex ways [13,53]. However, a systematic investigation into the effect of data on each core capability of LLMs -specifically isolating instruction following from reasoning and knowledge retrieval-has been limited. As a result, there is a lack in principled and practical guidelines on how to compose data to improve instruction-following capabilities. \n\nOur work addresses this gap by focusing explicitly ONLY on the Instruction-Following capabilities of LLMs. We first introduce a systematic analysis of instruction diversity through a controlled symbolic task-string rewrites-inspired by the Turing-complete Markov algorithm [33]. By isolating the effects of instruction diversity, we focus on the model's ability to follow instructions without conflating this with reasoning capabilities. Through controlled experiments, we examine the impact of instruction diversification across various semantic domains on the model's ability to adapt to unseen instruction semantics.",
            "score": 0.44454148642159375,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2324
                },
                {
                    "start": 2325,
                    "end": 2506
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 1391,
                    "end": 1394,
                    "matchedPaperCorpusId": "268032247"
                },
                {
                    "start": 2160,
                    "end": 2164,
                    "matchedPaperCorpusId": "118070944"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0034160614013671875
        },
        {
            "corpus_id": "266312608",
            "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
            "text": "this problem, it is difficult to empirically study today. Most prior work on alignment has either confronted this core challenge head-on-but been restricted to primarily theoretical frameworks and toy problems (Irving et al., 2018;Christiano et al., 2018;Leike et al., 2018;Demski & Garrabrant, 2019;Hubinger et al., 2019), or empirically studied humans supervising today's models-without addressing the core challenges that may arise with superhuman models (Christiano et al., 2017;Wu et al., 2021;Ouyang et al., 2022;Bowman et al., 2022;Saunders et al., 2022). In contrast, we would ideally like to have a setup that captures core challenges of aligning future superhuman models while also being able to make iterative empirical progress today. \n\nWe propose a simple setup for studying the problem of humans supervising superhuman models by considering an analogy: can we use weak models to supervise strong models? We can empirically test this by finetuning large (strong) pretrained models on labels generated by small (weak) models and observing how they generalize. Just like the problem of humans supervising superhuman models, our setup is an instance of what we call the weak-to-strong learning problem. \n\nWhy should weak-to-strong learning be possible? On the one hand, the strong model could simply learn to imitate the weak supervisor, including its errors, since that is what we would naively train it to do. On the other hand, strong pretrained models should already have good representations of the alignment-relevant tasks we care about. For example, if a model can generate complicated code, then it should intuitively also know whether that code faithfully adheres to the user's instructions. As a result, for the purposes of alignment we do not need the weak supervisor to teach the strong model new capabilities; instead, we simply need the weak supervisor to elicit what the strong model already knows. This gives us hope that the strong model can generalize beyond the weak supervision, solving even hard problems for which the weak supervisor can only give incomplete or flawed training labels. We call this phenomenon weak-to-strong generalization.",
            "score": 0.44370766394021643,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 519,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00394439697265625
        },
        {
            "corpus_id": "269982303",
            "title": "Instruction Tuning With Loss Over Instructions",
            "text": "However, it does align LMs to act in accordance with the user's intentions [34]. To enable this transfer, various methods for aligning language models [3,14,44,73,58,79] have thus been proposed, one of which is instruction tuning (IT) [36,65,69]. Recent study [78] proposes Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learnt almost entirely during pretraining, only minimal instruction tuning data is required to enable highquality outputs in the desired output style. Existing works [1,43,44,51,65,69,36] mainly perform instruction tuning by focusing the loss computation solely on the output segments. \n\nIn this work, we demonstrate that in many scenarios, incorporating the loss computation for instructions or prompts, which we refer to as INSTRUCTION MODELLING (IM) (see \u00a73), could substantially improve the performance of instruction tuning on both various NLP tasks (e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks (e.g., MT-Bench and AlpacaEval), as shown in Figure 1. Remarkably, in the most favourable case, our proposed method IM boosts performance on AlpacaEval 1.0 by over 100%. As illustrated in Figure 2, our study further identifies two key factors influencing the effectiveness of IM: (1) The ratio between instruction length and output length (see Figure 2 Left). Our analysis shows that our approach IM is especially beneficial for datasets characterised by lengthy instructions or prompts paired with comparably brief outputs, such as Code Alpaca [13] and Less MMLU Chat [68]; (2) The number of training examples (see Figure 2 Right). We demonstrate that our approach IM performs better under the SAH, where a small amount of training examples are available (see \u00a74.2). \n\nRecent works [27,31,44,71,73] suggest that LMs can quickly memorise training examples even after seeing them just once.",
            "score": 0.44335883258928366,
            "section_title": "body",
            "char_start_offset": 1917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1876
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 79,
                    "matchedPaperCorpusId": "53745764"
                },
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "248118878"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "263310485"
                },
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "260866107"
                },
                {
                    "start": 239,
                    "end": 242,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "271745981"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 521,
                    "end": 524,
                    "matchedPaperCorpusId": "244478674"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 527,
                    "end": 530,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 530,
                    "end": 533,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 533,
                    "end": 536,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "271745981"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "260866107"
                },
                {
                    "start": 1267,
                    "end": 1270,
                    "matchedPaperCorpusId": "244478674"
                },
                {
                    "start": 1562,
                    "end": 1565,
                    "matchedPaperCorpusId": "237142385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.150146484375
        },
        {
            "corpus_id": "273654506",
            "title": "Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks",
            "text": "The potential of using Large Language Models (LLMs) themselves to evaluate LLM outputs offers a promising method for assessing model performance across various contexts. Previous research indicates that LLM-as-a-judge exhibits a strong correlation with human judges in the context of general instruction following. However, for instructions that require specialized knowledge, the validity of using LLMs as judges remains uncertain. In our study, we applied a mixed-methods approach, conducting pairwise comparisons in which both subject matter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks. We focused on two distinct fields: dietetics, with registered dietitian experts, and mental health, with clinical psychologist experts. Our results showed that SMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in mental health when evaluating overall preference. Additionally, the results indicated variations in SME-LLM agreement across domain-specific aspect questions. Our findings emphasize the importance of keeping human experts in the evaluation process, as LLMs alone may not provide the depth of understanding required for complex, knowledge specific tasks. We also explore the implications of LLM evaluations across different domains and discuss how these insights can inform the design of evaluation workflows that ensure better alignment between human experts and LLMs in interactive systems.",
            "score": 0.44294093993258843,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021572113037109375
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "Another advantage of UFT lies in combining SFT and alignment into one stage to avoid performance degradation. In this stage, we utilize the HelpSteer2 dataset of 20k examples for alignment [WDD + 24]. For UFT, the 20k examples from UltraChat and 20k examples from HelpSteer2 are merged and utilized for training. For comparison, the best performing SFT model of learning rate 1e \u22124 in the previous experiment is utilized for further fine-tuning using DPO, KTO and UNA. \n\nThe same tasks are utilized for evaluation. In the two HuggingFace open LLM leaderboards, Mistral+UFT outperforms all of Mistral+SFT+DPO, Mistral+SFT+KTO, and Mistral+SFT+UNA in 9 out of 12 tasks. In terms of average scores, Mistral+UFT surpasses all three sequential methods. Several aspects need further discussion. Firstly, the performance degradation problem is evident as the performances of Mistral+SFT+DPO, Mistral+SFT+KTO, and Mistral+SFT+UNA are worse than Mistral+SFT, a phenomenon also known as alignment tax. Additionally, we observe that Mistral+UFT shows significant improvements on ifeval, which tests the model's capability of instruction-following, and truthful, which assesses the model's alignment capabilities. These results indicate that UFT greatly enhances instruction-following and alignment capabilities, demonstrating its effectiveness. In terms of generation capability, Mistral+UFT outperforms the other three sequential methods and it does not seem to suffer from performance degradation, as it performs better than both Mistral+SFT and Mistral+UFT on instruction-tuning data alone. Moreover, Mistral+UFT does not bias towards long generation like the other three sequential methods, which is another advantage of UFT. Simiar observations can be found for Qwen 32B. Qwen+UFT outperforms Qwen+SFT+Alignment in 9 out of 14 tasks, especially the average of the new and old HuggingFace open LLM leaderboards as shown in Table. 4 and Table. 5.",
            "score": 0.4429205050519688,
            "section_title": "UFT vs SFT+Alignment",
            "char_start_offset": 10792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1938
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0015668869018554688
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "According to this hypothesis, the alignment process, primarily through supervised fine-tuning (SFT), does not inject new knowledge or improve inherent abilities but rather adjusts the output response format. This implies that the strong mathematical reasoning ability may not be significantly improved by a large amount of synthetic SFT data. \n\nIn this paper, we re-examine these two common beliefs mentioned above regarding mathematical reasoning abilities of LLMs. For the first belief, we introduce the Skywork-Math model series, which are supervised fine-tuned (SFT) on common 7B pre-trained LLM models without employing other complex alignment techniques such as RLHF (Bai et al., 2022;Casper et al., 2023) and DPO (Rafailov et al., 2024). Skywork-Math 7B models have achieved impressive accuracies of 51.2% on the competition-level MATH (Hendrycks et al., 2021) benchmark and 83.9% on the GSM8K (Cobbe et al., 2021) benchmark, notably outperforming an early version of GPT-4 on MATH. Our empirical findings, consistent with the conclusions in Li et al. (2024), suggest that strong mathematical reasoning ability can indeed exist in common 7B language models. Moreover, scaling up synthetic SFT data can further enhance the mathematical reasoning ability of Skywork-Math 7B models. \n\nFor the second belief, we propose Skywork-MathQA high-quality SFT dataset containing 2.5 million instances, which is much larger than open-sourced dataset of its kind to date, such as MetaMathQA (Yu et al., 2024) containing 395K samples. We empirically observe that the scaling law curve on the SFT alignment for mathematical reasoning in modern LLMs is far from being saturated (ref. Figure 5). We have carefully scaled the Skywork-MathQA SFT dataset with diverse and high-quality samples specifically within the mathematical domain to enhance the model's capability in understanding and solving mathematical problems.",
            "score": 0.4428501783269233,
            "section_title": "Introduction",
            "char_start_offset": 2127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 743,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 843,
                    "end": 867,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1484,
                    "end": 1501,
                    "matchedPaperCorpusId": "262084051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002132415771484375
        },
        {
            "corpus_id": "268667570",
            "title": "Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models",
            "text": "Initial work in this domain shows that LMs can modulate their probability estimates to accommodate a previously unlikely target word (e.g., A peanut falls in love) following a short licensing context (Michaelov et al., 2023;Hanna et al., 2023), results that are consistent with human data (Nieuwland and Van Berkum, 2006;Rueschemeyer et al., 2015). Nevertheless, probability-based judgments of LMs can also be adversely influenced by context, for example in cases where the context contains information that is not related to the task (for syntax: e.g., Sinha et al., 2022; for factual knowledge: e.g., Kassner and Sch\u00fctze, 2020). \n\nComparing LOGPROBS and PROMPTING. The direct interaction with LMs through natural language prompts is exciting for many reasons, including the ability to run the exact same experiments on models and on humans (Lampinen, 2022). Nevertheless, Hu and Levy (2023); Hu et al. (2024) showed that the use of metalinguistic prompts for model evaluation may underestimate their true capabilities. They compared LMs' syntactic/semantic knowledge across four minimal sentence pair datasets and showed that, on aver-age, direct probability measures were a better indicator of these knowledge types than answers to prompts (similar to us, they used DTFit as one of their datasets, but their prompts did not explicitly probe the notion of plausibility; thus, we chose to include DTFit in this work; see Appendix \u00a7B, Figure 6 for a more direct comparison). \n\nEvaluating the alignment of instruction-tuned models with humans. Even though instructiontuning has been claimed to better align the representations of LMs and those computed by the human brain (Aw et al., 2023), others show that it does not always help for the alignment at the behavioral level (Kuribayashi et al., 2024). However, the work in this domain is still sparse. \n\n3 Experiment 1: Single-Sentence Plausibility Judgments \n\nIn this section, we test LMs' knowledge of semantic plausibility in isolated sentences.",
            "score": 0.4428279955987233,
            "section_title": "Related Work",
            "char_start_offset": 7302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 224,
                    "matchedPaperCorpusId": "256080487"
                },
                {
                    "start": 224,
                    "end": 243,
                    "matchedPaperCorpusId": "264339303"
                },
                {
                    "start": 289,
                    "end": 321,
                    "matchedPaperCorpusId": "207654867"
                },
                {
                    "start": 321,
                    "end": 347,
                    "matchedPaperCorpusId": "38952752"
                },
                {
                    "start": 603,
                    "end": 629,
                    "matchedPaperCorpusId": "218628691"
                },
                {
                    "start": 874,
                    "end": 892,
                    "matchedPaperCorpusId": "258833033"
                },
                {
                    "start": 1773,
                    "end": 1799,
                    "matchedPaperCorpusId": "265150440"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029010772705078125
        },
        {
            "corpus_id": "258833634",
            "title": "Self-QA: Unsupervised Knowledge Guided Language Model Alignment",
            "text": "With the emergence of GPT-based (Radford et al., 2018) large-scale models like InstructGPT (Ouyang et al., 2022), ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023), their remarkable conversational and generative capabilities have garnered widespread attention. These models not only have the capacity to understand complex language structures and grasp subtle meanings but also possess the remarkable capability to interact naturally and fluently with users, generating text that is both coherent and highly creative. This has pushed the boundaries of what was previously deemed impossible. The impact of these large-scale models extends beyond the academic realm of natural language processing (NLP) and has a profound influence in the domains of business and industry. They have opened up new possibilities for humanmachine interactions, intelligent customer service, and virtual assistant applications, revolutionizing these fields and paving the way for innovation and advancement. \n\nDespite the impressive capabilities of ChatGPT, constructing supervised fine-tuning (SFT) data for instruction tuning presents significant challenges. The human effort required for annotating data, along with issues related to data quality, diversity, accuracy, and others, hinder the development of this technique. Although Self-Instruct (Wang et al., 2022) has been proposed to mitigate this issue, it still relies on a small set of human-written seed instructions for guidance. Furthermore, the method is limited in its ability to control the domain coverage of generated instruction data and ensure the correctness of the generated answers. Consequently, there is a vast amount of untapped potential in utilizing the abundant unsupervised data, particularly domain-specific expertise. \n\nTherefore, in this paper, we introduce SELF-QA, a framework to generate SFT data from unsupervised knowledge, inspired by the human selfquestioning learning approach. SELF-QA replaces manually written seeds used in other self-alignment models (Wang et al., 2022;Sun et al., 2023;Xu et al., 2023) with a vast amount of unsupervised knowledge, alleviating the difficulty of language models in generating instruction data according to specific requirements.",
            "score": 0.44264325206073457,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00260162353515625
        },
        {
            "corpus_id": "275993560",
            "title": "Differentially Private Steering for Large Language Model Alignment",
            "text": "Despite the rapid advances in the capabilities of Large Language Models (LLMs), an important barrier to creating fully trustworthy systems remains. LLMs often generate inaccurate, biased or even harmful information that violates human values and preferences (Rawte et al., 2023). In response, recent research has increasingly focused on aligning LLMs towards certain desired behaviors (Konen et al., 2024) while preventing potentially harmful and unsafe outcomes. This has led to the development of several techniques for aligning LLMs, such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), instruction tuning (Wei et al., 2022), In-Context Learning (ICL) (Dong et al., 2022), and prompt engineering (Cheng et al., 2024). Nevertheless, several challenges remain, including the lack of diverse and representative datasets for alignment (Liu et al., 2024c), difficulties in addressing out-of-distribution issues (Liu et al., 2024a), the choice of alignment strategy (Ivison et al., 2024) and the lack of interpretability in traditional alignment methods (Lee et al., 2024). \n\nThe linear representation hypothesis (Park et al., 2024b) suggests that high-level concepts are linearly represented as directions in the representation space of LLMs. Recent evidence (Jain et al., 2024; The private steering vectors are then added to the activations of the LLM layers during inference which ensures the generated texts for any query are differentially private with respect to the paired demonstrations. Rimsky et al., 2024;Arditi et al., 2024) points to an interesting phenomenon in LLM outputs: positive (e.g., truthful) and negative generations (e.g., hallucination) form separate clusters within the activation space across different layers of an LLM. This observation has spurred a new direction of research, known as activation editing (Turner et al., 2023;von R\u00fctte et al., 2024), which aims to edit and 'steer' LLM activations during output text generation to improve alignment.",
            "score": 0.44244092900646687,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 278,
                    "matchedPaperCorpusId": "263831293"
                },
                {
                    "start": 385,
                    "end": 405,
                    "matchedPaperCorpusId": "267406162"
                },
                {
                    "start": 595,
                    "end": 616,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 637,
                    "end": 655,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 683,
                    "end": 702,
                    "matchedPaperCorpusId": "13490401"
                },
                {
                    "start": 727,
                    "end": 747,
                    "matchedPaperCorpusId": "265043631"
                },
                {
                    "start": 862,
                    "end": 881,
                    "matchedPaperCorpusId": "266551413"
                },
                {
                    "start": 937,
                    "end": 956,
                    "matchedPaperCorpusId": "261049411"
                },
                {
                    "start": 991,
                    "end": 1012,
                    "matchedPaperCorpusId": "270440774"
                },
                {
                    "start": 1079,
                    "end": 1097,
                    "matchedPaperCorpusId": "266755904"
                },
                {
                    "start": 1138,
                    "end": 1158,
                    "matchedPaperCorpusId": "265042984"
                },
                {
                    "start": 1285,
                    "end": 1303,
                    "matchedPaperCorpusId": "271212245"
                },
                {
                    "start": 1521,
                    "end": 1541,
                    "matchedPaperCorpusId": "266174252"
                },
                {
                    "start": 1541,
                    "end": 1561,
                    "matchedPaperCorpusId": "270560489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0035648345947265625
        },
        {
            "corpus_id": "273346025",
            "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
            "text": "The advancement of Large Language Models (LLMs) (Ope-nAI 2023; Yang et al. 2024) has profoundly revolutionized a variety of real-world tasks expressed in natural language (Wei et al. 2022;Luo et al. 2023). However, they still suffer from hallucinations and factual inconsistencies (Bang et al. 2023), impacting the authenticity of generated answers. Retrieval-Augmented Generation (RAG) has gained recognition as a promising solution, empowering LLMs to leverage reliable information from retrieved documents, thereby returning high-quality responses (Guu et al. 2020;Lewis et al. 2020). \n\nIn real-world interaction scenarios, users often deviate from standard templates when posing questions, instead of imposing diverse instructions on model outputs to meet specific task requirements (Jiang et al. 2023b;Chung et al. 2024). Consequently, improving instruction-following (IF) capabilities is foundational to the effective application of LLM and RAG systems. The core goal of IF is to enable models to adapt to the diverse intents of users, which has garnered widespread attention in the LLM community. \n\nExisting efforts on instruction-following alignment primarily focus on multi-grained evaluation (Zhou et al. 2023a;Jiang et al. 2024a;Wen et al. 2024) and high-quality instruction data synthesis (Sun et al. 2024a;Zhao et al. 2024) to enhance LLMs' natural instruction-following capabilities. However, in complex RAG scenarios, the diverse knowledge introduced by retrieval-augmented techniques presents significant challenges for LLMs in effectively handling complex instructions (Figure 1). As shown in Figure 2, after supervised fine-tuning on high-quality general and knowledgeintensive QA datasets, LLMs demonstrate robust performance in both IF and RAG tasks (Mistral-base vs. Mistral-SFT). However, these capabilities do not always generalize well to instruction-following tasks under RAG scenarios and may even conflict with the performance of other fundamen- Other Fundamental Abilities \n\nBase SFT \n\nFigure 2: The performance comparison between Mistral-7B base and SFT version models on different tasks.",
            "score": 0.4421266186029291,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1103
                },
                {
                    "start": 1106,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2000
                },
                {
                    "start": 2003,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 568,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 568,
                    "end": 586,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 807,
                    "end": 825,
                    "matchedPaperCorpusId": "253018554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016164779663085938
        },
        {
            "corpus_id": "267060796",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "text": "We assess the effectiveness of our Knowledge Consistent Alignment (KCA) method in reducing hallucinations during instruction-tuning, a prevalent approach for alignment. Our experiments feature various foundation LLMs with different architectures and scales, where we apply KCA to identify and handle knowledge inconsistencies. We evaluate our method across six public benchmarks, considering both metric-based and LLM-based assessments. Moreover, we explore how KCA impacts other model capabilities, such as helpfulness. Finally, we discuss the benefits of the three calibration strategies for addressing knowledge inconsistencies and their respective application scenarios.",
            "score": 0.4420264106015356,
            "section_title": "Experiments",
            "char_start_offset": 14914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 674
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0033626556396484375
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "Recently, several scaling laws are proposed to explore the synergistic effect of different aspects on the pretrained model performance, such as the bivariate model performance prediction regarding data quantity and domain composition ratio (Ge et al., 2024a), the quality-quantity tradeoff under different computing budget (Goyal et al., 2024), and the positive correlation between data quality and model scale under the same data quantity (Bi et al., 2024) 3 Supervised Fine-Tuning of LLM \n\nBased on the general knowledge and capabilities learned in the pretraining stage, supervised finetuning (SFT) is proposed to further improve LLMs with instruction-following ability and alignment with human expectations (Wei et al., 2021;Sanh et al., 2022;Ouyang et al., 2022). Although LLMs fined-tuned with existing instruction datasets have achieved remarkable performance in various NLP tasks, the impacts of instruction data management on fine-tuned models are still under debate. The data management process in the SFT stage can be summarized as illustrated in Figure 1(b), including task composition, data quality control, data quantity control and dynamic data-efficient learning. \n\nTable 2 summarizes the data management practices of prominent fine-tuned LLMs.",
            "score": 0.4410446535881924,
            "section_title": "Relations Among Domain Composition, Data Quantity and Data Quality",
            "char_start_offset": 15999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1260
                }
            ],
            "ref_mentions": [
                {
                    "start": 711,
                    "end": 729,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 747,
                    "end": 767,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0014781951904296875
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3). URIAL leverages in-context learning (ICL) through prompting with just a few carefully curated stylistic examples and a carefully designed system prompt to achieve impressive alignment results. We craft the in-context examples to begin by affirming the user query and introducing background information, then proceed to enumerate items or steps with comprehensive details, and finally conclude with an engaging summary that includes safety-related disclaimers. Surprisingly, we find that such a straightforward baseline method can significantly reduce the performance gap between base LLMs and aligned LLMs. To rigorously evaluate different alignment methods, we design a multi-aspect, interpretable evaluation protocol, detailed in Sec. 4. We create a dataset named just-eval-instruct which contains 1,000 diverse instructions from 9 existing datasets, such as those used by AlpacaEval (Li et al., 2023a), MT-bench (Zheng et al., 2023), andLIMA (Zhou et al., 2023). Our analysis encompasses six dimensions of LLM outputs: \n\nhelpfulness, clarity, factuality, depth, engagement, and safety. Our extensive results indicate that URIAL, using as few as three constant incontext examples, can effectively align base LLMs. Remarkably, URIAL surpass the LLMs aligned with SFT or SFT+RLHF on strong base LLMs such as Mistral-7b (Jiang et al., 2023a) and Llama-2-70b (Touvron et al., 2023), as reported in Fig. 1 and Tab. 1. \n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning. In this vein, our contributions in this work can support future research in the analysis and alignment of base LLMs.",
            "score": 0.440922530977205,
            "section_title": "Preprint",
            "char_start_offset": 3108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0850830078125
        },
        {
            "corpus_id": "268033102",
            "title": "Set the Clock: Temporal Alignment of Pretrained Language Models",
            "text": "We provide more detailed analyses of how temporal alignment improves model predictions and confirm that the improvement is not from trivial factors. \n\nPopular knowledge gets aligned better. We first illustrate the association between question popularity and models' improvement on them through temporal alignment, shown in Figure 4 (left). Overall, the target-year F1 scores of both unaligned and aligned models improve as the popularity grows, confirming that popular questions are easier for LMs to answer. Moreover, the improvement from temporal alignment also increases on popular questions, indicating that models have more potential to be aligned on popular topics, since such knowledge is better memorized during pretraining but not triggered due to the temporal chaos. \n\nImprovement is not from memorizing facts in finetuning. The surprising effectiveness of doing temporal alignment through finetuning might be weakened if it is mainly due to some knowledge overlap between the finetuning and testing sets. To confirm this is not the reason, Figure 4 (right) plots the relationship between testing questions'semantic similarity6 to the training set and models' performance on them. We see that the improvement from alignment does not increase as the similarity grows. This implies that memorization is not the reason for better performance on TAQA. \n\nModels are aligned beyond just QA formatting. We further test whether the temporally aligned LMs are learning to activate recent knowledge rather than just learning to answer questions in the correct format. We conduct another two finetunings, with 1) data randomly sampled from NaturalQuestions (NQ, Kwiatkowski et al., 2019) and 2) data randomly sampled from TAQA paired with answers picked from random years. We see from Figure 5 that both of them generally do not have the effect of aligning the model to a recent time. Interestingly, NQ actually aligns the model towards 2017, which we speculate is because the dataset was constructed using much information back then.",
            "score": 0.4400776121897728,
            "section_title": "Improvement Analysis",
            "char_start_offset": 16733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 1661,
                    "end": 1686,
                    "matchedPaperCorpusId": "86611921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004486083984375
        },
        {
            "corpus_id": "277065975",
            "title": "Augmented Adversarial Trigger Learning",
            "text": "To minimize the misalignment and systemic imperfections due to the reward modeling in RLHF, supervised learning methods directly optimize LLMs with either text-based feedback (Liu et al., 2023a;Scheurer et al., 2023) or ranking-based feedback (Malladi et al., 2023;Schick et al., 2021). \n\nTo understand how alignment process changes the generation behaviour and improves the safety, URIAL (Lin et al., 2023) observed that alignment process mainly changes the distribution of stylistic tokens. Concretely, they first feed the same question Q to both an aligned LLM and its base version. Second, they decode the aligned LLM's response and base model's response at each position. Finally, they categorize all tokens in the response into three groups based on its rank in the list of tokens sorted by probability from the base LLM. The significant distribution shift occurs at mostly stylistic, constituting discourse markers. Besides, LIMA (Zhou et al., 2023) argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. We observe that the format-related token set in ATLA intersects largely with the stylistic tokens defined in Lin et al. ( 2023) and the subdistribution of formats defined in Zhou et al. (2023). Regardless of the enormous effort, BEB (Wolf et al., 2023) formally investigates aligned LLMs and states that any alignment process that attenuates an undesired behavior but does not remove it altogether faces risks when confronted with adversarial prompts.",
            "score": 0.43964202932370483,
            "section_title": "A Related Work",
            "char_start_offset": 34746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 289,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00327301025390625
        },
        {
            "corpus_id": "266335873",
            "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
            "text": "In this section, we dissect the reason behind the decline on these world knowledge benchmarks during the expansion of fine-tuning data. We find this results from the occurrence of irreversible knowledge forgetting inside the LLM. \n\nThe performance on world knowledge benchmarks highly relies on the knowledge and skills learned during pre-training phase. To investigate the relationship between the performance on world knowledge benchmarks and the knowledge embedded in pre-trained models (Petroni et al., 2019;Roberts et al., 2020;AlKhamissi et al., 2022), we conduct fine-tuning solely on the CBQA dataset with 250k samples and run evaluation on the test sets without train-test overlap. Results in Figure 3 show initial training boosts performance significantly, especially the first 1% (approximately 1k samples), with limited gains thereafter. This is because early fine-tuning aligns existing knowledge with new instructions, improving CBQA results. However, due to minimal training-testing data over- Given this, it is naturally assumed that the diminished performance on knowledge benchmark stems from the damage of knowledge stored in the LLM due to large-scale instruction tuning. \n\nTo verify the hypothesis, we sequentially fine-tuned a model using two datasets, first excluding CBQA data, then with CBQA data. Results presented in Table 1 show a great decline in knowledge capabilities versus the original LLM. This indicates that the world knowledge within the model was compromised during the first stage of large-scale fine-tuning, resulting in the model's inability to forge the alignment between human instructions and the already destroyed knowledge in the subsequent stage of fine-tuning solely with CBQA. \n\nTo sum up, the pursuit of enhancing performance on downstream tasks through the expansion of training data conflicts the preservation of world knowledge within the model in vanilla SFT.",
            "score": 0.4394963512490095,
            "section_title": "The Irreversible Knowledge Forgetting",
            "char_start_offset": 5237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011653900146484375
        },
        {
            "corpus_id": "265043685",
            "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
            "text": "In this section, we review the related work in the two aspects, namely reinforcement learning from human feedback and alignment without reinforcement learning. \n\nReinforcement learning from human feedback Large-scale pre-training empowers large language models (LLMs) to acquire extensive knowledge, underscoring their remarkable potential across diverse tasks (Brown et al., 2020;Kojima et al., 2022;Zhang et al., 2022;Chowdhery et al., 2022). Nonetheless, models exclusively focus on next token prediction in pre-training phrase, while do not consider human preferences. Consequently, this gives rise to unexpected behaviors like harmful or inaccurate information, and emphasizes the necessity to align language models with human preferences. The current mainstream approaches (Ouyang et al., 2022) to better harness the capabilities of LLMs include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To be specific, this involves three stages: firstly, using SFT to enable the model to better follow human instructions; subsequently, training a reward model (RM) using human preference data; and ultimately, tune the model to maximize the reward through the proximal policy optimization (PPO) (Schulman et al., 2017) algorithm. Furthermore, there are works exploring enhancement for this process (Ramamurthy et al., 2022;Lightman et al., 2023;Lee et al., 2023). However, RLHF presents challenges due to complex coding and hyper-parameters selecting. Besides, it requires loading three to four models simultaneously, resulting in high memory usage. These challenges propel researchers to explore alternative approaches to align language models with human feedback. \n\nAlignment without reinforcement learning Several studies are based on the rationale that language models have already acquired comprehensive knowledge during the pre-training, and only high-quality supervised fine-tuning data is required for further tuning (Zhou et al., 2023).",
            "score": 0.439245498345836,
            "section_title": "RELATED WORK",
            "char_start_offset": 4413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 381,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 381,
                    "end": 401,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 779,
                    "end": 800,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1332,
                    "end": 1357,
                    "matchedPaperCorpusId": "252693405"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0034427642822265625
        },
        {
            "corpus_id": "272593102",
            "title": "Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency",
            "text": "Supervised fine-tuning (SFT) is the key to aligning large language models (LLMs) with human beings, enabling them to complete various downstream tasks and adapt to specific domains such as healthcare and finance (Zhao et al., 2023a). The effectiveness of the SFT process relies on a high-quality instruction set, so as to ensure the performance of LLMs (Longpre et al., 2023;Wang et al., 2023;Xu et al., 2023). Temporarily, with the availability of various instruction sets, a new challenge has been raised, i.e., how to select and integrate existing datasets to obtain an optimized instruction set. To address this issue, previous research typically works by selecting and combining individual \"high-quality\" instructions. Then often construct proxy indicators to evaluate different aspects of quality, such as factual correctness, complexity, and informativeness. Then the raw instruction set could be refined by selecting instructions with the highest relative quality scores (Latif and Zhai, 2024;Li et al., 2024;Lu et al.;Zhao et al., 2023b). \n\nHowever, emerging evidence (Dong et al.;Yuan et al., 2023) and our analyses indicate that complex correlation and dependency relationships exist between different categories of instructions. Therefore, considering the quality of individual instructions alone can be a suboptimal approach for building a fine-tuning instruction set. Research indicates that these categories are interrelated; incorporating one category of instructions may enhance or diminish the model's performance in others (Dong et al., 2023;Huang and Chang, 2023). Additionally, the skills required for different tasks often form hierarchical taxonomies. For instance, solving a bioinformatics problem requires both biological knowledge and coding skills. Consequently, instructions are interconnected and collectively influence model performance. Ignoring these correlations can reduce the efficiency of instruction selection, as incorporating one category of instruction may even degrade the model's performance in another category. Moreover, the dependency between skills necessitates that models acquire foundational knowledge before progressing to more complex tasks; otherwise, the effectiveness of instruction tuning will be compromised (Longpre et al., 2023).",
            "score": 0.43872389095898295,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2287
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 375,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 979,
                    "end": 1001,
                    "matchedPaperCorpusId": "264145880"
                },
                {
                    "start": 1001,
                    "end": 1017,
                    "matchedPaperCorpusId": "261076515"
                },
                {
                    "start": 1090,
                    "end": 1108,
                    "matchedPaperCorpusId": "254854311"
                },
                {
                    "start": 1561,
                    "end": 1583,
                    "matchedPaperCorpusId": "254877753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022258758544921875
        },
        {
            "corpus_id": "258685390",
            "title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
            "text": "One might worry that our positive results are highly dependent on the specific input-output pairs we have chosen. We now seek to address this concern by asking whether the causal roles (i.e., alignments) found using Boundless DAS in one setting are preserved in new settings. This is crucial, as it tells how robustly the causal model is realized in the neural network. \n\nGeneralizing Across Two Different Instructions Here, we assess whether the learned alignments for the 'Left Boundary' causal model transfer between different specific price brackets in the instruction. To do this, we retrain Boundless DAS for the high-level model with a fixed instruction that says \"between 5.49 dollars and 8.49 dollars\". Then, we fix the learned rotation matrix and evaluate with another instruction that says \"between 2.51 dollars and 5.51 dollars\". For both, Alpaca is successful at the task, with around 94% accuracy. Our hypothesis is that if the found alignment of the highlevel variable is robust, it should transfer between these two settings, as the aligning variable is a boolean-type variable which is potentially agnostic to the specific comparison price. Table 1 gives our findings in the 'New Bracket' rows. Boundless DAS is able to find a good alignment for the training bracket with an IIA max that is about the same as the task performance at 94%. For our unseen bracket, the alignments also hold up extremely well, with no drop in IIA max . For both cases, the found alignments also highly correlate with the counterpart of our main experiment.",
            "score": 0.4385145583999668,
            "section_title": "Do Alignments Robustly Generalize to Unseen Instructions and Inputs?",
            "char_start_offset": 20557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1552
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002349853515625
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "Adopting graded concept theory may suggest a continuous measurement of abstraction alignment where the abstraction between concepts is weighted based on membership strength. \n\nLikewise, while abstraction graphs provide a structured framework for measuring alignment, they inherently constrain the type of knowledge that can be represented, raising ethical implications about what it means to be aligned and whose knowledge we are aligning to [138]. These graphs are well-suited to domains with explicit and formalized knowledge but struggle to accommodate tacit, subjective, or contextual knowledge that is harder to generalize and abstract into discrete concepts [5,65,109,116]. By privileging knowledge that is easily formalized, abstraction graphs may amplify dominant representations of knowledge, potentially marginalizing alternative ways of knowing [43,139,156]. This risks reinforcing existing power dynamics, encouraging the development of models that perpetuate dominant worldviews [45]. Future alignment research should critically examine and document the perspectives we align to [48] and explore more informal abstraction representations that better represent diverse forms of knowledge. \n\nNevertheless, our case studies demonstrate that the current instantiation of abstraction alignment helps domain experts interpret model and dataset alignment. While there are two limitations with the design of our case studies -they follow a largely exploratory protocol and only engage seven participants -these limitations map to our goal of understanding how grounding alignment analysis in abstractions can aid expert analysts. Thus, we did not conduct comparative evaluations (e.g., against alternative alignment methods), nor do our case studies help us gauge the the value of our interface design choices or the usefulness of abstraction alignment in broader contexts (e.g., with less-expert analysts). Nonetheless, our study design allowed us to gather rich, open-ended feedback from experts and evaluate abstraction alignment in real-world tasks. Future work could complement our findings through large-scale comparative studies that directly contrast abstraction alignment to alternative alignment techniques across diverse participants, tasks, and domains. For instance, studies could extend our exploration of abstraction alignment in dataset analysis by applying it to a real-world participatory dataset audit and comparing users' speed and findings against traditional auditing methods.",
            "score": 0.4384115875400687,
            "section_title": "Discussion and Future Work",
            "char_start_offset": 75347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2270
                },
                {
                    "start": 2271,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 442,
                    "end": 447,
                    "matchedPaperCorpusId": "231698739"
                },
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "227106336"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "23154832"
                },
                {
                    "start": 860,
                    "end": 864,
                    "matchedPaperCorpusId": "235436386"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "15660866"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "4421027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007694244384765625
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "To enable large language models (LLMs) to understand and generate natural language, they are constructed with billions of parameters and pretrained on datasets containing trillions of tokens [OAA + 24]. However, several challenges arise after the pretraining stage of LLMs [WBP + 24]. One major issue is that pretrained LLMs can only continue generation based on the previous context and often struggle to accurately answer user questions. To address this, supervised fine-tuning (SFT) is introduced, using pairs of questions and answers. For example, in models like Mistral, preset instructions such as ' [INST]' and '[/INST]' are used to frame a question as a prompt [JSM + 23]. The corresponding answer is then used as the target output. The model's probability of generating the correct answer is maximized through next-token prediction, employing the cross-entropy loss function to classify tokens across the entire token space. \n\nThe next challenge for LLMs lies in ethical concerns, where LLMs may inadvertently teach humans to engage in unethical activities, such as robbing banks [OWJ + 22]. To address this issue, various alignment methodologies have been proposed, including Reinforcement Learning from Human Feedback (RLHF) [OWJ + 22, BJN + 22] with Proximal Policy Optimization (PPO) [SWD + 17], Direct Preference Optimization (DPO) [RSM + 23], Kahneman & Tversky Optimization (KTO) [EXM + 24], and UNified Alignment (UNA) [WBH + 24]. The core idea of alignment is to equip LLMs with the ability to reject harmful requests by learning from human feedback. \n\nFor pretrained LLMs, SFT and alignment are traditionally performed in sequence. However, this staged approach often leads to performance degradation, where the model loses capabilities acquired in earlier phases. This paper seeks to address and mitigate this degradation. \n\nFigure 1: UFT integrates SFT and alignment through a generalized implicit reward function. It likens pre-training and fine-tuning of LLMs to Chinese proveb \"Read ten thousand books, travel ten thousand miles\".",
            "score": 0.43814086640855704,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2054
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020427703857421875
        },
        {
            "corpus_id": "278208140",
            "title": "Phi-4-reasoning Technical Report",
            "text": "Phi-4-reasoning after the SFT stage already performs strongly across diverse benchmarks. Despite the focus on reasoning-specific content from select domains (math, coding, and safety), the improvement in performance generalizes to tasks not directly targeted in the training data-such as calendar planning (Figure 8). While we have a relatively long SFT stage with 2+ passes over reasoning data sources, we do not see any catastrophic forgetting compared to the base Phi-4 model on more general capabilities. In fact, most general-purpose benchmarks improve significantly over Phi-4 as summarized in Table 2. \n\nFigure 4a shows the progression of key metrics throughout the SFT iterations. We observe through manual checks that the model begins to use explicit \"thinking\" tokens very early in training, indicating the superficial structured format itself is learned quickly. However, the efficacy of the chain-of-thought block and the ability of the model to reason improves throughout training as seen in Figure 4a, suggesting that the model is not merely copying format, but actually acquiring reasoning as a learned skill. Interestingly, unlike during reinforcement learning, we do not see increasing response lengths over the course of SFT. In fact, as shown in Figure 4b, average response length slightly decreases, suggesting the model is learning to use its token budget more efficiently as training progresses. \n\nIn the remainder of this section, we describe at a high level our experimentation process with reasoning SFT. \n\nEarly experiments made it clear that SFT recipes used for instruction finetuning of Phi-4 do not transfer directly to reasoning-focused training. For example, the optimal hyperparameters for reasoning data differed significantly from those used for alignment-focused tuning in Phi-4. As a result, we conducted extensive experiments to identify effective SFT configurations specifically suited for reasoning. \n\nTo systematically evaluate different training strategies, we used fixed benchmarks-AIME 2024 and GPQA diamond-as progress indicators. At a high-level, our experimental methodology can be divided in two stages: exploration and scaling. During exploration, we used shorter training horizons and limited data sources and domains to rapidly iterate and extract robust training recipes. In the subsequent scaling stage, we aggregated findings from earlier derisking runs and finalize the SFT setup.",
            "score": 0.43809013101941285,
            "section_title": "Training data",
            "char_start_offset": 21838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2323
                },
                {
                    "start": 2324,
                    "end": 2435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00885009765625
        },
        {
            "corpus_id": "267782585",
            "title": "Towards Robust Instruction Tuning on Multimodal Large Language Models",
            "text": "In recent years we have witnessed a surge in the family of large language models (LLMs).Thanks to the scaling laws for LLMs (Chowdhery et al., 2023), a series of works have established powerful foundation models which show superior zero-shot performance on the downstream tasks (Brown et al., 2020;Touvron et al., 2023a,b;Achiam et al., 2023;Team et al., 2023;Bai et al., 2023).Instruction fine-tuning (IFT), which aims to \"teach\" models to follow natural language instructions, has been proven to be an effective learning paradigm to further enhance LLMs' generalizability (Sanh et al., 2021;Wei et al., 2021;Chung et al., 2022;Taori et al., 2023).\n\nWhen deploying the instruction-tuned LLMs on unseen tasks, users from various backgrounds may write instructions with diverse wording styles Figure 1: Zero-shot performance on MULTIIN-STRUCT test set (9 tasks) by OFA tuned on each instruction-following dataset.By expanding the instruction set several times using automatically generated instructions (\"MINS,59K\" to \"MINS+,59K\"), the average score is close to that tuned using 10x more data (\"MINS+,59K\" compared to \"MINS,564K\", highlighted by the arrow).\n\nto test the performance of the same task.In light of this, high-quality instruction-following data covering a wide range of wording styles are required in the alignment of large language models.The conventional approach to obtain a large instruction-following dataset resembles data annotation, which recruits crowdsourced workers who engage in NLP research and possess rich knowledge about the target tasks to write instructions for each task/instance (Sanh et al., 2021;Wang et al., 2022b).Zhou et al. (2023) further eliminates redundant annotations, proving that IFT on only well-crafted 1,000 instructions can align as huge as 65B models with user preferences.",
            "score": 0.4378389430089651,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 88,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 649
                },
                {
                    "start": 651,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1156
                },
                {
                    "start": 1158,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1650
                },
                {
                    "start": 1650,
                    "end": 1822
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 148,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 278,
                    "end": 298,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1630,
                    "end": 1649,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0015306472778320312
        },
        {
            "corpus_id": "276421668",
            "title": "Policy-to-Language: Train LLMs to Explain Decisions with Flow-Matching Generated Rewards",
            "text": "The training epochs of DPO is 10 and the learning rate is initialized as 5e-6 recommended by the TRL library and then decays linearly to zero. \n\nKTO is an advanced approach for aligning LLMs with human preferences or specific task objectives. It draws inspiration from prospect theory, a behavioral economics framework that models how humans evaluate potential gains and losses under uncertainty. In this context, KTO optimizes the alignment process by weighting outputs based on their perceived utility, rather than treating all errors equally. The core idea of KTO is to model alignment as an optimization problem where the goal is to maximize expected utility under a prospect-theoretic framework. In our work, we construct a preference dataset comprising samples paired with explanations generated by OpenAI o1-mini. The training epochs of KTO is 10 and the learning rate is initialized as 5e-7 recommended by the TRL library and then decays linearly to zero. \n\nSFT-CoT is a fine-tuning method that enhances the reasoning capabilities of LLMs by combining SFT with the structured reasoning paradigm. CoT uses explicit programmatic representations, such as pseudo-code or structured logic, to model complex problem-solving tasks. In this approach, SFT is performed using datasets annotated with both input-output pairs and detailed programmatic reasoning traces. These traces serve as templates for step-by-step reasoning and enable the model to break down complex problems, such as mathematical reasoning or logical inference, into manageable sub-tasks. The explicit program-like structure helps the model perform multi-step computations and enhances interpretability, making it especially useful for domains requiring precision and transparency. \n\nReFT is a training approach designed to enhance the reasoning capabilities of LLMs by combining supervised fine-tuning (SFT) with reinforcement learning. In ReFT, the initial training begins with SFT, where the model is fine-tuned using datasets annotated with reasoning traces, such as step-by-step explanations or logical chains of thought. Once the model achieves a baseline performance, reinforcement learning is applied to further refine its reasoning capabilities.",
            "score": 0.4378360314004658,
            "section_title": "A.4. Baselines",
            "char_start_offset": 33801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00254058837890625
        },
        {
            "corpus_id": "276394950",
            "title": "Abstraction Alignment: Comparing Model-Learned and Human-Encoded Conceptual Relationships",
            "text": "Abstraction is the process of distilling many individual data instances into a set of fundamental concepts and relationships that capture essential characteristics of the data [3,87,160]. It is a key feature of human cognition as it allows us to flexibly reason at the level of specificity appropriate for our task and generalize our knowledge by fitting abstracted patterns to new data [42,150,160]. As a result, abstractions form the basis for human information encodings across domains like linguistics [35,90], biology [59,86], and medicine [157,158]. In machine learning, abstractions are built into many tasks, including image classification [32,77] and medical coding [67,68]. Even datasets without built-in abstractions are often linked to existing abstractions by matching their outputs to corresponding concepts [121]. Encouragingly, researchers have recently integrated human abstractions into model training pipelines, resulting in increased model generalization [97], and advocated for using conceptual relationships, like abstractions, to advance our understanding of foundation models [153]. Building on this rich history, abstraction alignment leverages abstractions to better understand human-AI alignment. \n\nRelated research has studied formal representations of human knowledge, known as knowledge graphs [61,66]. Knowledge graphs reflect the relationships between entities, like distance (Eiffel Tower -is near\u2192 Arc de Triomphe) or connectivity (BOS -direct flight\u2192 MEX) [61]. In abstraction alignment, we represent human abstractions as an abstraction graph, a type of knowledge graph where nodes are concepts and edges encode abstractions from specific to general concepts (e.g., cardiologist -type of \u2192 doctor or Montreal -located in\u2192 Quebec). We make this distinction because we are interested in understanding whether a model has learned to reason with human-like abstractions. These human abstraction graphs provide an explicit representation of formal human knowledge (Section 3.1) that allows us to quantify alignment.",
            "score": 0.4376881591719395,
            "section_title": "Related Work 2.1 Abstraction and Human Knowledge",
            "char_start_offset": 8061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "202721456"
                },
                {
                    "start": 387,
                    "end": 391,
                    "matchedPaperCorpusId": "7674831"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "202721456"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "1671874"
                },
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "88766"
                },
                {
                    "start": 545,
                    "end": 550,
                    "matchedPaperCorpusId": "31594479"
                },
                {
                    "start": 648,
                    "end": 652,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "33285731"
                },
                {
                    "start": 822,
                    "end": 827,
                    "matchedPaperCorpusId": "786357"
                },
                {
                    "start": 1328,
                    "end": 1331,
                    "matchedPaperCorpusId": "211010433"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0038242340087890625
        },
        {
            "corpus_id": "271328900",
            "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
            "text": "In other words, LLMs tend to learn factual knowledge through pre-training, whereas fine-tuning4 teaches them to utilize it more efficiently (Gekhman et al., 2024;Zhou et al., 2023a;Ovadia et al., 2024). Ren et al. (2024a) also posit that instruction tuning is a form of self-alignment with existing internal knowledge rather than a process of learning new information. We conjecture that the debate on whether these processes truly introduce new knowledge stems from information conflicts. For example, the conflict between outdated information within LLMs and new external knowledge exacerbates their difficulty in learning new information. To mitigate information conflicts, Ni et al. (2023) propose first forgetting old knowledge then learning new knowledge. Another technique, retrieval-augmented generation (RAG) (Huang and Huang, 2024), while avoiding conflicts within internal parameters, still needs to manage conflicts between retrieved external information and LLMs' internal knowledge (Xu et al., 2024b). RAG also attempt to efficiently and effectively integrate new knowledge across passages or documents using multiple retrieval (Yang et al., 2024a) and hippocampal indexing (Guti\u00e9rrez et al., 2024). Besides, editing technologies, including knowledge and representation editing, exhibit promising potential for knowledge addition, modification, and erasure. Specifically, knowledge editing (Meng et al., 2022;Mitchell et al., 2022;Cao et al., 2021b;Zhang et al., 2024a;Wang et al., 2023d;Mazzia et al., 2023) aims to selectively modify model parameters responsible for specific knowledge retention, while representation editing (Zou et al., 2023;Wu et al., 2024) adjusts the model's conceptualization of knowledge to revise the stored knowledge within LLMs. Note that the other strategy for knowledge editing adds external parameters or memory banks for new knowledge while preserving models' parameters. We also provide the comparison of the above methods in \u00a7A for better understanding.",
            "score": 0.43750919156751594,
            "section_title": "Individual Evolution",
            "char_start_offset": 27355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 181,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1404,
                    "end": 1423,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1423,
                    "end": 1445,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 1445,
                    "end": 1463,
                    "matchedPaperCorpusId": "233289412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00257110595703125
        },
        {
            "corpus_id": "264146264",
            "title": "Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis",
            "text": "Training with model-generated data has recently become a prevailing research direction in both computer vision (e.g., He et al. (2022) for image classification, GeoDiffusion (Chen et al., 2023b;Gao et al., 2023;Liu et al., 2023b;Li et al., 2023) for object detection (Han et al., 2021;Li et al., 2022) and also StableRep (Tian et al., 2023) for contrastive learning (Chen et al., 2021;Liu et al., 2022) and masked image modeling (Chen et al., 2023a;Zhili et al., 2023)) and natural language processing (e.g., SELF (Lu et al., 2023) for instruction tuning), thanks to the remarkable development of AIGC. Our method also belongs to this direction, but different from previous works, we focus on utilizing the inherent discrimination ability of LLMs to enhance the generation capabilities, totally obviating the need for extra human intervention or external knowledge source, but still with solid theoretical support, as discussed in Sec. 4.2 and Appendix D. \n\nwhich simplifies to p(T |Y , X) \u221d p(Y |X, T ) under the assumption that p(Y |X) remains relatively stable during the fine-tuning process. To maintain the model's capability to produce a response given an instruction (i.e., preserving p(Y |X)), we combine original SFT data, both helpful (D helpful ) and harmless (D harmless ), with mistake analysis data during fine-tuning, rather than solely relying on the mistake analysis data. Thus, Eqn. \n\n(2) actually aims to explore how the additional mistake analysis data, along with the original SFT instruction-response pairs, can enhance the alignment performance. \n\nVerification of Eqn. \n\n(2) via harmless tag prediction. To verify p(T |Y , X) is indeed proportional to p(Y |X, T ) as previously discussed, we further conduct an additional experiment evaluating the LLM's ability to discern the harmfulness of responses.",
            "score": 0.4371871450659381,
            "section_title": "C MORE DISCUSSION",
            "char_start_offset": 33146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 385,
                    "matchedPaperCorpusId": "237346844"
                },
                {
                    "start": 385,
                    "end": 402,
                    "matchedPaperCorpusId": "249097646"
                },
                {
                    "start": 429,
                    "end": 449,
                    "matchedPaperCorpusId": "257834069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0015306472778320312
        },
        {
            "corpus_id": "268856712",
            "title": "HyperCLOVA X Technical Report",
            "text": "The first phase in alignment learning is SFT, in which HyperCLOVA, the pretrained LLM, is trained to maximize the likelihood of a completion given each prompt.This phase improves the model's ability to follow instructions and solve problems such as coding and creative writing.Furthermore, it allows the model to leverage knowledge from data across various domains, ranging from commonsense to humanities, sciences, and ethics.\n\nIn our SFT dataset, we define three special tokens: '<|user|>', '<|assistant|>', and '<|endofturn|>' to distinguish between the user's and the assistant's turns.Even if a token corresponding to a special token is part of the user input, it is processed as a regular token, thereby ensuring that each role in the context remains distinct from the user's instruction.For training on multi-turn samples, we apply loss masking on all text except for the assistant's turns.\n\nFor SFT training, we use an efficient batching strategy that groups sequences with similar lengths, in order to minimize padding within mini-batches and increase GPU utilization.The actual mini-batch size depends on the average length of sequences in each mini-batch, but the maximum number of tokens for each mini-batch is kept the same.",
            "score": 0.4369712645738082,
            "section_title": "Supervised Fine-tuning (SFT)",
            "char_start_offset": 8776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 427
                },
                {
                    "start": 429,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 897
                },
                {
                    "start": 899,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00133514404296875
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "Our study fine-tunes model pre-training checkpoints to understand the dynamics of pre-training and fine-tuning on model performance. \n\nFine-tuning teaches additional task format but leads to forgetting unused abilities. Our results show that fine-tuning guides the model to under-stand the format and complete a given task. As this information diminishes, the model's overall ability improves. Additionally, more pre-training will lead to a model that reacts better to instruction-style prompts, and the ability to interpret such instruction will not be lost when the model is fine-tuned in a different format. However, fine-tuning comes at the expense of other model abilities, such as the capability of solving tasks or domains that are unrelated or weakly related to the fine-tuning task. This insight can be helpful in our understanding of the multitask abilities of LLMs, where certain tasks can introduce conflicts during multi-task training (Mueller et al., 2022). Some datasets can be learned without finetuning. We discover a dichotomy between datasets. Some are learned during model pre-training, while others show no improvements during pre-training. Furthermore, the datasets learned during pretraining do not benefit from fine-tuning. This observation, combined with our study about what is learned during fine-tuning ( \u00a74) suggests that some tasks are presented in a manner that aligns with what the model sees during pre-training, and thus fine-tuning provides no additional information. It may be possible to modify tasks to better align with pre-training and thus make them learnable. \n\nPre-training can improve models in unseen ways. Some datasets are not learned during pre-training but benefit significantly from fine-tuning ( \u00a75). However, these datasets still benefit from additional pre-training, even though those benefits are not revealed without fine-tuning ( \u00a76). The model learns important information to solve the task, even though it cannot express that information without fine-tuning.",
            "score": 0.4369641449743094,
            "section_title": "Discussion",
            "char_start_offset": 22473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 948,
                    "end": 970,
                    "matchedPaperCorpusId": "254591386"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0027370452880859375
        },
        {
            "corpus_id": "265457248",
            "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
            "text": "3.1 Which instructions are the valuable data for a given LLM Zhou et al. (2023) show that LLM's knowledge has been mostly learnt during pre-training. Instruction tuning is mainly used to teach a given LLM to learn how to follow a certain pattern when interacting with human, and only a small number of carefully crafted high-quality instructions are enough to equip the given LLM with powerful instruction-following capabilities. However, for different LLMs, as the knowledge and abilities they have learnt during the pre-training procedure are different, the instruction tuning data they require shoud be different as well. Consequently, how to select the most crucial data for a given LLM has garnered much attention of researchers. After analyzing some LLMs and instructions, we find that the valuable instruction tuning data for one given LLM are mainly decided by the following three aspects: Quality. \"Quality\" refers to the data quality of both the instructions and their corresponding responses in the dataset, which directly influences the knowledge LLM learns. As demonstrated in the work of (Zhou et al., 2023;Chen et al., 2023;Cao et al., 2023), high-quality instruction data can effectively enhance LLM's ability to follow instructions. \n\nCoverage. \"Coverage\" refers to the types of instrucitons the dataset includes. It represents the diversity of one instruction dataset. The more diverse instruction the dataset covers, the greater the potential of stimulating the capabilities of a large language model is. Researches of (Iyer et al., 2023;Wang et al., 2023bWang et al., , 2022b;;Longpre et al., 2023) also show that enhancing the diversity of instruction data can effectively enhance LLM's ability to follow instructions during fine-tuning. \n\nNecessity. \"Necessity\" indicates the importance and uniqueness of one instruction for fine-tuning a specific LLM. As described in the work of (Zhou et al., 2023), LLMs have already acquired a substantial amount of knowledge and capabilities during pre-training. Instruction tuning primarily fo-cuses on how to use a limited number of instruction data to stimulate LLM's capabilities, enabling LLMs to follow a certain pattern when interacting with human.",
            "score": 0.4367847173289746,
            "section_title": "Methodology",
            "char_start_offset": 7175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0015611648559570312
        },
        {
            "corpus_id": "274130606",
            "title": "BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment",
            "text": "In general, knowledge breadth B represents the model's range of knowledge across various subjects or domains, which is a measure of how many different areas or topics the model can understand and provide information on. In contrast, knowledge depth K refers to how well an LLM can provide indepth and detailed information on specific topics. It is a measure of the model's ability to delve into complexities, offer nuanced insights, and demonstrate expertise in narrow subject areas (Bai et al., 2024). Thus, we can simply define a knowledge source L as L = (B, K). \n\nWhile LLMs demonstrate striking knowledge breadth in extensive domains and areas, their knowledge depth for providing really in-depth and expert-level output is still not promising (Zhang et al., 2024;Bai et al., 2024). In this work, we propose to address this problem by first analyzing the relationship between knowledge breadth and depth, with prompts and responses in the alignment tuning dataset. Intuitively, a large and diverse set of instructions allows the LLMs to cover a wide range of topics and areas, thus expanding its knowledge breadth. Similarly, more responses of various quality provide LLMs with great opportunities to fully understand the question and explore the required knowledge, thus leading to LLMs being able to  provide in-depth insights and responses. However, if we take a closer look at alignment tuning datasets, there is a significant imbalance between the number of instructions and responses: a typical alignment tuning dataset is in the format of D = {(x 1 , y 1 1 , y 2 1 ), ..., (x n , y 1 n , y 2 n )}, in which each sample (x, y 1 , y 2 ) encompasses a prompt x, the winning response y 1 and the losing response y 2 . In this dataset, the instruction number n is usually tens of thousands and the response number is simply 2, where we have n >> 2. Here we argue that this imbalance between instruction and response numbers actually implies the unbalanced resource allocation for knowledge breadth and depth learning in alignment tuning time, limiting LLMs' exploration for more in-depth knowledge.",
            "score": 0.4367233879320009,
            "section_title": "Knowledge Breadth and Depth",
            "char_start_offset": 5160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 501,
                    "matchedPaperCorpusId": "259095491"
                },
                {
                    "start": 769,
                    "end": 786,
                    "matchedPaperCorpusId": "259095491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0028667449951171875
        },
        {
            "corpus_id": "278501457",
            "title": "xGen-small Technical Report",
            "text": "To perform supervised fine-tuning (SFT) on xGen-small base models, we began by curating a broad, high-quality instruction dataset. The dataset is organized into three macro categories: general-purpose helpfulness, safety and harmlessness, and reasoning. The general-purpose category encompasses data from a wide range of domains, including creative writing, project management, data analysis, education, and everyday problem-solving. The safety and harmlessness category focuses on ensuring the model avoids generating harmful, biased, or unsafe content across diverse contexts. The reasoning category includes data that require advanced cognitive and analytical abilities, drawing heavily from mathematics, coding, science, and other STEM fields. \n\nBoth xGen-small-4B and xGen-small-9B were trained on the SFT dataset for four epochs. We used a learning rate of 5 \u00d7 10 \u22126 with a warm-up ratio of 10%. We observed consistent performance improvement over the training epochs. \n\nThrough supervised fine-tuning, the model learns a broad foundation of core capabilities. These include accurate instruction following, step-by-step reasoning, factual correctness, and desirable alignment traits such as helpfulness, honesty, and harmlessness. This stage establishes the model's general competency and alignment with human values, preparing it for further fine-tuning.",
            "score": 0.43656036946783416,
            "section_title": "Supervised Fine-Tuning",
            "char_start_offset": 12561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1361
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00243377685546875
        },
        {
            "corpus_id": "269148977",
            "title": "CEM: A Data-Efficient Method for Large Language Models to Continue Evolving From Mistakes",
            "text": "With the exponential growth of data and model sizes, Large Language Models (LLMs) have demonstrated superior performance across numerous downstream tasks (Chang et al., 2023;Wei et al., 2022;Peng et al., 2023). However, in realworld applications, the continual emergence of new knowledge and evolving task requirements necessitate ongoing updates and task-specific adaptations for LLMs. Without these, models risk inducing hallucinations due to knowledge boundaries and may cause task misalignment (Huang et al., 2023). Additionally, addressing and correcting the shortcomings and errors exposed during practical use is crucial. Therefore, Continual Instruction Tuning 1 https://anonymous.4open.science/r/cem-BB25 (CIT) and Continual Pre-training (CPT) are proposed as the primary methods of Continual Learning (CL) to align LLMs with evolving knowledge and tasks, and to improve their shortcomings (Wu et al., 2024). \n\nFigure 1: Two potential triggers for poor model performance: (1) Task Schema Unfamiliarity, and (2) Lack of Task-relevant Knowledge. Unfamiliarity with the task schema can cause deviations from expected interaction styles, while insufficient task knowledge may lead to hallucinations. Instruction tuning has been shown to be effective for addressing the former, but poor for the latter (Gekhman et al., 2024a;Zhou et al., 2023). Despite CIT's smaller data size requirement and lower risk of overfitting compared to CPT, it is limited in new knowledge injection, as Zhou et al. (2023) highlight that instruction tuning is a superficial alignment focused on interaction styles (i.e., task schema) and does not effectively help the model acquire new knowledge. Thus, it cannot address all the issues shown in Figure 1, where LLMs' poor performance may stem from a lack of requisite knowledge rather than unfamiliarity with the task schema (Ren et al., 2023;Zhang et al., 2023b). In contrast, CPT is more effective in addressing the lack of knowledge. \n\nHowever, using CPT to supplement the model with lacking knowledge still presents several challenges.",
            "score": 0.4365056563327112,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 174,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 498,
                    "end": 518,
                    "matchedPaperCorpusId": "265067168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00530242919921875
        },
        {
            "corpus_id": "272423541",
            "title": "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",
            "text": "Large language models (LLMs) have demonstrated exceptional performance on many tasks in diverse applications, including mathematical reasoning, coding capabilities, and knowledge-based question  answering (Brown et al., 2020;Bubeck et al., 2023;OpenAI, 2023). Whilst LLMs have broad knowledge and reasoning skills, the pre-training objective is often misaligned from the objective of instruction following (Ouyang et al., 2022) according to human preferences (Christiano et al., 2017), and LLMs can exhibit undesirable behaviors including hallucinating, and providing harmful or biased instructions (Huang et al., 2023;Zhang et al., 2023). As LLMs become more commonplace, it is important for them to be aligned with human preferences for helpfulness, harmlessness, and honesty (Bai et al., 2022). \n\nA common practice for aligning LLMs to human preference is through Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022), which is based on a reward model trained to score model outputs according to human preference annotations. In RLHF alignment, a high quality reward model is required for policy learning (Ram\u00e9 et al., 2024). However, in practice, learned reward models are typically imperfect approximations of the \"true\" human reward label function (Gao et al., 2023), because they are trained on a fixed set of human preference data collected offline. When used within the RLHF policy optimization, they may see out-of-distribution (OOD) data when annotating the response of the model. Aligning according to an imperfect reward model can lead to worse performing language models as policy optimization continues to optimize a mis-specified reward. This can lead to an increased gap between the learned and true reward, a phenomena known as over-optimization and reward hacking (Gao et al., 2023;Skalse et al., 2022). \n\nRecently, Rafailov et al. (2024) proposed Direct Preference Optimization (DPO), and show that any reward model can be implicitly represented by the optimal policy learned by DPO and a reference policy under certain assumptions.",
            "score": 0.43641238448317266,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1840
                },
                {
                    "start": 1843,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 225,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 406,
                    "end": 427,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 917,
                    "end": 938,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1272,
                    "end": 1290,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 1801,
                    "end": 1819,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 1819,
                    "end": 1839,
                    "matchedPaperCorpusId": "258509720"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021495819091796875
        },
        {
            "corpus_id": "267069454",
            "title": "Decolonial AI Alignment: Openness, Visesa-Dharma, and Including Excluded Knowledges",
            "text": "The currently prevalent development lifecycle for applications infused with LLMs may be divided into two halves: steps carried out by model providers and steps carried out by application developers. In an imperfect analogy with teaching a child, the model provider does the basic steps of teaching the LLM to go from babbling words, to having fluency in language, to following instructions, to carrying on a conversation. The application developer, if so empowered, teaches the LLM culture, which may include steps on subject matter expertise, social norms, laws, customs, and beliefs. Getting to the point of language fluency may be termed pre-training the base model or foundation model. Any of the steps after language fluency may be called 'alignment, ' depending on the interlocutor. As mentioned earlier, the term 'alignment' is an empty signifier, so it is not fixed to refer to any specific step [73]. \n\nIn pre-training, some amount of enculturation is possible by curating the content of the training dataset to include an abundance of topics that the model provider wishes the LLM to be skilled in and filtering out taboo topics. As discussed further in Section 2.2.3, some amount of undesirable cultural knowledge leaks into the pre-training performed by the model provider. Filtering is computationally-intensive given the size of datasets being in the trillions of tokens. Data curation is followed by self-supervised learning (like a peekaboo game) to obtain the base model, which may take months despite using thousands of high-end graphical processing units. \n\nThe AI technologies to do any of the alignment steps on top of the base model are essentially the same, whether the goal is following instructions, behaving according to social norms, or something else. Several techniques exist with varying resource requirements for humans, data, and computation. Supervised fine-tuning (SFT) updates all of the model weights according to a smaller, but still large dataset containing data with both inputs and outputs. It is fairly computationally-intensive given that all weights are updated. If a model has already been trained to follow instructions, then a dataset with instructions, inputs, and outputs may be used.",
            "score": 0.43619195483341744,
            "section_title": "Large Language Model Development Lifecycle and Alignment",
            "char_start_offset": 7354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "263620620"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0030994415283203125
        },
        {
            "corpus_id": "268379670",
            "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model",
            "text": "Then, following common instruction templates [34,35], we transform the selected datasets into a data format of instruction tuning. \n\nThen, following an in-depth evaluation of popular MLLMs, we reveal that some of them still suffer from catastrophic forgetting (as showing in Fig. 1), similar to traditional continual CNN [22,30] or VIT [13,38] models. However, different from these models which learn representations and use a final layer to make predictions with a fixed output style, MLLMs work in a generative way. This motivates us to inquire whether MLLMs forget the knowledge required for reasoning or if the issue lies in their inability to follow instructions. Because instruction tuning primarily focuses on learning to align with task instructions [69,35,34], we hypothesize that the model mainly loses the capability of instruction following, rather than the maintained knowledge. To validate this hypothesis, in addition to checking the outputs with the ground truth, called Truth Alignment, we employ powerful LLM assistant for evaluating the reasoning knowledge, called Reasoning Capability, as shown in Fig. 2. \n\nAfter analyzing the results of these two evaluations, we reveal that the failure in instruction following assumes the main responsibility, instead of reasoning knowledge forgetting. Recently, Mixture-of-Experts (MoE) framework [54] leverages multiple distinct experts to acquire different knowledge and incorporates a gate function to modulate their contributions. We observe that this method resembles the architecture-based methods in traditional continual learning, providing the model with the ability to learn different instruction following from distinct experts. Therefore, we try to bring it into CoIN to mitigate the forgetting of alignments. Experimental results consistently demonstrate improvement after integrating with more experts. \n\nIn summary, the contributions of this paper are as follows: \n\n\u2022 A novel benchmark for MLLMs in continual instruction tuning is proposed, namely CoIN, which consists of 10 datasets spanning 8 different tasks for comprehensive and diverse evaluation. \n\n\u2022 A novel evaluation approach is introduced to assess the model's ability from two aspects: Truth Alignment and Reasoning Capability. Furthermore, we reveal that the catastrophic forgetting in MLLMs is primarily due to the decline in instruction following rather than reasoning knowledge. \n\n\u2022 Multiple state-of-the-art MLLMs are chosen for evaluation on CoIN.",
            "score": 0.4361798682667002,
            "section_title": "Introduction",
            "char_start_offset": 3256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2125
                },
                {
                    "start": 2128,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2416
                },
                {
                    "start": 2419,
                    "end": 2487
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 321,
                    "end": 325,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 762,
                    "end": 765,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1355,
                    "end": 1359,
                    "matchedPaperCorpusId": "12462234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0026721954345703125
        },
        {
            "corpus_id": "273695163",
            "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
            "text": "Generalisation to different test-time prompt formats. As observed in the IT literature, instruction-tuned models sometimes memorise instruction formats and struggle to follow paraphrased instructions at test time (Ghosh et al., 2024). \n\nIn Appendix G (Figure 9), we compare the performance of models on the SMNLI dataset when using the same focus instructions at training and test time versus using paraphrased instructions at test time. We generate 10 different test-time focus instructions of each instruction type defined in Equation (2) by paraphrasing the existing focus instruction using ChatGPT (OpenAI, 2022). The results show minimal variation in focus accuracy across different dataset splits and focus features, even when testing on paraphrased prompts, indicating that FIT indeed teaches models a general capacity to focus on or ignore features regardless of the specific way that focus instructions are phrased. \n\nInstruction Following After FIT. Prior studies suggest that SFT can impair the instruction-following abilities of LLMs (Fu et al., 2024;Dou et al., 2024). We evaluate whether FIT impacts instruction adherence by comparing pre-trained and FIT models trained on the SMNLI dataset (see Section 4.2). \n\nUsing 500 samples from the Alpaca-GPT instruction-tuning dataset (Peng et al., 2023), responses were rated by GPT-4o (Achiam et al., 2023)  For each model (columns), we report the pre-trained and FIT average GPT-40 ratings, and the two-sided Wilcoxon Signed-Rank p-value testing the difference between the distributions of ratings. \n\nTo test for significant differences in instruction-following performance, we conducted a two-sided Wilcoxon Signed-Rank Test (Wilcoxon, 1992) on paired ratings. The null hypothesis assumes no difference in the median ratings between pre-trained and FIT models. The results, summarized in Table 1, show no statistically significant differences (p > 0.05) in instruction-following performance across all models. These findings demonstrate that FIT maintains general instruction-following capabilities whilst additionally enhancing test-time steerability.",
            "score": 0.435418282587476,
            "section_title": "Ablation",
            "char_start_offset": 27536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2112
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016613006591796875
        },
        {
            "corpus_id": "276235718",
            "title": "Extracting and Understanding the Superficial Knowledge in Alignment",
            "text": "Alignment of large language models (LLMs) with human values and preferences, often achieved through fine-tuning based on human feedback, is essential for ensuring safe and responsible AI behaviors. However, the process typically requires substantial data and computation resources. Recent studies have revealed that alignment might be attainable at lower costs through simpler methods, such as in-context learning. This leads to the question: Is alignment predominantly superficial? In this paper, we delve into this question and provide a quantitative analysis. We formalize the concept of superficial knowledge, defining it as knowledge that can be acquired through easily token restyling, without affecting the model's ability to capture underlying causal relationships between tokens. We propose a method to extract and isolate superficial knowledge from aligned models, focusing on the shallow modifications to the final token selection process. By comparing models augmented only with superficial knowledge to fully aligned models, we quantify the superficial portion of alignment. Our findings reveal that while superficial knowledge constitutes a significant portion of alignment, particularly in safety and detoxification tasks, it is not the whole story. Tasks requiring reasoning and contextual understanding still rely on deeper knowledge. Additionally, we demonstrate two practical advantages of isolated superficial knowledge: (1) it can be transferred between models, enabling efficient offsite alignment of larger models using extracted superficial knowledge from smaller models, and (2) it is recoverable, allowing for the restoration of alignment in compromised models without sacrificing performance.",
            "score": 0.4353487993929209,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.256591796875
        },
        {
            "corpus_id": "264590778",
            "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
            "text": "We term this paradigm Stable Alignment because each final answer stored in memory reflects a group consensus rather than an individual opinion. This approach approximates how social values form during interactions-by simulating potential feedback from others and seeking common ground to facilitate effective communication. These shared social values emerge as a byproduct of developing empathy (Lee, 2021), the ability to understand and share the feelings of another, which informs us about the words and behaviors that are appreciated in daily social interactions. \n\nIn Figure 2, we also illustrate how we construct three types of alignment data from recorded interactions. As detailed in the main paper, we use the instruction template from Alpaca (Taori et al., 2023) that formats the input to the model as Instruction-Input-Response. By varying the content in these slots, we can create numerous sequences that guide the model on how to complete different tasks. Specifically, imitation data instructs the model on desired and undesired behaviors; self-critic data trains the model to compose rationales for value judgments; realignment data defends against \"jailbreaking prompting\" by including potential misaligned behavior in the instruction as a \"preview\", requiring the model to produce a realigned response. Consequently, we have generated approximately 42k alignment data samples for our version 1.0 release (and 93.8k for version 2.0). The diversity of our alignment data is demonstrated in Figure A2. \n\nA.3 DETAILED IMPLEMENTATION OF CONTRASTIVE IMITATION LEARNING Figure A3 illustrates the algorithm employed to learn alignment from simulated social interactions. Fundamentally, Stable Alignment operates as a contrastive learning procedure that rewards highrated responses and penalizes lower-rated ones. This approach diverges from traditional methods in two key aspects. First, the contrastive signal is derived from low-rated responses within the same mini-batch, as opposed to utilizing a twin network (Koch et al., 2015) or shifted embeddings (Gao et al., 2021). This strategy leverages the interactive nature of the data gathered in SANDBOX and the preceding data preparation step to enable effective contrastive learning.",
            "score": 0.435095342438316,
            "section_title": "A.2 DETAILS OF BACK-SCATTER",
            "char_start_offset": 31804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2244
                }
            ],
            "ref_mentions": [
                {
                    "start": 2022,
                    "end": 2041,
                    "matchedPaperCorpusId": "13874643"
                },
                {
                    "start": 2064,
                    "end": 2082,
                    "matchedPaperCorpusId": "233296292"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0029468536376953125
        },
        {
            "corpus_id": "273963027",
            "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
            "text": "Li et al. (2024c) further refines IFD by utilizing GPT-2 for efficient estimation. Approaches like Deita (Liu et al., 2023) consider both quality and difficulty when selecting datasets. Token length is also adopted as a metric, as discussed in (Xia et al., 2024b;Liu et al., 2023). Selective Reflection-Tuning Li et al. (2024b) approach selects and refines existing instruction-following datasets to address the inconsistency between teacher and student models. \n\nOur investigation complements existing research on alignment data selection by shifting the focus to the response generation process itself, as illustrated in Figure 1. While prior studies have concentrated on selecting the most effective instruction-response pairs with an existing instruction dataset, we explore the crucial role that response generators play in influencing the quality of instruction tuning. \n\n3 Which Models are the most effective teachers for instruction tuning? \n\n3.1 Preliminaries Instruction Datasets. An instruction dataset can be represented as \n\n, where each sample (x i , y i ) consists of an instruction x i and its corresponding response y i . In this paper, we investigate how the response generator, denoted as M, impacts the instruction-following capabilities of models fined-tuned with D with y i = M(x i ). \n\nSupervised Fine-Tuning. Supervised finetuning (SFT) is widely adopted to enhance instruction-following capabilities of LLMs. The SFT updates the parameters \u03b8 of a pre-trained language model to minimize the negative loglikelihood loss over the instruction dataset D. The SFT loss can be formally expressed as:",
            "score": 0.43508364528603705,
            "section_title": "Related Work",
            "char_start_offset": 7045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 263,
                    "matchedPaperCorpusId": "273346777"
                },
                {
                    "start": 310,
                    "end": 327,
                    "matchedPaperCorpusId": "267682220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00206756591796875
        },
        {
            "corpus_id": "267770042",
            "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
            "text": "The adaption of multilingual pre-trained LLMs into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models instruction-tuned on different language compositions on parallel instruction-tuning benchmarks across a selection of the most spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized and a large, multilingual LLMs by instruction-tuning them on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.",
            "score": 0.4349216012304797,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.020965576171875
        },
        {
            "corpus_id": "270357323",
            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
            "text": "Alignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023).3. Fine-tune an LLM with RL using PPO (Schulman et al., 2017) and the reward model.RLHF is the most commonly used method for preference alignment but often requires a lot of computation and steps for alignment.Various variants of RLHF have been proposed, such as using pure RL for training LLMs with human feedback in an online manner (Bai et al., 2022) and modifying the reward modeling with adversarial probing (Glaese et al., 2022).",
            "score": 0.4348306814551304,
            "section_title": "F Background and Related Work F.1 Alignment Methods",
            "char_start_offset": 34017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0020427703857421875
        },
        {
            "corpus_id": "275133416",
            "title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry",
            "text": "Pre-training on large corpus with next token prediction does not align well with users' objective, 2 https://libretexts.org/ 3 https://goldbook.iupac.org/ as users expect models to \"follow their instructions helpfully and safely\" (Zhang et al., 2023b). SFT effectively aligns LLMs with user expectations by training them on datasets consisting of (INSTRUC-TION, OUTPUT) pairs, where INSTRUCTION refers to specific chemistry tasks and OUTPUT represents the desired responses. Given the variety of chemistry tasks in the SFT dataset, it can be further categorized as follows: \n\n1. Multi-task SFT: We categorize commonly used chemistry tasks into four types: SMILES understanding, reaction understanding, notation alignment and chemistry-related QA, as detailed in Appendix B. The most significant distinction among different SFT models (Yu et al., 2024;Fang et al., 2023;Zhao et al., 2024b;Zhang et al., 2024a) lie in their data sources and the volume of data used, and the detailed data distribution is shown in Appendix B. The total dataset volume ranges from 1.5M to 3M, although Zhang et al. (2024a) does not provide exact figures, it is likely of a similar magnitude. The distribution of tasks within the SFT dataset determines the model's chemistry capabilities, as identified by (Feng et al., 2024) (2024d) propose hybrid instruction tuning on more than 1000 property tasks with LLaMA2-7b-chat (Touvron et al., 2023b), reporting up to a 16.6% average improvement over leading LLM baselines across all classification tasks. Additionally, Chen et al. ( 2023) also fine-tune LLaMA2-7B-chat with 13,878 pieces of structured material knowledge data to predict inorganic material synthesis pathways. \n\nIn addition to these chemistry tasks, chemical text mining is also a crucial foundation in chemical research, as much scientific knowledge is dispersed across the text, tables, and figures in millions of academic papers (Dagdelen et al., 2024).",
            "score": 0.43482407102119225,
            "section_title": "SFT",
            "char_start_offset": 8737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0010986328125
        },
        {
            "corpus_id": "270559708",
            "title": "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
            "text": "Recent studies have demonstrated that In-Context Learning (ICL), through the use of specific demonstrations, can align Large Language Models (LLMs) with human preferences known as In-Context Alignment (ICA), indicating that models can comprehend human instructions without requiring parameter adjustments. However, the exploration of the mechanism and applicability of ICA remains limited. In this paper, we begin by dividing the context text used in ICA into three categories: format, system prompt, and example. Through ablation experiments, we investigate the effectiveness of each part in enabling ICA to function effectively. We then examine how variants in these parts impact the model's alignment performance. Our findings indicate that the example part is crucial for enhancing the model's alignment capabilities, with changes in examples significantly affecting alignment performance. We also conduct a comprehensive evaluation of ICA's zero-shot capabilities in various alignment tasks. The results indicate that compared to parameter fine-tuning methods, ICA demonstrates superior performance in knowledge-based tasks and tool-use tasks. However, it still exhibits certain limitations in areas such as multi-turn dialogues and instruction following.",
            "score": 0.4346465986065168,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004085540771484375
        },
        {
            "corpus_id": "271923851",
            "title": "How Susceptible are LLMs to Influence in Prompts?",
            "text": "The title \"Father of Physics\" is often attributed to several historical figures, depending on the context and the aspect of physics being discussed. These figures include Isaac Newton and Albert Einstein. \n\nThere are many settings where input to Large Language Models (LLMs) is augmented by output from other models or external sources. This includes for example self-critique and oversight (Bai et al., 2022), retrieval-augmented generation (RAG) (Lewis et al., 2020) and collaborative multi-agent systems where LLMs interact with each other or with humans to solve complex tasks. As LLMs become increasingly integrated into real-world applications, understanding how they respond to and incorporate information from external sources becomes crucial. However, LLMs are known to show sycophantic behaviour (Perez et al., 2022;Sharma et al., 2023). Specifically, models tend to agree with the interacting users' views, even if these are different from their own, manifesting a Clever Hans effect 1 in LLMs. This behaviour is not generally desirable when interacting with human users, and may lead to further problems, such as the propagation of errors, the reinforcement of biases and the generation of outputs that are inconsistent with the model's actual knowledge or capabilities. \n\nThere are important questions that need to be addressed. What happens when the external information contradicts the models internal knowledge? Does the quality and correctness of a provided information make a difference? \n\nIn this work we study the influence from such augmented inputs in a question-answering setting and the factors that contribute to this susceptibility. Across a diverse spectrum of question-answering tasks, we present how an LLM (judge) changes their response when provided with influence from another model (advocate) that is instructed to argue for a particular answer. We consider a range of models and a wide spectrum of question-answering tasks. Specifically, we study the following tasks: PIQA, SIQA, CommonsenseQA, Open-BookQA, WikiQA, GPQA, QuALITY and BoolQ, and focus on three current open models (Llama2,Mixtral,Falcon). By covering a diverse set of tasks, we aim to provide a broad perspective on the influence of augmented inputs across different domains, that represent different levels of difficulty based on the models' capabilities.",
            "score": 0.43409268620129116,
            "section_title": "Assistant:",
            "char_start_offset": 306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 806,
                    "end": 826,
                    "matchedPaperCorpusId": "254854519"
                },
                {
                    "start": 826,
                    "end": 846,
                    "matchedPaperCorpusId": "264405698"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022525787353515625
        },
        {
            "corpus_id": "271693158",
            "title": "ChipExpert: The Open-Source Integrated-Circuit-Design-Specific Large Language Model",
            "text": "Besides the training dataset, we carefully constructed a ChipICD-Bench to evaluate the model's performance. \n\n\u2022 Continue Pretraining: Continue pretraining [4] in LLMs is a technique that involves further training an already pre-trained model on additional data or tasks to improve its performance and adapt it to specific domains or applications. Just like the pretraining of foundation model, continue pretraining is also an autoregressive training process on long-text corpora. Through continue pretraining, ChipExpert has learned a large amount of foundational knowledge and cutting-edge information related to IC design, gaining a deep understanding of the chip industry. Next, it needs to learn how to express its professional knowledge when users inquire about it. \n\n\u2022 Supervised Fine-tuning: Unlike continue pretraining process, which is an unsupervised training process on long-text corpora, supervised fine-tuning (SFT), also known as domain-specific assistant adaptation or instruct tuning, is a supervised learning process on instruction-response samples. We train ChipExpert on the IC design related question-answer pair dataset, and as the training loss decreases, the model gradually learns how to utilize its domain knowledge to respond to users' domain-specific questions. \n\n\u2022 Direct Preference Optimization: In the alignment phase, ChipExpert undergoes refinement through the incorporation of human preferences to mitigate potential risks and adverse impacts on individual users and society at large. Common possible ways of doing so include Reinforcement Learning from Human Feedback (RLHF) [11] and Direct Preference Optimization (DPO) [5]. Due to the tremendous computational resources required for RLHF and the significant instability inherent in the reinforcement learning process, We opt for DPO to accomplish the alignment process of ChipExpert. In our alignment training, we employ a two-phase approach. The initial phase involves utilizing a curated set of highquality, publicly accessible paired preference data. Subsequently, in the second phase, we undertake a comprehensive red teaming initiative to identify and isolate potentially harmful, unsafe, or illegal model outputs. The insights gleaned from this process are then leveraged to generate targeted alignment data, which is used to mitigate these undesirable responses and refine the model's behavior.",
            "score": 0.43362823427106356,
            "section_title": "body",
            "char_start_offset": 2191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 1655,
                    "end": 1658,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0011034011840820312
        },
        {
            "corpus_id": "268363404",
            "title": "Fine-tuning Large Language Models with Sequential Instructions",
            "text": "Instruction tuning (IT), or supervised fine-tuning (SFT), gives large language models (LLMs) the ability to execute new tasks specified by users (Mishra et al., 2022;Sanh et al., 2022;Wei et al., 2022a).Nevertheless, popular instruction mixtures contain rather straightforward instructions derived from conventional NLP tasks or open-ended dialogues (Sanh et al., 2022;Taori et al., 2023;Conover et al., 2023).Hence, they suffer from the absence of multi-step instructions.While this dataset design presumably mirrors the properties of natural data, where such instructions rarely occur, we speculate that this hinders the fine-tuned models from navigating a sequence of sub-tasks in a single command, which is arguably crucial for complex tasks requiring reasoning (e.g., coding and maths) or knowledge transfer (e.g., cross-lingual and cross-modal question answering, Shi et al., 2023;Zhang et al., 2023).Moreover, this detracts from user experience as models do not track whether all requests have been fulfilled.\n\nWe empirically verify this hypothesis by prompting various versions of state-of-the-art open-source LLMs (e.g.Llama 3 AI@Meta 2024 and Mistral Jiang et al. 2023) with simple two-step instructionsalready more than they can shake a stick at.After manually inspecting their answers, we find that not only did their accuracy degrade dramatically, but also that they often failed to follow the entire list of instructions, particularly for models fine-tuned on public datasets like Alpaca (Taori et al., 2023).To tackle this problem, we propose a sequential instruction tuning (SIT) paradigm which uses simple",
            "score": 0.4334476245885491,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1016
                },
                {
                    "start": 1018,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 166,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 166,
                    "end": 184,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 184,
                    "end": 202,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 870,
                    "end": 887,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 887,
                    "end": 906,
                    "matchedPaperCorpusId": "265212710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016040802001953125
        },
        {
            "corpus_id": "270258300",
            "title": "Bayesian WeakS-to-Strong from Text Classification to Generation",
            "text": "With the increase in computing power and the amount of training data available, the capabilities of large language models (LLMs) have been continuously brought closer to humans in many aspects. Despite their impressive performance, the preferences and values of pre-trained LLMs do not always align with humans, and dedicated approaches are needed to tackle the problem. Based on large-scale instruction datasets, supervised finetuning (SFT) encourages LLMs to follow human instructions more strictly and respond more safely (Wei et al., 2022). Reinforcement learning (RL) is commonly applied to such alignment. By collecting model output values and the corresponding human feedback, the model can be finetuned by RL to avoid generating undesirable outputs (Ziegler et al., 2019;Bai et al., 2022a;Ouyang et al., 2022;Nakano et al., 2021;Askell et al., 2021). \n\nSince no current model has yet surpassed human intelligence, alignment methods, such as SFT and RL from human feedback (RLHF), remain effective. However, it is worthwhile considering future scenarios where artificial intelligence (AI) might surpass human intelligence in all aspects. Would the current alignment methods still be effective for such super AI models? How could humans supervise the super AI? To simulate this future scenario, an analogy situation is designed that downgrades both sides: using a weak model to simulate humans and a strong model to simulate future super AI (Burns et al., 2023), which is termed as superalignment. It has been demonstrated that adding a simple auxiliary loss can achieve effective Weak-to-Strong generalization, even if the weak model's supervision contains many errors, which offers hope of achieving superalignment. Nonetheless, this is just the beginning of exploring along the path of Weak-to-Strong. \n\n\u2022 We propose to generalize both Weak-to-Strong and WeakS-to-Strong from text classification to generation tasks, extending their scope from content regulation to content generation. \n\n\u2022 When applied to text generation, a token-level probability estimation is proposed to achieve soft labels for strong model training. We also propose the modified DPO algorithm under the Bayesian WeakS-to-Strong framework to further improve text generation performance.",
            "score": 0.43305117574781216,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 797,
                    "end": 817,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0025615692138671875
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "Limitation of Tuning-based Alignment. Alignment tuning through SFT and RLHF typically demands substantial resources, such as GPU nodes, a large amount of instruction data, and human annotations, making the process both costly and time-consuming. This restricts ordinary labs from aligning extreme-scale LLMs exceeding 30B, let alone the recent Falcon-180B (Almazrouei et al., 2023). Moreover, during the pre-training and continual training stages, efficiently estimating the downstream performance of a base model checkpoint becomes challenging if alignment tuning is always required to evaluate its instruction-following ability. Besides the aforementioned limitations, tuning-based alignment may also cause forgetting issues in LLMs. Wang et al. (2023) demonstrated that some SFTed LLMs perform significantly worse than their base counterparts on factual and reasoning benchmarks. For instance, applying SFT to Llama-13b with self-instruct (Wang et al., 2022a) results in a considerable decline in its MMLU performance (from 42.5 to 30.3) and Codex-Eval performance (from 26.6 to 13.4). Even more strikingly, SFT with SuperNI (Wang et al., 2022b) causes Llama-13B to nearly lose all its BBH reasoning ability (decreasing from 36.9 to 2.8). Moreover, Shen et al. (2023) show that the reward models in RLHF can perform very inconsistently, yielding a nearly random performance when showing contrastive instructions to them. These findings imply that alignment tuning may lead to the forgetting of previously acquired knowledge in base LLMs, which is also shown in our experiments. Superficial alignment hypothesis. LIMA (Zhou et al., 2023) employs only 1k examples to finetune a 65B LLM and discovers that such a slightly tuned LLM surprisingly achieves a high win rate over ChatGPT, thus implying that the alignment tuning is superficial. Similar observations are also reported by other recent studies (Chen et al., 2023a;Lee et al., 2023).",
            "score": 0.4328163645086571,
            "section_title": "ALIGNMENT TUNING & SUPERFICIAL ALIGNMENT HYPOTHESIS",
            "char_start_offset": 35071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 942,
                    "end": 962,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1128,
                    "end": 1148,
                    "matchedPaperCorpusId": "253098274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0316162109375
        },
        {
            "corpus_id": "267320913",
            "title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese",
            "text": "With the release of ChatGPT in November 2022, there has been an increase in interest in models that went through an alignment process (e.g., instruction tuning, preference modeling, etc.), making them more attuned to follow the commands of people without the need for sophisticated prompting or further fine-tuning, becoming, in general, more helpful tools (a.k.a. assistants) to their users. Nowadays, there are many assistant models like ChatGPT (Corr\u00eaa, 2023a;Taori et al., 2023;Touvron et al., 2023b;Jiang et al., 2023;Geng et al., 2023;Conover et al., 2023;K\u00f6pf et al., 2023), which, besides being an object of interest to the general public, have become one of the most used laboratories for alignment research (Askell et al., 2021). \n\nOur base models can follow instructions with minimal prompting, given that they were already exposed to millions of instructions during training. To further expand these capabilities, we fine-tuned the 460m parameter version of TTL on an instructional dataset to create a chat version of our larger base model, TTL-460m-Chat. Like in the Alpaca study (Taori et al., 2023), we trained our chat model via SFT on a synthetically generated dataset. This dataset contains a collection of single-turn conversations between an assistant and a user, generated by prompting models that already went through an alignment process (ChatGPT, Vicuna, LLama 2, Open-Assistant, etc.). The dataset is available in Brazilian Portuguese and English and contains approximately 81K samples (Corr\u00eaa, 2023b).9 \n\nFor the SFT, we used the same software stack utilized to pre-train our models. TTL-460m-Chat was trained for three epochs using almost the same configurations documented in Table 3. The only modifications are in the number of warm-up steps (1,000) and learning rate (1 \u00d7 10 \u22125 ). The full details are available in our GitHub repository. 10",
            "score": 0.4326808811756444,
            "section_title": "Alignment",
            "char_start_offset": 30019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1870
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00243377685546875
        },
        {
            "corpus_id": "270123084",
            "title": "InstructionCP: A fast approach to transfer Large Language Models into target language",
            "text": "Firstly, the KTO loss function is based on the Kahneman-Tversky value function for monetary gains and losses, which may not accurately reflect how humans perceive the relative goodness of text.Nevertheless, LLMs trained using RLHF demonstrate enhanced safety in completions, a crucial factor for companies and groups intending to open-source their models (Stiennon et al., 2022).Yet, as highlighted in Qi et al. (2023), challenges persist in maintaining RLHF capabilities when fine-tuning GPT-4(OpenAI, 2023)  Figure 1: n illustration to demonstrate the difference between the traditional approach and our method.\n\nIn the traditional approach, considerable effort is expended in collecting a plethora of contextual data for continual pre-training (CP), various types of instruction-following data for instruction tuning, and significant human resources are allocated to label data for reinforcement learning from human feedback (RLHF).However, with our method, Instruction Continual Pre-training (InsCP), these processes are streamlined into a single step\n\nIn this work, we propose a novel fine-tuning approach called Instruction Continual Pretraining (InsCP) for LLMs to adapt to non-English languages.This process draws inspiration from merging CP and SFT into a unified one-step training process.Additionally, we investigate whether LLMs, equipped with their own templates, can recognize tags during CP.Furthermore, we hypothesize that providing a chat template during CP prevents the model from forgetting its conversational abilities, as it resembles its original training conditions.Our approach begins with CP on a specific dataset, where we augment each piece of data with special instruction tokens, such as < |begin_of _text| > in LLaMA3(AI@Meta, 2024).This augmentation enables the model to respond to target language inputs in the target language and effectively handle offensive input based on its original RLHF capabilities.\n\nWe evaluate the effectiveness of InsCP on LLMs, primarily focusing on the LLaMA3instruct model, across three key aspects: language alignment, reliability, and knowledge benchmarks.Language alignment tests the model's proficiency in learning the desired language, while reliability evaluates its retention of RLHF capabilities.",
            "score": 0.4324703902867397,
            "section_title": "Introduction",
            "char_start_offset": 1815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 613
                },
                {
                    "start": 615,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1055
                },
                {
                    "start": 1057,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1589
                },
                {
                    "start": 1589,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1938
                },
                {
                    "start": 1940,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013828277587890625
        },
        {
            "corpus_id": "259375779",
            "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning",
            "text": "The response tone alignment problem is a part of a broader intent alignment topic. In principle, LLMs are not aligned with users' intents because their language modeling objective, e.g., predicting the next token of a training document, is different from the following instruction target. \n\nOne successful approach for aligning both objectives is to prompt models using zero-or n-shot techniques, where the response would look like a completion of a document containing QA (Brown et al. 2020, Radford et al. 2018). \n\nAnother approach is to instruct and tune a vanilla model on tuples of instruction and response, so the model, as part of learning, acquires skills to imitate the correct format response (Alpaca: Taori et al. 2023, Self-Instruct: Wang et al. 2023). \n\nIn the InstructGPT paper (Ouyang et al. 2022), the criterion \"fails to follow the correct instruction / task\" was included in the list of human evaluation metadata for a reward model (RM) used in the PPO algorithm (Schulman et al. 2017) to fine-tune the SFT models to maximize their reward. \n\nWe aim to isolate and understand the tone component by evaluating each strategy as a style formatting problem rather than using knowledge and language understanding-based metrics, e.g., MMLU (Hendrycks et al. 2021). \n\n3 Instruction Following Index",
            "score": 0.4323124584445693,
            "section_title": "Background and Related Work",
            "char_start_offset": 4754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018825531005859375
        },
        {
            "corpus_id": "271218517",
            "title": "CCoE: A Compact and Efficient LLM Framework with Multi-Expert Collaboration for Resource-Limited Settings",
            "text": "For example, it can easily add an expert for targeting English-Chinese translation task without worrying about the task conflicts with other experts.\n\n3 Dilemma of Supervised Fine-tuning LLM Large Language model (LLM) Supervised fine-tuning (SFT) is the process of continually training the pre-trained models on smaller, specific datasets to refine their capabilities and improve the performance in a specific task.SFT can also be used to inject new domain knowledge into the model.The SFT process is about turning a general-purpose model and transform it into specialized model with the domain-related dataset.For example, there is a limited knowledge in pre-trained model on law domain.We can inject the knowledge of the law into LLM to make it become a law specialized model.The SFT brings a gap between pre-trained model and fine-tuned model.Nowadays, LLM fine-tuning has become an indispensable approach for enterprises to enhance their operational process.Through training LLMs for specific task with domain datasets, we can push the knowledge of LLMs to the boundaries of different areas [12] [13] [14].\n\nThere are many ad-hoc attempts on SFT for enhancing LLM performance on individual capabilities in open community.However, when the widely used LLMs have been fine-tuned, it may affect the general knowledge stored in LLMs since there is a distribution shift between SFT data and the original training data.The study of SFT is crucial for certain practical applications of LLMs.In most real cases, users want the LLMs to be enhanced in specific domain capabilities while preserving their general capabilities.A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge during the SFT process [15] [16].The naive way of reducing the CF is to mix the supervised data with the pre-trained data at certain ratios.Recently, many researches have proposed various approaches to alleviate the problem of CF such as Dual-Stage Mixed Fine-tuning [17], Recall and Learning [18], etc.However, there are still many remaining challenges left to the community.",
            "score": 0.4322468371711482,
            "section_title": "Analogy to Mixture of Experts",
            "char_start_offset": 6341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 151,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1225
                },
                {
                    "start": 1225,
                    "end": 1417
                },
                {
                    "start": 1417,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1887
                },
                {
                    "start": 1887,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0008358955383300781
        },
        {
            "corpus_id": "273351188",
            "title": "Improving Data Efficiency via Curating LLM-Driven Rating Systems",
            "text": "In recent years, large language models (LLMs) have shown remarkable success across various downstream tasks, from natural language understanding to generative AI applications. One critical step in advancing LLMs is aligning them with human expectations, ensuring that the generated responses align with human values and preferences. While reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022) has been a popular approach for alignment, another widely adopted approach is instruction finetuning or supervised fine-tuning (SFT). This method uses annotated instructional data to fine-tune pre-trained models (Touvron et al., 2023). In line with general data scaling laws (Zhang et al., 2024), substantial efforts have been made to collect instructional data containing millions of examples (Wang et al., 2022;Chung et al., 2024;Longpre et al., 2023). \n\nHowever, recent studies suggest that most of the knowledge in LLM is acquired during pre-training, and a small, high-quality dataset curated through human annotations may suffice for effective alignment (Zhou et al., 2024), challenging traditional data scaling laws. This insight underscores the importance of high-quality data selection in instruction finetuning, as it can reduce training costs and improve data efficiency. Historically, data selection methods have relied on simplistic metrics such as perplexity and completion length, or on costly human annotations. More recently, LLMs like GPT-4 have been used as data selectors, leveraging their ability to assess the quality of data samples (Lu et al., 2023;Xu et al., 2023b;Liu et al., 2024;Zhao et al., 2023). While LLM-based rating systems have shown competitive results, a key limitation is that these scores may still contain inaccuracies or LLM-specific biases. Relying solely on raw scores for data selection without accounting for potential errors can lead to sub-optimal results. \n\nIn this work, we start by analyzing the error patterns presented in LLM-generated scores. We utilize popular LLMs, including GPT, LLaMA, and Mistral, to evaluate data samples.",
            "score": 0.432071893082732,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1914
                },
                {
                    "start": 1917,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 410,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 824,
                    "end": 843,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 843,
                    "end": 864,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 1567,
                    "end": 1584,
                    "matchedPaperCorpusId": "260887200"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00238800048828125
        },
        {
            "corpus_id": "277626677",
            "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models",
            "text": "To comprehensively understand how LLMs process and internalize arithmetic principles, we conducted extensive experiments on knowledge internalization through fine-tuning approaches. \n\nOur investigation examined three key aspects: knowledge source variability (general, domainspecific, and task-specific), fine-tuning methodologies (supervised fine-tuning (SFT) and reinforcement learning (RL) approaches), and their combined effects on mathematical understanding. Specifically, we employed Direct Preference Optimization (DPO) (Rafailov et al., 2023) and its advanced variant RPO(SFT+DPO) (Pang et al., 2024) for reinforcement learning-based training. To evaluate the effectiveness of these approaches, we assessed model performance on both numerical and symbolic addition tasks post-fine-tuning. We also included comparative baselines from specialized mathematical reasoning models, including Eu-rus2 (Cui et al., 2025) with its unique combination of SFT and Process Reward Modeling (PRM) (Ma et al., 2023), as well as advanced reasoning models like OpenAI o1 (OpenAI, 2024b) and DeepSeek R1 (DeepSeek-AI, 2025), along with their distilled variants. For training data preparation, we sampled responses from various LLMs in zero-shot and symbolic settings, using correct answers from the training split as positive examples and incorrect responses as negative samples for reinforcement learning. Detailed specifications of the fine-tuning protocols are provided in Appendix A.3. \n\nThe fine-tuning results presented in Table 5 reveal distinct patterns in knowledge internalization and generalization. Task-specific fine-tuning demonstrates a clear trade-off: while SFT achieves the highest within-domain performance improvement, suggesting effective knowledge internalization, it exhibits poor cross-domain generalization. RL-based approaches (DPO, RPO) show lower absolute performance but superior generalization capabilities. Notably, RPO's significant transfer performance drop, despite combining SFT and DPO, indicates that SFT's pattern-matching tendency dominates the learning process. This suggests that current fine-tuning approaches, particularly SFT, may optimize for task-specific pattern recognition rather than abstract principle learning.",
            "score": 0.430868720916611,
            "section_title": "Rule Internalization",
            "char_start_offset": 21842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 527,
                    "end": 550,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 589,
                    "end": 608,
                    "matchedPaperCorpusId": "269457506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022258758544921875
        },
        {
            "corpus_id": "272881354",
            "title": "Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories",
            "text": "Mocha presents novel interaction mechanisms inspired by two human cognition theories. First, the Variation Theory of human concept learning [44] informed a new approach to generate synthetic counterfactual data for users to annotate. Secondly, the Structural Alignment Theory [27] guides the design of Mocha's interface for presenting generated counterfactual examples in batch, which assists users in perceiving and comprehending the alignable differences between data items and annotating these data items in batch. \n\nCounterfactual Data Generation. In the counterfactual data generation phase, once a user annotates a small initial dataset, Mocha employs a Variation Theory (VT) [44]-based pipeline to create synthetic data. VT posits that human learning occurs when learners experience variation across critical and superficial aspects of a concept-through exposure to contrasting examples that systematically vary along different critical and superficial feature dimensions. Inspired by VT, our pipeline starts with the neurosymbolic model's current (and potentially imperfect) learned pattern rules, which can be thought of as feature dimensions. It then generates counterfactual data that are syntactically and semantically similar enough to an already-annotated datum that they would be given the same label by the neuro-symbolic model's pattern rule, but different enough that they would be given a different label by a standard pre-trained large language model. Therefore, the generated data poses the hypothetical question [19]: \"How should the model's prediction change if certain aspects of the input were altered?\" Consider this analogy to illustrate the counterfactual approach for refining concept boundaries; a user and a model are negotiating how to define a sandwich. Although both may start with their own definition, neither is accurate or specific. We suppose that the user starts with a definition \"a sandwich is two slices of bread with meat in between.\" Although this definition may be a good candidate, it misses important features that make a sandwich a sandwich. In this analogy, our proposed approach would ask the user if grilled cheese is a sandwich as a counterfactual proposition.",
            "score": 0.4303521705163915,
            "section_title": "Introduction",
            "char_start_offset": 3642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "1745309"
                },
                {
                    "start": 1534,
                    "end": 1538,
                    "matchedPaperCorpusId": "218901061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0079345703125
        },
        {
            "corpus_id": "271903390",
            "title": "Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs",
            "text": "SFT and Alignment Fine-tuning is a prevalent strategy to enhance model performance on downstream tasks, evidenced in domains such as coding (Wei et al., 2023;Luo et al., 2024) and arithmetic (Yue et al., 2024). Other work has highlighted the importance of consistency in format (Liang et al., 2023), data quality (Chung et al., 2022), and mixing tasks from different categories (Longpre et al., 2023;Iyer et al., 2022) in SFT. As LLMs evolve, the risk of generating unsafe content increases (Su et al., 2024;Wang et al., 2023a). Established methods for LLM alignment include instruction fine-tuning and reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022). Instruction fine-tuning, also known as SFT, refines pre-trained models using annotated instructional data, often preceding RLHF to aid initial alignment (Touvron et al., 2023). RLHF employs reinforcement learning to adapt models based on feedback on generated responses. Although RLHF has been pivotal for developing systems like ChatGPT (OpenAI, 2021), isolated instruction fine-tuning can yield comparable outcomes (Sun et al., 2023) with much less computational and labor costs. Packing While packing is relatively less researched, it is a technique extensively used in frameworks like Hugging Face's SFT Trainer1 to expedite inference and training. To prevent crosscontamination during self-attention calculation, existing packing approaches involve concatenating sequences into a single tensor and using masking to disregard elements from other sequences during computation (Kundu et al., 2024). This method, including variations like LongAlign (Bai et al., 2024) and Prepacking (Zhao et al., 2024a), enhances training efficiency and minimizes the crosscontamination impact on model performance. However, it necessitates calculating a distinct attention mask for each batch, complicating implementation and increasing memory consumption for masks, which can hinder the effectiveness of flash attention.",
            "score": 0.43002802085021,
            "section_title": "Related Work",
            "char_start_offset": 4153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 175,
                    "matchedPaperCorpusId": "259164815"
                },
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "261696697"
                },
                {
                    "start": 378,
                    "end": 400,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 508,
                    "end": 527,
                    "matchedPaperCorpusId": "259202782"
                },
                {
                    "start": 653,
                    "end": 674,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1093,
                    "end": 1111,
                    "matchedPaperCorpusId": "258479665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013885498046875
        },
        {
            "corpus_id": "267897555",
            "title": "Unintended Impacts of LLM Alignment on Global Representation",
            "text": "For example, the Starling 7B Reward Model (Zhu et al., 2023) gives higher scores to responses claiming to be from Englishspeaking Western nations and lower scores for Middle Eastern and African nations (See Figure 1). In this work, we take a closer look at the effects these design decisions have on a model's ability to serve a global population, which is key to understanding if the general use of aligned LLMs (Eloundou et al., 2023) is likely to be positively adopted globally. \n\nExisting performance evaluations of chat assistants mainly focus on tasks such as reasoning (Clark et al., 2018;Zellers et al., 2019;Sakaguchi et al., 2021;Cobbe et al., 2021), multitask knowledge (Hendrycks et al., 2021;Suzgun et al., 2023), truthfulness (Lin et al., 2022), multi-turn instruction following (Zheng et al., 2023), and similar variations of broad knowledge/reasoning/skills (Chen et al., 2021;Zhong et al., 2023). Instead, we explore a set of representative domains covering variations common in diverse global user bases: English dialects, multilingualism, and global opinions, and show a direct impact on model performance. \n\nOur evaluations focus on measuring how alignment makes LLMs more agreeable and helpful for different groups of possible global users. While prior works ( \u00a72) have explored the representation of global opinions in language models (Durmus et al., 2023;Santurkar et al., 2023), they only study the final model. However, the process of transforming a base language model to a user-facing chat model involves two key sequential steps: supervised fine-tuning (SFT) and preference tuning (PT). The impacts of alignment are the product of the base model, SFT, and PT. In addition to evaluating surveys, we study performance gaps on downstream tasks that occur throughout the alignment process for several variations common in global user bases. Together, these evaluations assess whether alignment procedures make LLMs both more agreeable and helpful for a global user base. In summary, our contributions are as follows:",
            "score": 0.4299352344102255,
            "section_title": "Introduction",
            "char_start_offset": 1593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 617,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 617,
                    "end": 640,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 681,
                    "end": 705,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 705,
                    "end": 725,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 740,
                    "end": 758,
                    "matchedPaperCorpusId": "237532606"
                },
                {
                    "start": 1357,
                    "end": 1378,
                    "matchedPaperCorpusId": "257834040"
                },
                {
                    "start": 1378,
                    "end": 1401,
                    "matchedPaperCorpusId": "257834040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0032863616943359375
        },
        {
            "corpus_id": "249674934",
            "title": "The Manifold Hypothesis for Gradient-Based Explanations",
            "text": "In this work, we focus on a particular aspect of feature attributions: whether they are aligned with the tangent space of the data manifold. The objective of this paper is not to claim that the gradients of existing models provide good explanations, or that any particular post-hoc explanation method works especially well. Instead, we would like to contribute to a line of work that, independently of particular algorithms, develops criteria by which explanations can be judged. As we demonstrate in Sections 4 and 5.2, the question of whether an attribution is aligned with the data manifold is amendable to empirical and theoretical analysis. \n\nWhile current models and algorithms provide only imperfect alignment, it is an open question whether this is due to the fact that we have not yet found the right model architecture or algorithm, or because the problem is more difficult than classification alone. To the best of our knowledge, the question of how model gradients can be aligned with the data manifold is essentially unexplored in the machine learning literature. Although we are, to the best of our knowledge, the first to conduct a systematic evaluation of the manifold hypothesis, aspects of it are implicit in previous works [33,7,11,62,22].",
            "score": 0.4298270680369187,
            "section_title": "Conclusion",
            "char_start_offset": 28954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1258
                }
            ],
            "ref_mentions": [
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "85543329"
                },
                {
                    "start": 1246,
                    "end": 1248,
                    "matchedPaperCorpusId": "220646566"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "52962991"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "54439061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003482818603515625
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "It likens pre-training and fine-tuning of LLMs to Chinese proveb \"Read ten thousand books, travel ten thousand miles\". In pre-training, the LLM processes vast amounts of data without feedback, gaining broad language understanding. In fine-tuning, it generates responses to prompts and receives feedback, refining its abilities and improving performance on specific tasks. \n\nDrawing inspiration from UNA's effectiveness in handling score-based feedback, we propose extending UNA to also fulfill the goals of SFT. By converting SFT data into a format compatible with alignment training, we enable the use of a unified objective and loss function. This approach allows for effective fine-tuning of pretrained LLMs while preserving their previously acquired capabilities across tasks. \n\nThe contributions of this paper are listed as follow: \n\n1. Prove that both UNA and SFT maximize the likelihood of the response in instruction-tuning data, and UNA outperforms SFT on downstream tasks when fine-tuning on instruction-tuning data. \n\n2. UFT that unifies SFT and alignment solves the performance degradation on some tasks elegantly and surpasses the performance of the original sequential application order in downstream tasks. \n\n3. UFT builds a unified post-training framework that is parallel to pretraining where the goal lies in generating responses for given prompts, receiving score-based feedback from different labelers and improve its capability on downstream tasks.",
            "score": 0.4296711059554748,
            "section_title": "Introduction",
            "char_start_offset": 1951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1469
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001934051513671875
        },
        {
            "corpus_id": "273653892",
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "text": "A recent advance in instruction tuning is the potential to complement or replace few-shot in-context learning with parameter-efficient fine-tuning. Compared to instruction tuning, parameter-efficient fine-tuning can achieve performance comparable to full parameter tuning while being computationally more cost-effective. Previous studies [97], [84], [98], [99] have demonstrated that parameter-efficient fine-tuning can be effectively integrated with instruction tuning, either before or after the instruction tuning process. Additionally, this body of research highlights that parameter-efficient fine-tuning can enhance the performance and applicability of instruction tuning across different domains. \n\n2) Alignment Tuning and RLHF: Despite the emergent abilities brought by increasing parameters of language models, hallucination exhibit to become a challenge for LLMs to produce satisfying response. To address this issue, alignment tuning is applied to align the models with specific human preferences. There are three primary targets for alignment tuning, respectively presented as helpfulness, honesty and harmlessness. From the targets' names, it can be concluded that the alignment criteria are closely associated with human's recognition, making it difficult to formulate them as optimization objectives for LLMs. Therefore, human feedback is widely adopted as an assistance to reinforce LLMs' performance. \n\nRLHF [100], [101] emerged as a method to fine-tune language models using human feedback, aiming to align the LLMs with human preferences, and consequently enhancing alignment performance. \n\nGenerally, an RLHF system [34] comprises three key components: a pre-trained language model, a reward model learned from human feedback, and a reinforcement learning algorithm to train the language model. Figure 2 shows the three key steps. \n\n\u201a Supervised Fine-Tuning (SFT): Initially, a supervised dataset consisting of input prompts and desired outputs is applied to fine-tune the language model. These prompts and outputs can be written by human labelers for some specific tasks while ensuring the diversity of tasks. This step helps the model learn expected behaviors. \u201a Reward Model Training: A reward model is trained using human feedback data. The LLM is employed to generate a certain number of output texts using sam- pled prompts as input. Then human labelers rank these output pairs based on their preferences.",
            "score": 0.4294529240495471,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 33052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1850
                },
                {
                    "start": 1853,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "248693283"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1425,
                    "end": 1430,
                    "matchedPaperCorpusId": "5613334"
                },
                {
                    "start": 1432,
                    "end": 1437,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0023975372314453125
        },
        {
            "corpus_id": "267616740",
            "title": "Rethinking Data Selection for Supervised Fine-Tuning",
            "text": "However, although these two factors are highly significant for developing AI systems in the pre-LLM era, we question their relevance under the SFT scenario in the current era of LLMs, given the superficial nature of SFT. \n\nAdhering to such nature of SFT, we hypothesize that data selection for SFT should focus on picking demonstrations that reflect human style the most. For example, a demonstration of teaching LLM that Paris is the capital of France is unhelpful since LLM already gained such knowledge during the pretraining phase. On the contrary, demonstrations with human-like responses containing words such as \"thanks\" and \"first of all\", and structured responses containing numbered lists are helpful for SFT (Lin et al., 2023). However, directly identifying such demonstrations is not a straightforward process; thus, we investigate a simple heuristic by selecting demonstrations with long responses, inspired by the observation that if a response meets the instruction requirement in the first place, a longer version with more details is deemed more helpful. Two illustrative examples are given in Appendix Table 9 and 10 where both human and GPT-4 prefer the longer response with more details. \n\nSurprisingly, even such a simple heuristic leads to models with strong instruction-following capabilities. For Alpaca dataset (Taori et al., 2023), finetuning LLaMA-2-7B (Touvron et al., 2023b) with only top 1K instances with long responses leads to an average win rate of 68% versus 20% when utilizing entire dataset, 63% versus 20% when utilizing same amount of instances selected based on the quality aspect, and 70% versus 15% when utilizing instances with high diversity instructions. \n\nOverall, starting from the potentially superficial nature of SFT, we question the existing data selection strategies based on quality and diversity aspects and propose to select true important instances from a perspective of mimicking human style. To make an initial effort in this direction, we show that even a naive heuristic leads to strong performances compared with baseline selection strategies. This also sheds light on better curating SFT datasets instead of only selecting from an existing data pool.",
            "score": 0.4293754422137319,
            "section_title": "Introduction",
            "char_start_offset": 1598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2104
                },
                {
                    "start": 2105,
                    "end": 2212
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00511932373046875
        },
        {
            "corpus_id": "274342032",
            "title": "Baichuan Alignment Technical Report",
            "text": "In recent years, Large Language Models (LLMs) have achieved significant breakthroughs [67,23,69,10,18,1,3,4,81,79], showing early signs of advancing toward Artificial General Intelligence (AGI). Through the mechanism of next-token prediction, LLMs undergo self-supervised pre-training on vast datasets, thereby acquiring a diverse spectrum of capabilities that empower them to perform an array of tasks, such as text continuation, summarization, and creative composition. Further advancements in alignment methodologies, such as Prompt Augmentation Systems (PAS) [103], Supervised Fine-Tuning (SFT), and preference modeling [65], have become important to the evolution of LLMs. These developments have enhanced the models' ability to comprehend user intentions and adhere to instructions, thereby enhancing their conversational skills and adaptability to complex real-world scenarios. Despite its critical importance, a comprehensive understanding of alignment remains largely inaccessible to the broader public [5,95,84,19,98,8,55,31,37,94,81,1]. The broad and intricate nature of alignment results in fragmented research efforts, which often provide only a narrow insight into specialized areas, making it challenging to present a holistic perspective of the alignment landscape. Additionally, alignment is frequently regarded as a proprietary cornerstone in the LLMs training, shrouded in corporate confidentiality. To foster advancement within the LLM community, we present a comprehensive and systematic exposition of Baichuan Alignment, featuring a suite of advanced and practical alignment techniques. \n\nBaichuan Alignment comprises three critical phases: Prompt Augmentation Systems (PAS), Supervised Fine-Tuning (SFT) and Preference Alignment. The PAS stage aims to transform user queries into instructions that are more comprehensible and actionable for LLMs through automated Prompt Engineering (PE) techniques. The SFT stage equips LLMs with the ability to engage in dialogue and handle complex tasks using a large corpus of high-quality and diverse data. The Preference Alignment further aligns the LLMs with human values and preferences. This report primarily focuses on four aspects: optimization, data, key capability enhancement, and system evaluation, which are critical elements of Baichuan Alignment.",
            "score": 0.4293230394684151,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00229644775390625
        },
        {
            "corpus_id": "265034123",
            "title": "The Alignment Problem in Context",
            "text": "Each comparison provides only a sparse signal -a relative preference over a set of model outputs. Meaningfully evaluating complex behavioural criteria requires large and diverse datasets in order to generalize robustly. Indeed, the upper bound on model performance with respect to alignment-sensitive prompts is determined by the quality of the human feedback. However, large volumes of unbiased, consistent comparisons can be difficult to collect. \n\nDespite these technical challenges, RLHF has been shown to significantly improve LLM performance with respect to helpfulness, harmlessness, and truthfulness based on human evaluations (Bai et al. 2022, OpenAI 2022, Glaese et al. 2022, Touvron et al. 2023). By combining next-token prediction pre-training with RLHF fine-tuning, one can steer the behaviour of LLMs towards producing outputs that are not just statistically likely, but also generally preferred by humans in alignment-sensitive contexts. When asked to produce hate speech or instructions to make a bomb, for example, ChatGPT will politely decline the request. Likewise, asking the model about its personal opinions, particularly on controversial topics, will trigger a statement explaining that it does not have opinions as a machine learning model. On the surface, at least, RLHF is an effective solution to the technical challenge of the value alignment problem for LLMs. \n\nReinforcement learning from direct human feedback is not the only way to fine-tune a base LLM for alignment. Another method, known as instruction tuning, consists in fine-tuning a base LLM using a dataset of instruction-output pairs, where the instruction provides a natural language description of the desired task, behaviour, or capabilities, and the output demonstrates the expected model response (Ouyang et al. 2022, Zhang et al. 2023). The instruction dataset is constructed either by integrating existing human-annotated datasets or by automatically generating new demonstrations using another Ver sion 1 LLM. The fine-tuning process then involves sequential token prediction given an instruction-input pair, where the model learns to generate the target output. Through this process, LLMs can learn to map user instructions to desired outputs based on demonstrations designed to reflect human preferences.",
            "score": 0.4289996693850684,
            "section_title": "Fine-tuning strategies",
            "char_start_offset": 27915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 1792,
                    "end": 1811,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0048675537109375
        },
        {
            "corpus_id": "269449935",
            "title": "Hallucination of Multimodal Large Language Models: A Survey",
            "text": "Before moving to multimodal large language models, it is essential to introduce the concept of large language models. Typically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets. Prominent examples include GPT-3 [14], PaLM [31], LLaMA [153], and . Through scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning [14], Chain-of-Thought prompting [168] and instruction following [130], among others. \n\nThe characteristics and behaviors of LLMs are intricately linked to their training processes. LLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Below, we provide a concise overview of each stage to facilitate comprehension. \n\nPre-trianing. Pre-training serves as a fundamental phase in the learning process of LLMs [219]. During this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities. This pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively. \n\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge and skills, it's important to acknowledge that its primary focus is on optimizing for completion. Consequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user's objective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-Tuning (SFT) [204] has been introduced. SFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs. \n\nReinforcement Learning from Human Feedback. Although SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.",
            "score": 0.42899735032077485,
            "section_title": "Large Language Models",
            "char_start_offset": 5855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 468,
                    "end": 473,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002208709716796875
        },
        {
            "corpus_id": "271854857",
            "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment",
            "text": "Aligning language models with preferences is a critical component in LLM development, significantly enhancing model capabilities, safety, and adherence to human values (Christiano et al., 2017;Ouyang et al., 2022;Bai et al., 2022). These preferences can be expressed through preference pairs (output y l \u227a y w for input x), which offer a richer signal than individual outputs and enable more expressive learning objectives. Recently, contrastive learning objectives have made alignment more accessible (Rafailov et al., 2024b). \n\nDespite these advantages, alignment outcomes can be suboptimal (Eisenstein et al., 2023;Feng et al., 2024;Park et al., 2024). In this paper, we reason through the nature of alignment, focusing on (i) the preference signal expressed by the data, and (ii) the training dynamics of contrastive objectives. We find that across both these axes, conventional alignment methods are underspecified. To solve this, we argue that (i) preference data should be minimally contrastive, and (ii) alignment objectives should account for distinct alignment situations (see Figure 1). This sheds light on suboptimal alignment outcomes. For example, we show in Section 5 how a model aligned using highquality outputs can actually degrade if the pairs differ in multiple uncontrolled aspects. \n\nThese insights lead to two new contributions. First, we introduce Contrastive Learning from AI Revisions (CLAIR), a method for creating prefer-ence pairs which minimally revises one output to express a preference. The pairs created by CLAIR result in a more precise learning signal, as opposed to conventional methods which use a judge to select a preferred response. Second, we introduce Anchored Preference Optimization (APO), a family of contrastive objectives which explicitly account for distinct relationships between model and data during alignment. The tailored training dynamics of APO results in more performant alignment compared to conventional objectives. \n\nIn order to study the role of both (i) minimally contrastive preference data, and (ii) distinct alignment training dynamics, we individually align a model across four comparable preference datasets using five alignment objectives. One dataset is created through our CLAIR method.",
            "score": 0.42883062250213033,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 193,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 193,
                    "end": 213,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 502,
                    "end": 526,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00313568115234375
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "Our work has obvious limitations, including (1) Our analysis focuses solely on open-domain instruction following, and we acknowledge that fine-tuning for specific domains or tasks might enable models to gain new skills and knowledge. (2) Our analysis is limited to uni-modal language only IT, and (3) We do not study the effects of more advanced alignment methods like DPO (Rafailov et al., 2023) and RLHF and leave this for future work. (4) We do not explore retrieval-augmented generation, which decouples knowledge extraction from the model. (5) Finally, the findings of our paper are only confined to general-purpose IT datasets (datasets with an amalgamation of all kinds of general-purpose everyday instruction-response pairs) and IT datasets tailored to improve specific tasks (e.g., IT datasets tailored for imitating step-wise thinking to improve math reasoning (?). may not obey our findings.",
            "score": 0.4285158710504051,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 35185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0024929046630859375
        },
        {
            "corpus_id": "259108887",
            "title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
            "text": "According to the ablation experiment of Expert-Alignment, we found that the knowledge injection of large language models mainly occurs in the further pre-train stage to a certain extent, while learning to express the knowledge and generating patterns occur in the SFT stage. For different types of SFT data, the model has a learning recipe. Similar to human beings, we learn to understand words before we can read and learn to communicate before we can discuss a specific field. So, we hypothesize that there is a similar phenomenon in learning to use tools. Thus, we conducted ablation experiments to evaluate different foundation models trained with the same tools. \n\nIn this experiment, we perform tool instruction tuning with the GeoTool dataset on both Alpaca-7B (i.e., LLaMA model tuned on the Alpaca-GPT4 dataset) and K2-7B, which undergoes both further pre-training and instruction tuning. This comparison is similar to a student educated in a general subject and a student specializing in geosciences using a geoscience literature search tool for knowledge queries and question answering. In this work, we need to compare the generative thoughts (illustrated in Figure 7). We take ChatGPT as a referee to decide which model generates better thoughts and ask ChatGPT to give a score based on the meaning of these three factors via simple prompts, following previous studies [16]. During this process, we use the 50 open geoscience questions mentioned in the subjective tasks above. \n\nFinally, according to the results shown in Table 9, K2 may have better thoughts on how to write search queries since K2 knows more geoscience knowledge than Alpaca-GPT4 leading to a better understanding towards geoscience questions.",
            "score": 0.4283230041150956,
            "section_title": "Exploration on Tool Learning",
            "char_start_offset": 31141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 1382,
                    "end": 1386,
                    "matchedPaperCorpusId": "257766307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013828277587890625
        },
        {
            "corpus_id": "270559868",
            "title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization",
            "text": "LLM Fine-Tuning and Alignment After obtaining sufficient world knowledge during the pretraining stage, LLMs will be specifically fine-tuned before deployment. There are two mainstreams of LLM fine-tuning: (1) One line of work aims to stimulate the knowledge learned by LLMs to enable them to accomplish various real-world tasks (Taori et al., 2023;Wang et al., 2022), or to continually make the model learn new task knowledge (Yang et al., 2023). Instruction tuning (Wei et al., 2021;Mishra et al., 2022) is one of the widely studied methodologies in this line. (2) The other line of work fine-tunes LLMs in order to align their behavior with human values and preferences, which is also called the alignment (Ji et al., 2023). Alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017;Bai et al., 2022a), Direct Preference Optimization (DPO) (Rafailov et al., 2024) and a series methods based on DPO (Azar et al., 2024;Park et al., 2024;Meng et al., 2024), are proven to be crucial and effective on improving helpfulness (Ouyang et al., 2022), harmlessness (Dai et al., 2023) and honesty (Cheng et al., 2024) of LLMs. However, all these studies are conducted under the assumption that humans are strong supervisors to LLMs, while we study in a superalignment case.",
            "score": 0.4283137480928656,
            "section_title": "RELATED WORK",
            "char_start_offset": 5651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1311
                }
            ],
            "ref_mentions": [
                {
                    "start": 466,
                    "end": 484,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 484,
                    "end": 504,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 889,
                    "end": 912,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1068,
                    "end": 1089,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02801513671875
        },
        {
            "corpus_id": "276903291",
            "title": "Large Language Models Often Say One Thing and Do Another",
            "text": "Our research introduces a novel evaluation benchmark, Words and Deeds Consistency Test (WDCT), to evaluate the consistency between the words and the deeds of LLMs across four different domains. \n\nEvaluation results reveal a significant inconsistency between words and deeds across LLMs, highlighting a critical gap in the reliability of these models. Furthermore, we conduct separate alignment on words or deeds by SFT and DPO. Experiment results show that aligning LLMs from a single aspect -either word or deed -has poor and unpredictable effects on the other aspect. This supports our hypothesis that the underlying knowledge guiding LLMs' choices of words or deeds is not contained within a unified space. \n\nSpecifically, we use a 3-shot CoT, considering that the model struggles with a 0-shot CoT prompt. Demonstrations of input-answer pairs are randomly sampled from a manually constructed set of 50. The reported experimental results in the paper are the average of three evaluations to mitigate the influence of demonstration selection on the outcomes. \n\nA  We conducted experiments with learning rates of [1e-6, 5e-6, 1e-5, 5e-7, 1e-7]. Figure 8 shows the performance of the model using different learning rates during the SFT stage, and Figure 9 shows the performance of the model using different learning rates during the DPO stage.",
            "score": 0.4275858339591404,
            "section_title": "CONCLUSION",
            "char_start_offset": 22231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.002704620361328125
        },
        {
            "corpus_id": "263830425",
            "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models",
            "text": "These tasks, due to their inherent simplicity, fail to challenge the capabilities of large-scale models adequately. Furthermore, a significant drawback lies in the fact that some of these datasets have previously appeared in the instruction tuning (Chung et al., 2022) sets of these LLMs, suggesting that the models may have already learned these tasks during training. Secondly, prior benchmarks have primarily focused on metrics that assess the performance of the models on target sequential tasks. Yet, for aligned models, aspects like generalization to new tasks, the ability to follow human instructions, and safety preservation are of paramount importance. Regrettably, these dimensions have not been extensively studied or incorporated into assessments. \n\nTo facilitate further research, we present TRACE, a continual learning benchmark designed for aligned Large Language Models. Our benchmark consists of 8 distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. To ensure task balance, we have sampled 5,000 instances for each task, and for classification tasks, we have ensured an equal distribution of samples across all classes. Additionally, all datasets have been standardized into a unified format, simplifying the evaluation process. To evaluate continual learning in aligned LLMs, we introduce three metrics: \"General Ability Delta,\" \"Instruction Following Delta,\" and \"Safety Delta\" to assess models' forgetfulness in such scenarios. \n\nWe conduct a comprehensive evaluation of 5 aligned LLMs on TRACE. Evaluation results reveal several key findings: 1) Nearly all models exhibit a significant decline in general abilities, especially in math and reasoning. For instance, when trained on TRACE, the accuracy of llama2-chat 13B on the gsm8k dataset dropped from 28.8% to a mere 2%. 2) Unlike other skills, LLMs' multilingual abilities generally improve. For example, llama2-chat 7B's performance on the TydiQA dataset surged from an F1 score of 23.47 to 33.23. 3) Full-parameter training, compared to LoRA training, more easily fits the target tasks, but it also leads to a more pronounced decline in general abilities. 4) LLMs' instruction-following capabilities also suffer a significant reduction after continual learning.",
            "score": 0.4273516194562131,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2336
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.001926422119140625
        },
        {
            "corpus_id": "268553812",
            "title": "Improving the Robustness of Large Language Models via Consistency Alignment",
            "text": "\u2026 the LLMs may generate inconsistent responses due to the different verbalized instructions (Li et al., 2023b), data distribution shift (Li et al., 2023a), or even discrepancies in instruction formats (Gu et al., 2022).Based on these observations, Li et al. (2023a) and Liang et al. (2023) propose to optimize the instruction to identify the optimal task instruction that elicits the best performance for LLMs.Nevertheless, there is an absence of quantitative analysis of the current state, along with a systemic solution to improve the instruction-tuned LLMs.\n\nIn this paper, we first quantitatively analyze the generation robustness of current LLMs in terms of our consistency metrics.We then propose a novel training framework for LLMs via consistency alignment to mitigate the inconsistency problem in current LLMs.Concretely, our training framework sequentially performs the following two training stages: instruction augmented supervised finetuning and response consistency alignment.(1) In the augmented supervised fine-tuning (SFT) stage, we first paraphrase the original instruction in the SFT dataset and then pair each paraphrased instruction with the original response to form a new augmented training sample.Finally, all augmented training samples are then added to the SFT dataset to fine-tune the LLMs.(2) In the consistency alignment stage, we feed the paraphrased instructions to LLMs to generate candidate responses, and then construct <good, bad> response pairs where each response is individually evaluated by the consistency score.Finally, we optimize the LLMs to directly learn the preferences through an offline training algorithm (Yuan et al., 2023).\n\nWe conduct extensive experiments on publicly available models including Vicuna-7B, Vicuna-13B, Llama2-7B, and Llama2-13B on the instructionfollowing tasks.The experimental results show that by explicitly adding consistency self-alignment, these LLMs can obtain robustness improvements and generalize better on following instructions.\n\nOur contributions are as follows:\n\n1. We propose an integrated training framework to enhance the robustness of LLMs.",
            "score": 0.4271101317308931,
            "section_title": "Introduction",
            "char_start_offset": 1532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 560
                },
                {
                    "start": 562,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1674
                },
                {
                    "start": 1676,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2009
                },
                {
                    "start": 2011,
                    "end": 2044
                },
                {
                    "start": 2046,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 1654,
                    "end": 1673,
                    "matchedPaperCorpusId": "258059818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0017957687377929688
        },
        {
            "corpus_id": "273502615",
            "title": "Exploring Continual Fine-Tuning for Enhancing Language Ability in Large Language Model",
            "text": "Cahyawijaya et al. (2023) propose InstructAlign which uses cross-lingual alignment and episodic replay to align an LLM's pre-trained languages to unseen languages but requires parallel data and previous task data. Shaham et al. (2024) introduces multilinguality during the first instruction fine-tuning phase which improves an LLM's instruction following capability across languages. He et al. (2023) show catastrophic forgetting during CFT and use techniques such as joint fine-tuning and model regularization to mitigate it. However, these techniques are computationally expensive or require access to previous task data. \n\nLanguage Adaption. This set of works looks at language and task adaption by adjusting the model to understand new languages and enhancing its performance on specific tasks through fine-tuning, respectively (Chen et al., 2023;Zhao et al., 2024;Pfeiffer et al., 2020). For instance, Chen et al. (2023) perform task adaption by fine-tuning the model on downstream task data. For language adaption, they fine-tune only the token embedding layer, helping the model learn specific lexical meanings of new languages. Language and task ability are either trained in parallel or sequentially. However, in this paper, we try to incorporate language ability in models with the constraint that they may have already learned task ability (e.g., MISTRAL-7B-INSTRUCT). To the best of our knowledge, this is a first attempt at studying the effect of task and language self-instruct datasets on an LLM's multilingual ability through CFT.",
            "score": 0.42688740909494793,
            "section_title": "Related Work",
            "char_start_offset": 6743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1546
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0016546249389648438
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "In this work we showed the performance improvement of a model on a task when it is finetuned with increasing task-specific data. However, frontier LLMs are trained to excel at multiple tasks, and we don't thoroughly understand how finetuning for one task or domain affects the performance on others. A big open question would be investigating how to take advantage of this scaling behavior while preventing model degradation on existing capabilities. Similarly, we showed how models can learn new knowledge beyond their pre-training data cutoff, but the issue of hallucination isn't solved. Further research in effectively introducing new knowledge, like continual learning methods during post-training can shed light on this. \n\nIn this work, we also limited the scope to supervised finetuning, on tasks that involve text generation. However, the implications from this opens up several interesting directions to explore further. Most notably, LLM post-training involves RLHF after supervised finetuning, and it would be interesting to see how RLHF can contribute to these improvements and how it scales with more data. \n\nFrom table 5, we see that continuing pretraining with new knowledge on the base model or LIMA-style fine-tuning is not effective at introducing new knowledge. \n\nNote that although SFT + RAG models have a perfect score on Direct Question, it is because the SFT models are finetuned on Direct Question-Answer in the first place and is thus, a result of memorization.",
            "score": 0.4267686003730397,
            "section_title": "Limitations and future work:",
            "char_start_offset": 22952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1486
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0386962890625
        },
        {
            "corpus_id": "275134139",
            "title": "ChipAlign: Instruction Alignment in Large Language Models for Chip Design via Geodesic Interpolation",
            "text": "DAPT and DAFT often drastically change the weights of LLMs to emphasize domain knowledge, resulting in a loss of the instructionfollowing capabilities originally present in general-purpose LLMs [9]. However, instruction alignment is crucial for real-world applications, such as a chatbot assistant for chip designers. In such settings, designers may seek guidance on design methodologies, troubleshooting steps, or explanations of specific design concepts, often phrased as direct instructions. Additionally, they may instruct the chatbot to respond solely based on a given context, which ensures the answer is grounded in relevant and context-specific information, as shown in Figures 5 and 6. Hence, the ability to understand and respond appropriately to these instructions is vital for the practical usability of a chip LLM, making instruction alignment an essential feature. \n\nA straightforward approach to enhance the instruction alignment of chip LLMs involves multi-task learning, which simultaneously trains a model on chip domain-specific data and instruction-following data to effectively integrate both sets of capabilities. However, access to high-quality instruction data is limited, as datasets used by advanced models like GPT-4 and the LLaMA series remain proprietary. While open-source instruction datasets are valuable [11], they often lack the scale and diversity needed to train models effectively for complex instruction-following tasks. Besides, even when data is available, the costs associated with finetuning on large-scale instruction datasets are prohibitively high, particularly for models with billions of parameters.",
            "score": 0.4264555080886897,
            "section_title": "B. Instruction Alignment in Chip Design LLMs",
            "char_start_offset": 7455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1646
                }
            ],
            "ref_mentions": [
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "258179434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0013151168823242188
        },
        {
            "corpus_id": "273323267",
            "title": "Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning",
            "text": "Large Language Models (LLMs), despite being primarily trained for next-token predictions, have shown impressive reasoning capabilities (Bubeck et al., 2023;Anthropic, 2024;Team et al., 2023). However, despite recent progress reviewed below, it is not well understood what knowledge LLMs represent internally and how they represent it. Improving such understanding could enable valuable progress relevant to transparency, interpretability, fairness and robustness, for example \u2022 discovering and correcting inaccuracies to improve model reliability. \n\n\u2022 discovering and correcting bias, \u2022 revealing and removing dangerous knowledge (relevant to bioweapon design, say), \u2022 detecting deceptive behavior where models deliberately output information inconsistent with its knowledge. \n\nThe goal of this paper is to deepen our our understanding of learned knowledge representations by focusing specifically on the representations learned of knowledge graphs (KGs), which are loosely speaking a discrete set of entities with various relations between them. KGs provide a valuable testbed because, although they are simpler and more structured than the totality of implicit knowledge LLM training corpora, they can nonetheless capture a massive amount of valuable human knowledge. We focus on applying the approach of mechanistic interpretability not to learned algorithms, but to learned knowledge. \n\nThe rest of this paper is organized as follows: We relate our approach to prior work in Section 2. \n\nIn Section 3, we formally describe our problem settings, and Section 4 explains various methods of measuring representation alignment, including model stitching. Section 5 presents hints of universality via LLM stitching, and Section 6 takes a further glimpse into the geometrical structure of LLMs' representations. We discuss our starvation hypothesis for hints of universality in Section 7, and summarize our conclusions in Section 8.",
            "score": 0.4263463073057897,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 775
                },
                {
                    "start": 778,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1929
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006290435791015625
        },
        {
            "corpus_id": "271916117",
            "title": "CIPHER: Cybersecurity Intelligent Penetration-Testing Helper for Ethical Researcher",
            "text": "Techniques like instruction fine-tuning (FLAN [20]) and model mimicry (Orca [21]) further improve reasoning capabilities. Despite advancements, challenges in LLM reasoning persist, motivating ongoing research [7,22]. \n\nLLM-Based Chatbots: LLM-based chatbots like ChatGPT excel in customer support, education, and complex problem-solving by synthesizing large volumes of information into detailed responses. However, they lack the specialized knowledge required for offensive penetration testing [23]. \n\nSupervised Fine-Tuning: Supervised fine-tuning enhances model performance on domain-specific datasets, particularly in areas like penetration testing, ensuring accurate application of specialized language. Unlike Retrieval-Augmented Generation (RAG), finetuning improves the model's domain comprehension [24]. \n\nLLM Alignment: Efforts to align language models with human values focus on ensuring these models exhibit traits such as helpfulness and truthfulness. Reinforcement Learning with Human Feedback (RLHF) fine-tunes large language models (LLMs) to achieve this alignment. This process involves training a reward model to predict human preferences and then using algorithms like Proximal Policy Optimization (PPO) for fine-tuning. Although PPO generally yields better results, Direct Preference Optimization (DPO) simplifies the process by fine-tuning directly with human-labeled preferences [25,26]. \n\nIncorporating Domain-Specific Knowledge: Domain-specific knowledge enhances LLM accuracy in specialized fields like medicine [27] and cybersecurity. Techniques such as Domain-Adaptive Pretraining (DAPT) and adaptive fine-tuning (AdaptLLM) are crucial for developing specialized models tailored for specific tasks, leveraging domain-specific datasets for improved insights [24,28].",
            "score": 0.42625373117773163,
            "section_title": "Background and Related Works",
            "char_start_offset": 7866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1405,
                    "end": 1408,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0021820068359375
        },
        {
            "corpus_id": "269982232",
            "title": "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners",
            "text": "Intuitively, LLMs have abilities to acclimatize themselves to the multilingual environment through appropriate training (Shi et al., 2022).Many existing methods rely on instruction-tuning on the multilingual instruction-tuning datasets (Kew et al., 2023;Liu et al., 2024).However, given the question alignment paradigm, utilizing multilingual alignment is also helpful for improving LLMs' multilingual abilities.Additionally, we focus on question alignment in our work to eliminate the interference of task-related data with annotated answers from our analysis of multilingual alignment.Based on the findings above, can LLMs achieve better multilingual alignment across different languages efficiently through appropriate methods?\n\nIn this work, we investigate the multilingual alignment of LLMs, where we only train the LLMs on the parallel data without annotated answers (only queries) in a few languages.Following question alignment, we conduct the experiments on models in different types (English-centric or not) and parameter sizes, and test across a wide range of languages on different benchmarks.We find that question alignment following Zhu et al. (2024) can effectively enhance the multilingual capabilities of LLMs, which indicates that models can effectively utilize the relevant knowledge and capabilities learned during the pretraining process with question alignment, consisting with the \"Superficial Alignment Hypothesis\" (Zhou et al., 2024).Our results also indicate that conducting question alignment in a small number of languages brings significantly better multilingual alignment even between English and many languages unseen during instruction-tuning process, which implies good language generalization.Furthermore, we also use logit lens (Nostalgebraist, 2020) and dimensionality reduction techniques (Pearson, 1901) to study the latent states of LLMs, providing more comprehensive perspectives and empirical results for the alignment improvements in our experiments.",
            "score": 0.4262496955774461,
            "section_title": "Introduction",
            "char_start_offset": 2023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 730
                },
                {
                    "start": 732,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 1439,
                    "end": 1458,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1826,
                    "end": 1841,
                    "matchedPaperCorpusId": "125037489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06756591796875
        },
        {
            "corpus_id": "266163054",
            "title": "Is Ignorance Bliss? The Role of Post Hoc Explanation Faithfulness and Alignment in Model Trust in Laypeople and Domain Experts",
            "text": "In summary, to investigate how the faithfulness and alignment of post hoc explanations affect model trust in laypeople and domain experts, we conducted a user study with computer science students and doctors. We find that for laypeople, trust is associated with explanation faithfulness, while for domain experts trust is associated with explanation alignment. We hypothesize that this is because domain experts, with their strong domain knowledge may experience a cognitive bias, causing them to focus on the alignment of explanations with prior knowledge, while laypeople, with no prior knowledge, have only explanation faithfulness to base their trust on. \n\nExplanation faithfulness is a fundamental and foremost criterion of a model explanation: a user ought to first establish that an explanation is faithful, i.e., accurately represents the underlying model, before interpreting information in the explanation, such as explanation alignment. In this study, we find that laypeople correctly focus on explanation faithfulness, while domain experts tend to focus on explanation alignment without first checking explanation faithfulness. \n\nOne potential limitation of this study is that it uses engineered post hoc explanations to precisely design explanations that are faithful or unfaithful and aligned or unaligned. While these engineered explanations enable us to draw precise conclusions, they may not generalize perfectly to real-world post hoc explanations. In addition, while the study includes a satisfactory number of participants, it can still benefit from a larger number of participants. \n\nTo our knowledge, this work is the first to show that (1) different aspects of post hoc explanations affect laypeople and domain experts' trust in a model and (2) domain experts are subject to specific biases due to their expertise when interpreting post hoc explanations. By uncovering this phenomenon and exposing this cognitive bias, this work motivates the need to educate end users about how to properly interpret explanations and overcome their own cognitive biases, and motivates the development of simple and interpretable faithfulness metrics for end users. This research is particularly important and timely as post hoc explanations are increasingly being used in high-stakes, real-world settings such as medicine.",
            "score": 0.42591695432279886,
            "section_title": "Conclusion",
            "char_start_offset": 22114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2329
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006145477294921875
        },
        {
            "corpus_id": "273346804",
            "title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models",
            "text": "If this retrieval process is flawed, it might not secure the necessary information, resulting in inappropriate responses. Additionally, a misalignment in knowledge representation could hinder RAG's comprehension of the context (Gao et al., 2023). Another approach is Supervised Fine-Tuning (SFT) (Dettmers et al., 2024), which improves a model by continuing its training on relevant tasks. Instruction tuning (Zhang et al., 2023c;Taori et al., 2023) under SFT has shown substantial enhancements in language model performance, though it doesn't necessarily enrich the model's knowledge base (Ouyang et al., 2022;Chia et al., 2023;Zhou et al., 2024). Techniques in fine-tuning also include reinforcement learning (RL) strategies such as RLHF (Touvron et al., 2023;Achiam et al., 2023) and DPO (Rafailov et al., 2024), which refine the model's alignment post-training but do not expand its knowledge capacity. Hence, developing a precise supervised mechanism for raw knowledge implementation poses a significant challenge. A third strategy is Continual Pre-Training (CPT) (Ke et al., 2023), or unsupervised fine-tuning, where the model is further trained on specific knowledge datasets tailored to certain tasks or domains (Wu et al., 2024). While CPT does not require labeled data, structuring the data in a format that closely re-flects the specific goals and tasks can be particularly effective. Recent studies have garnered attention for these three knowledge injection strategies (Ovadia et al., 2023;Balaguer et al., 2024;Mecklenburg et al., 2024), yet there remains a gap in knowledge ingestion. Given unprocessed and unstructured knowledge, two key questions arise: (i) What data representations are most effective for each injection strategy? (ii) How can we systematically construct diverse and high-quality representations to facilitate effective knowledge injection?",
            "score": 0.4258681501205246,
            "section_title": "Synthetic Knowledge Ingestion",
            "char_start_offset": 5245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 319,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 590,
                    "end": 611,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 629,
                    "end": 647,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 791,
                    "end": 814,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003040313720703125
        },
        {
            "corpus_id": "271874331",
            "title": "Leveraging Web-Crawled Data for High-Quality Fine-Tuning",
            "text": "Relationship with RAG The widely discussed RAG (Gao et al., 2023b;Komeili et al., 2022;Thoppilan et al., 2022;Schick et al., 2023) technology is conducted during the inference period. Providing references to the model and allowing the model to refer to these references in generating answers, helps the model reduce \"hallucinations\", especially for knowledge-intensive tasks. Our method can be seen as RAG during the training process. Distilling the model's unknown knowledge into the training data can further enhance the model's capabilities. The injection of knowledge can also positively impact the model's generalization in related domains. \n\nPossible Applications in Other Domains A core idea of our paper is that: the effective use of appropriate data formats, derived from pretraining datasets, can facilitate the efficient SFT. Therefore, our method can be extended to various scenarios. Numerous open-source high-quality datasets can be used to create paired data through alignment with web-crawled resources. For instance, by aggregating relevant Wikipedia entries for specific QA datasets, one can train a model to generate pertinent questions and answers corresponding to those entries. Furthermore, in niche scenarios featuring unique personal corpora, it is feasible to initiate training with a small amount of seed data to produce high-quality SFT data, thereby integrating this knowledge into the model.",
            "score": 0.4258179402515667,
            "section_title": "Discussions",
            "char_start_offset": 28704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1420
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 87,
                    "matchedPaperCorpusId": "236034557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0014553070068359375
        },
        {
            "corpus_id": "273233831",
            "title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization",
            "text": "To answer our research questions, we conduct weak-to-strong alignment experiments on helpfulness and harmlessness, investigate the impact of varying the number of weak teachers, evaluate the performance of weak teachers during iterations, and conduct ablation studies. Additionally, we introduce case studies to further assess the effectiveness of MACPO.  2 present the third-party reward model and GPT-4 evaluation results for the helpfulness and harmlessness alignment datasets. Across all metrics, MACPO consistently outperforms baseline methods on the HH-helpful, HH-harmless and PKU-SafeRLHF datasets. Based on these results, we have three main observations: \n\n\u2022 MACPO consistently outperforms strong-to-weak alignment baselines in terms of helpfulness and harmlessness, across HH-Helpful, HH-Harmless and PKU-SafeRLHF test sets. Strong-toweak alignment methods RLAIF and RLCD assume teachers are stronger than students and only require students to learn from teachers. However, in the weak-to-strong alignment setting, without continuous alignment ability improvement of weak teachers, weak teachers inevitably introduce noise. It indicates the importance of iterative mutual learning of weak teachers and strong students in the weak-to-strong alignment setting. \u2022 During the multi-round iterative optimization process, MACPO consistently outperforms self-alignment methods without collapse, in helpfulness and harmlessness. As shown in Table 1, the alignment performance of SPIN and Self-rewarding starts to decrease after the first and second iteration, respectively, while MACPO continues to improve the alignment performance through three rounds iteration. This finding aligns with Shumailov et al. (2024) and Wenger (2024): self-alignment methods use self-generated data to continually train LLMs, leading to collapse during multiple iterative optimization rounds. This underscores the effectiveness and necessity of encouraging weak teachers and strong students to learn from each other to reinforce unfamiliar positive behaviors. \u2022 MACPO significantly outperforms existing weak-to-strong alignment baselines in terms of helpfulness and harmlessness. Although Naive SFT and Confident loss can improve the alignment performance by reinforcing high-quality positive behavior, they ignore penalizing negative behavior. This underscores the effectiveness of penalizing negative behavior.",
            "score": 0.42577888864373803,
            "section_title": "EXPERIMENTAL RESULTS AND ANALYSIS",
            "char_start_offset": 21254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2327
                },
                {
                    "start": 2328,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 1692,
                    "end": 1715,
                    "matchedPaperCorpusId": "271448069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0022525787353515625
        },
        {
            "corpus_id": "263830508",
            "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
            "text": "Training LLMs on extensive text corpora has demonstrated remarkable capabilities, leading to state-of-the-art performance on numerous tasks (Brown et al., 2020;Kaplan et al., 2020). However, this does not automatically make language models effective in responding to user instructions (Wei et al., 2022;Sanh et al., 2022). To better align LLMs to human preferences, the most effective approach has been to perform SFT followed by the application of RLHF (Wang et al., 2023a;Chiang et al., 2023;Peng et al., 2023). In SFT, human annotators provide demonstrations of instructions and responses for the model to imitate (Taori et al., 2023;Zhang et al., 2023). RLHF goes a step further to enable models to generate responses that human annotators prefer to alternative responses (Bai et al., 2022;Ouyang et al., 2022;K\u00f6pf et al., 2023a). \n\nHowever, despite its success, there are limitations to this approach. First, using SFT alone does not allow the model to distinguish between highquality and low-quality responses leading to lower performance than RLHF (Wang et al., 2023a). Using RLHF for model alignment however, substantially increase the complexity of the training setup (Snell et al., 2023;Yuan et al., 2023), limiting its public adoption (Zhang et al., 2023;Dettmers et al., 2023;Zhou et al., 2023). Furthermore, RLHF treats human preference of model responses as monodimensional without regard for the diversity of aspects (e.g. helpfulness, humor, toxicity) that contribute to such preferences (Bai et al., 2022;Ouyang et al., 2022) and thereby limiting users' ability to adjust individual aspects at inference time based on their use cases. \n\nTo address these limitations, we introduce STEERLM, a novel approach to model alignment through SFT that overcomes the limitations associated with conventional SFT and RLHF methods.",
            "score": 0.4257406937354066,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0018033981323242188
        },
        {
            "corpus_id": "271947166",
            "title": "Building and better understanding vision-language models: insights and future directions",
            "text": "Similar to the approach commonly used with LLMs (Touvron et al., 2023), fine-tuning is typically done in two stages: supervised fine-tuning (SFT) followed by an alignment phase. \n\nWhich datasets should be used for the SFT? The literature offers many high-quality datasets containing diverse images and covering a wide range of tasks. They are often annotated by humans, ensuring accurate QA pairs. Although most of them are relatively small individually, when combined, they provide a sufficient number of examples for an effective SFT. \n\nInspired by previous work on LLMs (Wei et al., 2022;Sanh et al., 2022), InstructBLIP (Dai et al., 2023) and M3IT (Li et al., 2023) were among the first to introduce curated mixtures of academic datasets for fine-tuning VLMs. Building on these efforts, The Cauldron (Lauren\u00e7on et al., 2024) introduced a collection of 50 high-quality datasets covering a broad range of tasks, including general visual question answering, counting, captioning, text transcription, document understanding, chart/figure analysis, table understanding, visual reasoning, geometry, spotting differences between two images, and converting screenshots into functional code. Each dataset in this compilation is formatted into a standardized question/answer format, and when multiple QA pairs exist per image, they are combined into a multi-turn conversation. However, a drawback of academic datasets is that their answers tend to be concise, which may lead the model to generate similarly brief responses, which are often less preferred by users. A potential solution is to use an LLM to expand and rephrase the answers, as in M3IT (Li et al., 2023) and Llava 3-V (Dubey et al., 2024). \n\nAlignment phase There are several reasons to include an alignment stage following supervised fine-tuning. The first objective is to align the model's output with human preferences, making it more intuitive and better at following complex instructions.",
            "score": 0.42509945836701457,
            "section_title": "Fine-tuning",
            "char_start_offset": 28529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 609,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 624,
                    "end": 642,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0024242401123046875
        },
        {
            "corpus_id": "272367487",
            "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options",
            "text": "This study investigates the critical thinking capabilities of LLMs when confronted with multiple-choice questions lacking correct answers. Our findings reveal that many LLMs prioritize instruction adherence over critical judgment, often selecting incorrect options when no valid answer is provided. This tendency highlights a crucial gap in their ability to deviate from prescribed rules when necessary. Interestingly, we observed that the ability to exercise reflective judgment scales with model size, suggesting it may be an emergent property of larger models. However, our comparison between base models and their aligned counterparts reveals a potential trade-off between alignment for helpfulness and the preservation of critical reasoning skills, raising important questions about current alignment methodologies. \n\nOur parallel human study uncovered similar biases towards rule-following, even when it contradicts logical reasoning, suggesting that the challenges observed in LLMs might reflect broader cognitive patterns in human decision-making. While the Chain of Thought approach significantly improved reflective judgment capabilities in some models, it did not universally solve the problem. Additionally, our analysis of human preference datasets used in model alignment revealed potential quality issues, with a significant portion of annotated answers being incorrect, underscoring the need for rigorous curation of training data. \n\nThese findings have significant implications for the development and application of LLMs across various domains, including decision support systems and educational settings. Future work should focus on developing more comprehensive datasets to evaluate LLMs across various aspects of reasoning, exploring novel alignment techniques that preserve reflective judgment capabilities, and investigating the relationship between model architecture, size, and the emergence of critical thinking skills. Addressing these challenges is crucial for developing AI systems that can truly augment human decision-making across complex and nuanced domains.",
            "score": 0.4247442046097583,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 30492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2091
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00511932373046875
        },
        {
            "corpus_id": "248118878",
            "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
            "text": "perform very poorly (much worse than chance) on the other. Fortunately, we find that PMs trained on a mixture of both datasets can nevertheless learn the right lessons and behave helpfully when appropriate, while encouraging the polite refusal of harmful requests. With preference models in hand, we then train helpful and harmless assistants via reinforcement learning, using the PM scores as rewards. We evaluate both PM performance and the more relevant performance characteristics of our RLHF-trained models. As can be seen in Figure 1, purely helpful RLHF-trained models are far easier to red-team, while helpful+harmless models are both very helpful and much less harmful.\n\nA question that's often raised about alignment training is whether it will compromise AI capabilities. We find that when RLHF is applied to large language models, the answer seems to be an almost-categorical no. Our RLHF-trained models tend to perform better than their raw, generative counterparts on virtually all evaluations, as summarized in Figure 3. We also argue that one can mix specialized skills with alignmentrelated training without compromising either alignment or performance. In practice, aligned models are likely to be more user-friendly and deployable than their raw counterparts, which suggests that there's little reason to deploy models that have not been finetuned for alignment.",
            "score": 0.42461491154902525,
            "section_title": "Introduction",
            "char_start_offset": 2600,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00441741943359375
        }
    ],
    "quotes": {
        "cost": 0.023622,
        "quotes": [
            {
                "idx": 0,
                "key": "[265608902 | Lin et al. | 2023 | Citations: 198]",
                "snippets": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored.\n\nOur findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.\n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored.\n\nOur findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.\n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[273350763 | Li et al. | 2024 | Citations: 3]",
                "snippets": "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS (SSAH)",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 282,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 282
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.013071,
        "cot": "For the given query, I need to identify key dimensions to help explain the Superficial Alignment Hypothesis (SAH) and its implications. The query is somewhat fragmented but seems to be asking about the validity of the superficial alignment hypothesis, which relates to how language models gain knowledge versus how they learn to present that knowledge.\n\nLooking at the provided quotes, I'll organize my answer into the following dimensions:\n\n1. Introduction/Background: This is essential to provide basic context about what the Superficial Alignment Hypothesis is before diving into details. Since this is explanatory in nature, a synthesis format makes most sense.\n\n2. Definition of Superficial Alignment Hypothesis: This dimension will directly explain what the hypothesis states based on the quotes. Quote [1] provides a clear definition, and quote [0] adds context. A synthesis format is appropriate to explain this concept cohesively.\n\n3. Evidence Supporting the Hypothesis: The quotes mention studies that provide evidence for this hypothesis, including LIMA and URIAL. A synthesis format will help connect these findings into a coherent narrative.\n\n4. Implications for Model Development: Quote [0] mentions rethinking alignment research, which suggests important implications. This should be in synthesis format to properly analyze the consequences.\n\n5. Knowledge Acquisition: Pre-training vs. Alignment: This dimension explores the distinction between when models acquire knowledge versus when they learn presentation formats. Both quotes touch on this, and a synthesis format will help explain this nuanced concept.",
        "plan": {
            "Introduction to Alignment in Language Models (synthesis)": [],
            "Definition of Superficial Alignment Hypothesis (synthesis)": [
                0,
                1
            ],
            "Evidence Supporting the Superficial Alignment Hypothesis (synthesis)": [
                0
            ],
            "Implications for Model Development and Research (synthesis)": [
                0
            ],
            "Knowledge Acquisition: Pre-training vs. Alignment (synthesis)": [
                0,
                1
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Alignment in Language Models",
                "tldr": "Alignment refers to making language models behave according to human values and preferences. The alignment process typically involves techniques like Reinforcement Learning from Human Feedback (RLHF) that aim to make models helpful, harmless, and honest. (LLM Memory)",
                "text": "\nAlignment in the context of large language models (LLMs) refers to the process of ensuring that these models act in accordance with human values, intentions, and preferences. As LLMs have grown in capabilities, ensuring they are aligned with human values has become increasingly important. The alignment process typically aims to make models helpful (providing useful information), harmless (avoiding generating harmful content), and honest (being truthful and admitting uncertainty when appropriate). <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe most common technique for alignment is Reinforcement Learning from Human Feedback (RLHF), which involves three key steps: (1) collecting human preferences on model outputs, (2) training a reward model based on these preferences, and (3) optimizing the language model using reinforcement learning to maximize this reward function. Other alignment methods include Constitutional AI, Direct Preference Optimization (DPO), and Supervised Fine-Tuning (SFT) on carefully curated instruction datasets. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe alignment challenge arises because the objective we optimize during training (typically next-token prediction) doesn't inherently capture the complex human values we want these systems to embody. This creates a gap between what models learn to do well (predict tokens) and what we actually want them to do (be helpful, harmless, and honest in a way that aligns with human intentions). <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Definition of Superficial Alignment Hypothesis",
                "tldr": "The Superficial Alignment Hypothesis suggests that alignment processes don't teach language models new knowledge or capabilities, but rather teach them which output formats and styles to use when interacting with users. (2 sources)",
                "text": "\nThe Superficial Alignment Hypothesis (SAH) proposes a fundamental distinction between what happens during pre-training versus alignment training of large language models. According to this hypothesis, \"A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users\" <Paper corpusId=\"273350763\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. In other words, the alignment process doesn't necessarily impart new knowledge or reasoning abilities to the model, but rather instructs it on how to present its existing knowledge in ways that meet user expectations and preferences.\n\nThis hypothesis was originally proposed in the LIMA study by Zhou et al., which demonstrated that Supervised Fine-Tuning (SFT) with a remarkably small dataset of just 1,000 examples could produce high-quality aligned models <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This finding challenged the conventional wisdom that extensive alignment training was necessary for models to develop helpful, harmless, and honest behaviors. Instead, it suggested that the primary role of alignment might be to teach models to adopt the language style and presentation format typical of responsible AI assistants.\n\nSubsequent research has provided more direct evidence supporting the Superficial Alignment Hypothesis. For example, Lin et al. conducted token distribution shift analysis that demonstrated alignment tuning primarily focuses on adopting the language style of responsible AI assistants while leveraging knowledge already acquired during pre-training <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Their URIAL system showed surprisingly strong performance, further substantiating the hypothesis and prompting researchers to reconsider their understanding of alignment processes in LLMs.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 273350763,
                            "title": "Superficial Safety Alignment Hypothesis",
                            "authors": [
                                {
                                    "authorId": "2326007326",
                                    "name": "Jianwei Li"
                                },
                                {
                                    "authorId": "2326001415",
                                    "name": "Jung-Eun Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7001953125
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored.\n\nOur findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.\n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.63720703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evidence Supporting the Superficial Alignment Hypothesis",
                "tldr": "Multiple studies have provided compelling evidence for the Superficial Alignment Hypothesis through token distribution analysis and experiments with minimal fine-tuning. These findings suggest alignment processes primarily teach models to adopt appropriate response formats rather than imparting new knowledge. (1 source)",
                "text": "\nBuilding on the foundations of the Superficial Alignment Hypothesis, researchers have conducted studies that provide substantial direct evidence supporting this perspective. Lin et al. performed a comprehensive token distribution shift analysis that offered conclusive support for the hypothesis, demonstrating that alignment tuning primarily focuses on teaching models to adopt the language style characteristic of responsible AI assistants <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Their analysis revealed both quantitative and qualitative evidence showing that alignment processes largely leverage knowledge already acquired during pre-training rather than teaching new information or capabilities.\n\nThe surprisingly strong performance of their proposed system, URIAL, further substantiated the Superficial Alignment Hypothesis <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This success has prompted researchers to reconsider fundamental assumptions about alignment processes in large language models. The evidence suggests that the apparent improvements in helpfulness, harmlessness, and honesty after alignment may not stem from new knowledge acquisition but rather from learning to present existing knowledge in more appropriate formats.\n\nThis growing body of evidence has important implications for how we understand the distinct roles of pre-training versus alignment tuning. It suggests that accurately distinguishing which knowledge and reasoning capabilities originate in pre-training versus those acquired through alignment is crucial for advancing our understanding of large language models <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This clearer distinction could help researchers develop more efficient alignment methods that focus specifically on format selection rather than attempting to teach new capabilities during the alignment phase.",
                "citations": [
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored.\n\nOur findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.\n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.63720703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Implications for Model Development and Research",
                "tldr": "The Superficial Alignment Hypothesis challenges researchers to reconsider alignment strategies, suggesting more efficient training approaches that focus on format selection rather than knowledge acquisition. This could lead to faster, more resource-efficient model development and improved evaluation methods that separate pre-training capabilities from alignment effects. (1 source)",
                "text": "\nThe growing evidence supporting the Superficial Alignment Hypothesis has profound implications for how researchers and developers approach language model training and evaluation. If alignment primarily teaches models to select appropriate response formats rather than imparting new knowledge, this suggests that current resource-intensive alignment processes could potentially be streamlined significantly. Development teams might achieve comparable results with much smaller, carefully curated alignment datasets focused specifically on teaching format selection rather than attempting to instill new capabilities during this phase <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThis insight could transform the economics of model development, particularly for organizations with limited computational resources. By distinguishing more clearly between the knowledge acquisition phase (pre-training) and the format selection phase (alignment), developers could allocate resources more efficiently and potentially create more accessible paths to developing helpful AI assistants. The surprisingly strong performance of systems like URIAL demonstrates that this more targeted approach to alignment can yield impressive results, challenging conventional wisdom about what's required for effective model alignment <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nFor researchers, the Superficial Alignment Hypothesis necessitates new evaluation frameworks that can more accurately distinguish between capabilities gained during pre-training versus those resulting from alignment. Without such frameworks, it becomes difficult to accurately assess where a model's strengths and limitations originate, potentially leading to misguided improvement efforts. As the field continues to evolve, developing evaluation methods that can separate these effects will be crucial for advancing our understanding of large language models and creating more effective training paradigms <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored.\n\nOur findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.\n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.63720703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Knowledge Acquisition: Pre-training vs. Alignment",
                "tldr": "The Superficial Alignment Hypothesis suggests a clear division of labor between pre-training and alignment, with knowledge and capabilities acquired primarily during pre-training. Evidence from token distribution analyses confirms that alignment mainly teaches models how to format outputs rather than adding new knowledge. (2 sources)",
                "text": "\nThe Superficial Alignment Hypothesis (SAH) presents a paradigm shift in how we understand the distinct roles of pre-training versus alignment in language model development. According to this hypothesis, \"A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users\" <Paper corpusId=\"273350763\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This clear division of labor between the two training phases suggests that pre-training serves as the primary source of a model's knowledge and reasoning capabilities, while alignment merely teaches the model how to present this existing knowledge in user-friendly formats.\n\nThe token distribution shift analysis conducted by Lin et al. provides substantial direct evidence supporting this hypothesis. Their research demonstrates that \"alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This finding suggests that when we observe improvements in model performance after alignment, what we're actually seeing is not the acquisition of new knowledge but rather the model learning to express its pre-existing knowledge in more appropriate ways.\n\nThe distinction between knowledge acquisition and format selection has significant implications for how we conceptualize language model training. If knowledge and capabilities are indeed primarily acquired during pre-training, this suggests that researchers should focus their efforts on improving pre-training processes for enhancing model capabilities, while reserving alignment processes specifically for teaching appropriate formatting and presentation styles. As Lin et al. note, \"To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This clearer delineation could lead to more efficient and targeted approaches to both phases of model development.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users (Zhou et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 273350763,
                            "title": "Superficial Safety Alignment Hypothesis",
                            "authors": [
                                {
                                    "authorId": "2326007326",
                                    "name": "Jianwei Li"
                                },
                                {
                                    "authorId": "2326001415",
                                    "name": "Jung-Eun Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7001953125
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored.\n\nOur findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.\n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.63720703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.064797
    }
}